<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>分布式存储架构下的矩阵乘与编译器 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">分布式存储架构下的矩阵乘与编译器</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">分布式存储架构下的矩阵乘与编译器</h1><div class="post-meta">2024-11-07<span> | </span><span class="category"><a href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 5.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 26</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%AE%E5%89%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%9F%A9%E9%98%B5%E4%B9%98%E6%96%B9%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">1. 目前常用的分布式矩阵乘方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cannon%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">2. Cannon算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#summa%E7%AE%97%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">3. SUMMA算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-1"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90-1"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#d-summa%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">4. 3D SUMMA算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-2"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90-2"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#d-summa-%E7%AE%97%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">5. 2.5D SUMMA 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-3"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90-3"><span class="toc-number">5.3.</span> <span class="toc-text">5.3 分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%842.5d%E4%B8%8E3d%E5%B9%B6%E8%A1%8C"><span class="toc-number">6.</span> <span class="toc-text">6. 大模型中的2.5D与3D并行</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tesseract"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 Tesseract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nus%E7%89%883d%E5%B9%B6%E8%A1%8C"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 NUS版3D并行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#t10-inter-core-connected-compiler"><span class="toc-number">7.</span> <span class="toc-text">7. T10: Inter-core Connected
Compiler</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#rtensor%E6%8A%BD%E8%B1%A1"><span class="toc-number">7.1.</span> <span class="toc-text">7.1 rTensor抽象</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%90%9C%E7%B4%A2rtensor"><span class="toc-number">7.2.</span> <span class="toc-text">7.2 自动搜索rTensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E5%AD%90%E5%86%85%E4%B8%8E%E7%AE%97%E5%AD%90%E9%97%B4trade-off"><span class="toc-number">7.3.</span> <span class="toc-text">7.3 算子内与算子间trade-off</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%90%E5%BC%A0%E9%87%8F%E6%94%BE%E7%BD%AE"><span class="toc-number">7.4.</span> <span class="toc-text">7.4 子张量放置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90-4"><span class="toc-number">7.5.</span> <span class="toc-text">7.5 分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#parallel-matrix-multiplication-a-systematic-journey"><span class="toc-number">8.</span> <span class="toc-text">8. Parallel Matrix
Multiplication: A Systematic Journey</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">9.</span> <span class="toc-text">9. 总结</span></a></li></ol></div></div><div class="post-content"><p>分布式内存计算机的出现主要是为了满足大规模计算任务对计算能力和内存容量的需求,
但是由于物理限制与成本考虑, 单处理器的性能提升存在极限,
而分布式内存计算机通过使用多个相对简单/成本较低的处理器组成集群,
可以在不突破物理限制的情况下, 以较低的成本实现更高的计算性能.</p>
<span id="more"></span>
<p>分布式内存架构是一个多处理器计算机系统,
其中每个处理器都有自己的私有内存, 以及处理器之间某种形式的互联.
计算任务只能对本地数据进行操作, 如果需要远程数据,
则计算任务必须与一个或多个远程处理器通信.</p>
<p><img
src="/2024/11/07/mesh-matmul/Distributed-Memory-Architecture.png" /></p>
<p>在分布式内存架构下, 数据可以静态分布, 也可以通过节点移动;
既可以按需移动数据, 也可以提前将数据推送到新节点.
因此他的编程模型更复杂, 实现高性能计算任务时,
除了通常数据局部性还需要考虑通信开销,
也就是需要在考虑拓扑结构的同时设计数据的存储,通信的策略等.
仅仅是矩阵乘的计算就有非常多种方式, 接下来我将逐一介绍.</p>
<h1 id="目前常用的分布式矩阵乘方法">1. 目前常用的分布式矩阵乘方法</h1>
<p>目前大家对于大模型分布式可能比较熟悉, 因此我从大模型中的TP开始举例,
首先原始的TP是在一维拓扑上切分模型的权重,
也就是每个处理器节点存储<code>A[M,K], B[K,N/P]</code>大小的数据:</p>
<p><img src="/2024/11/07/mesh-matmul/TP%201D.png" /></p>
<p>套用<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.15032">oneflow</a>中sbp的表示方式,
这里的切分逻辑可以表示为: <span class="math display">\[
\begin{aligned}
  (broadcast) \times (split(1)) = (split(1))
\end{aligned}
\]</span></p>
<p>那么假设扩展到二维拓扑下, 我们可以自然的想到对K维度进行切分,
那么每个阶段存储<code>A[M,K/P], B[K/P,N/P]</code>大小的数据:</p>
<p><img src="/2024/11/07/mesh-matmul/TP%202D.png" /></p>
<p>当然这里别忘记K维度切分后需要做Core间reduce, 使用sbp可以按如下描述
<span class="math display">\[
\begin{aligned}
  (broadcast, split(1)) \times (split(1), split(0)) = (split(1),
partialsum)\\
  (split(1), partialsum) \xrightarrow{\text{boxing}} (split(1),
broadcast)
\end{aligned}
\]</span></p>
<p>同样还可以切分M和N:</p>
<p><span class="math display">\[
\begin{aligned}
  (split(0), broadcast) \times (broadcast, split(1)) = (split(0),
split(1))
\end{aligned}
\]</span></p>
<p>不知道大家有没有发现, 在sbp的表示方法下,
它所指示的切分代表实际存储的数据分片, 并且计算的数据完全来自本地数据,
那么在它的表示下, 矩阵乘总是有一个维度是无法被切分的,
因为在M,K,N同时被二维拓扑切分时, 按他的逻辑所计算出的结果会不完整.
这带来的第一个问题即是每个节点的内存占用较高.</p>
<h1 id="cannon算法">2. Cannon算法</h1>
<h2 id="原理">2.1 原理</h2>
<p>Cannon算法来自于1969年的论文<a
target="_blank" rel="noopener" href="https://dl.acm.org/doi/book/10.5555/905686">A cellular computer to
implement the Kalman Filter Algorithm</a>, 它适用在均匀的2D
mesh上实现分布式矩阵乘, 他的优势在于完全对三个维度进行切分,
不会增加内存占用:</p>
<p><img src="/2024/11/07/mesh-matmul/Cannon%20Alg.png" /></p>
<p>它将K维度的reduce依旧放在同一个core上执行,
但是同时需要把K维度进行切分,
通过时间上的迭代逐渐<code>shift</code>需要的k到目标core上.
因此需要对矩阵A,B进行预先分布,
并且此时的切分状态就无法使用类似SBP的形式表示, 需要用映射的方式来表示:
<span class="math display">\[
\begin{aligned}
  [j, (i+j) \% \sqrt{P}] \rightarrow [m,k] \\
  [(i+j) \% \sqrt{P}, i] \rightarrow [k,n] \\
\end{aligned}
\]</span></p>
<p>下图展示了在二维mesh的第一行处理器上所分布的数据状态:</p>
<p><img src="/2024/11/07/mesh-matmul/Cannon.png" /></p>
<p>接下来分别是<code>矩阵乘累加</code>以及<code>shift</code>,
<code>shift</code>表示在x和y方向上对A,B的数据分片进行ring的传输,
下图展示的是<code>shift</code>后数据分片变化情况:</p>
<p><img src="/2024/11/07/mesh-matmul/Canon%20Compute.gif" /></p>
<h2 id="实现">2.2 实现</h2>
<p>下面是一个基于mpi4py实现的cannon算法实例:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> viztracer <span class="keyword">import</span> log_sparse</span><br><span class="line">np.set_printoptions(suppress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul</span>(<span class="params">c: np.ndarray, a, b</span>):</span><br><span class="line">  c += a @ b</span><br><span class="line"></span><br><span class="line"><span class="meta">@log_sparse(<span class="params">stack_depth=<span class="number">3</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cannon</span>(<span class="params">A: np.ndarray, B: np.ndarray, C: np.ndarray, P, comm2d: MPI.Cartcomm</span>):</span><br><span class="line">  (M, K) = A.shape</span><br><span class="line">  N = B.shape[<span class="number">1</span>]</span><br><span class="line">  (mTile, nTile, kTile) = (M // P, N // P, K // P)</span><br><span class="line"></span><br><span class="line">  (j, i) = comm2d.Get_coords(rank)  <span class="comment"># topology is row major</span></span><br><span class="line">  <span class="comment"># align data</span></span><br><span class="line">  k = (i + j) % P</span><br><span class="line">  a = np.ascontiguousarray(A[j * mTile:(j + <span class="number">1</span>) * mTile, k * kTile:(k + <span class="number">1</span>) * kTile].copy())</span><br><span class="line">  b = np.ascontiguousarray(B[k * kTile:(k + <span class="number">1</span>) * kTile, i * nTile:(i + <span class="number">1</span>) * nTile].copy())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compute and shift</span></span><br><span class="line">  c = np.empty([mTile, nTile], np.float32)</span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(P):</span><br><span class="line">    matmul(c.view(), a, b)</span><br><span class="line">    <span class="keyword">if</span> t == P - <span class="number">1</span>: <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># top right is (0,0)</span></span><br><span class="line">    right, left = comm2d.Get_cart_rank([j, (i + <span class="number">1</span>) % P]), comm2d.Get_cart_rank([j, (i - <span class="number">1</span>) % P])</span><br><span class="line">    comm2d.Sendrecv_replace(a, dest=left, source=right)</span><br><span class="line">    top, down = comm2d.Get_cart_rank([(j - <span class="number">1</span>) % P, i]), comm2d.Get_cart_rank([(j + <span class="number">1</span>) % P, i])</span><br><span class="line">    comm2d.Sendrecv_replace(b, dest=top, source=down)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compare result</span></span><br><span class="line">  ref = C[j * mTile:(j + <span class="number">1</span>) * mTile, i * nTile:(i + <span class="number">1</span>) * nTile]</span><br><span class="line">  <span class="keyword">assert</span> np.allclose(c, ref)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  A, B, C = np.load(<span class="string">&#x27;A.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>), np.load(<span class="string">&#x27;B.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>), np.load(<span class="string">&#x27;C.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">  P = <span class="number">3</span></span><br><span class="line">  comm = MPI.COMM_WORLD</span><br><span class="line">  rank = comm.Get_rank()</span><br><span class="line">  comm2d = comm.Create_cart([P, P], [<span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line">  cannon(A, B, C, P, comm2d)</span><br></pre></td></tr></table></figure>
<p>我这里构造了<code>M:11520,K:7680,N:12288</code>大小的矩阵,
并使用9个处理器执行: <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">mpiexec -n 9 python cannon.py               </span><br></pre></td></tr></table></figure></p>
<p>此代码的profiling结果如下: <img
src="/2024/11/07/mesh-matmul/Cannon%20Trace.png" /></p>
<h2 id="分析">2.3 分析</h2>
<p>首先我们使用<a
target="_blank" rel="noopener" href="https://www.cs.utexas.edu/~pingali/CSE392/2011sp/lectures/Conc_Comp.pdf">此处</a>所提出的alpha-beta模型分析代价:</p>
<p><img src="/2024/11/07/mesh-matmul/Cost%20Model.png" /></p>
<p>对于Cannon算法, 假设处理器个数为<span
class="math inline">\(P\)</span>, 并且矩阵三个维度均为<span
class="math inline">\(n\)</span>, 在忽略初始分布开销的情况下,
通信代价下限为: <span class="math display">\[
\begin{aligned}
  Cost = 2(\sqrt{P}-1)(\alpha + \frac{n^2}{P} \beta)
\end{aligned}
\]</span></p>
<h1 id="summa算法">3. SUMMA算法</h1>
<p>SUMMA算法来源于1995年的论文<a
target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.5555/899248">Scalable Universal Matrix
Multiplication Algorithm</a>,
他克服了cannon算法限制处理器阵列必须为方形的缺点,
提出了一种更加通用以及便于实现的算法:</p>
<p><img src="/2024/11/07/mesh-matmul/SUMMA%20Alg.png" /></p>
<h2 id="原理-1">3.1 原理</h2>
<p>他是典型的计算的存储分离的模式, 他同样将数据进行了完全切分,
每个节点上没有重复存储, 此时可以使用类似SBP的方式来表示: <span
class="math display">\[
\begin{aligned}
    (split(0), split(1)) \times (split(0), split(1)) = (split(0),
split(1))
\end{aligned}
\]</span></p>
<p>对应的示意图如下:</p>
<p><img src="/2024/11/07/mesh-matmul/SUMMA.png" /></p>
<p>计算和使用SIMD指令进行外积形式的矩阵乘一致, 在K维度迭代,
只是数据需要从存储它的节点broadcast到其他节点:</p>
<p><img src="/2024/11/07/mesh-matmul/SUMMA%20Compute.gif" /></p>
<h2 id="实现-1">3.2 实现</h2>
<p>为了简单起见, 依旧使用<code>M:11520,K:7680,N:12288</code>大小的矩阵,
并使用9个处理器执行: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">from</span> viztracer <span class="keyword">import</span> log_sparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.set_printoptions(suppress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul</span>(<span class="params">c: np.ndarray, a, b</span>):</span><br><span class="line">  c += a @ b</span><br><span class="line"></span><br><span class="line"><span class="meta">@log_sparse(<span class="params">stack_depth=<span class="number">5</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">summa</span>(<span class="params">A: np.ndarray, B: np.ndarray, C: np.ndarray, P, comm2d: MPI.Cartcomm</span>):</span><br><span class="line">  col_comm = comm2d.Sub([<span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line">  row_comm = comm2d.Sub([<span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line"></span><br><span class="line">  (M, K) = A.shape</span><br><span class="line">  N = B.shape[<span class="number">1</span>]</span><br><span class="line">  (mTile, nTile, kTile) = (M // P, N // P, K // P)</span><br><span class="line">  (j, i) = comm2d.Get_coords(rank)  <span class="comment"># topology is row major</span></span><br><span class="line">  <span class="comment"># align data</span></span><br><span class="line">  a = np.ascontiguousarray(A[j * mTile:(j + <span class="number">1</span>) * mTile, i * kTile:(i + <span class="number">1</span>) * kTile].copy())</span><br><span class="line">  b = np.ascontiguousarray(B[j * kTile:(j + <span class="number">1</span>) * kTile, i * nTile:(i + <span class="number">1</span>) * nTile].copy())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compute and broadcast</span></span><br><span class="line">  c = np.empty([mTile, nTile], np.float32)</span><br><span class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(P):</span><br><span class="line">    Atemp = a <span class="keyword">if</span> k == i <span class="keyword">else</span> np.empty_like(a)</span><br><span class="line">    Btemp = b <span class="keyword">if</span> k == j <span class="keyword">else</span> np.empty_like(b)</span><br><span class="line">    row_comm.Bcast(Atemp, root=k)</span><br><span class="line">    col_comm.Bcast(Btemp, root=k)</span><br><span class="line">    matmul(c.view(), Atemp, Btemp)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compare result</span></span><br><span class="line">  ref = C[j * mTile:(j + <span class="number">1</span>) * mTile, i * nTile:(i + <span class="number">1</span>) * nTile]</span><br><span class="line">  <span class="keyword">assert</span> np.allclose(c, ref)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  A, B, C = np.load(<span class="string">&#x27;A.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>), np.load(<span class="string">&#x27;B.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>), np.load(<span class="string">&#x27;C.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">  P = <span class="number">3</span></span><br><span class="line">  comm = MPI.COMM_WORLD</span><br><span class="line">  rank = comm.Get_rank()</span><br><span class="line">  comm2d = comm.Create_cart([P, P], [<span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line">  summa(A, B, C, P, comm2d)</span><br></pre></td></tr></table></figure></p>
<p>此代码的profiling结果如下: <img
src="/2024/11/07/mesh-matmul/SUMMA%20Trace.png" /></p>
<h2 id="分析-1">3.3 分析</h2>
<p>同样保持与之前类似的假设, 计算得通信代价下界为:</p>
<p><span class="math display">\[
\begin{aligned}
  Cost =  2 \sqrt{P} (\alpha\log \sqrt{P} + \frac{n^2}{P}\beta)
\end{aligned}
\]</span></p>
<h1 id="d-summa算法">4. 3D SUMMA算法</h1>
<p>3D SUMMA算法来自于论文<a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/5389455">A three-dimensional
approach to parallel matrix multiplication</a>,
主要用于三维拓扑结构下进行矩阵乘计算, 他的优点是通信量比2d
summa更小:</p>
<p><img src="/2024/11/07/mesh-matmul/SUMMA%203D%20Alg.png" /></p>
<h2 id="原理-2">4.1 原理</h2>
<p>它首先把处理器节点构造为一个立方体拓扑结构,
然后将数据尽量切分映射到不同的节点上:</p>
<p><img src="/2024/11/07/mesh-matmul/SUMMA%203D.png" /></p>
<p>使用SBP的表示方式来描述: <span class="math display">\[
\begin{aligned}
  &amp;(split(1), split(1), split(0)) \times  (split(1), split(0),
split(0)) = (split(1), split(1), split(0)) \\
  &amp;A:[i,j+l] \rightarrow [m,k]\\
  &amp;B:[l,j+i] \rightarrow [k,n]\\
  &amp;C:[i,j+l] \rightarrow [m,n]
\end{aligned}
\]</span></p>
<p>为了尽量减少存储开销, 对于A,B都在三个维度进行了切分,
同时其中两个维度切的是同一个轴.
先通过<code>Allgather</code>将A,B收集并广播,然后计算矩阵乘得到<span
class="math inline">\(D_{ijl}\)</span>,
然后通过一个<code>Alltoall</code>将z轴的k换成了n的切分,
然后每个core单独执行reduce操作即可:</p>
<p><img src="/2024/11/07/mesh-matmul/SUMMA%203D%20Compute.png" /></p>
<h2 id="实现-2">4.2 实现</h2>
<p>为了简单起见, 依旧使用<code>M:11520,K:7680,N:12288</code>大小的矩阵,
并使用8个处理器执行: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">from</span> viztracer <span class="keyword">import</span> log_sparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.set_printoptions(suppress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@log_sparse(<span class="params">stack_depth=<span class="number">3</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">summa_3d</span>(<span class="params">A: np.ndarray, B: np.ndarray, C: np.ndarray, p, comm3d: MPI.Cartcomm</span>):</span><br><span class="line">  x_comm = comm3d.Sub([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line">  y_comm = comm3d.Sub([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line">  z_comm = comm3d.Sub([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line"></span><br><span class="line">  (M, K) = A.shape</span><br><span class="line">  N = B.shape[<span class="number">1</span>]</span><br><span class="line">  (MTile, NTile, KTile) = (M // p, N // p, K // p)</span><br><span class="line">  (mTile, nTile, kTile) = (MTile // p, NTile // p, KTile // p)</span><br><span class="line">  (l, j, i) = comm3d.Get_coords(rank)  <span class="comment"># topology is row major</span></span><br><span class="line">  <span class="comment"># align data</span></span><br><span class="line">  a = np.ascontiguousarray(A[i * MTile:(i + <span class="number">1</span>) * MTile, l * KTile:(l + <span class="number">1</span>) * KTile]</span><br><span class="line">                           [:, j * kTile: (j + <span class="number">1</span>) * kTile].copy())</span><br><span class="line">  b = np.ascontiguousarray(B[l * KTile:(l + <span class="number">1</span>) * KTile, j * NTile:(j + <span class="number">1</span>)</span><br><span class="line">                           * NTile][:, i * nTile: (i + <span class="number">1</span>) * nTile].copy())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compute and passing data</span></span><br><span class="line">  c = np.zeros([MTile, nTile], np.float32)</span><br><span class="line">  Atemp = np.empty([p, MTile, kTile], np.float32)</span><br><span class="line">  Btemp = np.empty([p, KTile, nTile], np.float32)</span><br><span class="line">  y_comm.Allgather(a, Atemp)</span><br><span class="line">  x_comm.Allgather(b, Btemp)</span><br><span class="line">  Atemp = Atemp.transpose([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]).reshape(MTile, KTile)</span><br><span class="line">  Btemp = Btemp.transpose([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]).reshape(KTile, NTile)</span><br><span class="line">  Dl = np.dot(Atemp, Btemp)  <span class="comment"># [MTile, NTile]</span></span><br><span class="line">  Dr = np.empty([p, MTile, nTile], np.float32)</span><br><span class="line">  <span class="comment"># Note that mpi4py will send data sequentially. So, we want to resplit on N. We had to split it first.</span></span><br><span class="line">  Dsend = np.ascontiguousarray(np.stack(np.split(Dl, p, axis=-<span class="number">1</span>)))</span><br><span class="line">  z_comm.Alltoall(Dsend, Dr)</span><br><span class="line">  c += np.<span class="built_in">sum</span>(Dr, axis=<span class="number">0</span>, keepdims=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compare result</span></span><br><span class="line">  ref = C[i * MTile:(i + <span class="number">1</span>) * MTile, j * NTile:(j + <span class="number">1</span>) * NTile][:, l * nTile:(l + <span class="number">1</span>) * nTile]</span><br><span class="line">  <span class="keyword">assert</span> np.allclose(c, ref)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  A, B, C = np.load(<span class="string">&#x27;A.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>), np.load(<span class="string">&#x27;B.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>), np.load(<span class="string">&#x27;C.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">  p = <span class="number">2</span></span><br><span class="line">  comm = MPI.COMM_WORLD</span><br><span class="line">  rank = comm.Get_rank()</span><br><span class="line">  comm3d = comm.Create_cart([p, p, p], [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line">  summa_3d(A, B, C, p, comm3d)</span><br></pre></td></tr></table></figure>
注意由于<code>mpi4py</code>所提供一些接口限制,
中间引入了一些必要的数据重排操作来保证通信的正确性,
此代码的profiling结果如下: <img
src="/2024/11/07/mesh-matmul/SUMMA%203D%20Trace.png" /></p>
<h2 id="分析-2">4.3 分析</h2>
<p>同样保持与之前类似的假设, 计算得通信代价下界为: <span
class="math display">\[
\begin{aligned}
  Cost &amp;= 2(\alpha\log\sqrt[3]{P} +
\frac{(\sqrt[3]{P}-1)}{\sqrt[3]{P}}\frac{N^2}{P}\beta) +
(\alpha\log\sqrt[3]{P} + \frac{(\sqrt[3]{P}-1)}{\sqrt[3]{P}}
\frac{N^2}{P^{\frac{2}{3}}} \beta ) \\
      &amp;= 3\alpha\log\sqrt[3]{P} +
\frac{(\sqrt[3]{P}-1)}{\sqrt[3]{P}}(\frac{2N^2}{P} +
\frac{N^2}{P^{\frac{2}{3}}})\beta
\end{aligned}
\]</span></p>
<p>总的来说, 3D算法可以比2D算法少传输<span
class="math inline">\(P^{\frac{1}{6}}\)</span>倍的数据.</p>
<h1 id="d-summa-算法">5. 2.5D SUMMA 算法</h1>
<p>实际上2.5D算法有两种, 一种是<a
target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-642-23397-5_10">基于Cannon改进版</a>,
另一种是<a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/6114461">基于SUMMA改进版</a>.不过他们都是在$p
p d,1 d p $的拓扑结构下执行的一种泛化算法:</p>
<p><img src="/2024/11/07/mesh-matmul/2.5D%20Grid.png" /></p>
<p>其中Cannon改进版还有存在拓扑结构为方阵的约束, 并且计算逻辑复杂:</p>
<p><img src="/2024/11/07/mesh-matmul/Cannon%202.5D%20ALG.png" /></p>
<p>后来作者提出SUMMA改进版, 可以推广到非方阵拓扑, 计算过程简单,
并且和上面的算法有相同的理论下限:</p>
<p><img src="/2024/11/07/mesh-matmul/SUMMA%202.5D%20ALG.png" /></p>
<h2 id="原理-3">5.1 原理</h2>
<p>支持可以变化的拓扑结构, d可以降低到1退化为2D并行,
也可以提高到p进化为3d并行. 使用SBP的表示方式来描述: <span
class="math display">\[
\begin{aligned}
  &amp;(broadcast, split(1), split(0)) \times (broadcast, split(1),
split(0)) = (partialsum, split(1), split(0)) \\
  &amp;A:[i,j] \rightarrow [m,k]\\
  &amp;B:[i,j] \rightarrow [k,n]\\
  &amp;C:[i,j,l] \rightarrow [m,n,k^*]
\end{aligned}
\]</span></p>
<p>首先在z轴为0处分布A,B矩阵, 然后broadcast到整个阵列.
接着是横向纵向重新分布a,b, 实现在z轴重新切分K,
最后矩阵乘并在z轴reduce得到结果:</p>
<p><img src="/2024/11/07/mesh-matmul/SUMMA%202.5D%20Compute.png" /></p>
<h2 id="实现-3">5.2 实现</h2>
<p>为了简单起见, 依旧使用<code>M:11520,K:7680,N:12288</code>大小的矩阵,
并使用18个处理器执行: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">from</span> viztracer <span class="keyword">import</span> log_sparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.set_printoptions(suppress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@log_sparse(<span class="params">stack_depth=<span class="number">3</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">summa_2_5d</span>(<span class="params">A: np.ndarray, B: np.ndarray, C: np.ndarray, p, d, comm3d: MPI.Cartcomm</span>):</span><br><span class="line">  x_comm = comm3d.Sub([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line">  y_comm = comm3d.Sub([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line">  z_comm = comm3d.Sub([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line"></span><br><span class="line">  (M, K) = A.shape</span><br><span class="line">  N = B.shape[<span class="number">1</span>]</span><br><span class="line">  (MTile, NTile, KPTile, KDTile) = (M // p, N // p, K // p, K // d)</span><br><span class="line">  (l, j, i) = comm3d.Get_coords(rank)  <span class="comment"># topology is row major</span></span><br><span class="line">  <span class="comment"># align data</span></span><br><span class="line">  a = np.ascontiguousarray(A[i * MTile:(i + <span class="number">1</span>) * MTile, j * KPTile:(j + <span class="number">1</span>)</span><br><span class="line">                           * KPTile]) <span class="keyword">if</span> l == <span class="number">0</span> <span class="keyword">else</span> np.ones([MTile, KPTile], np.float32)</span><br><span class="line">  b = np.ascontiguousarray(B[i * KPTile:(i + <span class="number">1</span>) * KPTile, j * NTile:(j + <span class="number">1</span>)</span><br><span class="line">                           * NTile]) <span class="keyword">if</span> l == <span class="number">0</span> <span class="keyword">else</span> np.ones([KPTile, NTile], np.float32)</span><br><span class="line">  c = np.zeros([MTile, NTile], dtype=<span class="built_in">float</span>)</span><br><span class="line">  <span class="comment"># compute and passing data</span></span><br><span class="line"></span><br><span class="line">  z_comm.Bcast(a, root=<span class="number">0</span>)</span><br><span class="line">  z_comm.Bcast(b, root=<span class="number">0</span>)</span><br><span class="line">  ktile = math.gcd(KPTile, KDTile)</span><br><span class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>((l * KDTile) // ktile, ((l + <span class="number">1</span>) * KDTile) // ktile):</span><br><span class="line">    aroot = ((k * ktile) // KPTile)</span><br><span class="line">    Atemp = np.copy(a[:, (k * ktile) - (aroot * KPTile):</span><br><span class="line">                      ((k + <span class="number">1</span>) * ktile) - (aroot * KPTile)]) <span class="keyword">if</span> aroot == j <span class="keyword">else</span> np.empty([MTile, ktile], np.float32)</span><br><span class="line">    y_comm.Bcast(Atemp, root=aroot)</span><br><span class="line"></span><br><span class="line">    broot = ((k * ktile) // KPTile)</span><br><span class="line">    Btemp = np.copy(b[(k * ktile) - (broot * KPTile):</span><br><span class="line">                      ((k + <span class="number">1</span>) * ktile) - (broot * KPTile), :]) <span class="keyword">if</span> broot == i <span class="keyword">else</span> np.empty([ktile, NTile], np.float32)</span><br><span class="line">    x_comm.Bcast(Btemp, root=broot)</span><br><span class="line">    np.add(c, np.dot(Atemp, Btemp), out=c)</span><br><span class="line"></span><br><span class="line">  cr = np.empty([MTile, NTile], dtype=<span class="built_in">float</span>) <span class="keyword">if</span> l == <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">  z_comm.Reduce(c, cr)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compare result</span></span><br><span class="line">  <span class="keyword">if</span> l == <span class="number">0</span>:</span><br><span class="line">    ref = C[i * MTile:(i + <span class="number">1</span>) * MTile, j * NTile:(j + <span class="number">1</span>) * NTile]</span><br><span class="line">    <span class="keyword">assert</span> np.allclose(cr, ref)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  A, B, C = np.load(<span class="string">&#x27;A.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>), np.load(<span class="string">&#x27;B.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>), np.load(<span class="string">&#x27;C.npy&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">  p = <span class="number">3</span></span><br><span class="line">  d = <span class="number">2</span></span><br><span class="line">  comm = MPI.COMM_WORLD</span><br><span class="line">  rank = comm.Get_rank()</span><br><span class="line">  comm3d = comm.Create_cart([d, p, p], [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line">  summa_2_5d(A, B, C, p, d, comm3d)</span><br></pre></td></tr></table></figure></p>
<p>实现的时候需要考虑跟多的细节, 因为他这里实际上重新对K进行了切分,
所以迭代k的时候需要按更小的块来广播, 此代码的profiling结果如下: <img
src="/2024/11/07/mesh-matmul/SUMMA%202.5D%20Trace.png" /></p>
<h2 id="分析-3">5.3 分析</h2>
<p>假设处理器维度为<span class="math inline">\(p,p,d\)</span>,
并且忽略第一次的分布的代价, 计算通信代价为: <span
class="math display">\[
\begin{aligned}
  Cost &amp; = 2\alpha \log p  + 2\frac{n^2}{pd} \beta +  \alpha \log d+
2\frac{(d-1)n^2}{d}  \beta \\
\end{aligned}
\]</span></p>
<p>注意到SUMMA 2.5D实际上在d上多存储A,B矩阵.</p>
<h1 id="大模型中的2.5d与3d并行">6. 大模型中的2.5D与3D并行</h1>
<h2 id="tesseract">6.1 <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.14500">Tesseract</a></h2>
<p>Tesseract实际上就是在大模型训练中被大家所熟知的2.5D并行,
他注意到了SUMMA 2.5D算法额外的内存开销问题, 提出进一步切分A矩阵:</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>A</th>
<th>B</th>
<th>C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="/2024/11/07/mesh-matmul/Tesseract%20A.png" /></td>
<td><img src="/2024/11/07/mesh-matmul/Tesseract%20B.png" /></td>
<td><img src="/2024/11/07/mesh-matmul/Tesseract%20C.png" /></td>
</tr>
</tbody>
</table>
<p>并且把拓扑结构构造为<span
class="math inline">\(p,p,d\)</span>的形式来更有效的分配数据和计算.</p>
<p>可以在结果中发现2.5D在进化到3D并行时取得了最好的效果.</p>
<p><img src="/2024/11/07/mesh-matmul/Tesseract%20Result.png" /></p>
<h2 id="nus版3d并行">6.2 <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.14450">NUS版3D并行</a></h2>
<p>这篇论文有三个改进, 分别是负载均衡, 优化矩阵矩阵乘, 优化矩阵向量乘.
但他这里的提到原始的3D并行只在两个维度切分A,B矩阵这是不对的,
本文第四节严格按照3D并行原论文进行了实现,
实际上是对于A矩阵的K是在<code>j,l</code>,以及B矩阵的N在<code>j,i</code>都做了进一步的切分的.</p>
<p>不过NUS版对于A矩阵的进一步切分维度确实与原论文不同: <span
class="math display">\[
\begin{align}
  A:[i+j,l] \rightarrow [M,K]\\
  B:[l, j+i] \rightarrow [K,N]
\end{align}
\]</span></p>
<p><img src="/2024/11/07/mesh-matmul/NUS%203D.png" /></p>
<p>最终取得了良好的效果, 但是他这里并没有和Tesseract来对比:</p>
<p><img src="/2024/11/07/mesh-matmul/NUS%203D%20Result.png" /></p>
<h1 id="t10-inter-core-connected-compiler">7. <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.04808">T10: Inter-core Connected
Compiler</a></h1>
<p>T10是针对核间互联架构IPU所提出的专用编译器,
对于不太了解的读者可以先参考<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1899480206">冯Jungle​的解读</a>,
本文就直接分析T10所提出的rTensor抽象的底层逻辑.
首先T10提出了在核间互联架构上有一种内存高效的计算模式:</p>
<p><img src="/2024/11/07/mesh-matmul/T10%20Compute%20Pattern.png" /></p>
<p>即多个core之间切分矩阵, 计算后通过ring的方式交换子矩阵,
再计算得下一部分的结果, 这样可以在空间和时间的角度同时切分矩阵,
从而最小化内存的使用量.</p>
<h2 id="rtensor抽象">7.1 rTensor抽象</h2>
<p>为了在编译器中表达这样一种计算模式, 因此他提出了rTensor的抽象:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RotatingTensor</span> &#123;</span><br><span class="line">  vector &lt; <span class="type">size_t</span> &gt; shape ; </span><br><span class="line">  DataType type ;</span><br><span class="line">  vector &lt; <span class="type">size_t</span> &gt; spatial_partition_factor ;</span><br><span class="line">  vector &lt; <span class="type">size_t</span> &gt; temporal_partition_factor ; </span><br><span class="line">  vector &lt; <span class="type">size_t</span> &gt; rotating_pace ;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>rTensor中的参数解释:</p>
<p><span class="math display">\[
\begin{aligned}
  &amp;f_s^X : \textbf{Spatial Partition Factor},\ \text{Spatially
partitions a tensor X into sub-tensors.} \\
  &amp;f_t^X : \textbf{Temporal Partition Factor},\ \text{Temporally
partitions a sub-tensor of X into sub-tensor partitions.} \\
  &amp;rp : \textbf{Rotating Pace},\ \text{Specifies how sub-tensor
partitions are shifted among cores.}\\
  &amp;F_{op}:  \textbf{ Operator Partition Factor},\ \text{Spatially
partitions an entire operator into sub-operators.}
\end{aligned}
\]</span></p>
<p>下面给定一个rTensor具体实例, 通过执行示意图来理解每个参数的含义:</p>
<p><img src="/2024/11/07/mesh-matmul/T10%20rTensor.png" /></p>
<p>上图中首先通过<span
class="math inline">\(f_s\)</span>在空间上切分矩阵,
然后由于矩阵在计算上的数据依赖, 是无法再使用空间上的并行, 因此采用<span
class="math inline">\(f_t\)</span>在时间维度上切分矩阵, 最后使用<span
class="math inline">\(rp\)</span>选择每次传输的数据块大小.
并且看到右下角的例子中<span
class="math inline">\(rp\)</span>可以小于时间上的分块,
对应到实际计算中就可以用于平衡计算和通信的时间.</p>
<h2 id="自动搜索rtensor">7.2 自动搜索rTensor</h2>
<p>目前有了形式化的描述方法,
那么就可以考虑如何构造搜索域并自动化搜索最优执行方式.
T10并没有采用随机添加rTensor的配置来构造搜索域,
而是通过对原始计算语义上的切分来先行构造rTensor可选的配置域.
这也就是上一小节中<span class="math inline">\(F_{op}\)</span>的作用,
他根据当前op的计算定义来<strong>对矩阵计算进行切分</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
  &amp;C[M,N] \mathrel{+}= A[M,K] * B[K,N] \\
  \text{if } &amp;F_{op}[m,k,n] :\\
  \text{each core: } &amp;C[\frac{M}{m},\frac{N}{n}] \mathrel{+}=
A[\frac{M}{m},\frac{K}{k}] * B[\frac{K}{k},\frac{N}{n}] \\
  &amp;\text{total cores} = \prod_{i=0}^{2} F_{op}[i]
\end{aligned}
\]</span></p>
<p>假设<span class="math inline">\(F_{op}\)</span>确定时,
就可以确定总core数, 以及每个core上的子任务所依赖的数据量: <span
class="math display">\[
\begin{aligned}
  &amp;  C[2,3] \mathrel{+}= A[2,6] * B[6,3] \\
  &amp;F_{op} = [2,1,3] :\\
  \text{each core: } &amp; C[1,1] \mathrel{+}= A[1,6] * B[6,1] \\
  &amp;\text{total cores} = 6
\end{aligned}
\]</span></p>
<p>通过子任务的数据量,
就可以<strong>推导出三个矩阵的空间切分参数</strong>: <span
class="math display">\[
\begin{aligned}
\begin{matrix}
   &amp; \textbf{m} &amp; \textbf{k} &amp; \textbf{n} \\
  F_{op} &amp; 2 &amp; 1 &amp; 3 \\
  f_s^A &amp; 2 &amp; 1 &amp; \\
  f_s^B &amp;  &amp; 1 &amp; 3 \\
  f_s^C &amp; 2 &amp;  &amp; 3
\end{matrix}
\end{aligned}
\]</span></p>
<p>观察上表可以发现A,B,C矩阵都有一个维度不参与他们的空间切分,
但实际上为了计算出最终结果,
这个不参与空间切分的维度是被当前矩阵完全依赖的,
T10称这个维度为<code>missing axis</code>,
因此采用时间切分来分割<code>missing axis</code>. 比如当前一共6个core,
B矩阵在空间上被切分了3组, 那么还剩下<span class="math inline">\(P =
\frac{6}{3} = 2\)</span>组可以用于切分,
因此按这个逻辑<strong>推导时间切分</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
\begin{matrix}
   &amp; \textbf{m} &amp; \textbf{k} &amp; \textbf{n} \\
  F_{op} &amp; 2 &amp; 1 &amp; 3 \\
  f_s^A &amp; 2 &amp; 1 &amp; \\
  f_t^A &amp; \color{blue}1 &amp; \color{red}3 &amp; \\
  f_s^B &amp;  &amp; 1 &amp; 3 \\
  f_t^B &amp; &amp; \color{red}2 &amp; \color{blue}1 \\
  f_s^C &amp; 2 &amp;  &amp; 3 \\
  f_t^C &amp; \color{blue}1 &amp; &amp; \color{blue}1 \\
\end{matrix}
\end{aligned}
\]</span></p>
<p>注意, T10限制了<span class="math inline">\(\frac{P}{\prod
f_t}\)</span>必须是整数, 因为它其实就是ring的圈数, 如果非整数就无法支持.
当<span class="math inline">\(\frac{P}{\prod f_t}\)</span>大于1时,
则表明ring的圈不止一个, 那么为了保证每个圈都可以得到正确的数据,
就需要把子张量复制<span class="math inline">\(\frac{P}{\prod
f_t}\)</span>次.</p>
<p>最后进行<strong>旋转参数对齐</strong>, 按照上面的切分,
可以发现A的K维度在分为3组, 而B的K维度被分为2组,
那么每个节点上所拥有的K分块大小并不一样,
这个时候只能按小的K分块进行计算和旋转:</p>
<p><span class="math display">\[
\begin{aligned}
  rp^A &amp;= [0,k^A]\\
  rp^B &amp;= [k^B,0] \\
  k^A , k^B &amp;\in [1,\min(2,3)]
\end{aligned}
\]</span></p>
<p>整体的流程如图所示:</p>
<p><img src="/2024/11/07/mesh-matmul/T10%20Partitioning.png" /></p>
<h2 id="算子内与算子间trade-off">7.3 算子内与算子间trade-off</h2>
<p>对于每个算子,都存在大量涉及不同空间和时间以及旋转因子确定的执行计划.
此外一个端到端的模型由众多算子组成, 这就产生了一个巨大的组合优化空间.
T10采用的是两级权衡, 首先对每个算子搜索执行时间和内存消耗的最优平衡,
再在不同算子间最优计划之上优化全局内存分配, 这里就不进一步展开了.</p>
<h2 id="子张量放置">7.4 子张量放置</h2>
<p>优化内存分配之后, 还需要考虑数据放置问题, 因为ring
shift只能在相邻节点之间传输数据, 如果初始数据放置的位置不合理,
会导致计算错误, 这在Cannon算法中称为<code>align data</code>过程.
而T10可以从编译器的角度, 看到上下算子的执行计算,
这给他们提供了全局张量放置优化机会.
比如一个算子的计算依赖于另一个算子的输出,
T10会将这些相关的子张量放置在合适的核上, 减少数据准备过程的开销.
并且T10还总是按照阵列轴的方向升序排列数据, 避免出现乱序带来的错误.</p>
<p>在这个3x3阵列上, T10所放置的子张量如图所示:</p>
<p><img src="/2024/11/07/mesh-matmul/T10%20Align.png" /></p>
<p>可以回到第一节查看Cannon算法的初始放置, 两者是一模一样的.</p>
<h2 id="分析-4">7.5 分析</h2>
<p>因此T10的本质是形式化与泛化了Cannon算法, 并将其用于核间互联架构.
那么他的问题也与Cannon一样, 即通信量会比SUMMA 3D等方法大, 比SUMMA
2D方法少<span class="math inline">\(\frac{1}{\sqrt{P}}\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
  Cost = 2(\sqrt{P}-1)(\alpha + \frac{n^2}{P} \beta)
\end{aligned}
\]</span></p>
<h1 id="parallel-matrix-multiplication-a-systematic-journey">8. <a
target="_blank" rel="noopener" href="https://epubs.siam.org/doi/10.1137/140993478">Parallel Matrix
Multiplication: A Systematic Journey</a></h1>
<p>这篇文章是blis作者Flame等人所写, 他通过系统性分析,
提出一种形式化的描述推导出SUMMA 2D/3D矩阵乘,
并分析不同的尺寸的矩阵下如何选择不同的数据广播策略.</p>
<p>首先他通过一种类似<span
class="math inline">\(x[(0,1),()]\)</span>来表示数据在节点上的切分状态,
这里表示的是x的M在阵列的行列均切分, N没有切分,
然后基于mpi的通信原语对切分状态进行转换: <img
src="/2024/11/07/mesh-matmul/Flame%20Formula.png" /></p>
<p>而后基于切分状态描述不同的算法,
并且根据不同的输入尺寸设计不同的计算方法, 下图左侧是Stationary C算法,
右侧是Stationary A算法:</p>
<p><img src="/2024/11/07/mesh-matmul/Flame%20ALG.png" /></p>
<p>Stationary C 算法, 在<span class="math inline">\(m=n\)</span>,
且方阵的情况对于任意的k值都有较好的弱可扩展性. Stationary A 算法,
虽然其通信开销相对较大, 但在<span
class="math inline">\(m\)</span>和<span
class="math inline">\(k\)</span>较大而<span
class="math inline">\(n\)</span>较小时, 该算法能够实现较好的并行性,
因为在这种情况下通信量减少.</p>
<p>最终, 总结在二维阵列上计算矩阵乘的通信量下限为<span
class="math inline">\(\Omega(\frac{n^2}{\sqrt{P}}\)</span>,
在二维阵列上计算矩阵乘的通信量下限为<span
class="math inline">\(\Omega(\frac{n^2}{\sqrt[3]{P^2}})\)</span>,
但是具体的场景还需要根据矩阵乘的尺寸带来的不同的通信量,
来选择不同的计算方法.</p>
<h1 id="总结">9. 总结</h1>
<ol type="1">
<li>传统分布式矩阵乘的论文所计算的通信量通常假设矩阵大小相同,
但实际上需要分析当前场景下不同尺寸的通信开销.</li>
<li>T10是一个好的例子, 但是他的计算模式还在固定在Cannon算法,
如何推广到SUMMA?</li>
<li>如何推广更高维度阵列下的矩阵乘切分和映射?</li>
<li>如何对于一个支持RMA(Remote Memory Access)的架构,
是直接读取远端数据还是通过broadcast的方式把计算模式回落到SUMMA或Cannon?
<ol type="1">
<li>理论上RMA所需要读取的数据量与SUMMA是一致的,
但实现性能可能需要进一步测试.</li>
</ol></li>
</ol>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag">分布式</a></li></ul></div><div class="post-nav"><a class="pre" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a><a class="next" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/03/13/vllm/sglang_attn/">vllm/sglang_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/trt_attn/">vllm/trt_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/vllm_attn/">vllm/vllm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/tvm_attn/">vllm/tvm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/23/torch-trick/">Pytorch中遇到的一些问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>