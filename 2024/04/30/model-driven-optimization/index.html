<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Model Driven Optimization | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Model Driven Optimization</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Model Driven Optimization</h1><div class="post-meta">2024-04-30<span> | </span><span class="category"><a href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 13</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%95%E5%86%85%E5%AD%98%E5%B1%82%E7%BA%A7%E5%BB%BA%E6%A8%A1"><span class="toc-number">1.</span> <span class="toc-text">1. 单内存层级建模</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%86%85%E5%AD%98%E5%B1%82%E7%BA%A7%E5%BB%BA%E6%A8%A1"><span class="toc-number">2.</span> <span class="toc-text">2. 多内存层级建模</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%B0%E8%AE%A1%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4"><span class="toc-number">3.</span> <span class="toc-text">3. 估计执行时间</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tilesize%E4%B8%8E%E5%BE%AA%E7%8E%AF%E6%8E%92%E5%88%97%E9%80%89%E6%8B%A9"><span class="toc-number">4.</span> <span class="toc-text">4. tilesize与循环排列选择</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#micro-kernel"><span class="toc-number">5.</span> <span class="toc-text">5. micro kernel</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%96%91%E9%97%AE"><span class="toc-number">6.</span> <span class="toc-text">6. 疑问</span></a></li></ol></div></div><div class="post-content"><p>关于<code>Model-Driven Optimization For Tensor Computations</code>论文的阅读笔记.</p>
<span id="more"></span>
<h1 id="单内存层级建模">1. 单内存层级建模</h1>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (i2 = <span class="number">0</span>; i2 &lt; Ni; i2+=Ti1)</span><br><span class="line">  <span class="keyword">for</span> (j2 = <span class="number">0</span>; j2 &lt; Nj; j2+=Tj1)</span><br><span class="line">    <span class="keyword">for</span> (k2 = <span class="number">0</span>; k2 &lt; Nk; k2+=Tk1)</span><br><span class="line">      <span class="keyword">for</span> (i1 = <span class="number">0</span>; i1 &lt; Ti1; i1++)</span><br><span class="line">        <span class="keyword">for</span> (j1 = <span class="number">0</span>; j1 &lt; Tj1; j1++)</span><br><span class="line">          <span class="keyword">for</span> (k1 = <span class="number">0</span>; k1 &lt; Tk1; k1++)</span><br><span class="line">            C[i1+i2][j1+j2] += A[i1+i2][k1+k2] * B[k1+k2][j1+j2];</span><br></pre></td></tr></table></figure>
<p>以这样的矩阵乘作为例子, 进行单层内存层级的tiling建模.
首先循环变量表示为<span
class="math inline">\(i_{1},i_{2},\ldots,i_{l+1}\)</span>, 其中<span
class="math inline">\(l\)</span>为tiling层级(我理解这个应该就是循环层级),
<span class="math inline">\(l==0\)</span>表示的是statement, <span
class="math inline">\(l+1\)</span>表示的就是最外层循环,<span
class="math inline">\(l==1\)</span>表示最内层循环.
tilesize变量表示为<span
class="math inline">\(T_{i_{1}},T_{i_{2}},\ldots,T{i_{l+1}}\)</span>.</p>
<p>对于一个固定的循环order, 我们可以用上面定义的变量来建模数据移动.
令<span class="math inline">\(DF(A,i)\)</span>表示的是数组<span
class="math inline">\(A\)</span>从循环<span
class="math inline">\(i\)</span>开始的被访问过的<strong>不同</strong>元素的数量(Data
Footprint).令<span class="math inline">\(DM(A,i)\)</span>表示在循环<span
class="math inline">\(i\)</span>处数组<span
class="math inline">\(A\)</span>从主存到cache的数据移动(Data Movement).
下面这段代码展示令如何计算这两个不同的项目,
其实他们的区别也就是在于存在数据复用的时候, 内存足迹是不改变的,
但是内存移动量在内存足迹大于cache size的时候是需要重复load的.
根据这个计算方式, 也就是表示如果有一个维度足够小,
可以让数据完整的放在cache中, 那么他的就不需要重复load.</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">foreach</span> (loop i <span class="keyword">in</span> Loops.Reverse())  &#123;</span><br><span class="line">  <span class="keyword">if</span> (i == <span class="number">0</span>) &#123; <span class="comment">// statement level</span></span><br><span class="line">    <span class="keyword">foreach</span>(tensor A <span class="keyword">in</span> Tensors) &#123;</span><br><span class="line">      DM(A, i) = DF(A, i) = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">foreach</span>(tensor A <span class="keyword">in</span> Tensors) &#123;</span><br><span class="line">      <span class="keyword">if</span> (A.Indices.Contains(i)) &#123;</span><br><span class="line">        DM(A, i) = DM(A, i − <span class="number">1</span>) * range(i);</span><br><span class="line">        DF(A, i) = DF(A, i − <span class="number">1</span>) * range(i);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        DF(A, i) = DF(A, i − <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (Sum(Tensors, A =&gt; DF(A, i − <span class="number">1</span>)) &lt; CacheCapacity) &#123;</span><br><span class="line">          DM(A, i) = DM(A, i − <span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          DM(A, i) = DM(A, i − <span class="number">1</span>) * range(i);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="多内存层级建模">2. 多内存层级建模</h1>
<p>目前的现代处理架构通常有多个内存层级,
更高的内存层级会有更大的存储以及更小的带宽.
假设每一个循环只能tile到其中一个内存层级,
对于一个两级内存层级的tiling示例代码如下: <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (i3 = <span class="number">0</span>; i3 &lt; Ni; i3+=Ti2)</span><br><span class="line">  <span class="keyword">for</span> (j3 = <span class="number">0</span>; j3 &lt; Nj; j3+=Tj2)</span><br><span class="line">    <span class="keyword">for</span> (k3 = <span class="number">0</span>; k3 &lt; Nk; k3+=Tk2)</span><br><span class="line">      <span class="keyword">for</span> (i2 = <span class="number">0</span>; i2 &lt; Ti2; i2+=Ti1)</span><br><span class="line">        <span class="keyword">for</span> (j2 = <span class="number">0</span>; j2 &lt; Tj2; j2+=Tj1)</span><br><span class="line">          <span class="keyword">for</span> (k2 = <span class="number">0</span>; k2 &lt; Tk2; k2+=Tk1)</span><br><span class="line">            <span class="keyword">for</span> (i1 = <span class="number">0</span>; i1 &lt; Ti1; i1++)</span><br><span class="line">              <span class="keyword">for</span> (j1 = <span class="number">0</span>; j1 &lt; Tj1; j1++)</span><br><span class="line">                <span class="keyword">for</span> (k1 = <span class="number">0</span>; k1 &lt; Tk1; k1++)</span><br><span class="line">                  C[i1+i2+i3][j1+j2+j3] += A[i1+i2+i3][k1+k2+k3] * B[k1+k2+k3][j1+j2+j3];</span><br></pre></td></tr></table></figure></p>
<p>为了适配多内存层级, 需要修改<span
class="math inline">\(DM\)</span>的定义为<span
class="math inline">\(DM(A,i,l)\)</span>表示数组<span
class="math inline">\(A\)</span>在循环<span
class="math inline">\(i\)</span>上从内存层级<span
class="math inline">\(l\)</span>到<span
class="math inline">\(l+1\)</span>的数据移动,
但原始论文只说了更新建模的代码从<code>Sum(Tensors, A =&gt; DF(A, i − 1)) &lt; CacheCapacity</code>到<code>Sum(Tensors, A =&gt; DF(A, i − 1)) &lt; CacheCapacity(l)</code>,
并没有给出完整的实现.
这里实际上每一个出现<code>DM</code>的地方都需要添加<span
class="math inline">\(l\)</span>参数,
那么可能需要在最外层再添加关于内存层级的循环,
但是从直觉上来说内存层级的选择这里是多分支的,
不是简单的双循环就可以构造出来的.</p>
<h1 id="估计执行时间">3. 估计执行时间</h1>
<p>这里假设访存是主要的瓶颈,
那么通过统计每个内存层级的数据移动量以及对应的带宽大小来估计总时间.
令<span class="math inline">\(L\)</span>表示内存层级编号, <span
class="math inline">\(C_1,\ldots,C_{L}\)</span>表示各个内层层级, 而<span
class="math inline">\(C_0\)</span>表示计算单元,<span
class="math inline">\(C_{L+1}\)</span>表示主存. 令<span
class="math inline">\(BW_1,\ldots,BW_{L+1}\)</span>表示各个内存层级的带宽.
令<span class="math inline">\(C\_DM_(l)\)</span>表示从内存层级<span
class="math inline">\(l\)</span>到<span
class="math inline">\(l-1\)</span>的数据移动量,<span
class="math inline">\(C\_Time(\mathcal{P},
l)\)</span>在特定的循环排列下从内存层级<span
class="math inline">\(l\)</span>到<span
class="math inline">\(l-1\)</span>的数据移动时间.
那么给定一个固定的循环排列<span
class="math inline">\(\mathcal{P}\)</span>,对应的时间为: <span
class="math display">\[
\begin{aligned}
  C\_Time(\mathcal{P}, l) &amp;= C\_DM(l) / BW_l \\
  TotTime(\mathcal{P}) &amp;= \max_{l = 1}^{L+1} \left(
C\_Time(\mathcal{P}, l) \right)
\end{aligned}
\]</span></p>
<h1 id="tilesize与循环排列选择">4. tilesize与循环排列选择</h1>
<p>首先对于一个固定循环排列下的tile size选择就变成了一个约束求解问题:
<span class="math display">\[
\begin{aligned}
  \arg \min_{tile-sizes}(TotTime(\mathcal{P})) = \arg
\min_{tile-sizes}\left(\max_{l = 1}^{L+1} \left( C\_Time(\mathcal{P}, l)
\right)\right)
\end{aligned}
\]</span></p>
<p>为了减少搜索空间, 限制对于内存层级<span
class="math inline">\(l\)</span>的所有内存移动量<span
class="math inline">\(C\_DM(l)\)</span>必须小于等于内存层级的容量,
令<span
class="math inline">\(group\_outer(l)\)</span>表示属于内存层级<span
class="math inline">\(l\)</span>的最外层循环, 构建容量约束如下: <span
class="math display">\[
\begin{aligned}
  \forall l \in [1,L],\ \sum_{A\in Tensors} DM(A, group\_outer(l)) \leq
CacheCapacity(l)
\end{aligned}
\]</span></p>
<p>但这样还是没有考虑tile size对于不同层级的访存速度影响,
需要构建一个多级约束. 令<span
class="math inline">\(T\)</span>表示所有的tile变量集合, <span
class="math inline">\(T_l\)</span>表示会影响内存层级<span
class="math inline">\(l\)</span>的总数据移动量<span
class="math inline">\(C\_DM(l)\)</span>的tile变量子集.
也就是多个循环可以对应同一个内存层级,
那么这些循环对应的tile变量组成的子集,
这些变量每个都会影响当前内存层级的数据移动量.</p>
<p>假设<span class="math inline">\(j\)</span>是最大瓶颈的内存层级,
也就是他的时间大于其他内存层级时间: <span class="math display">\[
\begin{aligned}
  \forall i \in [1, L+1], \ (C\_DM(j)/C\_BW(j)) \geq (C\_DM(i)/C\_BW (i)
\end{aligned}
\]</span></p>
<p>当固定了<span class="math inline">\(j\)</span>层级的tile size后,
下一个瓶颈的内存层级可以使用如下公式找到: <span class="math display">\[
\begin{aligned}
  \argmin_{T-T_j} (\max_{l\in[1,L+1] - T_j} C\_Time(\mathcal{P},l))
\end{aligned}
\]</span></p>
<p>但是论文中并没有详细描述怎么进行多级优化, 这里的约束应该都还是变量,
除非是求解器可以通过某种方式添加优化指导.</p>
<p>接下来就是考虑不同的循环排列, 对于两级tiling,
就有9个循环那么排列方式就有9!种.
但其实只需要考虑每个tile出来的循环内部进行排序,也就是<span
class="math inline">\(3!\times3!\times3! = 216\)</span>种即可.
最终的解为: <span class="math display">\[
\begin{aligned}
final\_solution =  \argmin_{\mathcal{P}\in \mathcal{R}}
(\argmin_{tile\_sizes}(TotTime(\mathcal{P})))
\end{aligned}
\]</span></p>
<h1 id="micro-kernel">5. micro kernel</h1>
<p>对于包含SIMD的指令集的处理器来说,
需要考虑设计一个最大硬件利用率的micro kernel, 这里需要对micro
kernel也进行建模.
首先是<code>MaxIssue</code>表示一个时钟周期最大可以发射的指令数,
<code>WordPerVec</code>表示指令集宽度,
<code>Latency</code>表示一个指令周期所需要的时钟周期数.
假设在一个指令周期中每个时钟周期都可以发射<code>MaxIssue</code>个指令数,
并且他们之间是没有数据依赖的,
那么<code>MaxIssue * Latency</code>则是可以保证流水线打满的最小指令数,
<code>MaxIssue * Latency * WordPerVec</code>则表示最小寄存器容量.
比如在<code>BLIS</code>库中,通常使用外积的方式来设计micro kernel,
但是这个kernel它需要一块布局优化后的数据块才可以开始计算,
这就要求进行<strong>packing</strong>.</p>
<p>packing的一个好处是可以减少Conflict
Miss,现代处理器中cache的存放策略通常是<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cache_placement_policies#Set-associative_cache">set-associative</a>,
整个缓存被划分为多个set,而这些set内部进一步被细分为lines/ways.
一个映射函数决定内存地址到set的映射.
在每个set内,一个给定的内存地址可能出现在任意一个cache line上.
当在多个不同的内存地址由于缓存映射规则而被迫存储在同一个cache line时,
由于每个缓存行只能存储一个数据块,
当一个新的数据块需要被存储到这个缓存行时, 原有的数据块必须被替换掉,
在理想情况下, 被替换的应该是最近最少使用(LRU)的数据块, 然而在某些情况下,
由于映射规则的限制, 一个不是LRU的数据块也可能被替换,
这就是发生了Conflict Miss.</p>
<p>通过选择合适tile大小,并依赖于packing设计,可以将即数据元素的排列顺序与它们将被访问的顺序相同,从而避免Conflict
Miss. 虽然packed buffer在内存中是连续的存储.
但是实际上他们分散在cache中各个地方, 由于大部分的cache是无法编程的,
加载新的数据会将其他tensor从cache中驱除, 为了避免这种情况,
每个tensor拥有的cache line的数量要被小心的控制,
对于矩阵乘的例子这里计算分配给各个tensor的cache line数量: <span
class="math display">\[
\begin{aligned}
lineA = [DF(A,l) / (NumOfSets(l)*lineSize(l))] \\
lineB = [DF(B,l) / (NumOfSets(l)*lineSize(l))] \\
lineC = [DF(C,l) / (NumOfSets(l)*lineSize(l))] \\
s.t Line_A(l)+Line_B(l)+Line_C(l) \leq Associativity(l) \\
\end{aligned}
\]</span> 假设只访问A/B矩阵, 考虑在l层级的j循环, A矩阵并没有被j索引,
但是在k循环中多次访问A行, 此时A会停留在cache中,
而B矩阵的行j循环才会被访问, 因此B矩阵在cache中是流式传输的.</p>
<p>packing 会增加额外的数据移动, 为了减少packing带来的额外时间.
比如要复用pack过的数据, 但是由于cache容量的限制,
大部分情况是没办法存储整个的buffer的. 假设A时需要pack的tneosr, 设<span
class="math inline">\(IS\)</span>表示整个迭代空间, 令<span
class="math inline">\(IS_A\)</span>表示参与访问A的循环子集,
设packing在最后一层cache <span class="math inline">\(ll\)</span>,
那么packing的cost为包含从主存加载以及存储数据到<span
class="math inline">\(ll\)</span>层上:</p>
<p><span class="math display">\[
\begin{aligned}
PackCost^{A, buf\in Mem}_{mem \rightarrow l3} = \prod_{idx \in IS_A} idx
\end{aligned}
\]</span></p>
<p>假设l3级别的tiling循环为<span
class="math inline">\(i_1^{L3},i_2^{L3},\ldots,i_l^{L3}\)</span>,
假设只有<span class="math inline">\(i_2^{L3},
i_l^{L3}\)</span>是A的reuse index,那么意味着<span
class="math inline">\(i_2^{L3}, i_l^{L3} \notin IS_A\)</span>.
假设packed A在level 3进行构造, 那么代码可能类似这样: <span
class="math display">\[
\begin{aligned}
for\ loop&amp;\  i_1^{L3}\\
for\ &amp;loop\  i_2^{L3} \\
&amp; \ldots\\
&amp;for\ loop\  i_l^{L3}\\
&amp; \text{Packing buffer resides here;}
\end{aligned}
\]</span></p>
<p>注意这里packed A将会在<span
class="math inline">\(i_2^{L3}\)</span>的循环中被填充满, 即使<span
class="math inline">\(i_2^{L3}\)</span>是他的reuse index,
这意味着对于给定tenors的内部cache packing的总数据移动是tensor
size和packing cache level以上的所有reuse loops的乘积.
公式化的描述如下:</p>
<p><span class="math display">\[
\begin{gathered}
R D X^{L 3}=\left\{i_g^{L 3} \mid i_g \notin I S_A \wedge\left(\exists
i_h \in I S_A\right)\left[i_g^{L 3}&gt;i_h^{L 3}\right]\right\} \\
\text { PackCost } t_{m e m \rightarrow L b}^{A, b u f \in L 3}=\prod_{i
d x \in I S_A} i d x * \prod_{r d x \in R D X^{L 3}}
\operatorname{NIter}(r d x)
\end{gathered}
\]</span></p>
<p>首先令<span class="math inline">\(i^{L3}_p&gt;
i^{L3}_q\)</span>表示在L3内存层级中<span
class="math inline">\(i^{L3}_p\)</span>循环在<span
class="math inline">\(i^{L3}_q\)</span>之上. 再令<span
class="math inline">\(\operatorname{NIter}(i^{L3}_p)\)</span>表示循环的迭代次数,
令<span class="math inline">\(Tile(i^{L3}_p )\)</span>表示索引<span
class="math inline">\(p\)</span>的tile大小, <span
class="math inline">\(N_p\)</span>表示索引<span
class="math inline">\(p\)</span>的全局大小(我理解就是这个tensor在<span
class="math inline">\(p\)</span>维度的大小). 然后<span
class="math inline">\(RDX^{L3}\)</span>表示在与A无关的循环迭代中,
存在的迭代<span class="math inline">\(i_g^{L 3}\)</span>级别高于<span
class="math inline">\(i_h^{L 3}\)</span>,
将这些迭代的总次数累积起来与参与A的循环<span
class="math inline">\(idx\)</span>进行乘积.</p>
<p>对于任意内存层级的pack cost可以通过如下公式计算:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; R D X^{L_c}=\left\{i_g^{L_c} \mid i_g \notin I S_A
\wedge\left(\exists i_h \in I
S_A\right)\left[i_g^{L_c}&gt;i_h^{L_c}\right]\right\} \\
&amp; \text { PackCost } \operatorname{mem}_{m \rightarrow L_c}^{A, b u
f \in L_c}=\prod_{i d x \in I S_A} i d x * \prod_{r d x \in R D X^{L_c}}
\text { NIter }(r d x) \\
&amp; =\prod_{i d x \in I S_A} i d x * \prod_{i_p \in R D
X^{L_c}}\left(N_p / \text { Tile }\left(i_p^{L_c}\right)\right) \\
&amp;
\end{aligned}
\]</span></p>
<h1 id="疑问">6. 疑问</h1>
<ol type="1">
<li>我在思考他这里的DM和DF和显式的指定sub
tensor放置在哪个循环下是否有共同性.</li>
<li>他这里的<code>group_outer(l)</code>的函数是一个离散函数,
但是求解器只能支持连续的变量,
现在问题转化为如何通过连续的变量构造出分段函数?
这个方法实际上和对于多层memory的循环加在哪个位置息息相关.</li>
<li>在约束编程中通过什么方式控制几个变量属于一个区间?</li>
<li>怎么样进行多阶段求解?</li>
<li>他这里感觉没有考虑在l1上cache packed buffer,
实际上可以开很大一块buffer, 然后在l1的循环中每次load并pack一小块,
然后在l1中缓存起来, 这样切换m/n的时候可以尽量复用.</li>
</ol>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" rel="tag">后端优化</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" rel="tag">性能建模</a></li></ul></div><div class="post-nav"><a class="pre" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a><a class="next" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a> <a href="/tags/%E7%AE%97%E5%AD%90/" style="font-size: 15px;">算子</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/10/15/jax-reshard/">探究jax reshard优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/09/26/flashattn/">Flash Attention记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>