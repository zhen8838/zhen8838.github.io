<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>探索AMX: 解锁Apple Silicon隐藏性能 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">探索AMX: 解锁Apple Silicon隐藏性能</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">探索AMX: 解锁Apple Silicon隐藏性能</h1><div class="post-meta">2024-04-23<span> | </span><span class="category"><a href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4.2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 18</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>自从2020年Apple发布的芯片M1/M2/M3,
至少提供了四种不同的方式可以执行高负载的计算任务:</p>
<ol type="1">
<li><p>标准的ARMv8 SIMD/NEON向量指令集.</p></li>
<li><p>苹果尚未公开文档的AMX(Apple Matrix Co-processor)指令集,
由CPU发射, 在特殊的加速器上运行.</p></li>
<li><p>神经网络处理器ANE(Apple Neural Engine)</p></li>
<li><p>Metal GPU</p></li>
</ol>
<p>在M1 Max上单核计算单精度浮点矩阵乘法时, 使用SIMD指令集<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464740681">可达到102
GFLOPS左右的性能</a>, 而使用AMX指令集最多<a
target="_blank" rel="noopener" href="https://github.com/corsix/amx/blob/main/fma.md#performance-m1-max">可达到<strong>1475</strong>
GFLOPS!</a> 本文就来带领大家一同探索AMX指令集,
学习如何解锁这剩下的14倍算力.</p>
<span id="more"></span>
<h1 id="概述">1. 概述</h1>
<p>先通过一张图建立起对于AMX初步了解. 考虑一个32x32的计算单元网格,
每个单元可以执行16位乘积, 或者2x2单元子网格可以执行32位乘积,
或者4x4子网格可以执行64位乘积. 为了提供此网格的输入,
有一个包含32个16位元素(或16个32位元素, 或8个64位元素)的X寄存器池,
以及一个同样包含32个16位元素(或16个32位元素, 或8个64位元素)的Y寄存器池.
使用单个指令可以执行完整的外积:
将X寄存器的每个元素与Y寄存器的每个元素相乘, 并在相应位置与Z元素一起累加.
或者可以将X寄存器的每个元素与Y寄存器的每个元素进行内积,
得到一个点的结果.</p>
<figure>
<img src="/2024/04/23/mac-amx/fig2.png" alt="专利 US20180074824A1" />
<figcaption aria-hidden="true">专利 US20180074824A1</figcaption>
</figure>
<p>AMX为以上的操作提供了LDX/LDY, FMA, LDZ/SDZ等指令. 其中FMA分为Matrix
Mode和Vector Mode分别对应外积与内积两种不同的计算方式. 当然我们首选外积,
否则无法最大化利用硬件. 下面两张图分别展示了内积与外积的计算方式:</p>
<figure>
<img src="/2024/04/23/mac-amx/inner%20product.png" alt="内积" />
<figcaption aria-hidden="true">内积</figcaption>
</figure>
<figure>
<img src="/2024/04/23/mac-amx/outer%20product.png" alt="外积" />
<figcaption aria-hidden="true">外积</figcaption>
</figure>
<h1 id="最小计算流程">2. 最小计算流程</h1>
<p>下面同样通过一张图解释清楚AMX中的寄存器规格与最小计算流程.
首先是X/Y寄存器池, 他们各有8个宽度为64 bytes的寄存器,
可以存储fp16/32/64或者i8/16/32,u8/16/32类型的数据. 接着是Z寄存器池,
他有64个宽度为64 bytes的寄存器, 用于存储X/Y寄存器外积或内积的结果.
根据第一节中描述, 2x2单元子网格来执行32位乘积,
因此执行外积时需要用到16个寄存器, 因此64个寄存器被分16组,
每组的间隔为4(64/16), 即外积结果并非是按寄存器连续存储的.
(注意AMX只支持从主存中加载与存储,
无法通过通用寄存器/向量寄存器加载与存储)</p>
<figure>
<img src="/2024/04/23/mac-amx/workflow.png" alt="最小计算流程" />
<figcaption aria-hidden="true">最小计算流程</figcaption>
</figure>
<p>通过外积计算可以获得一个partial sum存储在Z寄存器池中,
接下来我们可以切换K维度从而累积partial sum得到完整的结果:</p>
<figure>
<img src="/2024/04/23/mac-amx/workflow%20switch%20k.png"
alt="切换K维度" />
<figcaption aria-hidden="true">切换K维度</figcaption>
</figure>
<p>此时重新加载不同K维度的A/B矩阵,
然后将计算结果写入到与之前同一个Z的寄存器组中实现累加.</p>
<p>当然也可以迭代M/N, 比如这个例子中保持M不变, 迭代到下一个N,
此时我们需要在Z寄存器池中另外选一组存储当前M/N的累加值: <img
src="/2024/04/23/mac-amx/workflow%20switch%20n.png"
alt="切换K维度" /></p>
<h1 id="理论性能测试">3. 理论性能测试</h1>
<p>为了用好AMX, 首先需要更加了解AMX的相关参数,
接下来一起验证理论各项性能指标.</p>
<h2 id="理论计算性能">3.1 理论计算性能</h2>
<p>根据上一节我们清楚了Z寄存器组在float32模式下一共被分为了16组,
一次计算在每一组中得到一列结果, 因此ALU实际上被分为了4组.
这里就来验证使用不同个数的ALU组的峰值性能:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">perf_func = [&amp;z_nums]() &#123;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">uint64_t</span> a = <span class="built_in">matfp</span>().<span class="built_in">dtype_mode</span>(<span class="type">matfp_dtype_t</span>::f32f32).<span class="built_in">z_row</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">uint64_t</span> b = <span class="built_in">matfp</span>().<span class="built_in">dtype_mode</span>(<span class="type">matfp_dtype_t</span>::f32f32).<span class="built_in">z_row</span>(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">uint64_t</span> c = <span class="built_in">matfp</span>().<span class="built_in">dtype_mode</span>(<span class="type">matfp_dtype_t</span>::f32f32).<span class="built_in">z_row</span>(<span class="number">2</span>);</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">uint64_t</span> d = <span class="built_in">matfp</span>().<span class="built_in">dtype_mode</span>(<span class="type">matfp_dtype_t</span>::f32f32).<span class="built_in">z_row</span>(<span class="number">3</span>);</span><br><span class="line">  <span class="built_in">AMX_MATFP</span>(a);</span><br><span class="line">  <span class="keyword">if</span> (z_nums &gt; <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">AMX_MATFP</span>(b);</span><br><span class="line">  <span class="keyword">if</span> (z_nums &gt; <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">AMX_MATFP</span>(c);</span><br><span class="line">  <span class="keyword">if</span> (z_nums &gt; <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">AMX_MATFP</span>(d);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>得到结果如下:</p>
<table>
<thead>
<tr class="header">
<th>ALU Nums</th>
<th>Gflop/s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>405.530</td>
</tr>
<tr class="even">
<td>2</td>
<td>826.912</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1244.570</td>
</tr>
<tr class="even">
<td>4</td>
<td>1666.952</td>
</tr>
</tbody>
</table>
<p>这表明虽然每组ALU是单独配置和发射, 但是他们之间是可以并行执行的.</p>
<h2 id="理论数据加载性能">3.2 理论数据加载性能</h2>
<p>这里是我设计的性能测试用例, 其中<code>reg nums</code>表示X/Y寄存器池.
<code>near</code>表示是否读取内存中连续的地址,在M2 Pro中l1 Dcache
大小为65536, 因此将K设计为数倍大于l1 cache size,
当<code>near == 0</code>时需要跨过两倍大小的cache size去读取.
<code>width</code>表示一次读取几个寄存器个数, 最大为4.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">constexpr</span> <span class="type">size_t</span> K = (<span class="number">65536</span> / <span class="number">4</span> / (<span class="number">16</span> * <span class="number">4</span>)) * <span class="number">4</span>;</span><br><span class="line"><span class="type">float</span> M[K * <span class="number">2</span>][<span class="number">16</span> * <span class="number">4</span>]&#123;&#125;;</span><br><span class="line"><span class="type">float</span> N[K * <span class="number">2</span>][<span class="number">16</span> * <span class="number">4</span>]&#123;&#125;;</span><br><span class="line">perf_func = [&amp;M, &amp;N, &amp;near, &amp;reg_num, &amp;x_width, &amp;y_width]() &#123;</span><br><span class="line">  <span class="keyword">auto</span> ldx = <span class="built_in">ldxy</span>().<span class="built_in">register_index</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">auto</span> ldy = <span class="built_in">ldxy</span>().<span class="built_in">register_index</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">if</span> (x_width &gt;= <span class="number">2</span>)</span><br><span class="line">    ldx = ldx.<span class="built_in">multiple</span>();</span><br><span class="line">  <span class="keyword">if</span> (x_width &gt;= <span class="number">4</span>)</span><br><span class="line">    ldx = ldx.<span class="built_in">multiple_four</span>();</span><br><span class="line">  <span class="keyword">if</span> (reg_num &gt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (y_width &gt;= <span class="number">2</span>)</span><br><span class="line">      ldy = ldy.<span class="built_in">multiple</span>();</span><br><span class="line">    <span class="keyword">if</span> (y_width &gt;= <span class="number">4</span>)</span><br><span class="line">      ldy = ldy.<span class="built_in">multiple_four</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (near) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; K; i++) &#123;</span><br><span class="line">      <span class="built_in">AMX_LDX</span>(ldx.<span class="built_in">bind</span>(M[i]));</span><br><span class="line">      <span class="keyword">if</span> (reg_num &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">AMX_LDY</span>(ldy.<span class="built_in">bind</span>(N[i]));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; K / <span class="number">2</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">size_t</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">        <span class="built_in">AMX_LDX</span>(ldx.<span class="built_in">bind</span>(M[j * K + i]));</span><br><span class="line">        <span class="keyword">if</span> (reg_num &gt; <span class="number">1</span>) &#123;</span><br><span class="line">          <span class="built_in">AMX_LDY</span>(ldy.<span class="built_in">bind</span>(N[j * K + i]));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>接下来我们计算数据加载时间, 得到下表:</p>
<table>
<thead>
<tr class="header">
<th>Reg Nums</th>
<th>Near</th>
<th>X Width</th>
<th>Y Width</th>
<th>GB/s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>87.1489</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>213.164</td>
</tr>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td>456.332</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>120.796</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>260.115</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>4</td>
<td>0</td>
<td>483.285</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>134.33</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>162.084</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>297.15</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>201.658</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>214.772</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>350.554</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>4</td>
<td>1</td>
<td>384.614</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>4</td>
<td>2</td>
<td>349.528</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>4</td>
<td>4</td>
<td>476.722</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>130.604</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>163.91</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>254.922</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>195.612</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>2</td>
<td>2</td>
<td>213.61</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0</td>
<td>2</td>
<td>4</td>
<td>298.603</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>4</td>
<td>1</td>
<td>310.308</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0</td>
<td>4</td>
<td>2</td>
<td>302.767</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>4</td>
<td>4</td>
<td>325.193</td>
</tr>
</tbody>
</table>
<p>可以发现加大读取宽度是可以翻倍带宽的, 因此这是无成本的.
读取连续摆放的数据相比于非连续读取略有提升,
这表明需要对数据摆放进行优化.
同时加载两个寄存器中也同样不会导致带宽降低, 表明可以同时加载A/B矩阵,
但需要注意尽量cache起来.</p>
<h2 id="理论数据存储性能">3.3 理论数据存储性能</h2>
<p>同样在store时设计了<code>reg_num</code>,<code>near</code>,<code>width</code>等选项,
值的注意的是, 因为Z寄存器池被分为了16组,
所以每一次store时都将16组全部输出: <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">constexpr</span> <span class="type">size_t</span> K = (<span class="number">65536</span> / <span class="number">4</span> / (<span class="number">16</span> * <span class="number">4</span>)) * <span class="number">2</span>;</span><br><span class="line"><span class="type">float</span> CNear[<span class="number">16</span>][<span class="number">16</span> * <span class="number">4</span>]&#123;&#125;;</span><br><span class="line"><span class="type">float</span> C[<span class="number">16</span>][K]&#123;&#125;;</span><br><span class="line">perf_func = [&amp;C, &amp;CNear, &amp;near, &amp;z_num, &amp;width]() &#123;</span><br><span class="line">  <span class="keyword">auto</span> ldst = width == <span class="number">2</span> ? <span class="built_in">ldstz</span>().<span class="built_in">multiple</span>() : <span class="built_in">ldstz</span>();</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> z = <span class="number">0</span>; z &lt; z_num; z++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> m = <span class="number">0</span>; m &lt; <span class="number">16</span>; m++) &#123;</span><br><span class="line">      <span class="built_in">AMX_STZ</span>(ldst.<span class="built_in">row_index</span>(m * <span class="number">4</span> + z * width)</span><br><span class="line">                  .<span class="built_in">bind</span>(near ? CNear[m] + <span class="number">16</span> * z * width</span><br><span class="line">                              : C[m] + <span class="number">16</span> * z * width));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>测试结果如下:</p>
<table>
<thead>
<tr class="header">
<th>Reg Nums</th>
<th>Near</th>
<th>Width</th>
<th>GB/s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
<td>10.3769</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>2</td>
<td>8.93052</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
<td>12.9423</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>2</td>
<td>12.3377</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>1</td>
<td>5.69731</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>2</td>
<td>12.3658</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0</td>
<td>1</td>
<td>7.55092</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>2</td>
<td>13.0133</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1</td>
<td>1</td>
<td>6.58085</td>
</tr>
<tr class="even">
<td>3</td>
<td>0</td>
<td>1</td>
<td>11.4118</td>
</tr>
<tr class="odd">
<td>4</td>
<td>1</td>
<td>1</td>
<td>8.8847</td>
</tr>
<tr class="even">
<td>4</td>
<td>0</td>
<td>1</td>
<td>9.85956</td>
</tr>
</tbody>
</table>
<p>可以发现使用多个寄存器并无法提升带宽, 说明基本上是串行输出,
但开启两倍宽度还是可以有效提升带宽.
然而整体速度相比于计算和加载慢了数倍,
这表明我们不能频繁地加载与存储Z.</p>
<h1 id="设计micro-kernel">3. 设计Micro Kernel</h1>
<p>基于上一节中的内容, 我们可以来尝试编写一个现实世界的矩阵乘法,
为了矩阵乘法的高效, 采用自底向上的准则来设计它. 也就是说,
首先需要设计一个充分利用硬件算力的micro kernel, 接着再通过合理调用micro
kernel达到极限算力.</p>
<p>首先回顾一下最基础的计算流程, 加载一列M, 加载一行N, 计算得到一块MxN.
此时明显可以发现对于X/Y/Z寄存器池都是没用充分利用的, 特别是Z寄存器池,
这意味着只使用了<span class="math inline">\(\frac{1}{4}\)</span>的算力,
所以首要目标是用满它.</p>
<p><img src="/2024/04/23/mac-amx/micro%20kernel%200.png" /></p>
<p>根据上一节, 我们可以已知最大理论计算性能以及带宽,
那么根据公式我们可以计算用满ALU时一次计算需要花费多少纳秒: <span
class="math display">\[
\begin{aligned}
Compute Time  &amp;= \frac{FLOPs}{GFLOPS}\ ~NanoSeconds\\
Load Time  &amp;= \frac{Bytes}{GBS}\ ~NanoSeconds
\end{aligned}
\]</span></p>
<p>接下来我们基于这个数据来设计不同的计算策略:</p>
<h2 id="计算策略1-加载多组n">3.1 计算策略1: 加载多组N</h2>
<p>在我的M2 Pro中, AMX最大一次可以加载<code>4*64</code> bytes,
因此可以加载1组M以及4组的N, 利用满计算单元得到<code>M*4N</code>,
然后在下一次循环中切换不同的K. 总的来说是加载一次, 计算四次,
由于X寄存器池大小限制只能多缓存一次. <img
src="/2024/04/23/mac-amx/micro%20kernel%201.png" /></p>
<p>查表可知,加载带宽为254.922 GB/s,得到计算时间与数据加载时间分别为:
<span class="math display">\[
\begin{aligned}
FLOPs &amp;= 2 * M * N * 4 = 2048 \\
Compute Time &amp;= FLOPs / 1666  = 1.229~NanoSeconds \\
Bytes &amp;= (1 * M + 4 * N) * 4 \\
Load Time &amp;= Bytes / 297.15 = 1.076~NanoSeconds
\end{aligned}
\]</span></p>
<p>两者时间基本差不多, 但他们之间的差只有<code>0.15ns</code>,
这点时间不足以在一次缓存中流水起来.</p>
<h2 id="计算策略2-加载多组m">3.2 计算策略2: 加载多组M</h2>
<p>同上一种策略类似, 同样也是加载一次计算四次,
计算时间与加载时间和上一小节类似,
但是这里存在一个额外问题则是对于A矩阵的列方向是不连续的,
需要先通过CPU进行转置4倍的M后才可以加载, 因此不考虑. <img
src="/2024/04/23/mac-amx/micro%20kernel%202.png" /></p>
<h2 id="计算策略3-加载两组mn">3.3 计算策略3: 加载两组M/N</h2>
<p>为了均匀X/Y寄存器的存储, 可以考虑加载同样大小的M/N, 也就是加载2组,
此时可以计算4次得到<code>2M*2N</code>大小的输出,
考虑到最大可以加载<code>4*64</code> bytes, 那么可以加载不同K两组M/N,
这样一次加载实际可以计算8次, 最大限度的利用算力.
此时对于X/Y寄存器池使用量是一样的, 也就是他们还可以多缓存一次: <img
src="/2024/04/23/mac-amx/micro%20kernel%203.png" /></p>
<p>来动手计算一下,加载带宽为254.922
GB/s,得到计算时间与数据加载时间分别为: <span class="math display">\[
\begin{aligned}
FLOPs &amp;= 2 * M * N * 4 * 2 = 4096 \\
Compute Time &amp;= FLOPs / 1666  = 2.458~NanoSeconds \\
Bytes &amp;= (4 * M + 4 * N) * 4 \\
Load Time &amp;= Bytes / 476.722 = 1.074~NanoSeconds
\end{aligned}
\]</span></p>
<p>此时计算时间减去数据加载时间为<code>1.384 ns</code>,
大于下一次的计算时间<code>1.074 ns</code>,
那么也就表明我们可以在下一次缓存中完美的流水起来,
从而发挥出极限算力.</p>
<h2 id="验证计算策略3">3.4 验证计算策略3</h2>
<p>为了达到峰值的数据加载性能, 我们需要对矩阵进行布局优化,
也就是将A/B矩阵分别以32为宽度进行pack, 这样加载时是满足内存连续读取,
同时一次可以计算得到<code>2*M*N</code>的结果, 其具体迭代流程图如下: <img
src="/2024/04/23/mac-amx/micro%20kernel%203%20detail.png" /></p>
<p>为了简单起见我假设M/K/N都满足最小计算单元的倍数,
同时要求输入A/B矩阵都是经过布局优化的, 下面则是具体的代码:
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">bool</span> LoadC, <span class="type">bool</span> StoreC, <span class="type">size_t</span> KTile, <span class="type">size_t</span> M, <span class="type">size_t</span> N&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">gepdot</span><span class="params">(<span class="type">size_t</span> curM, <span class="type">size_t</span> curK, <span class="type">size_t</span> curN,</span></span></span><br><span class="line"><span class="params"><span class="function">            <span class="type">const</span> <span class="type">float</span> packedA[KTile][<span class="number">32</span>], <span class="type">const</span> <span class="type">float</span> packedB[KTile][<span class="number">32</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">            <span class="type">float</span> C[M][N])</span> </span>&#123;</span><br><span class="line">  <span class="built_in">static_assert</span>(KTile % <span class="number">4</span> == <span class="number">0</span>, <span class="string">&quot;not support k%4!=0&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(LoadC)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// load acc value.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> om = <span class="number">0</span>; om &lt; <span class="number">2</span>; om++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">size_t</span> m = <span class="number">0</span>; m &lt; <span class="number">16</span>; m++) &#123;</span><br><span class="line">        <span class="built_in">AMX_STZ</span>(</span><br><span class="line">            <span class="built_in">ldstz</span>().<span class="built_in">row_index</span>(m * <span class="number">4</span> + om * <span class="number">2</span>).<span class="built_in">multiple</span>().<span class="built_in">bind</span>(C[om * <span class="number">16</span> + m]));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> k = curK; k &lt; curK + KTile; k += <span class="number">4</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> ok = k; ok &lt; k + <span class="number">4</span>; ok += <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="comment">// load [m0,k0], [m1,k0], [m0,k1], [m1,k1]</span></span><br><span class="line">      <span class="built_in">AMX_LDY</span>(<span class="built_in">ldxy</span>().<span class="built_in">register_index</span>(<span class="number">0</span>).<span class="built_in">multiple</span>().<span class="built_in">multiple_four</span>().<span class="built_in">bind</span>(</span><br><span class="line">          (<span class="type">void</span> *)packedA[ok]));</span><br><span class="line">      <span class="comment">// load [n0,k0], [n1,k0], [n0,k1], [n1,k1]</span></span><br><span class="line">      <span class="built_in">AMX_LDX</span>(<span class="built_in">ldxy</span>().<span class="built_in">register_index</span>(<span class="number">0</span>).<span class="built_in">multiple</span>().<span class="built_in">multiple_four</span>().<span class="built_in">bind</span>(</span><br><span class="line">          (<span class="type">void</span> *)packedB[ok]));</span><br><span class="line">      <span class="comment">// compute 8 times.</span></span><br><span class="line">      <span class="comment">// [[m0,n0],[m0,n1],</span></span><br><span class="line">      <span class="comment">//  [m1,n0],[m1,n1]]</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">size_t</span> ik = ok; ik &lt; ok + <span class="number">2</span>; ik++) &#123;</span><br><span class="line">        <span class="keyword">auto</span> fma = ik == <span class="number">0</span> ? <span class="built_in">fma32</span>().<span class="built_in">skip_z</span>() : <span class="built_in">fma32</span>();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> m = <span class="number">0</span>; m &lt; <span class="number">2</span>; m++) &#123;</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">size_t</span> n = <span class="number">0</span>; n &lt; <span class="number">2</span>; n++) &#123;</span><br><span class="line">            <span class="built_in">AMX_FMA32</span>(fma.<span class="built_in">z_row</span>(m * <span class="number">2</span> + n)</span><br><span class="line">                          .<span class="built_in">y_offset</span>((ik - ok) * <span class="number">2</span> * <span class="number">16</span> * <span class="number">4</span> + m * <span class="number">16</span> * <span class="number">4</span>)</span><br><span class="line">                          .<span class="built_in">x_offset</span>((ik - ok) * <span class="number">2</span> * <span class="number">16</span> * <span class="number">4</span> + n * <span class="number">16</span> * <span class="number">4</span>));</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// last time need store C.</span></span><br><span class="line">  <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(StoreC)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> om = <span class="number">0</span>; om &lt; <span class="number">2</span>; om++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">size_t</span> m = <span class="number">0</span>; m &lt; <span class="number">16</span>; m++) &#123;</span><br><span class="line">        <span class="built_in">AMX_STZ</span>(</span><br><span class="line">            <span class="built_in">ldstz</span>().<span class="built_in">row_index</span>(m * <span class="number">4</span> + om * <span class="number">2</span>).<span class="built_in">multiple</span>().<span class="built_in">bind</span>(C[om * <span class="number">16</span> + m]));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在<code>M = 16 * 2, K = 8192, N = 16 * 2</code>的情况下测试,
得到<code>1632.13 Gflop/s</code>的性能,
达到了峰值性能的<code>0.979%</code>.: <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[----------] 1 <span class="built_in">test</span> from test_amx</span><br><span class="line">[ RUN      ] test_amx.test_gepdot</span><br><span class="line">             Gflop/s: 1632.13</span><br><span class="line">[       OK ] test_amx.test_gepdot (1032 ms)</span><br><span class="line">[----------] 1 <span class="built_in">test</span> from test_amx (1032 ms total)</span><br></pre></td></tr></table></figure></p>
<h1 id="设计分块方案">4. 设计分块方案</h1>
<p>如果在没有AI Compiler提前优化的情况下,
我们是没办法得到内存优化好的输入数据的,
那么就需要考虑计算的同时进行布局优化.
并且上一节的例子中我没有设置很大的M和N, 而现实中需要切换M与N,
此时又带来另一个分块大小的问题.</p>
<p>如果是使用SIMD来进行计算, 那么论文<a
target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/1356052.1356053">Anatomy of
High-Performance Matrix
Multiplication</a>中已经给出了一个很好的分块解决方案:</p>
<p><img src="/2024/04/23/mac-amx/goto%20blas.png" /></p>
<p>即最内层使用<code>gebp</code>核.
但是<code>gebp</code>它需要反复的加载和存储C矩阵,
对于CPU寄存器来说他的带宽足够大,
只需要Nr远大于Kc就可以均摊掉访问C矩阵的内存开销,
对于AMX来说Z寄存器的带宽只有X/Y寄存器带宽的几十分之一, 显然是无法均摊的,
因此我们肯定不能采用论文中的计算方法: <img
src="/2024/04/23/mac-amx/gebp.png" /></p>
<p>因此我这里抛砖引玉, 实现了一个简易的两级分块策略,
M中循环N用于复用A矩阵, 把K迭代放在循环最内层以适配<code>gepdot</code>核,
同时在每个KTile中pack一小块A并缓存起来, 这样循环N时可以避免重复执行:</p>
<p><img src="/2024/04/23/mac-amx/gepdot%20general.png" /></p>
<p>最终代码如下: <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> matmul = [&amp;]() -&gt; <span class="type">void</span> &#123;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">size_t</span> KTile = <span class="number">32</span>, MTile = <span class="number">32</span>, NTile = <span class="number">32</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> mo = <span class="number">0</span>; mo &lt; M; mo += MTile) &#123;</span><br><span class="line">    <span class="type">float</span> PackedA[K][MTile];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> no = <span class="number">0</span>; no &lt; N; no += NTile) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">size_t</span> ko = <span class="number">0</span>; ko &lt; K; ko += KTile) &#123;</span><br><span class="line">        <span class="comment">// each time pack local innertile.</span></span><br><span class="line">        <span class="keyword">if</span> (no == <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">size_t</span> mi = <span class="number">0</span>; mi &lt; MTile; mi++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">size_t</span> i = ko; i &lt; ko + KTile; i++) &#123;</span><br><span class="line">              PackedA[i][mi] = A[mo + mi][i];</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">gepdot_general</span>&lt;KTile, M, K, N&gt;(<span class="literal">false</span>, (ko + KTile == K), mo, ko, no,</span><br><span class="line">                                        PackedA + ko, B, C);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>测试性能达到了Apple所提供库的70%性能:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[ RUN      ] test_amx.test_gepdot_general</span><br><span class="line">gepdot general   Gflop/s: 998.291</span><br><span class="line">cblas_sgemm      Gflop/s: 1398.95</span><br><span class="line">[       OK ] test_amx.test_gepdot_general (582 ms)</span><br></pre></td></tr></table></figure>
<p>其实计算策略3中是计算瓶颈的,
按理论值计算流水之后实际大约还剩<code>0.3ns</code>时间可以分配给数据加载,
如果可以很好的建模CPU内存操作耗时, 理论上是可以做到把pack
A矩阵的过程掩盖起来, 从而达到理论峰值. 不过从cblas_sgemm的实测性能来看,
应该也没有完全掩藏起来,
超越Apple闭源库这个具有挑战性的任务就交给读者们了👻.</p>
<h1 id="疑问">5. 疑问</h1>
<p>我设计了一个测试AMX加载数据是否会更新cache的用例: <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">constexpr</span> <span class="type">size_t</span> K = (<span class="number">65536</span> / <span class="number">4</span> / (<span class="number">16</span> * <span class="number">2</span>)) * <span class="number">8</span>; <span class="comment">/* 65536 是cache size */</span></span><br><span class="line"><span class="type">float</span> N[<span class="number">1</span>][<span class="number">16</span> * <span class="number">2</span>]&#123;&#125;;</span><br><span class="line"><span class="type">float</span> M[K][<span class="number">16</span> * <span class="number">2</span>]&#123;&#125;;</span><br><span class="line"><span class="type">float</span> *C = (<span class="type">float</span> *)<span class="built_in">malloc</span>(K * <span class="number">65536</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> func1 = [&amp;N]() &#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; K; i++) &#123;</span><br><span class="line">    <span class="built_in">AMX_LDX</span>(<span class="built_in">ldxy</span>().<span class="built_in">multiple</span>().<span class="built_in">register_index</span>(<span class="number">0</span>).<span class="built_in">bind</span>((<span class="type">void</span> *)N[<span class="number">0</span>]));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> func2 = [&amp;M]() &#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; K; i++) &#123;</span><br><span class="line">    <span class="built_in">AMX_LDX</span>(<span class="built_in">ldxy</span>().<span class="built_in">multiple</span>().<span class="built_in">register_index</span>(<span class="number">0</span>).<span class="built_in">bind</span>((<span class="type">void</span> *)M[i]));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> func3 = [&amp;C]() &#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; K; i++) &#123;</span><br><span class="line">    <span class="built_in">AMX_LDX</span>(</span><br><span class="line">        <span class="built_in">ldxy</span>().<span class="built_in">multiple</span>().<span class="built_in">register_index</span>(<span class="number">0</span>).<span class="built_in">bind</span>((<span class="type">void</span> *)(C + i * K)));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中func1永远只加载同一个位置的数据, func2按顺序加载连续的数据,
func3每次加载跨度大于l1 cache size的数据, 最终结果如下:
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">func1: 29.9743 GB/s</span><br><span class="line">func2: 219.201 GB/s</span><br><span class="line">func3: 9.74711 GB/s</span><br></pre></td></tr></table></figure></p>
<p>func2与func3的结果表示l1 cache对于AMX来说也是重要的,
但是我无法理解func1与func2的差异,
理论上被访问过的数据应该会缓存在cache中,
而从结果上来看被缓存的似乎是当前所加载地址后面部分的数据,
这个问题可能需要更了解体系结构的读者才能解答.</p>
<h1 id="总结">6. 总结</h1>
<p>至此本文所有的内容介绍完毕, 我所使用的全部测试代码在<a
target="_blank" rel="noopener" href="https://github.com/zhen8838/leetcode/blob/main/meta_program/test_amx.cpp">这里获取</a>(Verified
on M2 Pro). 而本文所依赖的信息来源于Peter Cawley等人的<a
target="_blank" rel="noopener" href="https://github.com/corsix/amx">逆向成果</a>,
其中包含了更多指令介绍以及使用方法.</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Apple/" rel="tag">Apple</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" rel="tag">指令集</a></li></ul></div><div class="post-nav"><a class="pre" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a><a class="next" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>