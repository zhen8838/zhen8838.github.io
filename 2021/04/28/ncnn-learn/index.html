<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>ncnn学习 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">ncnn学习</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">ncnn学习</h1><div class="post-meta">2021-04-28<span> | </span><span class="category"><a href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 8</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>对ncnn学习的一些汇总。</p>
<span id="more"></span>
<h1 id="mat内存分布">Mat内存分布</h1>
<p>Mat是ncnn所有的数据对象集合，因此我们必须对其有所了解，下面这个函数就是其中一个构造函数（这里需要吐槽一下，ncnn的设计就是单幅图像输入，所以mat最大只支持三维），其中需要注意到的就是内存分配时需要对齐：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Mat::create</span><span class="params">(<span class="type">int</span> _w, <span class="type">int</span> _h, <span class="type">int</span> _c, <span class="type">size_t</span> _elemsize, <span class="type">int</span> _elempack, Allocator* _allocator)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (dims == <span class="number">3</span> &amp;&amp; w == _w &amp;&amp; h == _h &amp;&amp; c == _c &amp;&amp; elemsize == _elemsize &amp;&amp; elempack == _elempack &amp;&amp; allocator == _allocator)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">release</span>();</span><br><span class="line"></span><br><span class="line">    elemsize = _elemsize;</span><br><span class="line">    elempack = _elempack;</span><br><span class="line">    allocator = _allocator;</span><br><span class="line"></span><br><span class="line">    dims = <span class="number">3</span>;</span><br><span class="line">    w = _w;</span><br><span class="line">    h = _h;</span><br><span class="line">    c = _c;</span><br><span class="line"></span><br><span class="line">    cstep = <span class="built_in">alignSize</span>((<span class="type">size_t</span>)w * h * elemsize, <span class="number">16</span>) / elemsize;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">total</span>() &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">size_t</span> totalsize = <span class="built_in">alignSize</span>(<span class="built_in">total</span>() * elemsize, <span class="number">4</span>);</span><br><span class="line">        <span class="keyword">if</span> (allocator)</span><br><span class="line">            data = allocator-&gt;<span class="built_in">fastMalloc</span>(totalsize + (<span class="type">int</span>)<span class="built_in">sizeof</span>(*refcount));</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            data = <span class="built_in">fastMalloc</span>(totalsize + (<span class="type">int</span>)<span class="built_in">sizeof</span>(*refcount));</span><br><span class="line">        refcount = (<span class="type">int</span>*)(((<span class="type">unsigned</span> <span class="type">char</span>*)data) + totalsize);</span><br><span class="line">        *refcount = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="块对齐-block-aligin">块对齐 (block aligin)</h2>
<p>这是nihui对于mat内存排布的说明。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mat shape w=3 h=2 c=4</span><br><span class="line"></span><br><span class="line">internal memory layout</span><br><span class="line">[(a00 a01 a02) (a10 a11 a12) pad pad]</span><br><span class="line">[(b00 b01 b02) (b10 b11 b12) pad pad]</span><br><span class="line">[(c00 c01 c02) (c10 c11 c12) pad pad]</span><br><span class="line">[(d00 d01 d02) (d10 d11 d12) pad pad]</span><br><span class="line"></span><br><span class="line">each channel is 16byte aligned, </span><br><span class="line">padding values may be filled in channel gaps</span><br><span class="line"></span><br><span class="line">mat.data -&gt; address of a00</span><br><span class="line">mat.row(1) -&gt; address of a10</span><br><span class="line">mat.channel(0).row(1) -&gt; address of a10</span><br><span class="line">mat.channel(1).row(1) -&gt; address of b10</span><br></pre></td></tr></table></figure>
<h3 id="by-channel-alignsize">by channel alignSize</h3>
<p>ncnn通常把以单个通道的图像(h*w)进行读取，然后进行一些卷积操作，所以要by
channel的对数据进行对齐，考虑到对于不同的元素有不同的<code>elemsize</code>,同时在分配时还需要对内存块大小进行对齐，为了快速的对hw的内存进行读写，他这里默认分配16bit的倍数。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// element size in bytes</span></span><br><span class="line"><span class="comment">// 4 = float32/int32</span></span><br><span class="line"><span class="comment">// 2 = float16</span></span><br><span class="line"><span class="comment">// 1 = int8/uint8</span></span><br><span class="line"><span class="comment">// 0 = empty</span></span><br><span class="line"><span class="type">size_t</span> elemsize;</span><br></pre></td></tr></table></figure>
<p>比如我们申请float矩阵<code>w=3,h=9,c=4</code>，那么每一个<code>channel</code>的内存块本来应该是<code>3*9=27 byte = 27*elemsize = 108 bit</code>，然而<code>108</code>不是<code>16</code>的倍数，所以用<code>alignSize</code>计算到最小16的倍数为112，然后再除elemsize得到cstep为28。
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cstep = <span class="built_in">alignSize</span>((<span class="type">size_t</span>)w * h * elemsize, <span class="number">16</span>) / elemsize;</span><br></pre></td></tr></table></figure></p>
<h3 id="whole-alignsize">whole alignSize</h3>
<p>然后根据上述思路，整体的大小是以4倍大小分配的。 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">size_t</span> totalsize = <span class="built_in">alignSize</span>(<span class="built_in">total</span>() * elemsize, <span class="number">4</span>);</span><br></pre></td></tr></table></figure></p>
<h2 id="内存申请-malloc">内存申请 (Malloc)</h2>
<p>同时申请到的内存位置也需要对齐在内存上，便于我们的cpu整块读取，下面是整体的函数，我这里执行的是<code>posix_memalign</code>，不过还是有必要讲讲具体思路。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> __AVX__</span></span><br><span class="line"><span class="comment">// the alignment of all the allocated buffers</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MALLOC_ALIGN 32</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="comment">// the alignment of all the allocated buffers</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MALLOC_ALIGN 16</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">void</span>* <span class="title">fastMalloc</span><span class="params">(<span class="type">size_t</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> _MSC_VER</span></span><br><span class="line">    <span class="keyword">return</span> _aligned_malloc(size, MALLOC_ALIGN);</span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> (defined(__unix__) || defined(__APPLE__)) &amp;&amp; _POSIX_C_SOURCE &gt;= 200112L || (__ANDROID__ &amp;&amp; __ANDROID_API__ &gt;= 17)</span></span><br><span class="line">    <span class="type">void</span>* ptr = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">posix_memalign</span>(&amp;ptr, MALLOC_ALIGN, size))</span><br><span class="line">        ptr = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> ptr;</span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> __ANDROID__ &amp;&amp; __ANDROID_API__ &lt; 17</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">memalign</span>(MALLOC_ALIGN, size);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span>* udata = (<span class="type">unsigned</span> <span class="type">char</span>*)<span class="built_in">malloc</span>(size + <span class="built_in">sizeof</span>(<span class="type">void</span>*) + MALLOC_ALIGN);</span><br><span class="line">    <span class="keyword">if</span> (!udata)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span>** adata = <span class="built_in">alignPtr</span>((<span class="type">unsigned</span> <span class="type">char</span>**)udata + <span class="number">1</span>, MALLOC_ALIGN);</span><br><span class="line">    adata[<span class="number">-1</span>] = udata;</span><br><span class="line">    <span class="keyword">return</span> adata;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="fastmalloc">fastMalloc</h3>
<h4 id="malloc">malloc</h4>
<p>假设我是内存对齐<code>MALLOC_ALIGN</code>是32位。 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">char</span>* udata = (<span class="type">unsigned</span> <span class="type">char</span>*)<span class="built_in">malloc</span>(size + <span class="built_in">sizeof</span>(<span class="type">void</span>*) + MALLOC_ALIGN);</span><br></pre></td></tr></table></figure>
这里加上<code>sizeof(void*)</code>是为了用来保存原始malloc出来的数据空间，然后加上<code>MALLOC_ALIGN</code>的大小，因为我们要进行多大的对齐，我们需要偏移的大小总是在<code>0~MALLOC_ALIGN</code>中，所以加上<code>MALLOC_ALIGN</code>即可。</p>
<p>如果我的<code>size</code>是<code>112</code>，那么实际申请的大小是<code>112+8+32=152</code>，假设我这里申请到的内存为<code>0x555555b1cb30</code>。</p>
<h4 id="align-ptr">align ptr</h4>
<p>接下来我们要对刚刚申请的<code>udata</code>进行一系列操作，首要的事情就是要把起步的内存块进行一个内存对齐，我们要对一个指针操作，那么需要先转成指针的指针，并且需要考虑到对内存进行偏移之后，我们需要保存原来的<code>block</code>的起始地址用于释放内存，否则就会出现问题，ncnn的思路就是原来申请的内存块的头部存放着原始地址，后面一块数据才是实际使用的。</p>
<p>所以我们会跳过一个小的内存开始对齐，其中对齐的方法也是和找到<code>MALLOC_ALIGN</code>：
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">char</span> **adata = <span class="built_in">alignPtr</span>((<span class="type">unsigned</span> <span class="type">char</span> **)udata + <span class="number">1</span>, MALLOC_ALIGN);</span><br></pre></td></tr></table></figure></p>
<p>然后我们返回对齐的指针adata，然后把adata前面一个地址空间保存raw的地址，最终的内存分布如下图所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">           align with MALLOC_ALIGN</span><br><span class="line">              0x40</span><br><span class="line">               |</span><br><span class="line">0x30    0x38   V       0x44     0x48    0x4C      0x50</span><br><span class="line">   |------|----|--------|--------|--------|--------|</span><br><span class="line">  head     head  data1     data2    data3    data4</span><br><span class="line">           addr</span><br></pre></td></tr></table></figure>
<h3 id="ncnn-x86-cpu加速">NCNN x86 cpu加速</h3>
<p>想要使用ncnn的一些加速方法，需要从内存管理就开始适配，比如我想给定输入大小进行malloc，ncnn底层就会自动帮我padding到4的倍数，这就需要十分注意。所以这里显示的h，w是正确的，但是cstep并不是6。
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">TEST</span>(cpp_lang, ncnn_mat_create_shape)</span><br><span class="line">&#123;</span><br><span class="line">    nncase::<span class="type">runtime_shape_t</span> in_shape &#123; <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span> &#125;;</span><br><span class="line">    <span class="function">Mat <span class="title">m</span><span class="params">((<span class="type">int</span>)in_shape[<span class="number">0</span>], (<span class="type">int</span>)in_shape[<span class="number">1</span>], (<span class="type">int</span>)in_shape[<span class="number">2</span>])</span></span>;</span><br><span class="line">    cout &lt;&lt; m.cstep &lt;&lt; endl; <span class="comment">// 8</span></span><br><span class="line">    cout &lt;&lt; m.w  &lt;&lt; endl; <span class="comment">// 2</span></span><br><span class="line">    cout &lt;&lt; m.h  &lt;&lt; endl; <span class="comment">// 3</span></span><br><span class="line">    cout &lt;&lt; in_shape[<span class="number">0</span>] * in_shape[<span class="number">1</span>] &lt;&lt; endl; <span class="comment">// 6</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="elempack">elempack</h4>
<p>首先获得输入的<code>elemsize</code>和<code>elempack</code>，然后默认输出的<code>out_elempack=1</code>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> w = bottom_blob.w;</span><br><span class="line"><span class="type">int</span> h = bottom_blob.h;</span><br><span class="line"><span class="type">int</span> channels = bottom_blob.c;</span><br><span class="line"><span class="type">size_t</span> elemsize = bottom_blob.elemsize;</span><br><span class="line"><span class="type">int</span> elempack = bottom_blob.elempack;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> kernel_extent_w = dilation_w * (kernel_w - <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> kernel_extent_h = dilation_h * (kernel_h - <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">Mat bottom_blob_bordered;</span><br><span class="line"><span class="built_in">make_padding</span>(bottom_blob, bottom_blob_bordered, opt);</span><br><span class="line"><span class="keyword">if</span> (bottom_blob_bordered.<span class="built_in">empty</span>())</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-100</span>;</span><br><span class="line"></span><br><span class="line">w = bottom_blob_bordered.w;</span><br><span class="line">h = bottom_blob_bordered.h;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> outw = (w - kernel_extent_w) / stride_w + <span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> outh = (h - kernel_extent_h) / stride_h + <span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> out_elempack = <span class="number">1</span>;</span><br></pre></td></tr></table></figure></p>
<p>根据平台特性设定<code>out_elempack</code>:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> __SSE2__</span></span><br><span class="line">    <span class="keyword">if</span> (opt.use_packing_layout)</span><br><span class="line">    &#123;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> __AVX__</span></span><br><span class="line">        out_elempack = num_output % <span class="number">8</span> == <span class="number">0</span> ? <span class="number">8</span> : num_output % <span class="number">4</span> == <span class="number">0</span> ? <span class="number">4</span> : <span class="number">1</span>; <span class="comment">// 如果是AVX，那么256bit一组，所以elempack设置为8</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">        out_elempack = num_output % <span class="number">4</span> == <span class="number">0</span> ? <span class="number">4</span> : <span class="number">1</span>; </span><br><span class="line">        <span class="comment">// 如果是SSE，那么128bit一组，所以elempack设置为4</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span> <span class="comment">// __SSE2__</span></span></span><br></pre></td></tr></table></figure>
<p>计算输出<code>elemsize</code>,假设原始输入c是3，他的elemsize是4，输入数据是3*4=12，
out channel是64，假设elemsize是4，输出数据是64*4=256 ,
现在输出要8个一组，所以256/8=32
所以输出的elemsize是32。最后输出<code>top blob</code>申请的内存就是<code>outw,outh,channel=8 (64/8),elemsize=32 (4*8)</code></p>
<p>所以ncnn这都是channel通道上的packing，所以对于channel数大的情况下，卷积速度就快。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">size_t</span> out_elemsize = elemsize / elempack * out_elempack; </span><br><span class="line"></span><br><span class="line">top_blob.<span class="built_in">create</span>(outw, outh, num_output / out_elempack, out_elemsize, out_elempack, opt.blob_allocator);</span><br><span class="line"><span class="keyword">if</span> (top_blob.<span class="built_in">empty</span>())</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-100</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="卷积执行操作">卷积执行操作</h4>
<p>x86的cpu优化主要还是看packing的，所以他的卷积函数选择都是看输入packing大小和输出packing大小，主要我看了一下就这一些选项，其中的卷积函数都是类似的。
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (elempack == <span class="number">8</span> &amp;&amp; out_elempack == <span class="number">8</span>)</span><br><span class="line"><span class="keyword">if</span> (elempack == <span class="number">1</span> &amp;&amp; out_elempack == <span class="number">8</span>)</span><br><span class="line"><span class="keyword">if</span> (elempack == <span class="number">4</span> &amp;&amp; out_elempack == <span class="number">8</span>)</span><br><span class="line"><span class="keyword">if</span> (elempack == <span class="number">8</span> &amp;&amp; out_elempack == <span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> (elempack == <span class="number">8</span> &amp;&amp; out_elempack == <span class="number">4</span>)</span><br></pre></td></tr></table></figure></p>
<p>同时这里卷积的输出后，他可能是packing的，所以ncnn还提供了<code>convert_packing</code>函数对pack的mat进行转换，不过他只支持output与pack的值为倍数关系时才能成功转换,这里进行转换之后原来的内存块padding的位置应该是被填充了，然后尾部会多余一些值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                                            h*w</span><br><span class="line">                                    w        w       w</span><br><span class="line">                                c1 [ 0  1] [ 2  3] [ 4  5] [ p  p]</span><br><span class="line">                                c2 [ 6  7] [ 8  9] [10 11] [ p  p]</span><br><span class="line">                                c3 [12 13] [14 15] [16 17] [ p  p]</span><br><span class="line">                                c4 [18 19] [20 21] [22 23] [ p  p]</span><br><span class="line">                                            |</span><br><span class="line">                                            | convert_packing</span><br><span class="line">                                            v</span><br><span class="line">                                           h*w</span><br><span class="line">                w                           w                             w</span><br><span class="line">c1 [ (0 6 12 18) (1 7 13 19) ] [ (2 8 14 20)  (3 9 15 21) ] [ (4 10 16 22)  (5 11 17 23)] [o o o o o o o o]</span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ncnn/" rel="tag">ncnn</a></li></ul></div><div class="post-nav"><a class="pre" href="/2021/05/04/nand2tetris-week1/">Nand2Tetris week1</a><a class="next" href="/2021/04/25/cpp-trick/">cpp挖坑&amp;爬坑</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>