---
title: æ¦‚ç‡æ¨¡å‹ç¬¬å››ç«  ï¼š å¤§æ•°å®šç†
mathjax: true
toc: true
categories:
  - æœºå™¨å­¦ä¹ 
date: 2019-07-28 20:12:41
tags:
-   æ¦‚ç‡è®º
-   Tensorflow
---

Tensorflow æ¦‚ç‡æ¨¡å‹å­¦ä¹ ï¼Œä»£ç è¿è¡Œäº`Tensorflow 1.14`ï¼Œæ–‡å­—åŠæœºå™¨ç¿»è¯‘ã€‚


<!--more-->



# Probabilistic Programming and Bayesian Methods for Hackers Chapter 4

---------
### Table of Contents
- Dependencies & Prerequisites
- The greatest theorem never told
  - The Law of Large Numbers
  - Intuition
  - How do we compute $Var(Z)$ though?
  - Expected values and probabilities
  - What does this all have to do with Bayesian statistics?
  - The Disorder of Small Numbers
  - Example: Aggregated geographic data
  - Example: Kaggle's U.S. Census Return Rate Challenge
  - Example: How to order Reddit submissions
    - Setting up the Praw Reddit API
    - Register your Application on Reddit
      - Reddit API Setup
    - Sorting!
    - But this is too slow for real-time!
  - Extension to Starred rating systems
  - Example: Counting Github stars
  - Conclusion
  - Appendix
    - Exercises
    - Kicker Careers Ranked by Make Percentage
    - Average Household Income by Programming Language
  - References

______




```python
#@title Imports and Global Variables  { display-mode: "form" }
"""
The book uses a custom matplotlibrc file, which provides the unique styles for
matplotlib plots. If executing this book, and you wish to use the book's
styling, provided are two options:
    1. Overwrite your own matplotlibrc file with the rc-file provided in the
       book's styles/ dir. See http://matplotlib.org/users/customizing.html
    2. Also in the styles is  bmh_matplotlibrc.json file. This can be used to
       update the styles in only this notebook. Try running the following code:

        import json
        s = json.load(open("../styles/bmh_matplotlibrc.json"))
        matplotlib.rcParams.update(s)
"""
from __future__ import absolute_import, division, print_function

#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)
warning_status = "ignore" #@param ["ignore", "always", "module", "once", "default", "error"]
import warnings
warnings.filterwarnings(warning_status)
with warnings.catch_warnings():
    warnings.filterwarnings(warning_status, category=DeprecationWarning)
    warnings.filterwarnings(warning_status, category=UserWarning)

import numpy as np
import os
#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))
matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']
import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)
import matplotlib.axes as axes;
from matplotlib.patches import Ellipse
from mpl_toolkits.mplot3d import Axes3D
import pandas_datareader.data as web
%matplotlib inline
import seaborn as sns; sns.set_context('notebook')
from IPython.core.pylabtools import figsize
#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)
notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']
%config InlineBackend.figure_format = notebook_screen_res
plt.rcParams['font.sans-serif']=['YaHei Consolas Hybrid']
import tensorflow as tf
tfe = tf.contrib.eager

# Eager Execution
#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)
#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)
use_tf_eager = False #@param {type:"boolean"}

# Use try/except so we can easily re-execute the whole notebook.
if use_tf_eager:
    try:
        tf.enable_eager_execution()
    except:
        pass

import tensorflow_probability as tfp
tfd = tfp.distributions
tfb = tfp.bijectors

  
def evaluate(tensors):
    """Evaluates Tensor or EagerTensor to Numpy `ndarray`s.
    Args:
    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,
      `namedtuple` or combinations thereof.
   
    Returns:
      ndarrays: Object with same structure as `tensors` except with `Tensor` or
        `EagerTensor`s replaced by Numpy `ndarray`s.
    """
    if tf.executing_eagerly():
        return tf.contrib.framework.nest.pack_sequence_as(
            tensors,
            [t.numpy() if tf.contrib.framework.is_tensor(t) else t
             for t in tf.contrib.framework.nest.flatten(tensors)])
    return sess.run(tensors)

class _TFColor(object):
    """Enum of colors used in TF docs."""
    red = '#F15854'
    blue = '#5DA5DA'
    orange = '#FAA43A'
    green = '#60BD68'
    pink = '#F17CB0'
    brown = '#B2912F'
    purple = '#B276B2'
    yellow = '#DECF3F'
    gray = '#4D4D4D'
    def __getitem__(self, i):
        return [
            self.red,
            self.orange,
            self.green,
            self.blue,
            self.pink,
            self.brown,
            self.purple,
            self.yellow,
            self.gray,
        ][i % 9]
TFColor = _TFColor()

def session_options(enable_gpu_ram_resizing=True, enable_xla=True):
    """
    Allowing the notebook to make use of GPUs if they're available.
    
    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear 
    algebra that optimizes TensorFlow computations.
    """
    config = tf.ConfigProto()
    config.log_device_placement = True
    if enable_gpu_ram_resizing:
        # `allow_growth=True` makes it possible to connect multiple colabs to your
        # GPU. Otherwise the colab malloc's all GPU ram.
        config.gpu_options.allow_growth = True
    if enable_xla:
        # Enable on XLA. https://www.tensorflow.org/performance/xla/.
        config.graph_options.optimizer_options.global_jit_level = (tf.OptimizerOptions.ON_1)
    return config


def reset_sess(config=None):
    """
    Convenience function to create the TF graph & session or reset them.
    """
    if config is None:
        config = session_options()
    global sess
    tf.reset_default_graph()
    try:
        sess.close()
    except:
        pass
    sess = tf.InteractiveSession(config=config)

reset_sess()
```

    WARNING: Logging before flag parsing goes to stderr.
    W0728 19:21:27.291700 139811709552448 lazy_loader.py:50] 
    The TensorFlow contrib module will not be included in TensorFlow 2.0.
    For more information, please see:
      * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
      * https://github.com/tensorflow/addons
      * https://github.com/tensorflow/io (for I/O related ops)
    If you depend on functionality not listed there, please file an issue.
    


## æœ€ä¼Ÿå¤§çš„å®šç†ä»æœªè¢«å‘ŠçŸ¥è¿‡

æœ¬ç« çš„é‡ç‚¹æ˜¯ä¸€ä¸ªæ€»æ˜¯åœ¨æˆ‘ä»¬è„‘æµ·ä¸­è¹¦è¹¦è·³è·³çš„æƒ³æ³•ï¼Œä½†å¾ˆå°‘åœ¨ä¸“é—¨ç”¨äºç»Ÿè®¡çš„ä¹¦ç±ä¹‹å¤–æ˜ç¡®è¡¨è¾¾ã€‚äº‹å®ä¸Šï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªä¾‹å­ä¸­éƒ½ä½¿ç”¨è¿‡è¿™ä¸ªç®€å•çš„æƒ³æ³•ã€‚

### å¤§æ•°å®šå¾‹

è®©$ Z_i $ä¸º$ N $æ¥è‡ªæŸäº›æ¦‚ç‡åˆ†å¸ƒçš„ç‹¬ç«‹æ ·æœ¬ã€‚æ ¹æ®*å¤§æ•°å®šå¾‹*ï¼Œåªè¦é¢„æœŸå€¼$ E[Z] $æ˜¯æœ‰é™çš„ï¼Œä»¥ä¸‹æˆç«‹ï¼Œ

$$\frac{1}{N} \sum_{i=1}^N Z_i \rightarrow E[ Z ],  \;\;\; N \rightarrow \infty.$$

æ–‡å­—è¡¨è¿°:

>   æ¥è‡ªç›¸åŒåˆ†å¸ƒçš„éšæœºå˜é‡åºåˆ—çš„å¹³å‡å€¼æ”¶æ•›äºè¯¥åˆ†å¸ƒçš„æœŸæœ›ã€‚

è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªæ— èŠçš„ç»“æœï¼Œä½†å®ƒå°†æ˜¯æ‚¨ä½¿ç”¨çš„æœ€æœ‰ç”¨çš„å·¥å…·ã€‚ä»–æ˜¯è®¡ç®—æœºæ•°å€¼è®¡ç®—çš„é‡è¦æ‰‹æ®µã€‚

### ç›´è§‰

å¦‚æœä¸Šè¿°æ³•å¾‹æœ‰äº›ä»¤äººæƒŠè®¶ï¼Œå¯ä»¥é€šè¿‡ç ”ç©¶ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥æ›´æ¸…æ¥šåœ°è¯´æ˜ã€‚

è€ƒè™‘ä¸€ä¸ªéšæœºå˜é‡$ Z $ï¼Œå®ƒåªèƒ½å¸¦ä¸¤ä¸ªå€¼ï¼Œ$ c_1 $å’Œ$ c_2 $ã€‚å‡è®¾æˆ‘ä»¬æœ‰å¤§é‡$ Z $çš„æ ·æœ¬ï¼Œè¡¨ç¤ºä¸€ä¸ªç‰¹å®šçš„æ ·æœ¬$ Z_i $ã€‚è¯¥å®šç†è§„å®šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¹³å‡æ‰€æœ‰æ ·æœ¬æ¥ä¼°è®¡$ Z $çš„é¢„æœŸå€¼ã€‚è€ƒè™‘å¹³å‡å€¼ï¼š

$$ \frac{1}{N} \sum_{i=1}^N \;Z_i $$


é€šè¿‡æ„é€ ï¼Œ$ Z_i $åªèƒ½æ¥å—$ c_1 $æˆ–$ c_2 $ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å¯¹è¿™ä¸¤ä¸ªå€¼è¿›è¡Œåˆ†åŒºï¼š
$$
\begin{align}
\frac{1}{N} \sum_{i=1}^N \;Z_i & =\frac{1}{N} \big(  \sum_{ Z_i = c_1}c_1 + \sum_{Z_i=c_2}c_2 \big) \\
& = c_1 \sum_{ Z_i = c_1}\frac{1}{N} + c_2 \sum_{ Z_i = c_2}\frac{1}{N} \\
& = c_1 \times \text{ (approximate frequency of $c_1$) } \\
& \;\;\;\;\;\;\;\;\; + c_2 \times \text{ (approximate frequency of $c_2$) } \\
& \approx c_1 \times P(Z = c_1) + c_2 \times P(Z = c_2 ) \\
& = E[Z]
\end{align}
$$

åœ¨æé™å¹³ç­‰ä¿æŒï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨å¹³å‡å€¼ä¸­ä½¿ç”¨è¶Šæ¥è¶Šå¤šçš„æ ·æœ¬æ¥è¶Šæ¥è¶Šè¿‘ã€‚è¯¥æ³•é€‚ç”¨äºå‡ ä¹*ä»»ä½•åˆ†å¸ƒ*ï¼Œå‡å»æˆ‘ä»¬ç¨åå°†é‡åˆ°çš„ä¸€äº›é‡è¦æ¡ˆä¾‹ã€‚

### ä¾‹å­
____


ä¸‹é¢æ˜¯ä¸‰ä¸ªä¸åŒæ³Šæ¾éšæœºå˜é‡åºåˆ—çš„å¤§æ•°å®šå¾‹å›¾ã€‚ 
 
 æˆ‘ä»¬ç”¨å‚æ•°$ \lambda = 4.5 $å¯¹`sample_size = 100000`æ³Šæ¾éšæœºå˜é‡è¿›è¡ŒæŠ½æ ·ã€‚ ï¼ˆå›æƒ³ä¸€ä¸‹æ³Šæ¾éšæœºå˜é‡çš„æœŸæœ›å€¼ç­‰äºå®ƒçš„å‚æ•°ã€‚ï¼‰æˆ‘ä»¬è®¡ç®—å‰$ n $ä¸ªæ ·æœ¬çš„å¹³å‡å€¼ï¼Œ$n=1$åˆ°`sample_size`ã€‚


```python
sample_size_ = 100000
expected_value_ = lambda_val_ = 4.5
N_samples = tf.range(start=1,
                      limit=sample_size_,
                      delta=100)

plt.figure(figsize(12.5, 4))
for k in range(3):
    samples = tfd.Poisson(rate=lambda_val_).sample(sample_shape=(sample_size_))
    [ samples_, N_samples_ ] = evaluate([ samples, N_samples ]) 

    partial_average_ = [ samples_[:i].mean() for i in N_samples_ ]        

    plt.plot( N_samples_, partial_average_, lw=1.5,label="$n$ä¸ªæ ·æœ¬çš„å¹³å‡å€¼ ; seq. %d"%k)

plt.plot( N_samples_, expected_value_ * np.ones_like( partial_average_), 
    ls = "--", label = "çœŸå®æœŸæœ›å€¼", c = "k" )

plt.ylim( 4.35, 4.65) 
plt.title( "éšæœºå˜é‡çš„å¹³å‡å€¼ä¸å…¶æœŸæœ›çš„æ”¶æ•›æ€§" )
plt.ylabel( "average of $n$ samples" )
plt.xlabel( "# of samples, $n$")
plt.legend();
```


![](tfp-ch4/output_6_0.png)


çœ‹ä¸€ä¸‹ä¸Šé¢çš„å›¾ï¼Œå¾ˆæ˜æ˜¾ï¼Œå½“æ ·æœ¬é‡å¾ˆå°æ—¶ï¼Œå¹³å‡å€¼ä¼šæœ‰è¾ƒå¤§çš„å˜åŒ–ï¼ˆæ¯”è¾ƒ*è·³è·ƒ*çš„æœ€åˆå¹³å‡å€¼ï¼Œç„¶å*å¹³æ»‘*ï¼‰ã€‚æ‰€æœ‰ä¸‰æ¡è·¯å¾„*æ¥è¿‘*å€¼4.5ï¼Œä½†éšç€$ N $å˜å¤§ï¼Œåªæ˜¯è°ƒæ•´å®ƒã€‚æ•°å­¦å®¶å’Œç»Ÿè®¡å­¦å®¶æœ‰å¦ä¸€ä¸ªåå­—ï¼šæ”¶æ•›ã€‚

æˆ‘ä»¬å¯ä»¥é—®çš„å¦ä¸€ä¸ªéå¸¸ç›¸å…³çš„é—®é¢˜æ˜¯*æˆ‘æ”¶æ•›åˆ°é¢„æœŸå€¼çš„é€Ÿåº¦æœ‰å¤šå¿«ï¼Ÿ*è®©æˆ‘ä»¬ç»˜åˆ¶ä¸€äº›æ–°çš„ä¸œè¥¿ã€‚å¯¹äºç‰¹å®šçš„$ N $ï¼Œè®©æˆ‘ä»¬è¿›è¡Œä¸Šè¿°è¯•éªŒæ•°åƒæ¬¡ï¼Œå¹¶è®¡ç®—å‡ºæˆ‘ä»¬ä¸çœŸå®é¢„æœŸå€¼çš„å¹³å‡è·ç¦»ã€‚ä½†ç­‰ç­‰â€”â€”*å¹³å‡è®¡ç®—*ï¼Ÿè¿™åªæ˜¯å¤§æ•°æ³•åˆ™ï¼ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯ï¼Œå¯¹äºç‰¹å®šçš„$ N $ï¼Œæ•°é‡ï¼š

$$D(N) = \sqrt{ \;E\left[\;\; \left( \frac{1}{N}\sum_{i=1}^NZ_i  - 4.5 \;\right)^2 \;\;\right] \;\;}$$

å¯¹äºæŸäº›$ N $ï¼Œä¸Šè¿°å…¬å¼å¯è§£é‡Šä¸ºè·ç¦»çœŸå®å€¼ï¼ˆå¹³å‡å€¼ï¼‰çš„è·ç¦»ã€‚ ï¼ˆæˆ‘ä»¬å–å¹³æ–¹æ ¹ï¼Œå› æ­¤ä¸Šè¿°æ•°é‡çš„ç»´æ•°å’Œæˆ‘ä»¬çš„éšæœºå˜é‡æ˜¯ç›¸åŒçš„ï¼‰ã€‚ç”±äºä¸Šé¢æ˜¯ä¸€ä¸ªæœŸæœ›å€¼ï¼Œå®ƒå¯ä»¥ä½¿ç”¨å¤§æ•°å®šå¾‹è¿‘ä¼¼ï¼šæˆ‘ä»¬è®¡ç®—ä»¥ä¸‹å¤šæ¬¡å¹¶å¹³å‡å®ƒä»¬ï¼Œè€Œä¸æ˜¯å¹³å‡$ Z_i $ï¼š

$$ Y_k = \left( \;\frac{1}{N}\sum_{i=1}^NZ_i  - 4.5 \; \right)^2 $$

é€šè¿‡è®¡ç®—ä¸Šé¢çš„$ N_y $æ¬¡ï¼ˆè®°ä½ï¼Œå®ƒæ˜¯éšæœºçš„ï¼‰ï¼Œå¹¶å¯¹å®ƒä»¬æ±‚å¹³å‡å€¼ï¼š

$$ \frac{1}{N_Y} \sum_{k=1}^{N_Y} Y_k \rightarrow E[ Y_k ] = E\;\left[\;\; \left( \frac{1}{N}\sum_{i=1}^NZ_i  - 4.5 \;\right)^2 \right]$$

æœ€åï¼Œå–å¹³æ–¹æ ¹ï¼š

$$ \sqrt{\frac{1}{N_Y} \sum_{k=1}^{N_Y} Y_k} \approx D(N) $$ 


```python
N_Y = tf.constant(250)  # ç”¨è¿™ä¹ˆå¤šæ¥è¿‘ä¼¼ D(N)
N_array = tf.range(1000., 50000., 2500) # åœ¨å¤§çº¦ä½¿ç”¨è¿™ä¹ˆå¤šæ ·å“ã€‚å·®å¼‚ã€‚
D_N_results = tf.zeros(tf.shape(N_array)[0])
lambda_val = tf.constant(4.5) 
expected_value = tf.constant(4.5) #for X ~ Poi(lambda) , E[ X ] = lambda

[
    N_Y_, 
    N_array_, 
    D_N_results_, 
    expected_value_, 
    lambda_val_,
] = evaluate([ 
    N_Y, 
    N_array, 
    D_N_results, 
    expected_value,
    lambda_val,
])

def D_N(n):
    """
    This function approx. D_n, the average variance of using n samples.
    """
    Z = tfd.Poisson(rate=lambda_val_).sample(sample_shape=(int(n), int(N_Y_)))
    average_Z = tf.reduce_mean(Z, axis=0)
    average_Z_ = evaluate(average_Z)
    
    return np.sqrt(((average_Z_ - expected_value_)**2).mean())

for i,n in enumerate(N_array_):
    D_N_results_[i] =  D_N(n)

plt.figure(figsize(12.5, 3))
plt.xlabel( "$N$" )
plt.ylabel( "é¢„æœŸä¸çœŸå®å€¼çš„å¹³æ–¹è·ç¦»" )
plt.plot(N_array_, D_N_results_, lw = 3, label="éšæœºå˜é‡$N$çš„é¢„æœŸå€¼ä¸å¹³å‡å€¼ä¹‹é—´çš„é¢„æœŸè·ç¦»ã€‚")
plt.plot( N_array_, np.sqrt(expected_value_)/np.sqrt(N_array_), lw = 3, ls = "--", label = r"$\frac{\sqrt{\lambda}}{\sqrt{N}}$" )
plt.legend()
plt.title( "æ ·æœ¬å¹³å‡æ”¶æ•›çš„â€œå¿«â€ç¨‹åº¦å¦‚ä½•ï¼Ÿ " );
```


![](tfp-ch4/output_8_0.png)


æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œéšç€$ N $å¢é•¿ï¼Œæˆ‘ä»¬çš„æ ·æœ¬å¹³å‡å€¼ä¸å®é™…é¢„æœŸå€¼ä¹‹é—´çš„é¢„æœŸè·ç¦»ä¼šç¼©å°ã€‚ä½†ä¹Ÿæ³¨æ„åˆ°*æ”¶æ•›ç‡*é™ä½ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬åªéœ€è¦10 000ä¸ªé¢å¤–æ ·æœ¬ä»0.020ç§»åŠ¨åˆ°0.015ï¼Œç›¸å·®0.005ï¼Œä½†æ˜¯*20000*æ›´å¤šæ ·æœ¬å†æ¬¡ä»0.015é™ä½åˆ°0.010ï¼Œå†æ¬¡åªæœ‰0.005å‡å°‘ã€‚

äº‹å®è¯æ˜ï¼Œæˆ‘ä»¬å¯ä»¥è¡¡é‡è¿™ç§æ”¶æ•›é€Ÿåº¦ã€‚ä¸Šé¢æˆ‘ç»˜åˆ¶äº†ç¬¬äºŒè¡Œï¼Œå‡½æ•°$ \sqrt {\lambda}/\sqrt {N} $ã€‚è¿™ä¸æ˜¯ä»»æ„é€‰æ‹©çš„ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œç»™å®šä¸€ç³»åˆ—éšæœºå˜é‡åˆ†å¸ƒå¦‚$ Z $ï¼Œå¤§æ•°å®šå¾‹çš„æ”¶æ•›ç‡ä¸º$ E [Z] $

$$ \frac{ \sqrt{ \; Var(Z) \; } }{\sqrt{N} }$$

è¿™æœ‰ç”¨çš„çŸ¥è¯†ï¼šå¯¹äºç»™å®šçš„å¤§$ N $ï¼Œæˆ‘ä»¬çŸ¥é“ï¼ˆå¹³å‡è€Œè¨€ï¼‰æˆ‘ä»¬ä¸ä¼°è®¡çš„è·ç¦»ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨è´å¶æ–¯ç¯å¢ƒä¸­ï¼Œè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªæ— ç”¨çš„ç»“æœï¼šè´å¶æ–¯åˆ†ææ˜¯ä¸ç¡®å®šçš„ï¼Œé‚£ä¹ˆæ·»åŠ é¢å¤–ç²¾ç¡®æ•°å­—çš„*ç»Ÿè®¡*ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿè™½ç„¶ç»˜å›¾æ ·æœ¬çš„è®¡ç®—æˆæœ¬å¯ä»¥å¾ˆä½ï¼Œä½†*è¶Šå¤§çš„*$N$ä¹Ÿå¾ˆå¥½ã€‚

### æˆ‘ä»¬å¦‚ä½•è®¡ç®—$ Varï¼ˆZï¼‰$ï¼Ÿ

æ–¹å·®åªæ˜¯å¦ä¸€ä¸ªå¯ä»¥è¿‘ä¼¼çš„é¢„æœŸå€¼ï¼è€ƒè™‘ä»¥ä¸‹æƒ…å†µï¼Œä¸€æ—¦æˆ‘ä»¬å¾—åˆ°é¢„æœŸå€¼ï¼ˆé€šè¿‡ä½¿ç”¨å¤§æ•°å®šå¾‹æ¥ä¼°è®¡å®ƒï¼Œè¡¨ç¤º$ \mu $ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ä¼°è®¡æ–¹å·®ï¼š

$$ \frac{1}{N}\sum_{i=1}^N \;(Z_i - \mu)^2 \rightarrow E[ \;( Z - \mu)^2 \;] = Var( Z )$$



### æœŸæœ›å€¼å’Œæ¦‚ç‡

æœŸæœ›å€¼ä¸ä¼°è®¡æ¦‚ç‡ä¹‹é—´çš„å…³ç³»æ›´ä¸ºæ˜æ˜¾ã€‚å®šä¹‰*æŒ‡æ ‡åŠŸèƒ½*

$$\mathbb{1}_A(x) = 
\begin{cases} 1 &  x \in A \\\\
              0 &  else
\end{cases}
$$

ç„¶åï¼Œæ ¹æ®å¤§æ•°å®šå¾‹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰è®¸å¤šæ ·æœ¬$ X_i $ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¼°è®¡äº‹ä»¶$ A $çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$ Pï¼ˆAï¼‰$ï¼š

$$ \frac{1}{N} \sum_{i=1}^N \mathbb{1}_A(X_i) \rightarrow E[\mathbb{1}_A(X)] =  P(A) $$

åŒæ ·ï¼Œç»è¿‡ä¸€æ®µæ—¶é—´çš„è§‚æµ‹åï¼Œè¿™æ˜¯ç›¸å½“æ˜æ˜¾çš„ï¼šå¦‚æœäº‹ä»¶å‘ç”Ÿï¼ŒæŒ‡æ ‡å‡½æ•°åªæœ‰1ï¼Œæ‰€ä»¥æˆ‘ä»¬åªå°†äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´ç›¸åŠ å¹¶é™¤ä»¥è¯•éªŒæ€»æ•°ï¼ˆè€ƒè™‘æˆ‘ä»¬é€šå¸¸å¦‚ä½•ä½¿ç”¨é¢‘ç‡é€¼è¿‘æ¦‚ç‡ï¼‰ ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬å¸Œæœ›ä¼°è®¡$ Z \sim Expï¼ˆ.5ï¼‰$å¤§äº5çš„æ¦‚ç‡ï¼Œå¹¶ä¸”æˆ‘ä»¬ä»$ Expï¼ˆ.5ï¼‰$åˆ†å¸ƒä¸­å¾—åˆ°è®¸å¤šæ ·æœ¬ã€‚

$$ P( Z > 5 ) =  \frac{1}{N}\sum_{i=1}^N \mathbb{1}_{z > 5 }(Z_i) $$


```python
N = 10000

print("æ¦‚ç‡ä¼°è®¡: ", len(np.where(evaluate(tfd.Exponential(rate=0.5).sample(sample_shape=N)) > 5))/N )
```

    æ¦‚ç‡ä¼°è®¡:  0.0001


### è¿™ä¸è´å¶æ–¯ç»Ÿè®¡æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ

ä½¿ç”¨æœŸæœ›å€¼è®¡ç®—è´å¶æ–¯æ¨æ–­ä¸­å°†åœ¨ä¸‹ä¸€ç« ä¸­ä»‹ç»çš„*ç‚¹ä¼°è®¡*ã€‚åœ¨æ›´å¤šåˆ†æçš„è´å¶æ–¯æ¨æ–­ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°è¡¨ç¤ºä¸ºå¤šç»´ç§¯åˆ†çš„å¤æ‚æœŸæœ›å€¼ã€‚ä¸å†ã€‚å¦‚æœæˆ‘ä»¬å¯ä»¥ç›´æ¥ä»åéªŒåˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œæˆ‘ä»¬åªéœ€è¦è¯„ä¼°å¹³å‡å€¼ã€‚æ›´å®¹æ˜“ã€‚å¦‚æœå‡†ç¡®æ€§æ˜¯ä¸€ä¸ªä¼˜å…ˆäº‹é¡¹ï¼Œé‚£ä¹ˆä¸Šé¢çš„å›¾è¡¨æ˜¾ç¤ºä½ æ”¶æ•›é€Ÿåº¦æœ‰å¤šå¿«ã€‚å¦‚æœéœ€è¦è¿›ä¸€æ­¥çš„ç²¾ç¡®åº¦ï¼Œåªéœ€ä»åéªŒä¸­é‡‡é›†æ›´å¤šæ ·æœ¬ã€‚

ä»€ä¹ˆæ—¶å€™å¤Ÿäº†ï¼Ÿä½ ä½•æ—¶å¯ä»¥åœæ­¢ä»åæ–¹æŠ½å–æ ·æœ¬ï¼Ÿè¿™æ˜¯ä»ä¸šè€…çš„å†³å®šï¼Œä¹Ÿå–å†³äºæ ·æœ¬çš„æ–¹å·®ï¼ˆä»ä¸Šé¢å›å¿†é«˜æ–¹å·®æ„å‘³ç€å¹³å‡å€¼ä¼šæ”¶æ•›å¾—æ›´æ…¢ï¼‰ã€‚

æˆ‘ä»¬ä¹Ÿåº”è¯¥ç†è§£å¤§æ•°å®šå¾‹ä½•æ—¶å¤±è´¥ã€‚é¡¾åæ€ä¹‰ï¼Œå¹¶å°†ä¸Šé¢çš„å›¾è¡¨ä¸å°$N$è¿›è¡Œæ¯”è¾ƒï¼Œè¯¥æ³•åˆ™é€‚ç”¨äºå¤§æ ·æœ¬é‡ã€‚æ²¡æœ‰è¿™ä¸ªï¼Œæ¸è¿‘ç»“æœæ˜¯ä¸å¯é çš„ã€‚äº†è§£å®šç†å¤±è´¥çš„æƒ…å†µå¯ä»¥è®©æˆ‘ä»¬å¯¹è‡ªå·±åº”è¯¥å¤šä¹ˆ*ä¸è‡ªä¿¡*å……æ»¡ä¿¡å¿ƒã€‚ä¸‹ä¸€èŠ‚å°†è®¨è®ºæ­¤é—®é¢˜ã€‚

### å°æ•°ç›®çš„ç´Šä¹±

å¤§æ•°å®šå¾‹åªæœ‰åœ¨$ N $å¾—åˆ°*æ— é™å¤§*æ—¶æ‰æœ‰æ•ˆï¼šæ°¸è¿œä¸å¯èƒ½å®ç°ã€‚è™½ç„¶å®šç†æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œä½†å¹¿æ³›åœ°åº”ç”¨å®ƒæ˜¯è›®å¹²çš„ã€‚æˆ‘ä»¬çš„ä¸‹ä¸€ä¸ªä¾‹å­è¯´æ˜äº†è¿™




### Example: æ±‡æ€»çš„åœ°ç†æ•°æ®

æ•°æ®é€šå¸¸ä»¥æ±‡æ€»å½¢å¼å‡ºç°ã€‚ä¾‹å¦‚ï¼Œæ•°æ®å¯ä»¥æŒ‰å·ï¼Œå¿æˆ–åŸå¸‚çº§åˆ«åˆ†ç»„ã€‚å½“ç„¶ï¼Œäººå£æ•°é‡å› åœ°ç†åŒºåŸŸè€Œå¼‚ã€‚å¦‚æœæ•°æ®æ˜¯æ¯ä¸ªåœ°ç†åŒºåŸŸçš„æŸäº›ç‰¹å¾çš„å¹³å‡å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»æ„è¯†åˆ°å¤§æ•°å®šå¾‹ä»¥åŠå®ƒå¯¹äºäººå£è¾ƒå°‘çš„åŒºåŸŸå¦‚ä½•*å¤±è´¥*ã€‚

æˆ‘ä»¬å°†åœ¨ç©å…·æ•°æ®é›†ä¸Šè§‚å¯Ÿåˆ°è¿™ä¸€ç‚¹ã€‚å‡è®¾æˆ‘ä»¬çš„æ•°æ®é›†ä¸­æœ‰äº”åƒä¸ªå¿ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªå·çš„äººå£æ•°é‡å‡åŒ€åˆ†å¸ƒåœ¨100åˆ°1500ä¹‹é—´ã€‚äººå£æ•°é‡çš„ç”Ÿæˆæ–¹å¼ä¸è®¨è®ºæ— å…³ï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½è¯æ˜è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯æµ‹é‡æ¯ä¸ªå¿çš„å¹³å‡èº«é«˜ã€‚æˆ‘ä»¬ä¸çŸ¥é“ï¼Œèº«é«˜ä¸ä¼šå› å¿è€Œå¼‚ï¼Œæ¯ä¸ªäººï¼Œæ— è®ºä»–æˆ–å¥¹ç›®å‰å±…ä½åœ¨å“ªä¸ªå¿ï¼Œéƒ½æœ‰ä¸ä»–ä»¬èº«é«˜ç›¸åŒçš„åˆ†å¸ƒï¼š

$$ \text{height} \sim \text{Normal}(\text{mu}=150, \text{sd}=15 ) $$

æˆ‘ä»¬æ±‡æ€»äº†å¿çº§çš„ä¸ªäººï¼Œå› æ­¤æˆ‘ä»¬åªæœ‰å¿å†…*å¹³å‡å€¼çš„æ•°æ®*ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯èƒ½æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ


```python
plt.figure(figsize(12.5, 4))

std_height = 15.
mean_height = 150.
n_counties = 500
smallest_population = 100
largest_population = 1500
pop_generator = np.random.randint
norm = np.random.normal

population_ = pop_generator(smallest_population, largest_population, n_counties)

# Our strategy to vectorize this problem will be to end-to-end concatenate the
# number of draws we need. Then we'll loop over the pieces.
d = tfp.distributions.Normal(loc=mean_height, scale= 1. / std_height)
x = d.sample(np.sum(population_))

average_across_county = []
seen = 0
for p in population_:
    average_across_county.append(tf.reduce_mean(x[seen:seen+p]))
    seen += p
average_across_county_full = tf.stack(average_across_county)

##located the counties with the apparently most extreme average heights.
[   average_across_county_,
    i_min, 
    i_max 
] = evaluate([
    average_across_county_full,
    tf.argmin( average_across_county_full ), 
    tf.argmax( average_across_county_full )
])

#plot population size vs. recorded average
plt.scatter( population_, average_across_county_, alpha = 0.5, c=TFColor[6])
plt.scatter( [ population_[i_min], population_[i_max] ], 
           [average_across_county_[i_min], average_across_county_[i_max] ],
           s = 60, marker = "o", facecolors = "none",
           edgecolors = TFColor[0], linewidths = 1.5, 
            label="æç«¯çš„é«˜åº¦")

plt.xlim( smallest_population, largest_population )
plt.title( "å¹³å‡é«˜åº¦ vs. å¿äººå£")
plt.xlabel("å¿äººå£")
plt.ylabel("å¿çš„å¹³å‡èº«é«˜")
plt.plot( [smallest_population, largest_population], [mean_height, mean_height], color = "k", label = "true expected \
height", ls="--" )
plt.legend(scatterpoints = 1);
```


![](tfp-ch4/output_15_0.png)


æˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä»€ä¹ˆï¼Ÿ *å¦‚æœä¸è€ƒè™‘äººå£è§„æ¨¡* æˆ‘ä»¬å†’ç€é€ æˆå·¨å¤§æ¨ç†é”™è¯¯çš„é£é™©ï¼šå¦‚æœæˆ‘ä»¬å¿½ç•¥äº†äººå£è§„æ¨¡ï¼Œæˆ‘ä»¬ä¼šè¯´æœ€çŸ­å’Œæœ€é«˜ä¸ªä½“çš„å¿å·²è¢«æ­£ç¡®åœˆå‡ºã€‚ä½†ç”±äºä»¥ä¸‹åŸå› ï¼Œè¿™ç§æ¨æ–­æ˜¯é”™è¯¯çš„ã€‚è¿™ä¸¤ä¸ªå¿*ä¸ä¸€å®š*å…·æœ‰æœ€æç«¯çš„é«˜åº¦ã€‚è®¡ç®—å‡ºçš„è¾ƒå°ç§ç¾¤çš„å¹³å‡å€¼ä¸èƒ½å¾ˆå¥½åœ°åæ˜ å‡ºäººå£çš„çœŸå®é¢„æœŸä»·å€¼ï¼ˆäº‹å®ä¸Šåº”è¯¥æ˜¯$ \mu = 150 $ï¼‰ã€‚æ ·æœ¬å¤§å°/äººå£è§„æ¨¡/ $ N $ï¼Œæ— è®ºä½ æƒ³è¦ä»€ä¹ˆï¼Œå®ƒéƒ½å¤ªå°äº†ï¼Œæ— æ³•æœ‰æ•ˆåœ°è°ƒç”¨å¤§æ•°å®šå¾‹ã€‚

æˆ‘ä»¬æä¾›äº†æ›´å¤šåå¯¹è¿™ç§æ¨è®ºçš„è¯æ®ã€‚å›æƒ³ä¸€ä¸‹ï¼Œäººå£æ•°é‡å‡åŒ€åˆ†å¸ƒåœ¨100åˆ°1500ä¹‹é—´ã€‚æˆ‘ä»¬çš„ç›´è§‰åº”è¯¥å‘Šè¯‰æˆ‘ä»¬ï¼Œäººå£æœ€é«˜æåº¦çš„å¿ä¹Ÿåº”è¯¥ç»Ÿä¸€åˆ†å¸ƒåœ¨100åˆ°1500ä¹‹é—´ï¼Œå½“ç„¶ä¹Ÿä¸ä¾èµ–äºè¯¥å¿çš„äººå£ã€‚ä¸æ˜¯è¿™æ ·ã€‚ä»¥ä¸‹æ˜¯å…·æœ‰æœ€æç«¯é«˜åº¦çš„å¿çš„äººå£è§„æ¨¡ã€‚


```python
print("10ä¸ªæœ€å°‘å¿çš„äººå£è§„æ¨¡ï¼š")
print(population_[ np.argsort( average_across_county_ )[:10] ], '\n')
print("10ä¸ªæœ€é«˜å¿çš„äººå£è§„æ¨¡ï¼š ")
print(population_[ np.argsort( -average_across_county_ )[:10] ])
```

    10ä¸ªæœ€å°‘å¿çš„äººå£è§„æ¨¡ï¼š
    [160 134 280 129 207 176 411 256 247 176] 
    
    10ä¸ªæœ€é«˜å¿çš„äººå£è§„æ¨¡ï¼š 
    [113 127 258 362 185 224 478 310 148 312]


åœ¨100åˆ°1500ä¹‹é—´æ ¹æœ¬æ²¡æœ‰ç»Ÿä¸€ã€‚è¿™æ˜¯å¤§æ•°å®šå¾‹çš„ç»å¯¹å¤±è´¥ã€‚

### ç¤ºä¾‹ï¼šKaggleçš„*ç¾å›½ã€‚äººå£æ™®æŸ¥é€€è´§ç‡æŒ‘æˆ˜*

ä»¥ä¸‹æ˜¯2010å¹´ç¾å›½äººå£æ™®æŸ¥çš„æ•°æ®ï¼Œè¯¥æ•°æ®å°†å¿ä»¥å¤–çš„äººå£åˆ’åˆ†ä¸ºè¡—åŒºé›†å›¢ï¼ˆåŸå¸‚è¡—åŒºæˆ–åŒç­‰åŸå¸‚çš„é›†åˆï¼‰ã€‚è¿™ä¸ªæ•°æ®é›†æ¥è‡ªKaggleæœºå™¨å­¦ä¹ ç«èµ›ï¼Œä¸€äº›åŒäº‹å’Œæˆ‘å‚ä¸äº†ã€‚ç›®çš„æ˜¯ä½¿ç”¨äººå£æ™®æŸ¥å˜é‡ï¼ˆä¸­ä½æ•°æ”¶å…¥ï¼Œå¥³æ€§äººæ•°ï¼‰é¢„æµ‹ä¸€ç»„ç¾¤ä½“çš„äººå£æ™®æŸ¥ä¿¡ä»¶å›é‚®ç‡ï¼Œæµ‹é‡å€¼åœ¨0åˆ°100ä¹‹é—´ã€‚è¡—åŒºé›†å›¢ï¼Œæ‹–è½¦åœè½¦åœºæ•°é‡ï¼Œå¹³å‡å„¿ç«¥äººæ•°ç­‰ï¼‰ã€‚ä¸‹é¢æˆ‘ä»¬ç»˜åˆ¶äººå£æ™®æŸ¥é‚®ä»¶å›å¤ç‡ä¸å—ç»„äººå£çš„å…³ç³»ï¼š


```python
reset_sess()

import wget
url = 'https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter4_TheGreatestTheoremNeverTold/data/census_data.csv'
filename = wget.download(url)
filename
```




    'census_data.csv'




```python
plt.figure(figsize(12.5, 6.5))
data_ = np.genfromtxt( "census_data.csv", skip_header=1, 
                        delimiter= ",")
plt.scatter( data_[:,1], data_[:,0], alpha = 0.5, c=TFColor[6])
plt.title("äººå£æ™®æŸ¥é‚®å¯„å›é‚®ç‡ä¸äººå£")
plt.ylabel("é‚®å¯„å›å¤ç‡")
plt.xlabel("å—ç»„äººå£")
plt.xlim(-100, 15e3 )
plt.ylim( -5, 105)

i_min = tf.argmin(  data_[:,0] )
i_max = tf.argmax(  data_[:,0] )

[ i_min_, i_max_ ] = evaluate([ i_min, i_max ])
 
plt.scatter( [ data_[i_min_,1], data_[i_max_, 1] ], 
             [ data_[i_min_,0], data_[i_max_,0] ],
             s = 60, marker = "o", facecolors = "none",
             edgecolors = TFColor[0], linewidths = 1.5, 
             label="æœ€æç«¯çš„ç‚¹")

plt.legend(scatterpoints = 1);
```


![](tfp-ch4/output_20_0.png)


ä»¥ä¸Šæ˜¯ç»Ÿè®¡å­¦ä¸­çš„ç»å…¸ç°è±¡ã€‚æˆ‘è¯´*ç»å…¸*æŒ‡çš„æ˜¯ä¸Šé¢æ•£ç‚¹å›¾çš„â€œå½¢çŠ¶â€ã€‚å®ƒéµå¾ªç»å…¸çš„ä¸‰è§’å½¢å½¢å¼ï¼Œéšç€æˆ‘ä»¬å¢åŠ æ ·æœ¬å¤§å°è€Œç´§ç¼©ï¼ˆéšç€å¤§æ•°å®šå¾‹å˜å¾—æ›´åŠ ç²¾ç¡®ï¼‰ã€‚

æˆ‘å¯èƒ½ä¼šè¿‡åˆ†å¼ºè°ƒè¿™ä¸€ç‚¹ï¼Œä¹Ÿè®¸æˆ‘åº”è¯¥æŠŠè¿™æœ¬ä¹¦å‘½åä¸ºâ€œä½ æ²¡æœ‰å¤§æ•°æ®é—®é¢˜ï¼â€ï¼Œä½†è¿™é‡Œå†æ¬¡ä¸¾ä¾‹è¯´æ˜*å°æ•°æ®é›†*çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯å¤§æ•°æ®é›†ã€‚ç®€å•åœ°è¯´ï¼Œä½¿ç”¨å¤§æ•°å®šå¾‹ä¸èƒ½å¤„ç†å°æ•°æ®é›†ã€‚ä¸å¯¹å¤§æ•°æ®é›†ï¼ˆä¾‹å¦‚å¤§æ•°æ®ï¼‰æ¯«ä¸è´¹åŠ›åœ°åº”ç”¨å®šç†ç›¸æ¯”è¾ƒã€‚æˆ‘ä¹‹å‰æåˆ°çŸ›ç›¾çš„æ˜¯ï¼Œå¤§æ•°æ®é¢„æµ‹é—®é¢˜æ˜¯é€šè¿‡ç›¸å¯¹ç®€å•çš„ç®—æ³•è§£å†³çš„ã€‚é€šè¿‡ç†è§£å¤§æ•°å®šå¾‹åˆ›å»º*ç¨³å®š*çš„è§£å†³æ–¹æ¡ˆï¼Œå³åŠ å…¥æˆ–å‡å°‘ä¸€äº›æ•°æ®ç‚¹ä¸ä¼šå¯¹è§£å†³æ–¹æ¡ˆäº§ç”Ÿå¤ªå¤§å½±å“ï¼Œå¯ä»¥éƒ¨åˆ†è§£å†³æ‚–è®ºã€‚å¦ä¸€æ–¹é¢ï¼Œå‘å°å‹æ•°æ®é›†æ·»åŠ æˆ–åˆ é™¤æ•°æ®ç‚¹å¯èƒ½ä¼šäº§ç”Ÿæˆªç„¶ä¸åŒçš„ç»“æœã€‚

ä¸ºäº†è¿›ä¸€æ­¥é˜…è¯»å¤§æ•°å®šå¾‹çš„éšæ‚£ï¼Œæˆ‘å¼ºçƒˆæ¨èä¼˜ç§€çš„æ‰‹ç¨¿[æœ€å±é™©çš„æ–¹ç¨‹å¼](http://nsm.uh.edu/~dgraur/niv/TheMostDangerousEquation.pdf)

### Example: è®¡ç®—Githubæ˜Ÿ

Githubå­˜å‚¨åº“çš„å¹³å‡æ˜Ÿæ•°æ˜¯å¤šå°‘ï¼Ÿä½ æ€ä¹ˆç®—è¿™ä¸ªï¼Ÿæœ‰è¶…è¿‡600ä¸‡ä¸ªå­˜å‚¨åº“ï¼Œå› æ­¤æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥è°ƒç”¨å¤§æ•°å®šå¾‹ã€‚è®©æˆ‘ä»¬å¼€å§‹æå–ä¸€äº›æ•°æ®ã€‚


```python
reset_sess()

import wget
url = 'https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/data/github_data.csv'
filename = wget.download(url)
filename
```




    'github_data (1).csv'




```python
# Github data scrapper
# See documentation_url: https://developer.github.com/v3/

from json import loads
import datetime
import numpy as np
from requests import get

"""
variables of interest:
    indp. variables
    - language, given as a binary variable. Need 4 positions for 5 langagues
    - #number of days created ago, 1 position
    - has wiki? Boolean, 1 position
    - followers, 1 position
    - following, 1 position
    - constant
    
    dep. variables
    -stars/watchers
    -forks
"""


MAX = 8000000
today =  datetime.datetime.today()
randint = np.random.randint
N = 10 #sample size. 
auth = ("zhen8838", "zqh19960305" )

language_mappings = {"Python": 0, "JavaScript": 1, "Ruby": 2, "Java":3, "Shell":4, "PHP":5}

#define data matrix: 
X = np.zeros( (N , 12), dtype = int )

for i in range(N):
    is_fork = True
    is_valid_language = False
    
    while is_fork == True or is_valid_language == False:
        is_fork = True
        is_valid_language = False
        
        params = {"since":randint(0, MAX ) }
        r = get("https://api.github.com/repositories", params = params, auth=auth )
        results = loads( r.text )[0]
        #im only interested in the first one, and if it is not a fork.
#         print(results)
        is_fork = results["fork"]
        
        r = get( results["url"], auth = auth)
        
        #check the language
        repo_results = loads( r.text )
        try: 
            language_mappings[ repo_results["language" ] ]
            is_valid_language = True
        except:
            pass

    #languages 
    X[ i, language_mappings[ repo_results["language" ] ] ] = 1
    
    #delta time
    X[ i, 6] = ( today - datetime.datetime.strptime( repo_results["created_at"][:10], "%Y-%m-%d" ) ).days
    
    #haswiki
    X[i, 7] = repo_results["has_wiki"]
    
    #get user information
    r = get( results["owner"]["url"] , auth = auth)
    user_results = loads( r.text )
    X[i, 8] = user_results["following"]
    X[i, 9] = user_results["followers"]
    
    #get dep. data
    X[i, 10] = repo_results["watchers_count"]
    X[i, 11] = repo_results["forks_count"]
    print()
    print(" -------------- ")
    print(i, ": ", results["full_name"], repo_results["language" ], repo_results["watchers_count"], repo_results["forks_count"]) 
    print(" -------------- ") 
    print() 
    
np.savetxt("github_data.csv", X, delimiter=",", fmt="%d" )
```

    
     -------------- 
    0 :  hbradlow/autograde Python 0 0
     -------------- 
    
    
     -------------- 
    1 :  pkellett/test JavaScript 2 0
     -------------- 
    
    
     -------------- 
    2 :  sputnikus/cmdoro Python 0 0
     -------------- 
    
    
     -------------- 
    3 :  theteam/vagrant-django-template Python 36 12
     -------------- 
    
    
     -------------- 
    4 :  contra/JMOT Java 17 9
     -------------- 
    
    
     -------------- 
    5 :  jcDesigns99/sample_app Ruby 1 0
     -------------- 
    
    
     -------------- 
    6 :  tbarho/base_app Ruby 1 0
     -------------- 
    
    
     -------------- 
    7 :  lvh/txscrypt Python 6 1
     -------------- 
    
    
     -------------- 
    8 :  Xand0r/Treebook JavaScript 1 0
     -------------- 
    
    
     -------------- 
    9 :  wingertge/ThumbsApplyGroupManager Java 1 0
     -------------- 
    


### ç»“è®º

è™½ç„¶å¤§æ•°å®šå¾‹å¾ˆé…·ï¼Œä½†åªæœ‰å®ƒçš„åå­—æš—ç¤ºå®ƒæ‰çœŸå®ï¼šåªæœ‰å¤§æ ·æœ¬é‡ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•é€šè¿‡ä¸è€ƒè™‘*æ•°æ®çš„å½¢çŠ¶*æ¥å½±å“æˆ‘ä»¬çš„æ¨ç†ã€‚

1. é€šè¿‡ï¼ˆç®€å•çš„ï¼‰ä»åéªŒåˆ†å¸ƒä¸­æŠ½å–è®¸å¤šæ ·æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿å¤§æ•°å®šå¾‹é€‚ç”¨äºæˆ‘ä»¬æ¥è¿‘æœŸæœ›å€¼ï¼ˆæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­è¿›è¡Œï¼‰ã€‚

2. è´å¶æ–¯æ¨ç†ç†è§£ï¼Œå¯¹äºå°æ ·æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°é‡ç”Ÿéšæœºæ€§ã€‚æˆ‘ä»¬çš„åéªŒåˆ†å¸ƒå°†é€šè¿‡æ›´å¹¿æ³›è€Œä¸æ˜¯ç´§å¯†é›†ä¸­æ¥åæ˜ è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¨è®ºåº”è¯¥æ˜¯å¯çº æ­£çš„ã€‚

3. ä¸è€ƒè™‘æ ·æœ¬å¤§å°æœ‰é‡å¤§å½±å“ï¼Œå°è¯•å¯¹ä¸ç¨³å®šçš„å¯¹è±¡è¿›è¡Œæ’åºä¼šå¯¼è‡´ç—…æ€æ’åºã€‚ä¸Šé¢æä¾›çš„æ–¹æ³•è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚

##### Exercises

1\. How would you estimate the quantity $E\left[ \cos{X} \right]$, where $X \sim \text{Exp}(4)$? What about $E\left[ \cos{X} | X \lt 1\right]$, i.e. the expected value *given* we know $X$ is less than 1? Would you need more samples than the original samples size to be equally accurate?

ä½ å¦‚ä½•ä¼°è®¡$ E \left [\cos {X} \right] $ï¼Œå…¶ä¸­$ X \sim \text {Exp}ï¼ˆ4ï¼‰$ï¼Ÿæˆ–è€…$ E \left [\cos {X} | X \lt 1 \right] $ï¼Œå³é¢„æœŸå€¼*ç»™å®š*æˆ‘ä»¬çŸ¥é“$ X $å°äº1ï¼Ÿæ‚¨æ˜¯å¦éœ€è¦æ¯”åŸå§‹æ ·æœ¬å¤§å°æ›´å¤šçš„æ ·æœ¬æ‰èƒ½åŒæ ·å‡†ç¡®ï¼Ÿ


```python
## Enter code here
import tensorflow as tf
import tensorflow_probability as tfp
tfd = tf.distributions

reset_sess()

exp = tfd.Exponential(rate=4.)
N = 10000
X = exp.sample(sample_shape=int(N))


e_cos_x=evaluate(tf.reduce_sum(tf.math.cos(X)/N))
e_cos_x_le_1=evaluate(tf.reduce_sum(tf.math.cos(X)*tf.cast(X<1,tf.float32)/N))
  
print('ğ¸[cosğ‘‹]',e_cos_x)
print('ğ¸[cosğ‘‹|ğ‘‹<1]',e_cos_x_le_1)
```

    ğ¸[cosğ‘‹] 0.9411032
    ğ¸[cosğ‘‹|ğ‘‹<1] 0.9354135


2\. The following table was located in the paper "Going for Three: Predicting the Likelihood of Field Goal Success with Logistic Regression" [2]. The table ranks football field-goal kickers by their percent of non-misses. What mistake have the researchers made?

-----

####  Kicker Careers Ranked by Make Percentage
<table><tbody><tr><th>Rank </th><th>Kicker </th><th>Make % </th><th>Number  of Kicks</th></tr><tr><td>1 </td><td>Garrett Hartley </td><td>87.7 </td><td>57</td></tr><tr><td>2</td><td> Matt Stover </td><td>86.8 </td><td>335</td></tr><tr><td>3 </td><td>Robbie Gould </td><td>86.2 </td><td>224</td></tr><tr><td>4 </td><td>Rob Bironas </td><td>86.1 </td><td>223</td></tr><tr><td>5</td><td> Shayne Graham </td><td>85.4 </td><td>254</td></tr><tr><td>â€¦ </td><td>â€¦ </td><td>â€¦</td><td> </td></tr><tr><td>51</td><td> Dave Rayner </td><td>72.2 </td><td>90</td></tr><tr><td>52</td><td> Nick Novak </td><td>71.9 </td><td>64</td></tr><tr><td>53 </td><td>Tim Seder </td><td>71.0 </td><td>62</td></tr><tr><td>54 </td><td>Jose Cortez </td><td>70.7</td><td> 75</td></tr><tr><td>55 </td><td>Wade Richey </td><td>66.1</td><td> 56</td></tr></tbody></table>

In August 2013, [a popular post](http://bpodgursky.wordpress.com/2013/08/21/average-income-per-programming-language/) on the average income per programmer of different languages was trending. Here's the summary chart: (reproduced without permission, cause when you lie with stats, you gunna get the hammer). What do you notice about the extremes?

------

#### Average household income by programming language

<table >
 <tr><td>Language</td><td>Average Household Income ($)</td><td>Data Points</td></tr>
 <tr><td>Puppet</td><td>87,589.29</td><td>112</td></tr>
 <tr><td>Haskell</td><td>89,973.82</td><td>191</td></tr>
 <tr><td>PHP</td><td>94,031.19</td><td>978</td></tr>
 <tr><td>CoffeeScript</td><td>94,890.80</td><td>435</td></tr>
 <tr><td>VimL</td><td>94,967.11</td><td>532</td></tr>
 <tr><td>Shell</td><td>96,930.54</td><td>979</td></tr>
 <tr><td>Lua</td><td>96,930.69</td><td>101</td></tr>
 <tr><td>Erlang</td><td>97,306.55</td><td>168</td></tr>
 <tr><td>Clojure</td><td>97,500.00</td><td>269</td></tr>
 <tr><td>Python</td><td>97,578.87</td><td>2314</td></tr>
 <tr><td>JavaScript</td><td>97,598.75</td><td>3443</td></tr>
 <tr><td>Emacs Lisp</td><td>97,774.65</td><td>355</td></tr>
 <tr><td>C#</td><td>97,823.31</td><td>665</td></tr>
 <tr><td>Ruby</td><td>98,238.74</td><td>3242</td></tr>
 <tr><td>C++</td><td>99,147.93</td><td>845</td></tr>
 <tr><td>CSS</td><td>99,881.40</td><td>527</td></tr>
 <tr><td>Perl</td><td>100,295.45</td><td>990</td></tr>
 <tr><td>C</td><td>100,766.51</td><td>2120</td></tr>
 <tr><td>Go</td><td>101,158.01</td><td>231</td></tr>
 <tr><td>Scala</td><td>101,460.91</td><td>243</td></tr>
 <tr><td>ColdFusion</td><td>101,536.70</td><td>109</td></tr>
 <tr><td>Objective-C</td><td>101,801.60</td><td>562</td></tr>
 <tr><td>Groovy</td><td>102,650.86</td><td>116</td></tr>
 <tr><td>Java</td><td>103,179.39</td><td>1402</td></tr>
 <tr><td>XSLT</td><td>106,199.19</td><td>123</td></tr>
 <tr><td>ActionScript</td><td>108,119.47</td><td>113</td></tr>
</table>

### References

1. Wainer, Howard. *The Most Dangerous Equation*. American Scientist, Volume 95.
2. Clarck, Torin K., Aaron W. Johnson, and Alexander J. Stimpson. "Going for Three: Predicting the Likelihood of Field Goal Success with Logistic Regression." (2013): n. page. [Web](http://www.sloansportsconference.com/wp-content/uploads/2013/Going%20for%20Three%20Predicting%20the%20Likelihood%20of%20Field%20Goal%20Success%20with%20Logistic%20Regression.pdf). 20 Feb. 2013.
3. http://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function
