<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2019-07-11">

<title>EM算法与EM路由 – Zheng’s Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-8b4baf804e461d9b72633f0de59a0cac.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7072389654d23eff08f359f9aa0d1ee7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Zheng’s Notes</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#em算法" id="toc-em算法" class="nav-link active" data-scroll-target="#em算法">EM算法</a>
  <ul class="collapse">
  <li><a href="#詹森不等式" id="toc-詹森不等式" class="nav-link" data-scroll-target="#詹森不等式">詹森不等式</a></li>
  <li><a href="#完整信息的最大似然" id="toc-完整信息的最大似然" class="nav-link" data-scroll-target="#完整信息的最大似然">完整信息的最大似然</a>
  <ul class="collapse">
  <li><a href="#使用最小化函数求解似然" id="toc-使用最小化函数求解似然" class="nav-link" data-scroll-target="#使用最小化函数求解似然">使用最小化函数求解似然</a></li>
  </ul></li>
  <li><a href="#当信息缺失时的最大似然" id="toc-当信息缺失时的最大似然" class="nav-link" data-scroll-target="#当信息缺失时的最大似然">当信息缺失时的最大似然</a>
  <ul class="collapse">
  <li><a href="#推导" id="toc-推导" class="nav-link" data-scroll-target="#推导">推导</a></li>
  <li><a href="#例子" id="toc-例子" class="nav-link" data-scroll-target="#例子">例子</a></li>
  <li><a href="#第一种直接的代码" id="toc-第一种直接的代码" class="nav-link" data-scroll-target="#第一种直接的代码">第一种直接的代码</a></li>
  <li><a href="#结果" id="toc-结果" class="nav-link" data-scroll-target="#结果">结果</a></li>
  <li><a href="#我的代码" id="toc-我的代码" class="nav-link" data-scroll-target="#我的代码">我的代码</a></li>
  <li><a href="#结果-1" id="toc-结果-1" class="nav-link" data-scroll-target="#结果-1">结果</a></li>
  </ul></li>
  <li><a href="#混合模型" id="toc-混合模型" class="nav-link" data-scroll-target="#混合模型">混合模型</a>
  <ul class="collapse">
  <li><a href="#k-means" id="toc-k-means" class="nav-link" data-scroll-target="#k-means">K-means</a></li>
  <li><a href="#结果-2" id="toc-结果-2" class="nav-link" data-scroll-target="#结果-2">结果</a></li>
  </ul></li>
  <li><a href="#高斯混合模型" id="toc-高斯混合模型" class="nav-link" data-scroll-target="#高斯混合模型">高斯混合模型</a>
  <ul class="collapse">
  <li><a href="#使用em算法" id="toc-使用em算法" class="nav-link" data-scroll-target="#使用em算法">使用EM算法</a></li>
  <li><a href="#代码" id="toc-代码" class="nav-link" data-scroll-target="#代码">代码</a></li>
  <li><a href="#结果-3" id="toc-结果-3" class="nav-link" data-scroll-target="#结果-3">结果</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#胶囊网络" id="toc-胶囊网络" class="nav-link" data-scroll-target="#胶囊网络">胶囊网络</a>
  <ul class="collapse">
  <li><a href="#矩阵胶囊" id="toc-矩阵胶囊" class="nav-link" data-scroll-target="#矩阵胶囊">矩阵胶囊</a></li>
  <li><a href="#胶囊投票机制" id="toc-胶囊投票机制" class="nav-link" data-scroll-target="#胶囊投票机制">胶囊投票机制</a></li>
  <li><a href="#胶囊分配" id="toc-胶囊分配" class="nav-link" data-scroll-target="#胶囊分配">胶囊分配</a></li>
  <li><a href="#计算胶囊激活和姿态矩阵" id="toc-计算胶囊激活和姿态矩阵" class="nav-link" data-scroll-target="#计算胶囊激活和姿态矩阵">计算胶囊激活和姿态矩阵</a></li>
  <li><a href="#em路由" id="toc-em路由" class="nav-link" data-scroll-target="#em路由">EM路由</a>
  <ul class="collapse">
  <li><a href="#新动态路由1" id="toc-新动态路由1" class="nav-link" data-scroll-target="#新动态路由1">新动态路由1</a></li>
  <li><a href="#新动态路由2" id="toc-新动态路由2" class="nav-link" data-scroll-target="#新动态路由2">新动态路由2</a></li>
  <li><a href="#新动态路由3" id="toc-新动态路由3" class="nav-link" data-scroll-target="#新动态路由3">新动态路由3</a></li>
  </ul></li>
  <li><a href="#新动态路由4" id="toc-新动态路由4" class="nav-link" data-scroll-target="#新动态路由4">新动态路由4</a></li>
  <li><a href="#最终的动态路由" id="toc-最终的动态路由" class="nav-link" data-scroll-target="#最终的动态路由">最终的动态路由</a></li>
  <li><a href="#代码-1" id="toc-代码-1" class="nav-link" data-scroll-target="#代码-1">代码</a>
  <ul class="collapse">
  <li><a href="#论文中的伪代码" id="toc-论文中的伪代码" class="nav-link" data-scroll-target="#论文中的伪代码">论文中的伪代码</a></li>
  <li><a href="#python实现" id="toc-python实现" class="nav-link" data-scroll-target="#python实现">python实现</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#参考" id="toc-参考" class="nav-link" data-scroll-target="#参考">参考</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">EM算法与EM路由</h1>
  <div class="quarto-categories">
    <div class="quarto-category">机器学习</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 11, 2019</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>搞定了yolo，终于可以学习点新的东西了。今天就学习一波胶囊网络中的EM路由。 先推荐一个课程资料，杜克大学的<a href="https://people.duke.edu/~ccc14/sta-663">统计学课程</a>，从python讲到c++，从矩阵计算讲到概率统计，从jit讲到cuda编程。看看人家本科生学的东西。。。</p>
<!--more-->
<section id="em算法" class="level1">
<h1>EM算法</h1>
<section id="詹森不等式" class="level2">
<h2 class="anchored" data-anchor-id="詹森不等式">詹森不等式</h2>
<p>对于一个凸函数<span class="math inline">\(f\)</span>,<span class="math inline">\(E[f(x)]\geq f(E[x])\)</span>,将其反转获得一个凹函数。 如果一个函数<span class="math inline">\(f(x)\)</span>在区间内都有<span class="math inline">\(f''(x)\geq 0\)</span>，那么它是一个凸函数。例如<span class="math inline">\(f(x)=log\ x\)</span>,<span class="math inline">\(f''(x)=-\frac{1}{x^2}\)</span>,说明他在<span class="math inline">\(x\in (0,+ \infty]\)</span>是一个凸函数，詹森不等式的直观说明如下。</p>
<p><img src="em-algm/EMAlgorithm_5_0.png" class="img-fluid"></p>
<p>其实只有当<span class="math inline">\(f(x)\)</span>为常数时，詹森不等式才会相等。</p>
<p>这里使用概率论表述的<span class="math inline">\(E\)</span>，其实际就是积分。也就是说先经过凸函数的期望值必然大于等于期望的凸函数值。</p>
</section>
<section id="完整信息的最大似然" class="level2">
<h2 class="anchored" data-anchor-id="完整信息的最大似然">完整信息的最大似然</h2>
<p>设置一个实验，硬币A向上的概率为<span class="math inline">\(\theta_A\)</span>的，硬币B向上的概率为<span class="math inline">\(\theta_B\)</span>,接着一共做m此实验，每次实验随机选择一个硬币，投掷n次，并记录向下和向下的次数。如果我们记录了每个样本所使用的硬币，那我们就有完整的信息可以估计<span class="math inline">\(\theta_A\)</span>和<span class="math inline">\(\theta_B\)</span>。</p>
<p>假设我们做了5次实验，每次向上的次数记录成向量<span class="math inline">\(x\)</span>，并且使用的硬币顺序为<span class="math inline">\(A,A,B,A,B\)</span>。他的似然函数为： <span class="math display">\[
\begin{aligned}
    L(\theta_A,\theta_B)&amp;= p(x_1; \theta_A)\cdot p(x_2; \theta_A)\cdot p(x_3; \theta_B) \cdot  p(x_4; \theta_A) \cdot p(x_5; \theta_B) \\
    &amp;=B(n,\theta_A).pmf(x_1)\cdot B(n,\theta_A).pmf(x_2)\cdot B(n,\theta_B).pmf(x_3)\cdot B(n,\theta_A).pmf(x_4)\cdot B(n,\theta_B).pmf(x_5)
\end{aligned}
\]</span> 其中二项分布的概率计算方式为： <span class="math display">\[
\begin{aligned}
    \because X &amp; \sim B(n,p)\\
    \therefore  P(X=k) &amp;= \binom{n}{k} p^k (1-p)^{n-k} \\
        &amp;=  C_n^k p^{k}(1-p)^{n-k}
\end{aligned}
\]</span></p>
<p>将似然函数对数化：</p>
<p><span class="math display">\[
\begin{aligned}
    L(\theta_A,\theta_B)=\log p(x_1; \theta_A) + \log p(x_2; \theta_A) +\log p(x_3; \theta_B) + \log p(x_4; \theta_A) +\log p(x_5; \theta_B)
\end{aligned}
\]</span></p>
<p><span class="math inline">\(p(x_i; \theta)\)</span>是二项式分布的概率质量函数，其中<span class="math inline">\(n=m,p=\theta\)</span>。我们会使用<span class="math inline">\(z_i\)</span>来表示第<span class="math inline">\(i\)</span>个硬币的标签。</p>
<section id="使用最小化函数求解似然" class="level3">
<h3 class="anchored" data-anchor-id="使用最小化函数求解似然">使用最小化函数求解似然</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.core.umath_tests <span class="im">import</span> matrix_multiply <span class="im">as</span> mm</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli, binom</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1234</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">""" 做五次实验 """</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span>  <span class="co"># 每次实验投掷10次</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>theta_A <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>theta_B <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>theta_0 <span class="op">=</span> [theta_A, theta_B]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 两个硬币对应两个不同thta的二项分布</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>coin_A <span class="op">=</span> bernoulli(theta_A)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>coin_B <span class="op">=</span> bernoulli(theta_B)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># 代表使用的硬币为哪个</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 得到实验结果</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.array([np.<span class="bu">sum</span>(coin.rvs(n)) <span class="cf">for</span> coin <span class="kw">in</span> [coin_A, coin_A, coin_B, coin_A, coin_B]])</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(xs) <span class="co"># [7 9 2 6 0]</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">""" 精确求解 """</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算所有的硬币A朝上的比例作为概率</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>ml_A <span class="op">=</span> np.<span class="bu">sum</span>(xs[[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>]]) <span class="op">/</span> (<span class="fl">3.0</span> <span class="op">*</span> n)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算所有的硬币B朝上的比例作为概率</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>ml_B <span class="op">=</span> np.<span class="bu">sum</span>(xs[[<span class="dv">2</span>, <span class="dv">4</span>]]) <span class="op">/</span> (<span class="fl">2.0</span> <span class="op">*</span> n)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ml_A, ml_B)  <span class="co"># 0.7333333333333333 0.1</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">""" 数值估计 """</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_loglik(thetas, n, xs, zs):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" 对数似然计算函数 </span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co">        这里的的二项分布是次数为n，概率可能为 theta_A 或 theta_B。</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co">        将每个二项分布对应x的对数概率密度函数求和后取相反数，接下来就是最小化此函数即可。</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    logpmf <span class="op">=</span> <span class="op">-</span>np.<span class="bu">sum</span>([binom(n, thetas[z]).logpmf(x) <span class="cf">for</span> (x, z) <span class="kw">in</span> <span class="bu">zip</span>(xs, zs)])</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logpmf</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>bnds <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">0</span>, <span class="dv">1</span>)]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用优化策略进行最小化求解</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> minimize(neg_loglik, [<span class="fl">0.5</span>, <span class="fl">0.5</span>], args<span class="op">=</span>(n, xs, zs),</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>               bounds<span class="op">=</span>bnds, method<span class="op">=</span><span class="st">'tnc'</span>, options<span class="op">=</span>{<span class="st">'maxiter'</span>: <span class="dv">100</span>})</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(res)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="co">""" fun: 7.655267754139319</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="co">     jac: array([-7.31859018e-05, -7.58504370e-05])</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="co"> message: 'Converged (|f_n-f_(n-1)| ~= 0)'</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co">    nfev: 17</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co">     nit: 6</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co">  status: 1</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"> success: True</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="co">       x: array([0.73333285, 0.09999965]) """</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>可以发现似然估计的估计值相当近似与精确计算所得到的结论。</p>
</section>
</section>
<section id="当信息缺失时的最大似然" class="level2">
<h2 class="anchored" data-anchor-id="当信息缺失时的最大似然">当信息缺失时的最大似然</h2>
<p>当我们没有记录实验所使用的硬币种类时，这个问题的解决就开始困难起来了。有一种解决问题的方式就是我们根据这个样本是由<span class="math inline">\(A\)</span>或<span class="math inline">\(B\)</span>产生的来给每个样本假设权重<span class="math inline">\(\boldsymbol{w}_i\)</span>。直觉上来想，权重应该是<span class="math inline">\(\boldsymbol{z}_i\)</span>的后验分布： <span class="math display">\[
\begin{aligned}
    w_i = p(z_i \ | \ x_i; \theta)
\end{aligned}
\]</span></p>
<p>假设我们有<span class="math inline">\(\theta\)</span>的一些估计值，如果我们知道<span class="math inline">\(z_i\)</span>，那我们就可以估计出<span class="math inline">\(\theta\)</span>，因为我们相当于拥有了全部的似然信息。EM算法的基本思想就是先猜测<span class="math inline">\(\theta\)</span>，然后计算出<span class="math inline">\(z_i\)</span>，接着继续更新<span class="math inline">\(\theta\)</span>再计算<span class="math inline">\(z_i\)</span>，重复数次直到收敛。</p>
<p>非书面的表述：首先考虑一个对数似然函数作为曲线(曲面)，他的<span class="math inline">\(x\)</span>轴表示<span class="math inline">\(\theta\)</span>。找到另一个<span class="math inline">\(\theta\)</span>的函数<span class="math inline">\(\boldsymbol{Q}\)</span>，他是对数似然函数的下界，在特定的<span class="math inline">\(\theta\)</span>值时对数似然函数与函数<span class="math inline">\(\boldsymbol{Q}\)</span>接触。接下来找到这个<span class="math inline">\(\theta\)</span>的值，并且使函数<span class="math inline">\(\boldsymbol{Q}\)</span>最大化，重复这个过程，是下界函数<span class="math inline">\(\boldsymbol{Q}\)</span>和对数似然函数的最大值相同，那么我们就得到了最大对数似然～</p>
<p>从下图可以理解，每次迭代都能找到新的下界函数<span class="math inline">\(\boldsymbol{Q}\)</span>和当前最大的对数似然，等到迭代到一定程度时，就找到了全局的最大的对数似然。</p>
<p><img src="em-algm/14_ExpectationMaximization_21_0.png" class="img-fluid"></p>
<p>当然，还有个问题就是如何找到这个对数似然函数的下界函数<span class="math inline">\(\boldsymbol{Q}\)</span>，这就需要使用詹森不等式来进行数学推理。</p>
<section id="推导" class="level3">
<h3 class="anchored" data-anchor-id="推导">推导</h3>
<p>在EM算法的<code>E-step</code>中，我们确定一个函数，他是对数似然的下界。 <span class="math display">\[
\begin{align}
l &amp;= \sum_i{\log p(x_i; \theta)} &amp;&amp; \text{定义对数似然函数} \\
&amp;= \sum_i \log \sum_{z_i}{p(x_i, z_i; \theta)} &amp;&amp; \text{在函数中加入隐变量$z$} \\
&amp;= \sum_i \log \sum_{z_i} Q_i(z_i) \frac{p(x_i, z_i; \theta)}{Q_i(z_i)} &amp;&amp; \text{贝叶斯定理得$Q_i$为$z_i$分布} \\
&amp;= \sum_i \log E_{z_i}[\frac{p(x_i, z_i; \theta)}{Q_i(z_i)}] &amp;&amp; \text{得到期望 - 就是EM中的E} \\
&amp;\geq \sum E_{z_i}[\log \frac{p(x_i, z_i; \theta)}{Q_i(z_i)}] &amp;&amp; \text{使用詹森不等式计算凹的对数似然函数} \\
&amp;\geq \sum_i \sum_{z_i} Q_i(z_i) \log \frac{p(x_i, z_i; \theta)}{Q_i(z_i)} &amp;&amp; \text{得到期望的定义}
\end{align}
\]</span></p>
<p>我们如何确定分布<span class="math inline">\(\boldsymbol{Q_i}\)</span>？我们想要<span class="math inline">\(\boldsymbol{Q}\)</span>函数与对数似然函数进行接触，并且也知道了詹森不等式相等的充要条件就是这个函数为常数，因此：</p>
<p><span class="math display">\[
\begin{align}
\frac{p(x_i, z_i; \theta)}{Q_i(z_i)} =&amp; c \\
\implies Q_i(z_i) &amp;\propto p(x_i, z_i; \theta)\\
\implies Q_i(z_i) &amp;= \frac{p(x_i, z_i; \theta) }{\sum_{z_i}{p(x_i, z_i; \theta) } } &amp;&amp;\text{因为 $\boldsymbol{Q}$ 是个分布且和为1} \\
\implies Q_i(z_i) &amp;= \frac{p(x_i, z_i; \theta) }{ {p(x_i, \theta) } } &amp;&amp; \text{边缘化 $z_i$}\\
\implies Q_i(z_i) &amp;= p(z_i | x_i; \theta) &amp;&amp; \text{根据条件概率定义}
\end{align}
\]</span></p>
<p>因此得到<span class="math inline">\(\boldsymbol{Q_i}\)</span>就是<span class="math inline">\(z_i\)</span>的后验概率，这就完成了<code>E-step</code>。</p>
<p>在<code>M-step</code>中，我找到最大化对数似然函数下界时的<span class="math inline">\(\theta\)</span>值，然后我们迭代<code>E</code>和<code>M</code>即可。</p>
<p>所以EM算法是在缺少信息的情况下最大化似然的优化算法，或者说他可以很方便的添加隐变量来简化最大似然计算。</p>
</section>
<section id="例子" class="level3">
<h3 class="anchored" data-anchor-id="例子">例子</h3>
<p>现在如果我们忘记记录了投掷硬币的顺序，那我们来求解一波A、B硬币向上的概率。 对于<code>E-step</code>,我们：</p>
<p><span class="math display">\[
\begin{align}
w_j &amp;= Q_i(z_i = j) \\
&amp;= p(z_i = j \mid x_i; \theta) \\
&amp;= \frac{p(x_i \mid z_i = j; \theta) p(z_i = j; \phi)}  {\sum_{l=1}^k{p(x_i \mid z_i = l; \theta) p(z_i = l; \phi) } }  &amp;&amp; \text{贝叶斯准则} \\
&amp;= \frac{\theta_j^h(1-\theta_j)^{n-h} \phi_j}{\sum_{l=1}^k \theta_l^h(1-\theta_l)^{n-h} \phi_l} &amp;&amp; \text{代入二项分布公式} \\
&amp;= \frac{\theta_j^h(1-\theta_j)^{n-h} }{\sum_{l=1}^k \theta_l^h(1-\theta_l)^{n-h} } &amp;&amp; \text{为了简单起见使 $\phi$ 为常数}
\end{align}
\]</span></p>
<p>对于<code>M-step</code>,我们需要找到最大时的<span class="math inline">\(\theta\)</span>值。</p>
<p><span class="math display">\[
\begin{align}
&amp; \sum_i \sum_{z_i} Q_i(z_i) \log \frac{p(x_i, z_i; \theta)}{Q_i(z_i)} \\
&amp;= \sum_{i=1}^m \sum_{j=1}^k w_j \log \frac{p(x_i \mid z_i=j; \theta) \, p(z_i = j; \phi)}{w_j} \\
&amp;= \sum_{i=1}^m \sum_{j=1}^k w_j \log \frac{\theta_j^h(1-\theta_j)^{n-h} \phi_j}{w_j} \\
&amp;= \sum_{i=1}^m \sum_{j=1}^k w_j \left( h \log \theta_j + (n-h) \log (1-\theta_j) + \log \phi_j - \log w_j \right)
\end{align}
\]</span></p>
<p>我们使用区间的方式解决<span class="math inline">\(\theta_s\)</span>导数消失的问题： <span class="math display">\[
\begin{align}
\sum_{i=1}^m w_s \left( \frac{h}{\theta_s} - \frac{n-h}{1-\theta_s} \right) &amp;= 0  \\
\implies \theta_s &amp;= \frac {\sum_{i=1}^m w_s h}{\sum_{i=1}^m w_s n}
\end{align}
\]</span></p>
</section>
<section id="第一种直接的代码" class="level3">
<h3 class="anchored" data-anchor-id="第一种直接的代码">第一种直接的代码</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.array([(<span class="dv">5</span>, <span class="dv">5</span>), (<span class="dv">9</span>, <span class="dv">1</span>), (<span class="dv">8</span>, <span class="dv">2</span>), (<span class="dv">4</span>, <span class="dv">6</span>), (<span class="dv">7</span>, <span class="dv">3</span>)])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.array([[<span class="fl">0.6</span>, <span class="fl">0.4</span>], [<span class="fl">0.5</span>, <span class="fl">0.5</span>]])  <span class="co"># 初始化参数A B (向上概率，向下概率)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>tol <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># 变化容忍度</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>max_iter <span class="op">=</span> <span class="dv">100</span>  <span class="co"># 迭代次数</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>ll_old <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    exp_A <span class="op">=</span> []</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    exp_B <span class="op">=</span> []</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    ll_new <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ! E-step: 计算可能的概率分布</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> xs:</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 求解当前theta下两个分布的对数似然</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        ll_A <span class="op">=</span> np.<span class="bu">sum</span>(x <span class="op">*</span> np.log(thetas[<span class="dv">0</span>]))  <span class="co"># 多项式分布的对数似然函数 (忽略常数).</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        ll_B <span class="op">=</span> np.<span class="bu">sum</span>(x <span class="op">*</span> np.log(thetas[<span class="dv">1</span>]))</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        w_A <span class="op">=</span> np.exp(ll_A) <span class="op">/</span> (np.exp(ll_A) <span class="op">+</span> np.exp(ll_B))  <span class="co"># 求出概率A的权重</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        w_B <span class="op">=</span> np.exp(ll_B) <span class="op">/</span> (np.exp(ll_A) <span class="op">+</span> np.exp(ll_B))  <span class="co"># 求出概率B的权重</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        exp_A.append(w_A <span class="op">*</span> x)  <span class="co"># 概率A权重乘上样本</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        exp_B.append(w_B <span class="op">*</span> x)  <span class="co"># 概率B权重乘上样本</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        ll_new <span class="op">+=</span> w_A <span class="op">*</span> ll_A <span class="op">+</span> w_B <span class="op">*</span> ll_B  <span class="co"># 计算当前的theta值对应的似然值</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ! M-step: 为给定的分布更新当前参数</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    thetas[<span class="dv">0</span>] <span class="op">=</span> np.<span class="bu">sum</span>(exp_A, <span class="dv">0</span>) <span class="op">/</span> np.<span class="bu">sum</span>(exp_A)  <span class="co"># 利用更新之后的样本值计算出当前A的theta值</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    thetas[<span class="dv">1</span>] <span class="op">=</span> np.<span class="bu">sum</span>(exp_B, <span class="dv">0</span>) <span class="op">/</span> np.<span class="bu">sum</span>(exp_B)  <span class="co"># 利用更新之后的样本值计算出当前B的theta值</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输出每个x和当前参数估计z的分布</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Iteration: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> (i <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"theta_A = </span><span class="sc">%.2f</span><span class="st">, theta_B = </span><span class="sc">%.2f</span><span class="st">, ll = </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (thetas[<span class="dv">0</span>, <span class="dv">0</span>], thetas[<span class="dv">1</span>, <span class="dv">0</span>], ll_new))</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.<span class="bu">abs</span>(ll_new <span class="op">-</span> ll_old) <span class="op">&lt;</span> tol:</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    ll_old <span class="op">=</span> ll_new</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="结果" class="level3">
<h3 class="anchored" data-anchor-id="结果">结果</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 1</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.71, theta_B = 0.58, ll = <span class="at">-32.69</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 2</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.75, theta_B = 0.57, ll = <span class="at">-31.26</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 3</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.77, theta_B = 0.55, ll = <span class="at">-30.76</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 4</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.78, theta_B = 0.53, ll = <span class="at">-30.33</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 5</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.79, theta_B = 0.53, ll = <span class="at">-30.07</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 6</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.79, theta_B = 0.52, ll = <span class="at">-29.95</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 7</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.80, theta_B = 0.52, ll = <span class="at">-29.90</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 8</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.80, theta_B = 0.52, ll = <span class="at">-29.88</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 9</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.80, theta_B = 0.52, ll = <span class="at">-29.87</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>NOTE：</strong> 他这里的对数似然函数是根据上面的公式推导之后简化而来的，所以直接对<span class="math inline">\(\theta\)</span>做对数即可，下面我按最基本的思路来写了一个例程。</p>
</section>
<section id="我的代码" class="level3">
<h3 class="anchored" data-anchor-id="我的代码">我的代码</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span>  <span class="co"># 实验次数</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>m_xs <span class="op">=</span> np.array([<span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">7</span>])  <span class="co"># 向上次数</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>theta_A <span class="op">=</span> <span class="fl">0.6</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>theta_B <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>tol <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># 变化容忍度</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>max_iter <span class="op">=</span> <span class="dv">100</span>  <span class="co"># 迭代次数</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>loglike_old <span class="op">=</span> <span class="dv">0</span>  <span class="co"># 初始对数似然值</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    cnt_A <span class="op">=</span> []</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    cnt_B <span class="op">=</span> []</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    loglike_new <span class="op">=</span> <span class="dv">0</span>  <span class="co"># 新的对数似然值</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ! E-step</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> m_xs:</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        pmf_A <span class="op">=</span> binom(n, theta_A).pmf(x)  <span class="co"># 当前theta下 A的概率</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        pmf_B <span class="op">=</span> binom(n, theta_B).pmf(x)  <span class="co"># 当前theta下 B的概率</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        logpmf_A <span class="op">=</span> binom(n, theta_A).logpmf(x)  <span class="co"># 当前theta下 A的对数概率</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        logpmf_B <span class="op">=</span> binom(n, theta_B).logpmf(x)  <span class="co"># 当前theta下 B的对数概率</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        weight_A <span class="op">=</span> pmf_A <span class="op">/</span> (pmf_A <span class="op">+</span> pmf_B)  <span class="co"># 求得权重</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        weight_B <span class="op">=</span> pmf_B <span class="op">/</span> (pmf_A <span class="op">+</span> pmf_B)  <span class="co"># 求得权重</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        cnt_A.append(weight_A <span class="op">*</span> np.array([x, n <span class="op">-</span> x]))  <span class="co"># 概率A权重乘上样本，得到新的硬币次数统计</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        cnt_B.append(weight_B <span class="op">*</span> np.array([x, n <span class="op">-</span> x]))  <span class="co"># 概率B权重乘上样本，得到新的硬币次数统计</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        loglike_new <span class="op">+=</span> weight_A <span class="op">*</span> logpmf_A <span class="op">+</span> weight_B <span class="op">*</span> logpmf_B  <span class="co"># 计算当前的theta值对应的似然值</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ! M-step</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    theta_A <span class="op">=</span> np.<span class="bu">sum</span>(cnt_A, <span class="dv">0</span>)[<span class="dv">0</span>] <span class="op">/</span> np.<span class="bu">sum</span>(cnt_A)  <span class="co"># 硬币A向上的次数除以总次数即为theta_A</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    theta_B <span class="op">=</span> np.<span class="bu">sum</span>(cnt_B, <span class="dv">0</span>)[<span class="dv">0</span>] <span class="op">/</span> np.<span class="bu">sum</span>(cnt_B)  <span class="co"># 硬币B向上的次数除以总次数即为theta_B</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输出每个x和当前参数估计z的分布</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Iteration: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> (i <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"theta_A = </span><span class="sc">%.2f</span><span class="st">, theta_B = </span><span class="sc">%.2f</span><span class="st">, ll = </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (theta_A, theta_B, loglike_new))</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.<span class="bu">abs</span>(loglike_new <span class="op">-</span> loglike_old) <span class="op">&lt;</span> tol:</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    loglike_old <span class="op">=</span> loglike_new</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="结果-1" class="level3">
<h3 class="anchored" data-anchor-id="结果-1">结果</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 1</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.71, theta_B = 0.58, ll = <span class="at">-10.91</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 2</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.75, theta_B = 0.57, ll = <span class="at">-9.49</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 3</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.77, theta_B = 0.55, ll = <span class="at">-8.99</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 4</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.78, theta_B = 0.53, ll = <span class="at">-8.56</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 5</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.79, theta_B = 0.53, ll = <span class="at">-8.30</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 6</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.79, theta_B = 0.52, ll = <span class="at">-8.18</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 7</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.80, theta_B = 0.52, ll = <span class="at">-8.13</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 8</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.80, theta_B = 0.52, ll = <span class="at">-8.11</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="ex">Iteration:</span> 9</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="ex">theta_A</span> = 0.80, theta_B = 0.52, ll = <span class="at">-8.10</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>NOTE：</strong> 就是对数似然值和例程计算的不一样。。</p>
</section>
</section>
<section id="混合模型" class="level2">
<h2 class="anchored" data-anchor-id="混合模型">混合模型</h2>
<p>从一个简单的混合模型开始，即<code>k-means</code>，<code>k-means</code>不使用<code>EM</code>算法，但是可以结合<code>EM</code>算法帮助理解<code>EM</code>算法如何用于高斯混合模型。</p>
<section id="k-means" class="level3">
<h3 class="anchored" data-anchor-id="k-means">K-means</h3>
<p>这个算法比较简单，初始化选择<span class="math inline">\(k\)</span>个中心点，然后做如下：</p>
<ol type="1">
<li>找到每个点与中心点的距离</li>
<li>给每个点分配最近的中心点</li>
<li>根据所分配的点来更新中心点位置</li>
</ol>
<p>下面给出一段程序示例：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.core.umath_tests <span class="im">import</span> inner1d</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kmeans(xs, k, max_iter<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""K-means 算法."""</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.random.choice(<span class="bu">len</span>(xs), k, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    cs <span class="op">=</span> xs[idx]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        ds <span class="op">=</span> np.array([inner1d(xs <span class="op">-</span> c, xs <span class="op">-</span> c) <span class="cf">for</span> c <span class="kw">in</span> cs])</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        zs <span class="op">=</span> np.argmin(ds, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        cs <span class="op">=</span> np.array([xs[zs <span class="op">==</span> i].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)])</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (cs, zs)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> sns.load_dataset(<span class="st">'iris'</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> iris.iloc[:, :<span class="dv">4</span>].values</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>cs, zs <span class="op">=</span> kmeans(data, <span class="dv">3</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>iris[<span class="st">'cluster'</span>] <span class="op">=</span> zs</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>sns.pairplot(iris, hue<span class="op">=</span><span class="st">'cluster'</span>, diag_kind<span class="op">=</span><span class="st">'kde'</span>, <span class="bu">vars</span><span class="op">=</span>iris.columns[:<span class="dv">4</span>])</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="结果-2" class="level3">
<h3 class="anchored" data-anchor-id="结果-2">结果</h3>
<p><img src="em-algm/kmeans.png" class="img-fluid"></p>
</section>
</section>
<section id="高斯混合模型" class="level2">
<h2 class="anchored" data-anchor-id="高斯混合模型">高斯混合模型</h2>
<p><span class="math inline">\(k\)</span>个高斯分布混合具有以下的概率密度函数： <span class="math display">\[
\begin{align}
p(x) = \sum_{j=1}^k \pi_j \phi(x; \mu_j, \sigma_j)
\end{align}
\]</span></p>
<p><span class="math inline">\(\pi_j\)</span>是第<span class="math inline">\(j\)</span>个高斯分布的权重，且：</p>
<p><span class="math display">\[
\begin{align}
\phi(x; \mu, \sigma) = \frac{1}{(2 \pi)^{d/2}|\sigma|^{1/2 } } \exp \left( -\frac{1}{2}(x-\mu)^T\sigma^{-1}(x-\mu) \right)
\end{align}
\]</span></p>
<p>假设我们观察<span class="math inline">\(y_1, y2, \ldots, y_n\)</span>作为来自高斯混合模型的样本，那么对数似然为： <span class="math display">\[
\begin{align}
l(\theta) = \sum_{i=1}^n \log \left( \sum_{j=1}^k \pi_j \phi(y_i; \mu_j, \sigma_j) \right)
\end{align}
\]</span></p>
<p>其中<span class="math inline">\(\theta = (\pi, \mu, \sigma)\)</span>，很难拟合这种对数似然最大值的参数，因为他们要在对数函数中求和。</p>
<section id="使用em算法" class="level3">
<h3 class="anchored" data-anchor-id="使用em算法">使用EM算法</h3>
<p>假设我们增加潜在变量<span class="math inline">\(z\)</span>，它表明我们观察到的<span class="math inline">\(y\)</span>来自于第<span class="math inline">\(k\)</span>个高斯分布。其中<code>E-step</code>和<code>M-step</code>的推导与之前的例子类似，只是变量更多。</p>
<p>在<code>E-step</code>我们想要计算样本<span class="math inline">\(x_i\)</span>输入第<span class="math inline">\(j\)</span>个类别的后验概率，给定参数<span class="math inline">\(\theta =(\pi,\mu,\sigma)\)</span></p>
<p><strong>NOTE：</strong> 这里后验概率有的地方使用公式<span class="math inline">\(p(j|x_i)\)</span>,这里使用<span class="math inline">\(w_j^i\)</span>来表示。</p>
<p><span class="math display">\[
\begin{align}
w_j^i &amp;= Q_i(z^i = j) \\
&amp;= p(z^i = j \mid y^i; \theta) \\
&amp;= \frac{p(x^i \mid z^i = j; \mu, \sigma) p(z^i = j; \pi)}  {\sum_{l=1}^k{p(y^i \mid z^i = l; \mu, \sigma) p(z^i = l; \pi) } }  &amp;&amp; \text{贝叶斯定理} \\
&amp;= \frac{\phi(x^i; \mu_j, \sigma_j) \pi_j}{\sum_{l=1}^k \phi(x^i; \mu_l, \sigma_l) \pi_l}
\end{align}
\]</span></p>
<p>在<code>M-step</code>中，我们要找到<span class="math inline">\(\theta = (w, \mu, \sigma)\)</span>来最大化<span class="math inline">\(\boldsymbol{Q}\)</span>函数，对应与真实对数似然函数的下界。</p>
<p><span class="math display">\[
\begin{align}
\sum_{i=1}^{m}\sum_{j=1}^{k} Q(z^i=j) \log \frac{p(x^i \mid z^i= j; \mu, \sigma) p(z^i=j; \pi)}{Q(z^i=j)}
\end{align}
\]</span></p>
<p>通过分别取<span class="math inline">\((w, \mu, \sigma)\)</span>的导数并求解（使用拉格朗日乘数构造约束<span class="math inline">\(\sum_{j=1}^k w_j = 1\)</span>来求解），我们得到：</p>
<p><span class="math display">\[
\begin{align}
\pi_j &amp;= \frac{1}{m} \sum_{i=1}^{m} w_j^i \\
\mu_j &amp;= \frac{\sum_{i=1}^{m} w_j^i x^i}{\sum_{i=1}^{m} w_j^i} \\
\sigma_j &amp;= \frac{\sum_{i=1}^{m} w_j^i (x^i - \mu)(x^i - \mu)^T}{\sum_{i1}^{m} w_j^i}
\end{align}
\]</span></p>
</section>
<section id="代码" class="level3">
<h3 class="anchored" data-anchor-id="代码">代码</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize(xs, axis<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return normalized marirx so that sum of row or column (default) entries = 1."""</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> axis <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> xs <span class="op">/</span> xs.<span class="bu">sum</span>()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> axis <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> xs <span class="op">/</span> xs.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> xs <span class="op">/</span> xs.<span class="bu">sum</span>(<span class="dv">1</span>)[:, <span class="va">None</span>]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mix_mvn_pdf(xs, pis, mus, sigmas):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([pi <span class="op">*</span> multivariate_normal(mu, sigma).pdf(xs) <span class="cf">for</span> (pi, mu, sigma) <span class="kw">in</span> <span class="bu">zip</span>(pis, mus, sigmas)])</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> em_gmm_orig(xs, pis, mus, sigmas, tol<span class="op">=</span><span class="fl">0.01</span>, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    n, p <span class="op">=</span> xs.shape</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> <span class="bu">len</span>(pis)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    ll_old <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        exp_A <span class="op">=</span> []</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        exp_B <span class="op">=</span> []</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        ll_new <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ！ E-step</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        ws <span class="op">=</span> np.zeros((k, n))</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mus)):</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 遍历所有的 mu，sigma，pi 来计算概率密度</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                ws[j, i] <span class="op">=</span> pis[j] <span class="op">*</span> multivariate_normal(mus[j], sigmas[j]).pdf(xs[i])</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        ws <span class="op">/=</span> ws.<span class="bu">sum</span>(<span class="dv">0</span>)  <span class="co"># 根据概率密度求权值</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># M-step</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co"> 下面的更新过程是根据公式来计算的！</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        pis <span class="op">=</span> np.zeros(k)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mus)):</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>                pis[j] <span class="op">+=</span> ws[j, i]  <span class="co"># 使用权值更新pi</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        pis <span class="op">/=</span> n</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>        mus <span class="op">=</span> np.zeros((k, p))</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>                mus[j] <span class="op">+=</span> ws[j, i] <span class="op">*</span> xs[i]  <span class="co"># 使用权值更新mu</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>            mus[j] <span class="op">/=</span> ws[j, :].<span class="bu">sum</span>()</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        sigmas <span class="op">=</span> np.zeros((k, p, p))</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>                ys <span class="op">=</span> np.reshape(xs[i] <span class="op">-</span> mus[j], (<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>                sigmas[j] <span class="op">+=</span> ws[j, i] <span class="op">*</span> np.dot(ys, ys.T)  <span class="co"># 使用权值更新sigma</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>            sigmas[j] <span class="op">/=</span> ws[j, :].<span class="bu">sum</span>()</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 更新对数似然函数</span></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        ll_new <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>            s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>                s <span class="op">+=</span> pis[j] <span class="op">*</span> multivariate_normal(mus[j], sigmas[j]).pdf(xs[i])</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>            ll_new <span class="op">+=</span> np.log(s)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">abs</span>(ll_new <span class="op">-</span> ll_old) <span class="op">&lt;</span> tol:</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        ll_old <span class="op">=</span> ll_new</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ll_new, pis, mus, sigmas</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a><span class="co"># 构建数据集</span></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>_mus <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">4</span>], [<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>]])</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>_sigmas <span class="op">=</span> np.array([[[<span class="dv">3</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="fl">0.5</span>]], [[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]]])</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>_pis <span class="op">=</span> np.array([<span class="fl">0.6</span>, <span class="fl">0.4</span>])</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.concatenate([np.random.multivariate_normal(mu, sigma, <span class="bu">int</span>(pi <span class="op">*</span> n))</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">for</span> pi, mu, sigma <span class="kw">in</span> <span class="bu">zip</span>(_pis, _mus, _sigmas)])</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a><span class="co"># 初始化预测值</span></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>pis <span class="op">=</span> normalize(np.random.random(<span class="dv">2</span>))  <span class="co"># pi 要经过归一化</span></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> np.random.random((<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>sigmas <span class="op">=</span> np.array([np.eye(<span class="dv">2</span>)] <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用EM算法拟合</span></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>ll1, pis1, mus1, sigmas1 <span class="op">=</span> em_gmm_orig(xs, pis, mus, sigmas)</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘图</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>intervals <span class="op">=</span> <span class="dv">101</span></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">8</span>, <span class="dv">8</span>, intervals)</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(ys, ys)</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>_ys <span class="op">=</span> np.vstack([X.ravel(), Y.ravel()]).T</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.zeros(<span class="bu">len</span>(_ys))</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pi, mu, sigma <span class="kw">in</span> <span class="bu">zip</span>(pis1, mus1, sigmas1):</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>    z <span class="op">+=</span> pi <span class="op">*</span> multivariate_normal(mu, sigma).pdf(_ys)</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> z.reshape((intervals, intervals))</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.subplot(<span class="dv">111</span>)</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>plt.scatter(xs[:, <span class="dv">0</span>], xs[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>plt.contour(X, Y, z, N<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">8</span>, <span class="dv">6</span>, <span class="op">-</span><span class="dv">6</span>, <span class="dv">8</span>])</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>ax.axes.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="结果-3" class="level3">
<h3 class="anchored" data-anchor-id="结果-3">结果</h3>
<p><img src="em-algm/mxiguass.png" class="img-fluid"></p>
</section>
</section>
</section>
<section id="胶囊网络" class="level1">
<h1>胶囊网络</h1>
<section id="矩阵胶囊" class="level2">
<h2 class="anchored" data-anchor-id="矩阵胶囊">矩阵胶囊</h2>
<p>现在的矩阵胶囊与之前的胶囊不是很一样了，因为胶囊是矩阵，就不能把模长作为激活了。所以多加一个标量<span class="math inline">\(a\)</span>来表示激活值。</p>
<p>矩阵胶囊的主体，名字也变了一下，叫做姿态矩阵，这里是<span class="math inline">\(4\times4\)</span>的矩阵。</p>
<p><img src="em-algm/capp.png" class="img-fluid"></p>
</section>
<section id="胶囊投票机制" class="level2">
<h2 class="anchored" data-anchor-id="胶囊投票机制">胶囊投票机制</h2>
<p>在胶囊网络中，通过寻找底层胶囊投票之间的一致性来检测一个高层特征(比如一张脸)，对于来自胶囊<span class="math inline">\(i\)</span>对父胶囊<span class="math inline">\(j\)</span>的投票<span class="math inline">\(\boldsymbol{V}_{ij}\)</span>，通过将胶囊<span class="math inline">\(i\)</span>的姿态矩阵<span class="math inline">\(\boldsymbol{P}_i\)</span>和视角不变矩阵<span class="math inline">\(W_{ij}\)</span>相乘得到。</p>
<p><span class="math display">\[
\begin{split}
&amp;\boldsymbol{V}_{ij} =\boldsymbol{P}_i W_{ij} \quad
\end{split}
\]</span></p>
<p>基于投票<span class="math inline">\(\boldsymbol{V}_{ij}\)</span>和其他投票<span class="math inline">\((\boldsymbol{V}_{o_{1}j} \ldots \boldsymbol{V}_{o_{k}j})\)</span>相似度来计算胶囊<span class="math inline">\(i\)</span>被分组到胶囊<span class="math inline">\(j\)</span>中的概率。</p>
</section>
<section id="胶囊分配" class="level2">
<h2 class="anchored" data-anchor-id="胶囊分配">胶囊分配</h2>
<p>EM算法对低层的胶囊进行聚类，也计算底层胶囊与上层胶囊间分配概率<span class="math inline">\(r_{ij}\)</span>。例如一个表示手的胶囊不属于脸的胶囊，他们的分配概率就被抑止到0，<span class="math inline">\(r_{ij}\)</span>也受到胶囊激活的影响。</p>
<p><img src="em-algm/c2.jpg" class="img-fluid"></p>
</section>
<section id="计算胶囊激活和姿态矩阵" class="level2">
<h2 class="anchored" data-anchor-id="计算胶囊激活和姿态矩阵">计算胶囊激活和姿态矩阵</h2>
<p>在EM聚类中，我们将数据使用正态分布来表现。在EM路由中，我们也对父胶囊使用正态分布来建模，姿态矩阵<span class="math inline">\(M\)</span>是<span class="math inline">\(4\times4\)</span>的矩阵，即16个分量。我们对姿态矩阵建立16个<span class="math inline">\(\mu\)</span>,16个<span class="math inline">\(\sigma\)</span>的高斯分布，每个<span class="math inline">\(\mu\)</span>就表征了姿态矩阵的每个分量。注意为了避免在概率密度函数中求逆，胶囊网络中的<span class="math inline">\(\sigma\)</span>是被固定成一个对角矩阵的。</p>
<p>让<span class="math inline">\(v_{ij}\)</span>作为底层胶囊<span class="math inline">\(i\)</span>对父胶囊<span class="math inline">\(j\)</span>的投票，<span class="math inline">\(v^{n}_{ij}\)</span>代表着第<code>n</code>个分量。我们使用高斯概率密度函数</p>
<p><span class="math display">\[
\begin{split}
P(x) &amp; = \frac{1}{\sigma \sqrt{2\pi } }e^{-(x - \mu)^{2}/2\sigma^{2} } \\
\end{split}
\]</span></p>
<p>来计算属于胶囊<span class="math inline">\(j\)</span>正态分布的投票<span class="math inline">\(v^{n}_{ij}\)</span>概率。 <span class="math display">\[
\begin{split}
p^n_{i \vert j} &amp; = \frac{1}{\sqrt{2 \pi ({\sigma^n_j})^2 } } \exp{(- \frac{(v^n_{ij}-\mu^n_j)^2}{2 ({\sigma^n_j})^2})} \\
\end{split}
\]</span></p>
<p>取其对数：</p>
<p><span class="math display">\[
\begin{split}
\ln(p^n_{i \vert j}) &amp;= \ln \frac{1}{\sqrt{2 \pi ({\sigma^n_j})^2 } } \exp{(- \frac{(v^n_{ij}-\mu^n_j)^2}{2 ({\sigma^n_j})^2})} \\
\\
&amp;= - \ln(\sigma^n_j) - \frac{\ln(2 \pi)}{2} - \frac{(v^n_{ij}-\mu^n_j)^2}{2 ({\sigma^n_j})^2}\\
\end{split}
\]</span></p>
<p>接下来估计胶囊激活的损失值，越低的损失值，代表胶囊越有可能被激活。如果损失值较大，就代表投票与父胶囊的高斯分布不匹配，因此激活概率较小。</p>
<p>让<span class="math inline">\(cost_{ij}\)</span>作为胶囊<span class="math inline">\(i\)</span>激活胶囊<span class="math inline">\(j\)</span>的损失，是负的对数似然也可以称之为<strong>熵</strong>(熵越小，那就意味<strong>似然估计越准</strong>)： <span class="math display">\[
cost^n_{ij} = - \ln(P^n_{i \vert j})
\]</span></p>
<p>由于胶囊<span class="math inline">\(i\)</span>与胶囊<span class="math inline">\(j\)</span>之间的联系并不相同，因此我们将在运行时按比例<span class="math inline">\(r_{ij}\)</span>来分配<span class="math inline">\(cost\)</span>，那下层胶囊的<span class="math inline">\(cost\)</span>计算为：</p>
<p><span class="math display">\[
\begin{split}
cost^n_j &amp;= \sum_i  r_{ij} cost^n_{ij} \\
&amp;= \sum_i - r_{ij} \ln(p^n_{i \vert j}) \\
&amp;= \sum_i r_{ij}  \big( \frac{(v^n_{ij}-\mu^n_j)^2}{2 ({\sigma^n_j})^2} + \ln(\sigma^n_j) + \frac{\ln(2 \pi)}{2} \big)\\
&amp;= \frac{\sum_i r_{ij} (\sigma^n_j)^2}{2 (\sigma^n_j)^2} + (\ln(\sigma^n_j) + \frac{\ln(2 \pi)}{2}) \sum_i r_{ij} \\
&amp;= \big(\ln(\sigma^n_j) + \beta_v \big) \sum_i r_{ij}  \quad \beta_v\text{是待优化参数}
\end{split}
\]</span></p>
<p>有了<span class="math inline">\(cost\)</span>值，使用<span class="math inline">\(a_j\)</span>来决定胶囊<span class="math inline">\(j\)</span>是否被激活： <span class="math display">\[
a_j = sigmoid(\lambda(\beta_a - \sum_h cost^n_j))
\]</span></p>
<p>这里面的<span class="math inline">\(\beta_a，\beta_v\)</span>是根据反向传播进行优化的，所以并不需要直接计算。</p>
<p>其中的<span class="math inline">\(r_{ij},\mu,\sigma,a_j\)</span>是根据EM路由来计算的。<span class="math inline">\(\lambda\)</span>是根据温度参数<span class="math inline">\(\frac{1}{temperature}\)</span>来计算(退火策略)，随着<span class="math inline">\(r_{ij}\)</span>越来越好，就降低<code>temperature</code>以获得更大的<span class="math inline">\(\lambda\)</span>，也就是增加<span class="math inline">\(sigmoid\)</span>的陡度，这样就更加在更加小的范围内微调<span class="math inline">\(r_{ij}\)</span>。</p>
</section>
<section id="em路由" class="level2">
<h2 class="anchored" data-anchor-id="em路由">EM路由</h2>
<p>这一块是重点，所以我要仔细看每个公式细节。并且要一步一步的渐进的看。</p>
<section id="新动态路由1" class="level3">
<h3 class="anchored" data-anchor-id="新动态路由1">新动态路由1</h3>
<p>首先用一个矩阵<span class="math inline">\(\boldsymbol{P}_{i}， i=1,\ldots,n\)</span>来表示第<span class="math inline">\(l\)</span>层的胶囊，用矩阵<span class="math inline">\(\boldsymbol{M}_j， j=1,\ldots,k\)</span>来表示第<span class="math inline">\(l+1\)</span>层的胶囊。在做EM路由的过程中，他将<span class="math inline">\(P_i\)</span>变成长16的向量，且协方差矩阵为对角阵。</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow N(\boldsymbol{P}_i;\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j) &amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{\pi_j p_{ij} }{\sum\limits_{j=1}^k\pi_j p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{R_{ij } }{\sum\limits_{i=1}^n R_{ij } } &amp;&amp; \text{计算归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n r_{ij}\boldsymbol{P}_i &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n r_{ij}(\boldsymbol{P}_i-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma \\
    &amp;\pi_j \leftarrow \frac{1}{n}\sum\limits_{i=1}^n R_{ij} &amp;&amp; \text{更新聚类中心}\pi
\end{aligned}
\]</span></p>
<p>这里的<span class="math inline">\(R_{ij}\)</span>就是后验概率，也就是在<code>EM</code>算法中使用的<span class="math inline">\(w_j\)</span>。</p>
</section>
<section id="新动态路由2" class="level3">
<h3 class="anchored" data-anchor-id="新动态路由2">新动态路由2</h3>
<p>前面讲道理有一个激活标量<span class="math inline">\(a_j\)</span>来衡量胶囊单元的显著程度，根据EM算法计算我们可以得到<span class="math inline">\(\pi_j\)</span>为第<span class="math inline">\(l+1\)</span>层胶囊的聚类中心，但我们不能选择<span class="math inline">\(\pi\)</span>因为：</p>
<ol type="1">
<li><span class="math inline">\(\pi_j\)</span>是归一化的，就是我们只想得到他的显著程度，而不是显著程度的概率。</li>
<li><span class="math inline">\(\pi_j\)</span>不能反映出类内的元素特征是否相似。</li>
</ol>
<p>现在再看前面所使用的激活标量<span class="math inline">\(a_j\)</span>，他既考虑了似然估计值<span class="math inline">\(cost_j^h\)</span>,又考虑了显著程度<span class="math inline">\(\pi_j\)</span>。所以作者直接将<span class="math inline">\(a_j\)</span>替换了<span class="math inline">\(\pi_j\)</span>，这样虽然不完全与原始EM算法相同，但是也能收敛，得到新的动态路由：</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow N(\boldsymbol{P}_i;\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j) &amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{a_j p_{ij} }{\sum\limits_{j=1}^k a_j p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{R_{ij } }{\sum\limits_{i=1}^n R_{ij } } &amp;&amp; \text{归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n r_{ij}\boldsymbol{P}_i &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n r_{ij}(\boldsymbol{P}_i-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma \\
   &amp; cost_j \leftarrow \left(\beta_v+\sum\limits_{l=1}^d \ln \boldsymbol{\sigma}_j^l \right)\sum\limits_i r_{ij} &amp;&amp; \text{计算熵} \\
   &amp; a_j \leftarrow sigmoid(\lambda(\beta_a - \sum_h cost^h_j)) &amp;&amp; \text{更新}a_j
\end{aligned}
\]</span></p>
</section>
<section id="新动态路由3" class="level3">
<h3 class="anchored" data-anchor-id="新动态路由3">新动态路由3</h3>
<p>上面好像没什么问题。但是没有使用底层胶囊的激活值<span class="math inline">\(a^{last}_i\)</span>，作者在计算归一化后验概率<span class="math inline">\(r_{ij}\leftarrow \frac{R_{ij } }{\sum\limits_{i=1}^n R_{ij } }\)</span>的时候加入：</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow N(\boldsymbol{P}_i;\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j) &amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{a_j p_{ij} }{\sum\limits_{j=1}^k a_j p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{a_i^{last}R_{ij } }{\sum\limits_{i=1}^n a_i^{last} R_{ij } } &amp;&amp; \text{计算归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n r_{ij}\boldsymbol{P}_i &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n r_{ij}(\boldsymbol{P}_i-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma \\
   &amp; cost_j \leftarrow \left(\beta_v+\sum\limits_{l=1}^d \ln \boldsymbol{\sigma}_j^l \right)\sum\limits_i r_{ij} &amp;&amp; \text{计算熵} \\
   &amp; a_j \leftarrow sigmoid(\lambda(\beta_a - \sum_h cost^h_j)) &amp;&amp; \text{更新}a_j
\end{aligned}
\]</span></p>
</section>
</section>
<section id="新动态路由4" class="level2">
<h2 class="anchored" data-anchor-id="新动态路由4">新动态路由4</h2>
<p>因为<span class="math inline">\(\sum\limits_i r_{ij}=1\)</span>，所以下面还可以化简。</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow N(\boldsymbol{P}_i;\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j) &amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{a_j p_{ij} }{\sum\limits_{j=1}^k a_j p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{a_i^{last}R_{ij } }{\sum\limits_{i=1}^n a_i^{last} R_{ij } } &amp;&amp; \text{计算归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n r_{ij}\boldsymbol{P}_i &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n r_{ij}(\boldsymbol{P}_i-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma \\
   &amp; cost_j \leftarrow \left(\beta_v+\sum\limits_{l=1}^d \ln \boldsymbol{\sigma}_j^l \right)\sum\limits_i a_i^{last} R_{ij} &amp;&amp; \text{计算熵} \\
   &amp; a_j \leftarrow sigmoid(\lambda(\beta_a - \sum_h cost^h_j)) &amp;&amp; \text{更新}a_j
\end{aligned}
\]</span></p>
</section>
<section id="最终的动态路由" class="level2">
<h2 class="anchored" data-anchor-id="最终的动态路由">最终的动态路由</h2>
<p>现在再把投票机制与视角不变矩阵加入进来,投票矩阵与上面描述一样<span class="math inline">\(\boldsymbol{V}_{ij}=\boldsymbol{P}_{i}\boldsymbol{W}_{ij}\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow N(\boldsymbol{V}_{ij};\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j) &amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{a_j p_{ij} }{\sum\limits_{j=1}^k a_j p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{a_i^{last}R_{ij } }{\sum\limits_{i=1}^n a_i^{last} R_{ij } } &amp;&amp; \text{计算归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n r_{ij}\boldsymbol{V}_{ij} &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n r_{ij}(\boldsymbol{V}_{ij}-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma \\
   &amp; cost_j \leftarrow \left(\beta_v+\sum\limits_{l=1}^d \ln \boldsymbol{\sigma}_j^l \right)\sum\limits_i a_i^{last} R_{ij} &amp;&amp; \text{计算熵} \\
   &amp; a_j \leftarrow sigmoid(\lambda(\beta_a - \sum_h cost^h_j)) &amp;&amp; \text{更新}a_j
\end{aligned}
\]</span></p>
</section>
<section id="代码-1" class="level2">
<h2 class="anchored" data-anchor-id="代码-1">代码</h2>
<section id="论文中的伪代码" class="level3">
<h3 class="anchored" data-anchor-id="论文中的伪代码">论文中的伪代码</h3>
<p>官方的<code>EM路由</code>的顺序是和之前使用的<code>EM</code>算法不一样的，他先做<code>M-step</code>再做<code>E-step</code>，因为一开始我们是不知道父胶囊的分布，所以我们没有办法通过概率密度函数来计算后验分布，他这里比较简单粗暴，直接先把后验概率<span class="math inline">\(R_{ij}\)</span>分配为1，再来算分配权重矩阵<span class="math inline">\(r_{ij}\)</span>。 <img src="em-algm/em.png" class="img-fluid"></p>
<p>在<code>M-step</code>中计算出父胶囊正态分布的<span class="math inline">\(\mu,\sigma\)</span>。然后计算熵<span class="math inline">\(cost\)</span>，再计算激活。</p>
<p><strong>NOTE：</strong> 在论文或者<code>tensorflow</code>里面，<span class="math inline">\(log\)</span>默认都是以<span class="math inline">\(e\)</span>为底的。并且这里的<code>h</code>就是我上面用的<code>n</code></p>
<p><img src="em-algm/em-m.png" class="img-fluid"></p>
<p>在<code>E-step</code>中，我们有了之前的父胶囊的<span class="math inline">\(\mu,\sigma\)</span>，那我们根据正态分布公式计算概率密度<span class="math inline">\(p_{ij}\)</span>，然后贝叶斯公式更新后验概率<span class="math inline">\(R_{ij}\)</span>。这样一个循环就形成了。 <img src="em-algm/em-e.png" class="img-fluid"></p>
</section>
<section id="python实现" class="level3">
<h3 class="anchored" data-anchor-id="python实现">python实现</h3>
<p>使用<code>Tensorflow 1.14</code>直接运行即可。主要就是看<code>EM</code>算法的实现部分，当然这里在计算<span class="math inline">\(a_j\)</span>的时候，用的是迂回的方式来计算，避免值溢出。我再尝试过这个算法之后，感觉真的不咋地，思路很高深，但是效果没有那些简单粗暴的算法好，我还是喜欢谷歌提出一些网络，虽然没那么多数学原理，但是很直接有效。不过这个笔记对我拿来写论文还是很有帮助的。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow.python <span class="im">as</span> tf</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow.contrib.slim <span class="im">as</span> slim</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.python.keras.datasets <span class="im">import</span> mnist</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.contrib.layers <span class="im">import</span> xavier_initializer</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-9</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv2d(inputs, kernel, out_channels, stride, padding, name, is_train<span class="op">=</span><span class="va">True</span>, activation_fn<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> slim.arg_scope([slim.conv2d], trainable<span class="op">=</span>is_train):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.variable_scope(name) <span class="im">as</span> scope:</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> slim.conv2d(inputs,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>                                 num_outputs<span class="op">=</span>out_channels,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                                 kernel_size<span class="op">=</span>[kernel, kernel], stride<span class="op">=</span>stride, padding<span class="op">=</span>padding,</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>                                 scope<span class="op">=</span>scope, activation_fn<span class="op">=</span>activation_fn)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>            tf.logging.info(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> output shape: </span><span class="sc">{</span>output<span class="sc">.</span>get_shape()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> primary_caps(inputs, kernel_size, out_capsules, stride, padding, pose_shape, name):</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""This constructs a primary capsule layer using regular convolution layer.</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co">    :param inputs: shape (N, H, W, C) (?, 14, 14, 32)</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co">    :param kernel_size: Apply a filter of [kernel, kernel] [5x5]</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co">    :param out_capsules: # of output capsule (32)</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="co">    :param stride: 1, 2, or ... (1)</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co">    :param padding: padding: SAME or VALID.</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="co">    :param pose_shape: (4, 4)</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="co">    :param name: scope name</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co">    :return: (P, a), (P (?, 14, 14, 32, 4, 4), a (?, 14, 14, 32))</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(name) <span class="im">as</span> scope:</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate the P matrics for the 32 output capsules</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        P <span class="op">=</span> conv2d(</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>            inputs,</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>            kernel_size,</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>            out_capsules <span class="op">*</span> pose_shape[<span class="dv">0</span>] <span class="op">*</span> pose_shape[<span class="dv">1</span>],</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>            stride,</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span>padding,</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'pose_stacked'</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        input_shape <span class="op">=</span> inputs.get_shape()</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape 16 scalar values into a 4x4 matrix</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        P <span class="op">=</span> tf.reshape(</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>            P, shape<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, input_shape[<span class="op">-</span><span class="dv">3</span>], input_shape[<span class="op">-</span><span class="dv">2</span>], out_capsules, pose_shape[<span class="dv">0</span>], pose_shape[<span class="dv">1</span>]],</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'P'</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate the activation for the 32 output capsules</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> conv2d(</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>            inputs,</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>            kernel_size,</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>            out_capsules,</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>            stride,</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span>padding,</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>            activation_fn<span class="op">=</span>tf.sigmoid,</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'activation'</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>        tf.summary.histogram(</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>            <span class="st">'a'</span>, a</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># P (?, 14, 14, 32, 4, 4), a (?, 14, 14, 32)</span></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> P, a</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kernel_tile(<span class="bu">input</span>, kernel, stride):</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""This constructs a primary capsule layer using regular convolution layer.</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a><span class="co">    :param inputs: shape (?, 14, 14, 32, 4, 4)</span></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a><span class="co">    :param kernel: 3</span></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a><span class="co">    :param stride: 2</span></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a><span class="co">    :return output: (?, 5, 5, 3x3=9, 136)</span></span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (?, 14, 14, 32x(16)=512)</span></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>    input_shape <span class="op">=</span> <span class="bu">input</span>.get_shape()</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> input_shape[<span class="dv">4</span>] <span class="op">*</span> input_shape[<span class="dv">5</span>] <span class="cf">if</span> <span class="bu">len</span>(input_shape) <span class="op">&gt;</span> <span class="dv">5</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> tf.reshape(<span class="bu">input</span>, shape<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, input_shape[<span class="dv">1</span>], input_shape[<span class="dv">2</span>], input_shape[<span class="dv">3</span>] <span class="op">*</span> size])</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>    input_shape <span class="op">=</span> <span class="bu">input</span>.get_shape()</span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (3, 3, 512, 9)</span></span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>    tile_filter <span class="op">=</span> np.zeros(shape<span class="op">=</span>[kernel, kernel, input_shape[<span class="dv">3</span>],</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>                                  kernel <span class="op">*</span> kernel], dtype<span class="op">=</span>np.float32)</span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(kernel):</span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(kernel):</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>            tile_filter[i, j, :, i <span class="op">*</span> kernel <span class="op">+</span> j] <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># (3, 3, 512, 9)</span></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (3, 3, 512, 9)</span></span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>    tile_filter_op <span class="op">=</span> tf.constant(tile_filter, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (?, 6, 6, 4608)</span></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> tf.nn.depthwise_conv2d(<span class="bu">input</span>, tile_filter_op, strides<span class="op">=</span>[</span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>                                    <span class="dv">1</span>, stride, stride, <span class="dv">1</span>], padding<span class="op">=</span><span class="st">'VALID'</span>)</span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>    output_shape <span class="op">=</span> output.get_shape()</span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> tf.reshape(output, shape<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, output_shape[<span class="dv">1</span>], output_shape[<span class="dv">2</span>], input_shape[<span class="dv">3</span>], kernel <span class="op">*</span> kernel])</span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> tf.transpose(output, perm<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>])</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (?, 6, 6, 9, 512)</span></span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a><span class="co"># import tensorflow.python as tf</span></span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mat_transform(inputs, output_cap_size, size):</span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute the vote.</span></span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a><span class="co">    :param inputs: shape (size, 288, 16)</span></span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_cap_size: 32</span></span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a><span class="co">    :return V: (24, 5, 5, 3x3=9, 136)</span></span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a>    caps_num_i <span class="op">=</span> <span class="bu">int</span>(inputs.get_shape()[<span class="dv">1</span>])  <span class="co"># 288</span></span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> tf.reshape(inputs, shape<span class="op">=</span>[size, caps_num_i, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">4</span>])  <span class="co"># (size, 288, 1, 4, 4)</span></span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> slim.variable(<span class="st">'W'</span>, shape<span class="op">=</span>[<span class="dv">1</span>, caps_num_i, output_cap_size, <span class="dv">4</span>, <span class="dv">4</span>], dtype<span class="op">=</span>tf.float32,</span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a>                      initializer<span class="op">=</span>tf.truncated_normal_initializer(mean<span class="op">=</span><span class="fl">0.0</span>, stddev<span class="op">=</span><span class="fl">1.0</span>))  <span class="co"># (1, 288, 32, 4, 4)</span></span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> tf.tile(W, [size, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># (24, 288, 32, 4, 4)</span></span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> tf.tile(P, [<span class="dv">1</span>, <span class="dv">1</span>, output_cap_size, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># (size, 288, 32, 4, 4)</span></span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> tf.matmul(P, W)  <span class="co"># (24, 288, 32, 4, 4)</span></span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> tf.reshape(V, [size, caps_num_i, output_cap_size, <span class="dv">16</span>])  <span class="co"># (size, 288, 32, 16)</span></span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> V</span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> coord_addition(V, H, W):</span>
<span id="cb8-140"><a href="#cb8-140" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Coordinate addition.</span></span>
<span id="cb8-141"><a href="#cb8-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a><span class="co">    :param V: (24, 4, 4, 32, 10, 16)</span></span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a><span class="co">    :param H, W: spaital height and width 4</span></span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a><span class="co">    :return V: (24, 4, 4, 32, 10, 16)</span></span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a>    coordinate_offset_hh <span class="op">=</span> tf.reshape(</span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a>        (tf.<span class="bu">range</span>(H, dtype<span class="op">=</span>tf.float32) <span class="op">+</span> <span class="fl">0.50</span>) <span class="op">/</span> H, [<span class="dv">1</span>, H, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a>    coordinate_offset_h0 <span class="op">=</span> tf.constant(</span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a>        <span class="fl">0.0</span>, shape<span class="op">=</span>[<span class="dv">1</span>, H, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>], dtype<span class="op">=</span>tf.float32</span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a>    coordinate_offset_h <span class="op">=</span> tf.stack(</span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a>        [coordinate_offset_hh, coordinate_offset_h0] <span class="op">+</span> [coordinate_offset_h0 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">14</span>)], axis<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># (1, 4, 1, 1, 1, 16)</span></span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-157"><a href="#cb8-157" aria-hidden="true" tabindex="-1"></a>    coordinate_offset_ww <span class="op">=</span> tf.reshape(</span>
<span id="cb8-158"><a href="#cb8-158" aria-hidden="true" tabindex="-1"></a>        (tf.<span class="bu">range</span>(W, dtype<span class="op">=</span>tf.float32) <span class="op">+</span> <span class="fl">0.50</span>) <span class="op">/</span> W, [<span class="dv">1</span>, <span class="dv">1</span>, W, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb8-159"><a href="#cb8-159" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-160"><a href="#cb8-160" aria-hidden="true" tabindex="-1"></a>    coordinate_offset_w0 <span class="op">=</span> tf.constant(</span>
<span id="cb8-161"><a href="#cb8-161" aria-hidden="true" tabindex="-1"></a>        <span class="fl">0.0</span>, shape<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>, W, <span class="dv">1</span>, <span class="dv">1</span>], dtype<span class="op">=</span>tf.float32</span>
<span id="cb8-162"><a href="#cb8-162" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-163"><a href="#cb8-163" aria-hidden="true" tabindex="-1"></a>    coordinate_offset_w <span class="op">=</span> tf.stack(</span>
<span id="cb8-164"><a href="#cb8-164" aria-hidden="true" tabindex="-1"></a>        [coordinate_offset_w0, coordinate_offset_ww] <span class="op">+</span> [coordinate_offset_w0 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">14</span>)], axis<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb8-165"><a href="#cb8-165" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># (1, 1, 4, 1, 1, 16)</span></span>
<span id="cb8-166"><a href="#cb8-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-167"><a href="#cb8-167" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (24, 4, 4, 32, 10, 16)</span></span>
<span id="cb8-168"><a href="#cb8-168" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> V <span class="op">+</span> coordinate_offset_h <span class="op">+</span> coordinate_offset_w</span>
<span id="cb8-169"><a href="#cb8-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-170"><a href="#cb8-170" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> V</span>
<span id="cb8-171"><a href="#cb8-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-172"><a href="#cb8-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-173"><a href="#cb8-173" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_capsule(inputs, shape, strides, iterations, batch_size, name):</span>
<span id="cb8-174"><a href="#cb8-174" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" This constructs a convolution capsule layer from a primary or convolution capsule layer.</span></span>
<span id="cb8-175"><a href="#cb8-175" aria-hidden="true" tabindex="-1"></a><span class="co">        i: input capsules (32)</span></span>
<span id="cb8-176"><a href="#cb8-176" aria-hidden="true" tabindex="-1"></a><span class="co">        o: output capsules (32)</span></span>
<span id="cb8-177"><a href="#cb8-177" aria-hidden="true" tabindex="-1"></a><span class="co">        batch size: 24</span></span>
<span id="cb8-178"><a href="#cb8-178" aria-hidden="true" tabindex="-1"></a><span class="co">        spatial dimension: 14x14</span></span>
<span id="cb8-179"><a href="#cb8-179" aria-hidden="true" tabindex="-1"></a><span class="co">        kernel: 3x3</span></span>
<span id="cb8-180"><a href="#cb8-180" aria-hidden="true" tabindex="-1"></a><span class="co">    :param inputs: a primary or convolution capsule layer with poses and a_j</span></span>
<span id="cb8-181"><a href="#cb8-181" aria-hidden="true" tabindex="-1"></a><span class="co">           pose: (24, 14, 14, 32, 4, 4)</span></span>
<span id="cb8-182"><a href="#cb8-182" aria-hidden="true" tabindex="-1"></a><span class="co">           activation: (24, 14, 14, 32)</span></span>
<span id="cb8-183"><a href="#cb8-183" aria-hidden="true" tabindex="-1"></a><span class="co">    :param shape: the shape of convolution operation kernel, [kh, kw, i, o] = (3, 3, 32, 32)</span></span>
<span id="cb8-184"><a href="#cb8-184" aria-hidden="true" tabindex="-1"></a><span class="co">    :param strides: often [1, 2, 2, 1] (stride 2), or [1, 1, 1, 1] (stride 1).</span></span>
<span id="cb8-185"><a href="#cb8-185" aria-hidden="true" tabindex="-1"></a><span class="co">    :param iterations: number of iterations in EM routing. 3</span></span>
<span id="cb8-186"><a href="#cb8-186" aria-hidden="true" tabindex="-1"></a><span class="co">    :param name: name.</span></span>
<span id="cb8-187"><a href="#cb8-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-188"><a href="#cb8-188" aria-hidden="true" tabindex="-1"></a><span class="co">    :return: (poses, a_j).</span></span>
<span id="cb8-189"><a href="#cb8-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-190"><a href="#cb8-190" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-191"><a href="#cb8-191" aria-hidden="true" tabindex="-1"></a>    P, a_last <span class="op">=</span> inputs</span>
<span id="cb8-192"><a href="#cb8-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-193"><a href="#cb8-193" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(name) <span class="im">as</span> scope:</span>
<span id="cb8-194"><a href="#cb8-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-195"><a href="#cb8-195" aria-hidden="true" tabindex="-1"></a>        stride <span class="op">=</span> strides[<span class="dv">1</span>]  <span class="co"># 2</span></span>
<span id="cb8-196"><a href="#cb8-196" aria-hidden="true" tabindex="-1"></a>        i_size <span class="op">=</span> shape[<span class="op">-</span><span class="dv">2</span>]  <span class="co"># 32</span></span>
<span id="cb8-197"><a href="#cb8-197" aria-hidden="true" tabindex="-1"></a>        o_size <span class="op">=</span> shape[<span class="op">-</span><span class="dv">1</span>]  <span class="co"># 32</span></span>
<span id="cb8-198"><a href="#cb8-198" aria-hidden="true" tabindex="-1"></a>        pose_size <span class="op">=</span> P.get_shape()[<span class="op">-</span><span class="dv">1</span>]  <span class="co"># 4</span></span>
<span id="cb8-199"><a href="#cb8-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-200"><a href="#cb8-200" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tile the input capusles' pose matrices to the spatial dimension of the output capsules</span></span>
<span id="cb8-201"><a href="#cb8-201" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Such that we can later multiple with the transformation matrices to generate the V.</span></span>
<span id="cb8-202"><a href="#cb8-202" aria-hidden="true" tabindex="-1"></a>        P <span class="op">=</span> kernel_tile(P, <span class="dv">3</span>, stride)  <span class="co"># (?, 14, 14, 32, 4, 4) -&gt; (?, 6, 6, 3x3=9, 32x16=512)</span></span>
<span id="cb8-203"><a href="#cb8-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-204"><a href="#cb8-204" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tile the a_j needed for the EM routing</span></span>
<span id="cb8-205"><a href="#cb8-205" aria-hidden="true" tabindex="-1"></a>        a_last <span class="op">=</span> kernel_tile(a_last, <span class="dv">3</span>, stride)  <span class="co"># (?, 14, 14, 32) -&gt; (?, 6, 6, 9, 32)</span></span>
<span id="cb8-206"><a href="#cb8-206" aria-hidden="true" tabindex="-1"></a>        spatial_size <span class="op">=</span> <span class="bu">int</span>(a_last.get_shape()[<span class="dv">1</span>])  <span class="co"># 6</span></span>
<span id="cb8-207"><a href="#cb8-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-208"><a href="#cb8-208" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape it for later operations</span></span>
<span id="cb8-209"><a href="#cb8-209" aria-hidden="true" tabindex="-1"></a>        P <span class="op">=</span> tf.reshape(P, shape<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> i_size, <span class="dv">16</span>])  <span class="co"># (?, 9x32=288, 16)</span></span>
<span id="cb8-210"><a href="#cb8-210" aria-hidden="true" tabindex="-1"></a>        a_last <span class="op">=</span> tf.reshape(a_last, shape<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, spatial_size, spatial_size, <span class="dv">3</span> <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> i_size])  <span class="co"># (?, 6, 6, 9x32=288)</span></span>
<span id="cb8-211"><a href="#cb8-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-212"><a href="#cb8-212" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.variable_scope(<span class="st">'V'</span>) <span class="im">as</span> scope:</span>
<span id="cb8-213"><a href="#cb8-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-214"><a href="#cb8-214" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Generate the V by multiply it with the transformation matrices</span></span>
<span id="cb8-215"><a href="#cb8-215" aria-hidden="true" tabindex="-1"></a>            V <span class="op">=</span> mat_transform(P, o_size, size<span class="op">=</span>batch_size <span class="op">*</span> spatial_size <span class="op">*</span> spatial_size)  <span class="co"># (864, 288, 32, 16)</span></span>
<span id="cb8-216"><a href="#cb8-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-217"><a href="#cb8-217" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reshape the vote for EM routing</span></span>
<span id="cb8-218"><a href="#cb8-218" aria-hidden="true" tabindex="-1"></a>            V_shape <span class="op">=</span> V.get_shape()</span>
<span id="cb8-219"><a href="#cb8-219" aria-hidden="true" tabindex="-1"></a>            V <span class="op">=</span> tf.reshape(</span>
<span id="cb8-220"><a href="#cb8-220" aria-hidden="true" tabindex="-1"></a>                V,</span>
<span id="cb8-221"><a href="#cb8-221" aria-hidden="true" tabindex="-1"></a>                shape<span class="op">=</span>[batch_size, spatial_size,</span>
<span id="cb8-222"><a href="#cb8-222" aria-hidden="true" tabindex="-1"></a>                       spatial_size, V_shape[<span class="op">-</span><span class="dv">3</span>],</span>
<span id="cb8-223"><a href="#cb8-223" aria-hidden="true" tabindex="-1"></a>                       V_shape[<span class="op">-</span><span class="dv">2</span>], V_shape[<span class="op">-</span><span class="dv">1</span>]])  <span class="co"># (24, 6, 6, 288, 32, 16)</span></span>
<span id="cb8-224"><a href="#cb8-224" aria-hidden="true" tabindex="-1"></a>            tf.logging.info(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> V shape: </span><span class="sc">{</span>V<span class="sc">.</span>get_shape()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-225"><a href="#cb8-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-226"><a href="#cb8-226" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.variable_scope(<span class="st">'routing'</span>) <span class="im">as</span> scope:</span>
<span id="cb8-227"><a href="#cb8-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-228"><a href="#cb8-228" aria-hidden="true" tabindex="-1"></a>            <span class="co"># beta_v and beta_a one for each output capsule: (1, 1, 1, 32)</span></span>
<span id="cb8-229"><a href="#cb8-229" aria-hidden="true" tabindex="-1"></a>            beta_v <span class="op">=</span> tf.get_variable(</span>
<span id="cb8-230"><a href="#cb8-230" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">'beta_v'</span>, shape<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, o_size], dtype<span class="op">=</span>tf.float32,</span>
<span id="cb8-231"><a href="#cb8-231" aria-hidden="true" tabindex="-1"></a>                initializer<span class="op">=</span>xavier_initializer()</span>
<span id="cb8-232"><a href="#cb8-232" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-233"><a href="#cb8-233" aria-hidden="true" tabindex="-1"></a>            beta_a <span class="op">=</span> tf.get_variable(</span>
<span id="cb8-234"><a href="#cb8-234" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">'beta_a'</span>, shape<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, o_size], dtype<span class="op">=</span>tf.float32,</span>
<span id="cb8-235"><a href="#cb8-235" aria-hidden="true" tabindex="-1"></a>                initializer<span class="op">=</span>xavier_initializer()</span>
<span id="cb8-236"><a href="#cb8-236" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-237"><a href="#cb8-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-238"><a href="#cb8-238" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use EM routing to compute the pose and activation</span></span>
<span id="cb8-239"><a href="#cb8-239" aria-hidden="true" tabindex="-1"></a>            <span class="co"># V (24, 6, 6, 3x3x32=288, 32, 16), a_last (?, 6, 6, 288)</span></span>
<span id="cb8-240"><a href="#cb8-240" aria-hidden="true" tabindex="-1"></a>            <span class="co"># poses (24, 6, 6, 32, 16), activation (24, 6, 6, 32)</span></span>
<span id="cb8-241"><a href="#cb8-241" aria-hidden="true" tabindex="-1"></a>            M, a_j <span class="op">=</span> matrix_capsules_em_routing(</span>
<span id="cb8-242"><a href="#cb8-242" aria-hidden="true" tabindex="-1"></a>                V, a_last, beta_v, beta_a, iterations, name<span class="op">=</span><span class="st">'em_routing'</span></span>
<span id="cb8-243"><a href="#cb8-243" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-244"><a href="#cb8-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-245"><a href="#cb8-245" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reshape it back to 4x4 pose matrix</span></span>
<span id="cb8-246"><a href="#cb8-246" aria-hidden="true" tabindex="-1"></a>            poses_shape <span class="op">=</span> M.get_shape()</span>
<span id="cb8-247"><a href="#cb8-247" aria-hidden="true" tabindex="-1"></a>            <span class="co"># (24, 6, 6, 32, 4, 4)</span></span>
<span id="cb8-248"><a href="#cb8-248" aria-hidden="true" tabindex="-1"></a>            M <span class="op">=</span> tf.reshape(</span>
<span id="cb8-249"><a href="#cb8-249" aria-hidden="true" tabindex="-1"></a>                M, [</span>
<span id="cb8-250"><a href="#cb8-250" aria-hidden="true" tabindex="-1"></a>                    poses_shape[<span class="dv">0</span>], poses_shape[<span class="dv">1</span>], poses_shape[<span class="dv">2</span>], poses_shape[<span class="dv">3</span>], pose_size, pose_size</span>
<span id="cb8-251"><a href="#cb8-251" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb8-252"><a href="#cb8-252" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-253"><a href="#cb8-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-254"><a href="#cb8-254" aria-hidden="true" tabindex="-1"></a>        tf.logging.info(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> pose shape: </span><span class="sc">{</span>M<span class="sc">.</span>get_shape()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-255"><a href="#cb8-255" aria-hidden="true" tabindex="-1"></a>        tf.logging.info(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> a_j shape: </span><span class="sc">{</span>a_j<span class="sc">.</span>get_shape()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-256"><a href="#cb8-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-257"><a href="#cb8-257" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> M, a_j</span>
<span id="cb8-258"><a href="#cb8-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-259"><a href="#cb8-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-260"><a href="#cb8-260" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> class_capsules(inputs, num_classes, iterations, batch_size, name):</span>
<span id="cb8-261"><a href="#cb8-261" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-262"><a href="#cb8-262" aria-hidden="true" tabindex="-1"></a><span class="co">    :param inputs: ((24, 4, 4, 32, 4, 4), (24, 4, 4, 32))</span></span>
<span id="cb8-263"><a href="#cb8-263" aria-hidden="true" tabindex="-1"></a><span class="co">    :param num_classes: 10</span></span>
<span id="cb8-264"><a href="#cb8-264" aria-hidden="true" tabindex="-1"></a><span class="co">    :param iterations: 3</span></span>
<span id="cb8-265"><a href="#cb8-265" aria-hidden="true" tabindex="-1"></a><span class="co">    :param batch_size: 24</span></span>
<span id="cb8-266"><a href="#cb8-266" aria-hidden="true" tabindex="-1"></a><span class="co">    :param name:</span></span>
<span id="cb8-267"><a href="#cb8-267" aria-hidden="true" tabindex="-1"></a><span class="co">    :return poses, a_j: poses (24, 10, 4, 4), activation (24, 10).</span></span>
<span id="cb8-268"><a href="#cb8-268" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-269"><a href="#cb8-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-270"><a href="#cb8-270" aria-hidden="true" tabindex="-1"></a>    P, a_last <span class="op">=</span> inputs  <span class="co"># (24, 4, 4, 32, 4, 4), (24, 4, 4, 32)</span></span>
<span id="cb8-271"><a href="#cb8-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-272"><a href="#cb8-272" aria-hidden="true" tabindex="-1"></a>    P_shape <span class="op">=</span> P.get_shape()</span>
<span id="cb8-273"><a href="#cb8-273" aria-hidden="true" tabindex="-1"></a>    spatial_size <span class="op">=</span> <span class="bu">int</span>(P_shape[<span class="dv">1</span>])  <span class="co"># 4</span></span>
<span id="cb8-274"><a href="#cb8-274" aria-hidden="true" tabindex="-1"></a>    pose_size <span class="op">=</span> <span class="bu">int</span>(P_shape[<span class="op">-</span><span class="dv">1</span>])    <span class="co"># 4</span></span>
<span id="cb8-275"><a href="#cb8-275" aria-hidden="true" tabindex="-1"></a>    i_size <span class="op">=</span> <span class="bu">int</span>(P_shape[<span class="dv">3</span>])        <span class="co"># 32</span></span>
<span id="cb8-276"><a href="#cb8-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-277"><a href="#cb8-277" aria-hidden="true" tabindex="-1"></a>    <span class="co"># P (24*4*4=384, 32, 16)</span></span>
<span id="cb8-278"><a href="#cb8-278" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> tf.reshape(P, shape<span class="op">=</span>[batch_size <span class="op">*</span> spatial_size <span class="op">*</span> spatial_size, P_shape[<span class="op">-</span><span class="dv">3</span>], P_shape[<span class="op">-</span><span class="dv">2</span>] <span class="op">*</span> P_shape[<span class="op">-</span><span class="dv">2</span>]])</span>
<span id="cb8-279"><a href="#cb8-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-280"><a href="#cb8-280" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(name) <span class="im">as</span> scope:</span>
<span id="cb8-281"><a href="#cb8-281" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.variable_scope(<span class="st">'V'</span>) <span class="im">as</span> scope:</span>
<span id="cb8-282"><a href="#cb8-282" aria-hidden="true" tabindex="-1"></a>            <span class="co"># P (384, 32, 16)</span></span>
<span id="cb8-283"><a href="#cb8-283" aria-hidden="true" tabindex="-1"></a>            <span class="co"># V: (384, 32, 10, 16)</span></span>
<span id="cb8-284"><a href="#cb8-284" aria-hidden="true" tabindex="-1"></a>            V <span class="op">=</span> mat_transform(P, num_classes, size<span class="op">=</span>batch_size <span class="op">*</span> spatial_size <span class="op">*</span> spatial_size)</span>
<span id="cb8-285"><a href="#cb8-285" aria-hidden="true" tabindex="-1"></a>            tf.logging.info(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> V shape: </span><span class="sc">{</span>V<span class="sc">.</span>get_shape()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-286"><a href="#cb8-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-287"><a href="#cb8-287" aria-hidden="true" tabindex="-1"></a>            <span class="co"># V (24, 4, 4, 32, 10, 16)</span></span>
<span id="cb8-288"><a href="#cb8-288" aria-hidden="true" tabindex="-1"></a>            V <span class="op">=</span> tf.reshape(V, shape<span class="op">=</span>[batch_size, spatial_size, spatial_size, i_size, num_classes, pose_size <span class="op">*</span> pose_size])</span>
<span id="cb8-289"><a href="#cb8-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-290"><a href="#cb8-290" aria-hidden="true" tabindex="-1"></a>            <span class="co"># (24, 4, 4, 32, 10, 16)</span></span>
<span id="cb8-291"><a href="#cb8-291" aria-hidden="true" tabindex="-1"></a>            V <span class="op">=</span> coord_addition(V, spatial_size, spatial_size)</span>
<span id="cb8-292"><a href="#cb8-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-293"><a href="#cb8-293" aria-hidden="true" tabindex="-1"></a>            tf.logging.info(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> V shape with coord addition: </span><span class="sc">{</span>V<span class="sc">.</span>get_shape()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-294"><a href="#cb8-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-295"><a href="#cb8-295" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.variable_scope(<span class="st">'routing'</span>) <span class="im">as</span> scope:</span>
<span id="cb8-296"><a href="#cb8-296" aria-hidden="true" tabindex="-1"></a>            <span class="co"># beta_v and beta_a one for each output capsule: (1, 10)</span></span>
<span id="cb8-297"><a href="#cb8-297" aria-hidden="true" tabindex="-1"></a>            beta_v <span class="op">=</span> tf.get_variable(</span>
<span id="cb8-298"><a href="#cb8-298" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">'beta_v'</span>, shape<span class="op">=</span>[<span class="dv">1</span>, num_classes], dtype<span class="op">=</span>tf.float32,</span>
<span id="cb8-299"><a href="#cb8-299" aria-hidden="true" tabindex="-1"></a>                initializer<span class="op">=</span>xavier_initializer()</span>
<span id="cb8-300"><a href="#cb8-300" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-301"><a href="#cb8-301" aria-hidden="true" tabindex="-1"></a>            beta_a <span class="op">=</span> tf.get_variable(</span>
<span id="cb8-302"><a href="#cb8-302" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">'beta_a'</span>, shape<span class="op">=</span>[<span class="dv">1</span>, num_classes], dtype<span class="op">=</span>tf.float32,</span>
<span id="cb8-303"><a href="#cb8-303" aria-hidden="true" tabindex="-1"></a>                initializer<span class="op">=</span>xavier_initializer()</span>
<span id="cb8-304"><a href="#cb8-304" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-305"><a href="#cb8-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-306"><a href="#cb8-306" aria-hidden="true" tabindex="-1"></a>            <span class="co"># V (24, 4, 4, 32, 10, 16) -&gt; (24, 512, 10, 16)</span></span>
<span id="cb8-307"><a href="#cb8-307" aria-hidden="true" tabindex="-1"></a>            V_shape <span class="op">=</span> V.get_shape()</span>
<span id="cb8-308"><a href="#cb8-308" aria-hidden="true" tabindex="-1"></a>            V <span class="op">=</span> tf.reshape(V, shape<span class="op">=</span>[batch_size, V_shape[<span class="dv">1</span>] <span class="op">*</span> V_shape[<span class="dv">2</span>] <span class="op">*</span> V_shape[<span class="dv">3</span>], V_shape[<span class="dv">4</span>], V_shape[<span class="dv">5</span>]])</span>
<span id="cb8-309"><a href="#cb8-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-310"><a href="#cb8-310" aria-hidden="true" tabindex="-1"></a>            <span class="co"># a_last (24, 4, 4, 32) -&gt; (24, 512)</span></span>
<span id="cb8-311"><a href="#cb8-311" aria-hidden="true" tabindex="-1"></a>            a_last <span class="op">=</span> tf.reshape(a_last, shape<span class="op">=</span>[batch_size,</span>
<span id="cb8-312"><a href="#cb8-312" aria-hidden="true" tabindex="-1"></a>                                               V_shape[<span class="dv">1</span>] <span class="op">*</span> V_shape[<span class="dv">2</span>] <span class="op">*</span> V_shape[<span class="dv">3</span>]])</span>
<span id="cb8-313"><a href="#cb8-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-314"><a href="#cb8-314" aria-hidden="true" tabindex="-1"></a>            <span class="co"># V (24, 512, 10, 16), a_last (24, 512)</span></span>
<span id="cb8-315"><a href="#cb8-315" aria-hidden="true" tabindex="-1"></a>            <span class="co"># poses (24, 10, 16), activation (24, 10)</span></span>
<span id="cb8-316"><a href="#cb8-316" aria-hidden="true" tabindex="-1"></a>            M, a_j <span class="op">=</span> matrix_capsules_em_routing(</span>
<span id="cb8-317"><a href="#cb8-317" aria-hidden="true" tabindex="-1"></a>                V, a_last, beta_v, beta_a, iterations, name<span class="op">=</span><span class="st">'em_routing'</span></span>
<span id="cb8-318"><a href="#cb8-318" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-319"><a href="#cb8-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-320"><a href="#cb8-320" aria-hidden="true" tabindex="-1"></a>        <span class="co"># M (24, 10, 16) -&gt; (24, 10, 4, 4)</span></span>
<span id="cb8-321"><a href="#cb8-321" aria-hidden="true" tabindex="-1"></a>        M <span class="op">=</span> tf.reshape(M, shape<span class="op">=</span>[batch_size, num_classes, pose_size, pose_size])</span>
<span id="cb8-322"><a href="#cb8-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-323"><a href="#cb8-323" aria-hidden="true" tabindex="-1"></a>        <span class="co"># M (24, 10, 4, 4), activation (24, 10)</span></span>
<span id="cb8-324"><a href="#cb8-324" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> M, a_j</span>
<span id="cb8-325"><a href="#cb8-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-326"><a href="#cb8-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-327"><a href="#cb8-327" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matrix_capsules_em_routing(V, a_last, beta_v, beta_a, iterations, name):</span>
<span id="cb8-328"><a href="#cb8-328" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The EM routing between input capsules (i) and output capsules (j).</span></span>
<span id="cb8-329"><a href="#cb8-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-330"><a href="#cb8-330" aria-hidden="true" tabindex="-1"></a><span class="co">    :param V: (N, OH, OW, kh x kw x i, o, 4 x 4) = (24, 6, 6, 3x3*32=288, 32, 16)</span></span>
<span id="cb8-331"><a href="#cb8-331" aria-hidden="true" tabindex="-1"></a><span class="co">    :param i_activation: activation from Level L (24, 6, 6, 288)</span></span>
<span id="cb8-332"><a href="#cb8-332" aria-hidden="true" tabindex="-1"></a><span class="co">    :param beta_v: (1, 1, 1, 32)</span></span>
<span id="cb8-333"><a href="#cb8-333" aria-hidden="true" tabindex="-1"></a><span class="co">    :param beta_a: (1, 1, 1, 32)</span></span>
<span id="cb8-334"><a href="#cb8-334" aria-hidden="true" tabindex="-1"></a><span class="co">    :param iterations: number of iterations in EM routing, often 3.</span></span>
<span id="cb8-335"><a href="#cb8-335" aria-hidden="true" tabindex="-1"></a><span class="co">    :param name: name.</span></span>
<span id="cb8-336"><a href="#cb8-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-337"><a href="#cb8-337" aria-hidden="true" tabindex="-1"></a><span class="co">    :return: (pose, activation) of output capsules.</span></span>
<span id="cb8-338"><a href="#cb8-338" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-339"><a href="#cb8-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-340"><a href="#cb8-340" aria-hidden="true" tabindex="-1"></a>    V_shape <span class="op">=</span> V.get_shape().as_list()</span>
<span id="cb8-341"><a href="#cb8-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-342"><a href="#cb8-342" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(name) <span class="im">as</span> scope:</span>
<span id="cb8-343"><a href="#cb8-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-344"><a href="#cb8-344" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Match R_ij (routing assignment) shape, a_last shape with V shape for broadcasting in EM routing</span></span>
<span id="cb8-345"><a href="#cb8-345" aria-hidden="true" tabindex="-1"></a>        <span class="co"># R_ij 就是后验概率</span></span>
<span id="cb8-346"><a href="#cb8-346" aria-hidden="true" tabindex="-1"></a>        <span class="co"># R_ij: [3x3x32=288, 32, 1]</span></span>
<span id="cb8-347"><a href="#cb8-347" aria-hidden="true" tabindex="-1"></a>        <span class="co"># R_ij: routing matrix from each input capsule (i) to each output capsule (o)</span></span>
<span id="cb8-348"><a href="#cb8-348" aria-hidden="true" tabindex="-1"></a>        R_ij <span class="op">=</span> tf.constant(</span>
<span id="cb8-349"><a href="#cb8-349" aria-hidden="true" tabindex="-1"></a>            <span class="fl">1.0</span> <span class="op">/</span> V_shape[<span class="op">-</span><span class="dv">2</span>], shape<span class="op">=</span>V_shape[<span class="op">-</span><span class="dv">3</span>:<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> [<span class="dv">1</span>], dtype<span class="op">=</span>tf.float32</span>
<span id="cb8-350"><a href="#cb8-350" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-351"><a href="#cb8-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-352"><a href="#cb8-352" aria-hidden="true" tabindex="-1"></a>        <span class="co"># a_last: expand_dims to (24, 6, 6, 288, 1, 1)</span></span>
<span id="cb8-353"><a href="#cb8-353" aria-hidden="true" tabindex="-1"></a>        a_last <span class="op">=</span> a_last[..., tf.newaxis, tf.newaxis]</span>
<span id="cb8-354"><a href="#cb8-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-355"><a href="#cb8-355" aria-hidden="true" tabindex="-1"></a>        <span class="co"># beta_v and beta_a: expand_dims to (1, 1, 1, 1, 32, 1]</span></span>
<span id="cb8-356"><a href="#cb8-356" aria-hidden="true" tabindex="-1"></a>        beta_v <span class="op">=</span> beta_v[..., tf.newaxis, :, tf.newaxis]</span>
<span id="cb8-357"><a href="#cb8-357" aria-hidden="true" tabindex="-1"></a>        beta_a <span class="op">=</span> beta_a[..., tf.newaxis, :, tf.newaxis]</span>
<span id="cb8-358"><a href="#cb8-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-359"><a href="#cb8-359" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> m_step(R_ij, V, a_last, beta_v, beta_a, inverse_temperature):</span>
<span id="cb8-360"><a href="#cb8-360" aria-hidden="true" tabindex="-1"></a>            <span class="co">"""The M-Step in EM Routing from input capsules i to output capsule j.</span></span>
<span id="cb8-361"><a href="#cb8-361" aria-hidden="true" tabindex="-1"></a><span class="co">            i: input capsules (32)</span></span>
<span id="cb8-362"><a href="#cb8-362" aria-hidden="true" tabindex="-1"></a><span class="co">            o: output capsules (32)</span></span>
<span id="cb8-363"><a href="#cb8-363" aria-hidden="true" tabindex="-1"></a><span class="co">            h: 4x4 = 16</span></span>
<span id="cb8-364"><a href="#cb8-364" aria-hidden="true" tabindex="-1"></a><span class="co">            output spatial dimension: 6x6</span></span>
<span id="cb8-365"><a href="#cb8-365" aria-hidden="true" tabindex="-1"></a><span class="co">            :param R_ij: routing assignments. shape = (kh x kw x i, o, 1) =(3x3x32, 32, 1) = (288, 32, 1)</span></span>
<span id="cb8-366"><a href="#cb8-366" aria-hidden="true" tabindex="-1"></a><span class="co">            :param V. shape = (N, OH, OW, kh x kw x i, o, 4x4) = (24, 6, 6, 288, 32, 16)</span></span>
<span id="cb8-367"><a href="#cb8-367" aria-hidden="true" tabindex="-1"></a><span class="co">            :param a_last: input capsule activation (at Level L). (N, OH, OW, kh x kw x i, 1, 1) = (24, 6, 6, 288, 1, 1)</span></span>
<span id="cb8-368"><a href="#cb8-368" aria-hidden="true" tabindex="-1"></a><span class="co">               with dimensions expanded to match V for broadcasting.</span></span>
<span id="cb8-369"><a href="#cb8-369" aria-hidden="true" tabindex="-1"></a><span class="co">            :param beta_v: Trainable parameters in computing cost (1, 1, 1, 1, 32, 1)</span></span>
<span id="cb8-370"><a href="#cb8-370" aria-hidden="true" tabindex="-1"></a><span class="co">            :param beta_a: Trainable parameters in computing next level activation (1, 1, 1, 1, 32, 1)</span></span>
<span id="cb8-371"><a href="#cb8-371" aria-hidden="true" tabindex="-1"></a><span class="co">            :param inverse_temperature: lambda, increase over each iteration by the caller.</span></span>
<span id="cb8-372"><a href="#cb8-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-373"><a href="#cb8-373" aria-hidden="true" tabindex="-1"></a><span class="co">            :return: (M, sigma, o_activation)</span></span>
<span id="cb8-374"><a href="#cb8-374" aria-hidden="true" tabindex="-1"></a><span class="co">            """</span></span>
<span id="cb8-375"><a href="#cb8-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-376"><a href="#cb8-376" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 用于计算归一化后验概率的零时变量</span></span>
<span id="cb8-377"><a href="#cb8-377" aria-hidden="true" tabindex="-1"></a>            R_ij_a_last <span class="op">=</span> R_ij <span class="op">*</span> a_last</span>
<span id="cb8-378"><a href="#cb8-378" aria-hidden="true" tabindex="-1"></a>            <span class="co"># R_ij_a_last_sum: sum over all input capsule i</span></span>
<span id="cb8-379"><a href="#cb8-379" aria-hidden="true" tabindex="-1"></a>            R_ij_a_last_sum <span class="op">=</span> tf.reduce_sum(R_ij_a_last, axis<span class="op">=-</span><span class="dv">3</span>, keepdims<span class="op">=</span><span class="va">True</span>, name<span class="op">=</span><span class="st">'R_ij_a_last_sum'</span>)</span>
<span id="cb8-380"><a href="#cb8-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-381"><a href="#cb8-381" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 计算聚类中心,也就是新样本</span></span>
<span id="cb8-382"><a href="#cb8-382" aria-hidden="true" tabindex="-1"></a>            M <span class="op">=</span> tf.reduce_sum(R_ij_a_last <span class="op">*</span> V, axis<span class="op">=-</span><span class="dv">3</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="op">/</span> R_ij_a_last_sum</span>
<span id="cb8-383"><a href="#cb8-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-384"><a href="#cb8-384" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 计算输出方差sigma:  sigma (24, 6, 6, 1, 32, 16)</span></span>
<span id="cb8-385"><a href="#cb8-385" aria-hidden="true" tabindex="-1"></a>            sigma <span class="op">=</span> tf.sqrt(</span>
<span id="cb8-386"><a href="#cb8-386" aria-hidden="true" tabindex="-1"></a>                tf.reduce_sum(</span>
<span id="cb8-387"><a href="#cb8-387" aria-hidden="true" tabindex="-1"></a>                    R_ij_a_last <span class="op">*</span> tf.square(V <span class="op">-</span> M), axis<span class="op">=-</span><span class="dv">3</span>, keepdims<span class="op">=</span><span class="va">True</span></span>
<span id="cb8-388"><a href="#cb8-388" aria-hidden="true" tabindex="-1"></a>                ) <span class="op">/</span> R_ij_a_last_sum</span>
<span id="cb8-389"><a href="#cb8-389" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-390"><a href="#cb8-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-391"><a href="#cb8-391" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 计算每component的差异 cost_n: (24, 6, 6, 1, 32, 16)</span></span>
<span id="cb8-392"><a href="#cb8-392" aria-hidden="true" tabindex="-1"></a>            cost_n <span class="op">=</span> (beta_v <span class="op">+</span> tf.log(sigma <span class="op">+</span> epsilon)) <span class="op">*</span> R_ij_a_last_sum</span>
<span id="cb8-393"><a href="#cb8-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-394"><a href="#cb8-394" aria-hidden="true" tabindex="-1"></a>            <span class="co"># cost: (24, 6, 6, 1, 32, 1)</span></span>
<span id="cb8-395"><a href="#cb8-395" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 计算求和得到熵</span></span>
<span id="cb8-396"><a href="#cb8-396" aria-hidden="true" tabindex="-1"></a>            cost <span class="op">=</span> tf.reduce_sum(cost_n, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-397"><a href="#cb8-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-398"><a href="#cb8-398" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">NOTE</span><span class="co"> 为了数值上的稳定性 计算输出激活值a_j的时候,利用分布之间的差异来算</span></span>
<span id="cb8-399"><a href="#cb8-399" aria-hidden="true" tabindex="-1"></a>            cost_mu <span class="op">=</span> tf.reduce_mean(cost, axis<span class="op">=-</span><span class="dv">2</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-400"><a href="#cb8-400" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 这是熵的sigma</span></span>
<span id="cb8-401"><a href="#cb8-401" aria-hidden="true" tabindex="-1"></a>            cost_sigma <span class="op">=</span> tf.sqrt(</span>
<span id="cb8-402"><a href="#cb8-402" aria-hidden="true" tabindex="-1"></a>                tf.reduce_sum(</span>
<span id="cb8-403"><a href="#cb8-403" aria-hidden="true" tabindex="-1"></a>                    tf.square(cost <span class="op">-</span> cost_mu), axis<span class="op">=-</span><span class="dv">2</span>, keepdims<span class="op">=</span><span class="va">True</span></span>
<span id="cb8-404"><a href="#cb8-404" aria-hidden="true" tabindex="-1"></a>                ) <span class="op">/</span> cost.get_shape().as_list()[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb8-405"><a href="#cb8-405" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-406"><a href="#cb8-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-407"><a href="#cb8-407" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 计算激活值之间的差异性  a_cost = (24, 6, 6, 1, 32, 1)</span></span>
<span id="cb8-408"><a href="#cb8-408" aria-hidden="true" tabindex="-1"></a>            a_cost <span class="op">=</span> beta_a <span class="op">+</span> (cost_mu <span class="op">-</span> cost) <span class="op">/</span> (cost_sigma <span class="op">+</span> epsilon)</span>
<span id="cb8-409"><a href="#cb8-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-410"><a href="#cb8-410" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 归一化激活值 (24, 6, 6, 1, 32, 1)</span></span>
<span id="cb8-411"><a href="#cb8-411" aria-hidden="true" tabindex="-1"></a>            a_j <span class="op">=</span> tf.sigmoid(inverse_temperature <span class="op">*</span> a_cost)</span>
<span id="cb8-412"><a href="#cb8-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-413"><a href="#cb8-413" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> M, sigma, a_j</span>
<span id="cb8-414"><a href="#cb8-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-415"><a href="#cb8-415" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> e_step(M, sigma, a_j, V):</span>
<span id="cb8-416"><a href="#cb8-416" aria-hidden="true" tabindex="-1"></a>            <span class="co">"""The E-Step in EM Routing.</span></span>
<span id="cb8-417"><a href="#cb8-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-418"><a href="#cb8-418" aria-hidden="true" tabindex="-1"></a><span class="co">            :param M: (24, 6, 6, 1, 32, 16)</span></span>
<span id="cb8-419"><a href="#cb8-419" aria-hidden="true" tabindex="-1"></a><span class="co">            :param sigma: (24, 6, 6, 1, 32, 16)</span></span>
<span id="cb8-420"><a href="#cb8-420" aria-hidden="true" tabindex="-1"></a><span class="co">            :param a_j: (24, 6, 6, 1, 32, 1)</span></span>
<span id="cb8-421"><a href="#cb8-421" aria-hidden="true" tabindex="-1"></a><span class="co">            :param V: (24, 6, 6, 288, 32, 16)</span></span>
<span id="cb8-422"><a href="#cb8-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-423"><a href="#cb8-423" aria-hidden="true" tabindex="-1"></a><span class="co">            :return: R_ij</span></span>
<span id="cb8-424"><a href="#cb8-424" aria-hidden="true" tabindex="-1"></a><span class="co">            """</span></span>
<span id="cb8-425"><a href="#cb8-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-426"><a href="#cb8-426" aria-hidden="true" tabindex="-1"></a>            o_p_unit0 <span class="op">=</span> <span class="op">-</span> tf.reduce_sum(tf.square(V <span class="op">-</span> M) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> tf.square(sigma)), axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-427"><a href="#cb8-427" aria-hidden="true" tabindex="-1"></a>            o_p_unit2 <span class="op">=</span> <span class="op">-</span> tf.reduce_sum(tf.log(sigma <span class="op">+</span> epsilon), axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-428"><a href="#cb8-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-429"><a href="#cb8-429" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 求出概率 p_ij</span></span>
<span id="cb8-430"><a href="#cb8-430" aria-hidden="true" tabindex="-1"></a>            <span class="co"># (24, 6, 6, 1, 32, 16)</span></span>
<span id="cb8-431"><a href="#cb8-431" aria-hidden="true" tabindex="-1"></a>            p_ij <span class="op">=</span> o_p_unit0 <span class="op">+</span> o_p_unit2</span>
<span id="cb8-432"><a href="#cb8-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-433"><a href="#cb8-433" aria-hidden="true" tabindex="-1"></a>            <span class="co"># R_ij: (24, 6, 6, 288, 32, 1)</span></span>
<span id="cb8-434"><a href="#cb8-434" aria-hidden="true" tabindex="-1"></a>            zz <span class="op">=</span> tf.log(a_j <span class="op">+</span> epsilon) <span class="op">+</span> p_ij</span>
<span id="cb8-435"><a href="#cb8-435" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 求出后验概率</span></span>
<span id="cb8-436"><a href="#cb8-436" aria-hidden="true" tabindex="-1"></a>            R_ij <span class="op">=</span> tf.nn.softmax(zz, dim<span class="op">=</span><span class="bu">len</span>(zz.get_shape().as_list()) <span class="op">-</span> <span class="dv">2</span>)</span>
<span id="cb8-437"><a href="#cb8-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-438"><a href="#cb8-438" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> R_ij</span>
<span id="cb8-439"><a href="#cb8-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-440"><a href="#cb8-440" aria-hidden="true" tabindex="-1"></a>        <span class="co"># inverse_temperature schedule (min, max)</span></span>
<span id="cb8-441"><a href="#cb8-441" aria-hidden="true" tabindex="-1"></a>        it_min <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb8-442"><a href="#cb8-442" aria-hidden="true" tabindex="-1"></a>        it_max <span class="op">=</span> <span class="bu">min</span>(iterations, <span class="fl">3.0</span>)</span>
<span id="cb8-443"><a href="#cb8-443" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> it <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb8-444"><a href="#cb8-444" aria-hidden="true" tabindex="-1"></a>            inverse_temperature <span class="op">=</span> it_min <span class="op">+</span> (it_max <span class="op">-</span> it_min) <span class="op">*</span> it <span class="op">/</span> <span class="bu">max</span>(<span class="fl">1.0</span>, iterations <span class="op">-</span> <span class="fl">1.0</span>)</span>
<span id="cb8-445"><a href="#cb8-445" aria-hidden="true" tabindex="-1"></a>            M, sigma, a_j <span class="op">=</span> m_step(</span>
<span id="cb8-446"><a href="#cb8-446" aria-hidden="true" tabindex="-1"></a>                R_ij, V, a_last, beta_v, beta_a, inverse_temperature<span class="op">=</span>inverse_temperature</span>
<span id="cb8-447"><a href="#cb8-447" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-448"><a href="#cb8-448" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> it <span class="op">&lt;</span> iterations <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb8-449"><a href="#cb8-449" aria-hidden="true" tabindex="-1"></a>                R_ij <span class="op">=</span> e_step(</span>
<span id="cb8-450"><a href="#cb8-450" aria-hidden="true" tabindex="-1"></a>                    M, sigma, a_j, V</span>
<span id="cb8-451"><a href="#cb8-451" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb8-452"><a href="#cb8-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-453"><a href="#cb8-453" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pose: (N, OH, OW, o 4 x 4) via squeeze M (24, 6, 6, 32, 16)</span></span>
<span id="cb8-454"><a href="#cb8-454" aria-hidden="true" tabindex="-1"></a>        M <span class="op">=</span> tf.squeeze(M, axis<span class="op">=-</span><span class="dv">3</span>)</span>
<span id="cb8-455"><a href="#cb8-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-456"><a href="#cb8-456" aria-hidden="true" tabindex="-1"></a>        <span class="co"># activation: (N, OH, OW, o) via squeeze o_activationis [24, 6, 6, 32]</span></span>
<span id="cb8-457"><a href="#cb8-457" aria-hidden="true" tabindex="-1"></a>        a_j <span class="op">=</span> tf.squeeze(a_j, axis<span class="op">=</span>[<span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb8-458"><a href="#cb8-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-459"><a href="#cb8-459" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> M, a_j</span>
<span id="cb8-460"><a href="#cb8-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-461"><a href="#cb8-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-462"><a href="#cb8-462" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> capsules_net(inputs, num_classes, iterations, batch_size, name<span class="op">=</span><span class="st">'capsule_em'</span>):</span>
<span id="cb8-463"><a href="#cb8-463" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Define the Capsule Network model</span></span>
<span id="cb8-464"><a href="#cb8-464" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-465"><a href="#cb8-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-466"><a href="#cb8-466" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(name) <span class="im">as</span> scope:</span>
<span id="cb8-467"><a href="#cb8-467" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ReLU Conv1</span></span>
<span id="cb8-468"><a href="#cb8-468" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Images shape (24, 28, 28, 1) -&gt; conv 5x5 filters, 32 output channels, strides 2 with padding, ReLU</span></span>
<span id="cb8-469"><a href="#cb8-469" aria-hidden="true" tabindex="-1"></a>        <span class="co"># nets -&gt; (?, 14, 14, 32)</span></span>
<span id="cb8-470"><a href="#cb8-470" aria-hidden="true" tabindex="-1"></a>        nets <span class="op">=</span> conv2d(</span>
<span id="cb8-471"><a href="#cb8-471" aria-hidden="true" tabindex="-1"></a>            inputs,</span>
<span id="cb8-472"><a href="#cb8-472" aria-hidden="true" tabindex="-1"></a>            kernel<span class="op">=</span><span class="dv">5</span>, out_channels<span class="op">=</span><span class="dv">32</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">'SAME'</span>,</span>
<span id="cb8-473"><a href="#cb8-473" aria-hidden="true" tabindex="-1"></a>            activation_fn<span class="op">=</span>tf.nn.relu, name<span class="op">=</span><span class="st">'relu_conv1'</span></span>
<span id="cb8-474"><a href="#cb8-474" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-475"><a href="#cb8-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-476"><a href="#cb8-476" aria-hidden="true" tabindex="-1"></a>        <span class="co"># PrimaryCaps</span></span>
<span id="cb8-477"><a href="#cb8-477" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (?, 14, 14, 32) -&gt; capsule 1x1 filter, 32 output capsule, strides 1 without padding</span></span>
<span id="cb8-478"><a href="#cb8-478" aria-hidden="true" tabindex="-1"></a>        <span class="co"># nets -&gt; (poses (?, 14, 14, 32, 4, 4), activations (?, 14, 14, 32))</span></span>
<span id="cb8-479"><a href="#cb8-479" aria-hidden="true" tabindex="-1"></a>        nets <span class="op">=</span> primary_caps(</span>
<span id="cb8-480"><a href="#cb8-480" aria-hidden="true" tabindex="-1"></a>            nets,</span>
<span id="cb8-481"><a href="#cb8-481" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span><span class="dv">1</span>, out_capsules<span class="op">=</span><span class="dv">32</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">'VALID'</span>,</span>
<span id="cb8-482"><a href="#cb8-482" aria-hidden="true" tabindex="-1"></a>            pose_shape<span class="op">=</span>[<span class="dv">4</span>, <span class="dv">4</span>], name<span class="op">=</span><span class="st">'primary_caps'</span></span>
<span id="cb8-483"><a href="#cb8-483" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-484"><a href="#cb8-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-485"><a href="#cb8-485" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ConvCaps1</span></span>
<span id="cb8-486"><a href="#cb8-486" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (poses, activations) -&gt; conv capsule, 3x3 kernels, strides 2, no padding</span></span>
<span id="cb8-487"><a href="#cb8-487" aria-hidden="true" tabindex="-1"></a>        <span class="co"># nets -&gt; (poses (24, 6, 6, 32, 4, 4), activations (24, 6, 6, 32))</span></span>
<span id="cb8-488"><a href="#cb8-488" aria-hidden="true" tabindex="-1"></a>        nets <span class="op">=</span> conv_capsule(</span>
<span id="cb8-489"><a href="#cb8-489" aria-hidden="true" tabindex="-1"></a>            nets, shape<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>], strides<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>], iterations<span class="op">=</span>iterations,</span>
<span id="cb8-490"><a href="#cb8-490" aria-hidden="true" tabindex="-1"></a>            batch_size<span class="op">=</span>batch_size, name<span class="op">=</span><span class="st">'conv_caps1'</span></span>
<span id="cb8-491"><a href="#cb8-491" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-492"><a href="#cb8-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-493"><a href="#cb8-493" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ConvCaps2</span></span>
<span id="cb8-494"><a href="#cb8-494" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (poses, activations) -&gt; conv capsule, 3x3 kernels, strides 1, no padding</span></span>
<span id="cb8-495"><a href="#cb8-495" aria-hidden="true" tabindex="-1"></a>        <span class="co"># nets -&gt; (poses (24, 4, 4, 32, 4, 4), activations (24, 4, 4, 32))</span></span>
<span id="cb8-496"><a href="#cb8-496" aria-hidden="true" tabindex="-1"></a>        nets <span class="op">=</span> conv_capsule(</span>
<span id="cb8-497"><a href="#cb8-497" aria-hidden="true" tabindex="-1"></a>            nets, shape<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>], strides<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>], iterations<span class="op">=</span>iterations,</span>
<span id="cb8-498"><a href="#cb8-498" aria-hidden="true" tabindex="-1"></a>            batch_size<span class="op">=</span>batch_size, name<span class="op">=</span><span class="st">'conv_caps2'</span></span>
<span id="cb8-499"><a href="#cb8-499" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-500"><a href="#cb8-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-501"><a href="#cb8-501" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Class capsules</span></span>
<span id="cb8-502"><a href="#cb8-502" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (poses, activations) -&gt; 1x1 convolution, 10 output capsules</span></span>
<span id="cb8-503"><a href="#cb8-503" aria-hidden="true" tabindex="-1"></a>        <span class="co"># nets -&gt; (poses (24, 10, 4, 4), activations (24, 10))</span></span>
<span id="cb8-504"><a href="#cb8-504" aria-hidden="true" tabindex="-1"></a>        nets <span class="op">=</span> class_capsules(nets, num_classes, iterations<span class="op">=</span>iterations,</span>
<span id="cb8-505"><a href="#cb8-505" aria-hidden="true" tabindex="-1"></a>                              batch_size<span class="op">=</span>batch_size, name<span class="op">=</span><span class="st">'class_capsules'</span>)</span>
<span id="cb8-506"><a href="#cb8-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-507"><a href="#cb8-507" aria-hidden="true" tabindex="-1"></a>        <span class="co"># poses (24, 10, 4, 4), activations (24, 10)</span></span>
<span id="cb8-508"><a href="#cb8-508" aria-hidden="true" tabindex="-1"></a>        poses, activations <span class="op">=</span> nets</span>
<span id="cb8-509"><a href="#cb8-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-510"><a href="#cb8-510" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> poses, activations</span>
<span id="cb8-511"><a href="#cb8-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-512"><a href="#cb8-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-513"><a href="#cb8-513" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> spread_loss(labels, activations, iterations_per_epoch, global_step, name):</span>
<span id="cb8-514"><a href="#cb8-514" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Spread loss</span></span>
<span id="cb8-515"><a href="#cb8-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-516"><a href="#cb8-516" aria-hidden="true" tabindex="-1"></a><span class="co">    :param labels: (24, 10] in one-hot vector</span></span>
<span id="cb8-517"><a href="#cb8-517" aria-hidden="true" tabindex="-1"></a><span class="co">    :param activations: [24, 10], activation for each class</span></span>
<span id="cb8-518"><a href="#cb8-518" aria-hidden="true" tabindex="-1"></a><span class="co">    :param margin: increment from 0.2 to 0.9 during training</span></span>
<span id="cb8-519"><a href="#cb8-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-520"><a href="#cb8-520" aria-hidden="true" tabindex="-1"></a><span class="co">    :return: spread loss</span></span>
<span id="cb8-521"><a href="#cb8-521" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-522"><a href="#cb8-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-523"><a href="#cb8-523" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Margin schedule</span></span>
<span id="cb8-524"><a href="#cb8-524" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Margin increase from 0.2 to 0.9 by an increment of 0.1 for every epoch</span></span>
<span id="cb8-525"><a href="#cb8-525" aria-hidden="true" tabindex="-1"></a>    margin <span class="op">=</span> tf.train.piecewise_constant(</span>
<span id="cb8-526"><a href="#cb8-526" aria-hidden="true" tabindex="-1"></a>        tf.cast(global_step, dtype<span class="op">=</span>tf.int32),</span>
<span id="cb8-527"><a href="#cb8-527" aria-hidden="true" tabindex="-1"></a>        boundaries<span class="op">=</span>[</span>
<span id="cb8-528"><a href="#cb8-528" aria-hidden="true" tabindex="-1"></a>            (iterations_per_epoch <span class="op">*</span> x) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">8</span>)</span>
<span id="cb8-529"><a href="#cb8-529" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb8-530"><a href="#cb8-530" aria-hidden="true" tabindex="-1"></a>        values<span class="op">=</span>[</span>
<span id="cb8-531"><a href="#cb8-531" aria-hidden="true" tabindex="-1"></a>            x <span class="op">/</span> <span class="fl">10.0</span> <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb8-532"><a href="#cb8-532" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb8-533"><a href="#cb8-533" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-534"><a href="#cb8-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-535"><a href="#cb8-535" aria-hidden="true" tabindex="-1"></a>    activations_shape <span class="op">=</span> activations.get_shape().as_list()</span>
<span id="cb8-536"><a href="#cb8-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-537"><a href="#cb8-537" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(name) <span class="im">as</span> scope:</span>
<span id="cb8-538"><a href="#cb8-538" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mask_t, mask_f Tensor (?, 10)</span></span>
<span id="cb8-539"><a href="#cb8-539" aria-hidden="true" tabindex="-1"></a>        mask_t <span class="op">=</span> tf.equal(labels, <span class="dv">1</span>)      <span class="co"># Mask for the true label</span></span>
<span id="cb8-540"><a href="#cb8-540" aria-hidden="true" tabindex="-1"></a>        mask_i <span class="op">=</span> tf.equal(labels, <span class="dv">0</span>)      <span class="co"># Mask for the non-true label</span></span>
<span id="cb8-541"><a href="#cb8-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-542"><a href="#cb8-542" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Activation for the true label</span></span>
<span id="cb8-543"><a href="#cb8-543" aria-hidden="true" tabindex="-1"></a>        <span class="co"># activations_t (?, 1)</span></span>
<span id="cb8-544"><a href="#cb8-544" aria-hidden="true" tabindex="-1"></a>        activations_t <span class="op">=</span> tf.reshape(tf.boolean_mask(activations, mask_t), shape<span class="op">=</span>(tf.shape(activations)[<span class="dv">0</span>], <span class="dv">1</span>))</span>
<span id="cb8-545"><a href="#cb8-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-546"><a href="#cb8-546" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Activation for the other classes</span></span>
<span id="cb8-547"><a href="#cb8-547" aria-hidden="true" tabindex="-1"></a>        <span class="co"># activations_i (?, 9)</span></span>
<span id="cb8-548"><a href="#cb8-548" aria-hidden="true" tabindex="-1"></a>        activations_i <span class="op">=</span> tf.reshape(</span>
<span id="cb8-549"><a href="#cb8-549" aria-hidden="true" tabindex="-1"></a>            tf.boolean_mask(activations, mask_i), [tf.shape(activations)[<span class="dv">0</span>], activations_shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>]</span>
<span id="cb8-550"><a href="#cb8-550" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-551"><a href="#cb8-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-552"><a href="#cb8-552" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> tf.reduce_sum(tf.square(tf.maximum(<span class="fl">0.0</span>, margin <span class="op">-</span> (activations_t <span class="op">-</span> activations_i))))</span>
<span id="cb8-553"><a href="#cb8-553" aria-hidden="true" tabindex="-1"></a>        tf.losses.add_loss(l)</span>
<span id="cb8-554"><a href="#cb8-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-555"><a href="#cb8-555" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> l</span>
<span id="cb8-556"><a href="#cb8-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-557"><a href="#cb8-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-558"><a href="#cb8-558" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_image(image, label):</span>
<span id="cb8-559"><a href="#cb8-559" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Scale the image value between -1 and 1.</span></span>
<span id="cb8-560"><a href="#cb8-560" aria-hidden="true" tabindex="-1"></a><span class="co">      :param image: An image in Tensor.</span></span>
<span id="cb8-561"><a href="#cb8-561" aria-hidden="true" tabindex="-1"></a><span class="co">      :return A scaled image in Tensor.</span></span>
<span id="cb8-562"><a href="#cb8-562" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-563"><a href="#cb8-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-564"><a href="#cb8-564" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tf.to_float(image)</span>
<span id="cb8-565"><a href="#cb8-565" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tf.subtract(image, <span class="fl">128.0</span>)</span>
<span id="cb8-566"><a href="#cb8-566" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tf.div(image, <span class="fl">128.0</span>)</span>
<span id="cb8-567"><a href="#cb8-567" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tf.reshape(image, (<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb8-568"><a href="#cb8-568" aria-hidden="true" tabindex="-1"></a>    label <span class="op">=</span> tf.one_hot(label, <span class="dv">10</span>)</span>
<span id="cb8-569"><a href="#cb8-569" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image, label</span>
<span id="cb8-570"><a href="#cb8-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-571"><a href="#cb8-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-572"><a href="#cb8-572" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_batch(batch_size<span class="op">=</span><span class="dv">32</span>) <span class="op">-&gt;</span> [tf.Tensor, tf.Tensor]:</span>
<span id="cb8-573"><a href="#cb8-573" aria-hidden="true" tabindex="-1"></a>    (x_train, y_train), (x_test, y_test) <span class="op">=</span> mnist.load_data()</span>
<span id="cb8-574"><a href="#cb8-574" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> (tf.data.Dataset.from_tensor_slices((x_train, y_train))</span>
<span id="cb8-575"><a href="#cb8-575" aria-hidden="true" tabindex="-1"></a>               .shuffle(<span class="dv">10000</span>, <span class="dv">66</span>)</span>
<span id="cb8-576"><a href="#cb8-576" aria-hidden="true" tabindex="-1"></a>               .repeat()</span>
<span id="cb8-577"><a href="#cb8-577" aria-hidden="true" tabindex="-1"></a>               .<span class="bu">map</span>(preprocess_image)</span>
<span id="cb8-578"><a href="#cb8-578" aria-hidden="true" tabindex="-1"></a>               .batch(batch_size))  <span class="co"># type: tf.data.Dataset</span></span>
<span id="cb8-579"><a href="#cb8-579" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dataset._make_one_shot_iterator().get_next()</span>
<span id="cb8-580"><a href="#cb8-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-581"><a href="#cb8-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-582"><a href="#cb8-582" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb8-583"><a href="#cb8-583" aria-hidden="true" tabindex="-1"></a>    tf.logging.set_verbosity(tf.logging.INFO)</span>
<span id="cb8-584"><a href="#cb8-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-585"><a href="#cb8-585" aria-hidden="true" tabindex="-1"></a>    seed <span class="op">=</span> <span class="dv">66</span></span>
<span id="cb8-586"><a href="#cb8-586" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb8-587"><a href="#cb8-587" aria-hidden="true" tabindex="-1"></a>    log_dir <span class="op">=</span> <span class="st">'./log/train'</span></span>
<span id="cb8-588"><a href="#cb8-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-589"><a href="#cb8-589" aria-hidden="true" tabindex="-1"></a>    config <span class="op">=</span> tf.ConfigProto()</span>
<span id="cb8-590"><a href="#cb8-590" aria-hidden="true" tabindex="-1"></a>    config.gpu_options.allow_growth <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-591"><a href="#cb8-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-592"><a href="#cb8-592" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb8-593"><a href="#cb8-593" aria-hidden="true" tabindex="-1"></a>    tf.set_random_seed(seed)</span>
<span id="cb8-594"><a href="#cb8-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-595"><a href="#cb8-595" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Slim dataset contains data sources, decoder, reader and other meta-information</span></span>
<span id="cb8-596"><a href="#cb8-596" aria-hidden="true" tabindex="-1"></a>    iterations_per_epoch <span class="op">=</span> <span class="dv">60000</span> <span class="op">//</span> batch_size  <span class="co"># 60,000/24 = 2500</span></span>
<span id="cb8-597"><a href="#cb8-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-598"><a href="#cb8-598" aria-hidden="true" tabindex="-1"></a>    <span class="co"># images: Tensor (?, 28, 28, 1)</span></span>
<span id="cb8-599"><a href="#cb8-599" aria-hidden="true" tabindex="-1"></a>    <span class="co"># labels: Tensor (?)</span></span>
<span id="cb8-600"><a href="#cb8-600" aria-hidden="true" tabindex="-1"></a>    next_images, next_labels <span class="op">=</span> load_batch(batch_size)</span>
<span id="cb8-601"><a href="#cb8-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-602"><a href="#cb8-602" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> tf.placeholder_with_default(next_images, (batch_size, <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb8-603"><a href="#cb8-603" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> tf.placeholder_with_default(next_labels, (batch_size, <span class="dv">10</span>))</span>
<span id="cb8-604"><a href="#cb8-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-605"><a href="#cb8-605" aria-hidden="true" tabindex="-1"></a>    <span class="co"># poses: Tensor(?, 10, 4, 4) activations: (?, 10)</span></span>
<span id="cb8-606"><a href="#cb8-606" aria-hidden="true" tabindex="-1"></a>    poses, activations <span class="op">=</span> capsules_net(images, num_classes<span class="op">=</span><span class="dv">10</span>, iterations<span class="op">=</span><span class="dv">3</span>, batch_size<span class="op">=</span>batch_size, name<span class="op">=</span><span class="st">'capsules_em'</span>)</span>
<span id="cb8-607"><a href="#cb8-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-608"><a href="#cb8-608" aria-hidden="true" tabindex="-1"></a>    global_step <span class="op">=</span> tf.train.get_or_create_global_step()</span>
<span id="cb8-609"><a href="#cb8-609" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> spread_loss(labels, activations, iterations_per_epoch, global_step, name<span class="op">=</span><span class="st">'spread_loss'</span>)</span>
<span id="cb8-610"><a href="#cb8-610" aria-hidden="true" tabindex="-1"></a>    tf.summary.scalar(<span class="st">'losses/spread_loss'</span>, loss)</span>
<span id="cb8-611"><a href="#cb8-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-612"><a href="#cb8-612" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> tf.train.AdamOptimizer(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb8-613"><a href="#cb8-613" aria-hidden="true" tabindex="-1"></a>    train_tensor <span class="op">=</span> slim.learning.create_train_op(loss, optimizer, global_step<span class="op">=</span>global_step, clip_gradient_norm<span class="op">=</span><span class="fl">4.0</span>)</span>
<span id="cb8-614"><a href="#cb8-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-615"><a href="#cb8-615" aria-hidden="true" tabindex="-1"></a>    slim.learning.train(</span>
<span id="cb8-616"><a href="#cb8-616" aria-hidden="true" tabindex="-1"></a>        train_tensor,</span>
<span id="cb8-617"><a href="#cb8-617" aria-hidden="true" tabindex="-1"></a>        logdir<span class="op">=</span>log_dir,</span>
<span id="cb8-618"><a href="#cb8-618" aria-hidden="true" tabindex="-1"></a>        log_every_n_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb8-619"><a href="#cb8-619" aria-hidden="true" tabindex="-1"></a>        save_summaries_secs<span class="op">=</span><span class="dv">60</span>,</span>
<span id="cb8-620"><a href="#cb8-620" aria-hidden="true" tabindex="-1"></a>        saver<span class="op">=</span>tf.train.Saver(max_to_keep<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb8-621"><a href="#cb8-621" aria-hidden="true" tabindex="-1"></a>        save_interval_secs<span class="op">=</span><span class="dv">600</span>,</span>
<span id="cb8-622"><a href="#cb8-622" aria-hidden="true" tabindex="-1"></a>        session_config<span class="op">=</span>config</span>
<span id="cb8-623"><a href="#cb8-623" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-624"><a href="#cb8-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-625"><a href="#cb8-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-626"><a href="#cb8-626" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb8-627"><a href="#cb8-627" aria-hidden="true" tabindex="-1"></a>    main()                                       </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
</section>
<section id="参考" class="level1">
<h1>参考</h1>
<ol type="1">
<li><p><a href="https://kexue.fm/archives/5155">苏建林的博客</a></p></li>
<li><p><a href="https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-Capsule-Network/">理解胶囊网络</a></p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/zhen8838\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>