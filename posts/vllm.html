<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-02-14">

<title>推理框架调研 – Zheng’s Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-8b4baf804e461d9b72633f0de59a0cac.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7072389654d23eff08f359f9aa0d1ee7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Zheng’s Notes</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#llm模型结构" id="toc-llm模型结构" class="nav-link active" data-scroll-target="#llm模型结构">LLM模型结构</a></li>
  <li><a href="#vllm" id="toc-vllm" class="nav-link" data-scroll-target="#vllm">vllm</a>
  <ul class="collapse">
  <li><a href="#安装-开发" id="toc-安装-开发" class="nav-link" data-scroll-target="#安装-开发">安装 &amp; 开发</a></li>
  <li><a href="#cuda-graph" id="toc-cuda-graph" class="nav-link" data-scroll-target="#cuda-graph">cuda graph</a></li>
  <li><a href="#attention-layer-size-trace" id="toc-attention-layer-size-trace" class="nav-link" data-scroll-target="#attention-layer-size-trace">attention layer size trace</a></li>
  <li><a href="#vllm-attention-detail" id="toc-vllm-attention-detail" class="nav-link" data-scroll-target="#vllm-attention-detail">vllm attention detail</a></li>
  <li><a href="#vllm-fused-moe" id="toc-vllm-fused-moe" class="nav-link" data-scroll-target="#vllm-fused-moe">vllm fused moe</a></li>
  <li><a href="#vllm并行模式" id="toc-vllm并行模式" class="nav-link" data-scroll-target="#vllm并行模式">vllm并行模式</a></li>
  </ul></li>
  <li><a href="#trt-llm" id="toc-trt-llm" class="nav-link" data-scroll-target="#trt-llm">trt llm</a>
  <ul class="collapse">
  <li><a href="#plugin" id="toc-plugin" class="nav-link" data-scroll-target="#plugin">plugin</a></li>
  <li><a href="#auto-parallel" id="toc-auto-parallel" class="nav-link" data-scroll-target="#auto-parallel">auto parallel</a></li>
  <li><a href="#pyexector" id="toc-pyexector" class="nav-link" data-scroll-target="#pyexector">PyExector</a></li>
  </ul></li>
  <li><a href="#mlc-llm" id="toc-mlc-llm" class="nav-link" data-scroll-target="#mlc-llm">mlc llm</a></li>
  <li><a href="#sglang" id="toc-sglang" class="nav-link" data-scroll-target="#sglang">sglang</a></li>
  <li><a href="#问题汇总" id="toc-问题汇总" class="nav-link" data-scroll-target="#问题汇总">问题汇总</a>
  <ul class="collapse">
  <li><a href="#vllm问题" id="toc-vllm问题" class="nav-link" data-scroll-target="#vllm问题">vllm问题</a></li>
  <li><a href="#trt-llm问题" id="toc-trt-llm问题" class="nav-link" data-scroll-target="#trt-llm问题">trt llm问题</a></li>
  <li><a href="#scaled-dot-product-attention-sdpa-精度问题" id="toc-scaled-dot-product-attention-sdpa-精度问题" class="nav-link" data-scroll-target="#scaled-dot-product-attention-sdpa-精度问题">scaled dot product attention (SDPA) 精度问题</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">推理框架调研</h1>
  <div class="quarto-categories">
    <div class="quarto-category">推理框架</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 14, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>记录一下学习vllm/trt llm等框架的内容。</p>
<!--more-->
<section id="llm模型结构" class="level1">
<h1>LLM模型结构</h1>
<p>虽然用编译器编译了挺久的LLM，但其实对于宏观上的模型结构还是理解的不够深入。</p>
<p><img src="vllm/llama_arch.png" class="img-fluid"></p>
<p>首先现在的LLM基本上是重复Attention + FFN的结构，Attention里面首先有三个权重矩阵来计算得到QKV。 然后把QKV转换为<code>[batch,num_head,seq_len,head_size]</code>的形式, 然后是<code>Q*K^T</code>,这一步是计算每个head的相似度，计算完之后head size进行了规约，得到<code>[batch,num_head,seq_len,total_seq_len]</code>，然后是scaling以及softmax。后面再和V的<code>total seq len</code>维度进行规约，最后又得到<code>[batch,num_head,seq_len,head_size]</code>的输出。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>q: [batch,num_head,target_len,head_size]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>k: [batch,num_head,source_len,head_size]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>v: [batch,num_head,source_len,head_size]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>s: [batch,num_head,target_len,source_len] <span class="op">=</span> q <span class="op">@</span> k.T</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> s <span class="op">*</span> scale <span class="op">+</span> mask</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>s: [batch,num_head,target_len,source_len] <span class="op">=</span> softmax(s,<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>d: [batch,num_head,target_len,head_size] <span class="op">=</span> s <span class="op">@</span> v</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><img src="vllm/attention.png" class="img-fluid"></p>
</section>
<section id="vllm" class="level1">
<h1>vllm</h1>
<p>vll支持的优化： 1. Continuous Batching 2. Paged Attention 3. Chunked Prefill 要把decode融合到prefill中一起执行，当然比如要求<code>[prefill_token,decode_token]</code>的排列顺序。</p>
<section id="安装-开发" class="level2">
<h2 class="anchored" data-anchor-id="安装-开发">安装 &amp; 开发</h2>
<p>为了构建一个方便调试的开发环境，可以采用官方镜像, 并且需要自己修改entrypoint：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> run <span class="at">-d</span> <span class="at">-it</span> <span class="dt">\</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--gpus</span> all <span class="dt">\</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--name</span> vllm_dev <span class="dt">\</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--cap-add</span><span class="op">=</span>NET_ADMIN <span class="dt">\</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">--network</span><span class="op">=</span>host <span class="dt">\</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">--privileged</span><span class="op">=</span>true <span class="dt">\</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">--shm-size</span> 50g <span class="dt">\</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">--entrypoint</span> /bin/bash <span class="dt">\</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  vllm/vllm-openai:latest <span class="dt">\</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">-c</span> <span class="st">"while true; do sleep 10; done"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>进入官方镜像之后，他自带一个python3的环境，并且安装好了所有依赖，所以直接clone最新的vllm的并只安装python部分即可：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://gitee.com/mirrors/vllm.git</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> vllm</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">VLLM_TARGET_DEVICE</span><span class="op">=</span>cuda</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">VLLM_USE_PRECOMPILED</span><span class="op">=</span>1</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> use_existing_torch.py</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">--no-build-isolation</span> <span class="at">-e</span> .</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="cuda-graph" class="level2">
<h2 class="anchored" data-anchor-id="cuda-graph">cuda graph</h2>
<p>基于qwen 2.5 0.5b， 在<code>vllm/worker/model_runner.py</code>中1915行，注意他这里capture并不是一次，而是类似shape bucket, 迭代这些batch size<code>[256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1]</code>，他们也只会在batch上进行capture。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._graph <span class="op">=</span> torch.cuda.CUDAGraph()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._graph.enable_debug_mode()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.cuda.graph(<span class="va">self</span>._graph, pool<span class="op">=</span>memory_pool, stream<span class="op">=</span>stream):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>            output_hidden_or_intermediate_states <span class="op">=</span> <span class="va">self</span>.model(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                input_ids<span class="op">=</span>input_ids, <span class="co"># [256]</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                positions<span class="op">=</span>positions, <span class="co"># [256]</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                kv_caches<span class="op">=</span>kv_caches, <span class="co"># [[2, 103168, 16, 2, 64], [2, 103168, 16, 2, 64],...] 24个</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                attn_metadata<span class="op">=</span>attn_metadata,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                intermediate_tensors<span class="op">=</span>intermediate_inputs,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                <span class="op">**</span>kwargs,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(output_hidden_or_intermediate_states, torch.Tensor):</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>                hidden_or_intermediate_states <span class="op">=</span> weak_ref_tensor(</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>                    output_hidden_or_intermediate_states)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="bu">isinstance</span>(output_hidden_or_intermediate_states,</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>                            IntermediateTensors):</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                hidden_or_intermediate_states <span class="op">=</span> IntermediateTensors(</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>                    tensors<span class="op">=</span>{</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>                        key: weak_ref_tensor(value)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> key, value <span class="kw">in</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                        output_hidden_or_intermediate_states.tensors.items()</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                    })</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> output_hidden_or_intermediate_states</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># make sure `output_hidden_or_intermediate_states` is deleted</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># in the graph's memory pool</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            gc.collect()</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        torch.cuda.synchronize()</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._graph.debug_dump(<span class="st">"/root/vllm_learn/graph.dot"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>不过他这里生成的cuda graph是这样的，并没有shape。 <img src="vllm/cuda_graph.png" class="img-fluid"></p>
</section>
<section id="attention-layer-size-trace" class="level2">
<h2 class="anchored" data-anchor-id="attention-layer-size-trace">attention layer size trace</h2>
<p>在模型初始化的时候，会用最大的batch size trace一次， 这是每一个att的打印代码：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        positions: torch.Tensor,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        hidden_states: torch.Tensor,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        kv_cache: torch.Tensor,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        attn_metadata: AttentionMetadata,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"hidden_states"</span>, hidden_states.shape) <span class="co"># [1, 896]</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        qkv, _ <span class="op">=</span> <span class="va">self</span>.qkv_proj(hidden_states) </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> qkv.split([<span class="va">self</span>.q_size, <span class="va">self</span>.kv_size, <span class="va">self</span>.kv_size], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"q"</span>, q.shape,<span class="st">"k"</span>, k.shape,<span class="st">"v"</span>, v.shape)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        q, k <span class="op">=</span> <span class="va">self</span>.rotary_emb(positions, q, k)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"ro q"</span>, q.shape,<span class="st">"ro k"</span>, k.shape)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> <span class="va">self</span>.attn(q, k, v, kv_cache, attn_metadata)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"attn_output"</span>, attn_output.shape)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        output, _ <span class="op">=</span> <span class="va">self</span>.o_proj(attn_output)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"output"</span>, output.shape)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>hidden_states torch.Size([<span class="dv">32768</span>, <span class="dv">896</span>])</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>q torch.Size([<span class="dv">32768</span>, <span class="dv">896</span>]) k torch.Size([<span class="dv">32768</span>, <span class="dv">128</span>]) v torch.Size([<span class="dv">32768</span>, <span class="dv">128</span>])</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>ro q torch.Size([<span class="dv">32768</span>, <span class="dv">896</span>]) ro k torch.Size([<span class="dv">32768</span>, <span class="dv">128</span>])</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>attn_output torch.Size([<span class="dv">32768</span>, <span class="dv">896</span>])</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>output torch.Size([<span class="dv">32768</span>, <span class="dv">896</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后是prefill：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>hidden_states torch.Size([<span class="dv">48</span>, <span class="dv">896</span>])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>q torch.Size([<span class="dv">48</span>, <span class="dv">896</span>]) k torch.Size([<span class="dv">48</span>, <span class="dv">128</span>]) v torch.Size([<span class="dv">48</span>, <span class="dv">128</span>])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>ro q torch.Size([<span class="dv">48</span>, <span class="dv">896</span>]) ro k torch.Size([<span class="dv">48</span>, <span class="dv">128</span>])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>attn_output torch.Size([<span class="dv">48</span>, <span class="dv">896</span>])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>output torch.Size([<span class="dv">48</span>, <span class="dv">896</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>接下来是decode:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>hidden_states torch.Size([<span class="dv">1</span>, <span class="dv">896</span>])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>q torch.Size([<span class="dv">1</span>, <span class="dv">896</span>]) k torch.Size([<span class="dv">1</span>, <span class="dv">128</span>]) v torch.Size([<span class="dv">1</span>, <span class="dv">128</span>])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>ro q torch.Size([<span class="dv">1</span>, <span class="dv">896</span>]) ro k torch.Size([<span class="dv">1</span>, <span class="dv">128</span>])</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>attn_output torch.Size([<span class="dv">1</span>, <span class="dv">896</span>])</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>output torch.Size([<span class="dv">1</span>, <span class="dv">896</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>所以最重要的就是看decode的时候是如何适配seq len增长的。</p>
</section>
<section id="vllm-attention-detail" class="level2">
<h2 class="anchored" data-anchor-id="vllm-attention-detail">vllm attention detail</h2>
<p>这是vllm中对于attention类的的设计： <img src="vllm/vllm_attn.png" class="img-fluid"></p>
<p>这是vllm调度出来的请求与attention的meta data的对应关系图： <img src="vllm/vllm scheduled requests.png" class="img-fluid"></p>
<p>在decode的时候，会调用<code>vllm/attention/layer.py</code>的attention：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        query: torch.Tensor,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        key: torch.Tensor,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        value: torch.Tensor,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        kv_cache: torch.Tensor,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        attn_metadata: AttentionMetadata,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: please avoid accessing `kv_cache` and `attn_metadata` arguments</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># directly, use `self.kv_cache` and</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `get_forward_context().attn_metadata` instead.</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.calculate_kv_scales:</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>            ctx_attn_metadata <span class="op">=</span> get_forward_context().attn_metadata</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ctx_attn_metadata.enable_kv_scales_calculation:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.calc_kv_scales(key, value)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_output:</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> torch.empty_like(query)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>            hidden_size <span class="op">=</span> query.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reshape the query, key, and value tensors.</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">NOTE</span><span class="co">(woosuk): We do this outside the custom op to minimize the</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># CPU overheads from the non-CUDA-graph regions.</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>            query <span class="op">=</span> query.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_size)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> output.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_size)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>                key <span class="op">=</span> key.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_kv_heads, <span class="va">self</span>.head_size)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> value <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>                value <span class="op">=</span> value.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_kv_heads, <span class="va">self</span>.head_size)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.use_direct_call:</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>                forward_context: ForwardContext <span class="op">=</span> get_forward_context()</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>                ctx_attn_metadata <span class="op">=</span> forward_context.attn_metadata</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>                self_kv_cache <span class="op">=</span> <span class="va">self</span>.kv_cache[forward_context.virtual_engine]</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.impl.forward(<span class="va">self</span>,</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>                                  query,</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>                                  key,</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>                                  value,</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>                                  self_kv_cache,</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>                                  ctx_attn_metadata,</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>                                  output<span class="op">=</span>output)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>                <span class="co"># note 上面把q都reshape到 [batch,num_heads,head_size]， kv转换为[num_kv_heads, head_size]</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>                <span class="co"># q [1,1,64] , k,v [1,2,64]</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>                torch.ops.vllm.unified_attention_with_output(</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>                    query, key, value, output, <span class="va">self</span>.layer_name)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> output.view(<span class="op">-</span><span class="dv">1</span>, hidden_size)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.use_direct_call:</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>                forward_context <span class="op">=</span> get_forward_context()</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>                ctx_attn_metadata <span class="op">=</span> forward_context.attn_metadata</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>                self_kv_cache <span class="op">=</span> <span class="va">self</span>.kv_cache[forward_context.virtual_engine]</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="va">self</span>.impl.forward(<span class="va">self</span>, query, key, value,</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>                                         self_kv_cache, ctx_attn_metadata)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> torch.ops.vllm.unified_attention(</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>                    query, key, value, <span class="va">self</span>.layer_name)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a><span class="co"># 虽然torch.ops.vllm.unified_attention_with_output看起来没有使用kv cache，但是实际上是使用了的。</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> unified_attention_with_output(</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    query: torch.Tensor,</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    key: torch.Tensor,</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    value: torch.Tensor,</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    output: torch.Tensor,</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>    layer_name: <span class="bu">str</span>,</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>    forward_context: ForwardContext <span class="op">=</span> get_forward_context()</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>    attn_metadata <span class="op">=</span> forward_context.attn_metadata</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span> <span class="op">=</span> forward_context.attn_layers[layer_name]</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>    kv_cache <span class="op">=</span> <span class="va">self</span>.kv_cache[forward_context.virtual_engine]</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.impl.forward(<span class="va">self</span>,</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>                      query,</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>                      key,</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>                      value,</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>                      kv_cache,</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>                      attn_metadata,</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>                      output<span class="op">=</span>output)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>检查当前的attn metadata：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>FlashAttentionMetadata(num_prefills<span class="op">=</span><span class="dv">0</span>, num_prefill_tokens<span class="op">=</span><span class="dv">0</span>, num_decode_tokens<span class="op">=</span><span class="dv">1</span>, slot_mapping<span class="op">=</span>tensor([<span class="dv">54</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>), multi_modal_placeholder_index_maps<span class="op">=</span>{}, enable_kv_scales_calculation<span class="op">=</span><span class="va">True</span>, seq_lens<span class="op">=</span>[<span class="dv">55</span>], seq_lens_tensor<span class="op">=</span>tensor([<span class="dv">55</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), max_prefill_seq_len<span class="op">=</span><span class="dv">0</span>, max_decode_seq_len<span class="op">=</span><span class="dv">55</span>, context_lens_tensor<span class="op">=</span>tensor([<span class="dv">54</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), block_tables<span class="op">=</span>tensor([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), use_cuda_graph<span class="op">=</span><span class="va">False</span>, max_query_len<span class="op">=</span><span class="dv">1</span>, max_decode_query_len<span class="op">=</span><span class="dv">1</span>, query_start_loc<span class="op">=</span>tensor([<span class="dv">0</span>, <span class="dv">1</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), seq_start_loc<span class="op">=</span>tensor([ <span class="dv">0</span>, <span class="dv">55</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), _cached_prefill_metadata<span class="op">=</span><span class="va">None</span>, _cached_decode_metadata<span class="op">=</span>FlashAttentionMetadata(num_prefills<span class="op">=</span><span class="dv">0</span>, num_prefill_tokens<span class="op">=</span><span class="dv">0</span>, num_decode_tokens<span class="op">=</span><span class="dv">1</span>, slot_mapping<span class="op">=</span>tensor([<span class="dv">54</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>), multi_modal_placeholder_index_maps<span class="op">=</span><span class="va">None</span>, enable_kv_scales_calculation<span class="op">=</span><span class="va">True</span>, seq_lens<span class="op">=</span><span class="va">None</span>, seq_lens_tensor<span class="op">=</span>tensor([<span class="dv">55</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), max_prefill_seq_len<span class="op">=</span><span class="dv">0</span>, max_decode_seq_len<span class="op">=</span><span class="dv">55</span>, context_lens_tensor<span class="op">=</span><span class="va">None</span>, block_tables<span class="op">=</span>tensor([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), use_cuda_graph<span class="op">=</span><span class="va">False</span>, max_query_len<span class="op">=</span><span class="dv">1</span>, max_decode_query_len<span class="op">=</span><span class="dv">1</span>, query_start_loc<span class="op">=</span>tensor([<span class="dv">0</span>, <span class="dv">1</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), seq_start_loc<span class="op">=</span>tensor([ <span class="dv">0</span>, <span class="dv">55</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), _cached_prefill_metadata<span class="op">=</span><span class="va">None</span>, _cached_decode_metadata<span class="op">=</span>FlashAttentionMetadata(num_prefills<span class="op">=</span><span class="dv">0</span>, num_prefill_tokens<span class="op">=</span><span class="dv">0</span>, num_decode_tokens<span class="op">=</span><span class="dv">1</span>, slot_mapping<span class="op">=</span>tensor([<span class="dv">54</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>), multi_modal_placeholder_index_maps<span class="op">=</span><span class="va">None</span>, enable_kv_scales_calculation<span class="op">=</span><span class="va">True</span>, seq_lens<span class="op">=</span><span class="va">None</span>, seq_lens_tensor<span class="op">=</span>tensor([<span class="dv">55</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), max_prefill_seq_len<span class="op">=</span><span class="dv">0</span>, max_decode_seq_len<span class="op">=</span><span class="dv">55</span>, context_lens_tensor<span class="op">=</span><span class="va">None</span>, block_tables<span class="op">=</span>tensor([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), use_cuda_graph<span class="op">=</span><span class="va">False</span>, max_query_len<span class="op">=</span><span class="dv">1</span>, max_decode_query_len<span class="op">=</span><span class="dv">1</span>, query_start_loc<span class="op">=</span>tensor([<span class="dv">0</span>, <span class="dv">1</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), seq_start_loc<span class="op">=</span>tensor([ <span class="dv">0</span>, <span class="dv">55</span>], device<span class="op">=</span><span class="st">'cuda:0'</span>, dtype<span class="op">=</span>torch.int32), _cached_prefill_metadata<span class="op">=</span><span class="va">None</span>, _cached_decode_metadata<span class="op">=</span><span class="va">None</span>, encoder_seq_lens<span class="op">=</span><span class="va">None</span>, encoder_seq_lens_tensor<span class="op">=</span><span class="va">None</span>, encoder_seq_start_loc<span class="op">=</span><span class="va">None</span>, max_encoder_seq_len<span class="op">=</span><span class="va">None</span>, num_encoder_tokens<span class="op">=</span><span class="va">None</span>, cross_slot_mapping<span class="op">=</span><span class="va">None</span>, cross_block_tables<span class="op">=</span><span class="va">None</span>), encoder_seq_lens<span class="op">=</span><span class="va">None</span>, encoder_seq_lens_tensor<span class="op">=</span><span class="va">None</span>, encoder_seq_start_loc<span class="op">=</span><span class="va">None</span>, max_encoder_seq_len<span class="op">=</span><span class="va">None</span>, num_encoder_tokens<span class="op">=</span><span class="va">None</span>, cross_slot_mapping<span class="op">=</span><span class="va">None</span>, cross_block_tables<span class="op">=</span><span class="va">None</span>), encoder_seq_lens<span class="op">=</span><span class="va">None</span>, encoder_seq_lens_tensor<span class="op">=</span><span class="va">None</span>, encoder_seq_start_loc<span class="op">=</span><span class="va">None</span>, max_encoder_seq_len<span class="op">=</span><span class="va">None</span>, num_encoder_tokens<span class="op">=</span><span class="va">None</span>, cross_slot_mapping<span class="op">=</span><span class="va">None</span>, cross_block_tables<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>kv_cache: torch.Size([<span class="dv">2</span>, <span class="dv">101291</span>, <span class="dv">16</span>, <span class="dv">2</span>, <span class="dv">64</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后走到了<code>FlashAttentionImpl</code>.</p>
<p>注意这里在启用chunked prefill之后，一批token里面会同时存在prefill和decode，因此需要拆分为两个部分分别执行prefill的decode的attention。 同时这个只是在动态的情况下会被执行到，如果开启了cuda graph，那么decode阶段会直接走cuda graph replay，同时</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co">    If the input tensors contain prompt tokens, the layout is as follows:</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    |&lt;--------------- num_prefill_tokens -----------------&gt;|    </span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    |&lt;--prefill_0--&gt;|&lt;--prefill_1--&gt;|...|&lt;--prefill_N-1---&gt;|</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Otherwise, the layout is as follows:    </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    |&lt;----------------- num_decode_tokens ------------------&gt;|  </span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">    |&lt;--decode_0--&gt;|..........|&lt;--decode_M-1--&gt;|&lt;--padding--&gt;|</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Generation tokens can contain padding when cuda-graph is used.</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Currently, prompt tokens don't contain any padding.</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">    The prompts might have different lengths, while the generation tokens</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">    always have length 1.</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">    If chunked prefill is enabled, prefill tokens and decode tokens can be</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">    batched together in a flattened 1D query.</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co">    |&lt;----- num_prefill_tokens ----&gt;|&lt;------- num_decode_tokens ---------&gt;|</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co">    |&lt;-prefill_0-&gt;|...|&lt;-prefill_N-1-&gt;|&lt;--decode_0--&gt;|...|&lt;--decode_M-1--&gt;|</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Currently, cuda graph is disabled for chunked prefill, meaning there's no</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co">    padding between prefill and decode tokens.</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        layer: AttentionLayer,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        query: torch.Tensor,</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        key: torch.Tensor,</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        value: torch.Tensor,</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        kv_cache: torch.Tensor,</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        attn_metadata: FlashAttentionMetadata,</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        output: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass with FlashAttention.</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="co">            query: shape = [num_tokens, num_heads, head_size]</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="co">            key: shape = [num_tokens, num_kv_heads, head_size]</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="co">            value: shape = [num_tokens, num_kv_heads, head_size]</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="co">            output: shape = [num_tokens, num_heads, head_size]</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="co">            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="co">                </span><span class="al">NOTE</span><span class="co">: kv_cache will be an empty tensor with shape [0]</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="co">                for profiling run.</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="co">            attn_metadata: Metadata for attention.</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="co">        </span><span class="al">NOTE</span><span class="co">: It in-place updates the output tensor.</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">(woosuk): FlashAttention does not support FP8 KV cache.</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        (num_prefill_query_tokens, num_prefill_kv_tokens,</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>        num_decode_query_tokens) <span class="op">=</span> <span class="op">\</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>            get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>        decode_query <span class="op">=</span> query[num_prefill_query_tokens:]</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>        decode_output <span class="op">=</span> output[num_prefill_query_tokens:]</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># QKV for prefill.</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> query[:num_prefill_query_tokens]</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>        prefill_output <span class="op">=</span> output[:num_prefill_query_tokens]</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> query.shape[<span class="dv">0</span>] <span class="op">==</span> num_prefill_query_tokens</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> decode_query.shape[<span class="dv">0</span>] <span class="op">==</span> num_decode_query_tokens</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> prefill_meta <span class="op">:=</span> attn_metadata.prefill_metadata:</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Prompt run.</span></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (kv_cache.numel() <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> prefill_meta.block_tables <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>                    <span class="kw">or</span> prefill_meta.block_tables.numel() <span class="op">==</span> <span class="dv">0</span>):</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>                <span class="co"># normal attention</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>                <span class="co"># When block_tables are not filled, it means q and k are the</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>                <span class="co"># prompt, and they have the same length.</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>                q_seq_start_loc, q_seq_len, k_seq_start_loc, k_seq_len <span class="op">=</span> <span class="op">\</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>                    _get_query_key_seq_metadata(prefill_meta, <span class="va">True</span>, attn_type)</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>                key <span class="op">=</span> key[:num_prefill_kv_tokens]</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>                value <span class="op">=</span> value[:num_prefill_kv_tokens]</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>                flash_attn_varlen_func(</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>                    q<span class="op">=</span>query,</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>                    k<span class="op">=</span>key,</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>                    v<span class="op">=</span>value,</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>                    cu_seqlens_q<span class="op">=</span>q_seq_start_loc,</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>                    cu_seqlens_k<span class="op">=</span>k_seq_start_loc,</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>                    max_seqlen_q<span class="op">=</span>q_seq_len,</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>                    max_seqlen_k<span class="op">=</span>k_seq_len,</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>                    softmax_scale<span class="op">=</span>softmax_scale,</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>                    causal<span class="op">=</span>_get_causal_option(attn_type),</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>                    window_size<span class="op">=</span>window_size,</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>                    alibi_slopes<span class="op">=</span>alibi_slopes,</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>                    softcap<span class="op">=</span>logits_soft_cap,</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>                    out<span class="op">=</span>prefill_output,</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>                    fa_version<span class="op">=</span><span class="va">self</span>.fa_version,</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>                <span class="co"># prefix-enabled attention</span></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> attn_type <span class="op">==</span> AttentionType.DECODER, (</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"Only decoder-only models support prefix caching"</span>)</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> prefill_meta.seq_lens <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>                max_seq_len <span class="op">=</span> <span class="bu">max</span>(prefill_meta.seq_lens)</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>                flash_attn_varlen_func(  <span class="co"># noqa</span></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>                    q<span class="op">=</span>query,</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>                    k<span class="op">=</span>key_cache,</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>                    v<span class="op">=</span>value_cache,</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>                    cu_seqlens_q<span class="op">=</span>prefill_meta.query_start_loc,</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>                    max_seqlen_q<span class="op">=</span>prefill_meta.max_query_len,</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>                    seqused_k<span class="op">=</span>prefill_meta.seq_lens_tensor,</span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>                    max_seqlen_k<span class="op">=</span>max_seq_len,</span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>                    softmax_scale<span class="op">=</span>softmax_scale,</span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>                    causal<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>                    window_size<span class="op">=</span>window_size,</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>                    alibi_slopes<span class="op">=</span>alibi_slopes,</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>                    block_table<span class="op">=</span>prefill_meta.block_tables,</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>                    softcap<span class="op">=</span>logits_soft_cap,</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>                    out<span class="op">=</span>prefill_output,</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>                    fa_version<span class="op">=</span><span class="va">self</span>.fa_version,</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> decode_meta <span class="op">:=</span> attn_metadata.decode_metadata:</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Decoding run.</span></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use flash_attn_varlen_func kernel for speculative decoding</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>            <span class="co"># because different queries might have different lengths.</span></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> decode_meta.max_decode_query_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a>            <span class="co"># use only for actual varlen decoding</span></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> decode_meta.max_decode_query_len <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> attn_type <span class="op">==</span> AttentionType.DECODER, (</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"Only decoder-only models support max_decode_query_len &gt; 1"</span></span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a>                flash_attn_varlen_func(</span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>                    q<span class="op">=</span>decode_query,</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>                    k<span class="op">=</span>key_cache,</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>                    v<span class="op">=</span>value_cache,</span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>                    cu_seqlens_q<span class="op">=</span>decode_meta.query_start_loc,</span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a>                    max_seqlen_q<span class="op">=</span>decode_meta.max_decode_query_len,</span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a>                    seqused_k<span class="op">=</span>decode_meta.seq_lens_tensor,</span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>                    max_seqlen_k<span class="op">=</span>decode_meta.max_decode_seq_len,</span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>                    softmax_scale<span class="op">=</span>softmax_scale,</span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a>                    causal<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>                    window_size<span class="op">=</span>window_size,</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>                    alibi_slopes<span class="op">=</span>alibi_slopes,</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>                    softcap<span class="op">=</span>logits_soft_cap,</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a>                    block_table<span class="op">=</span>decode_meta.block_tables,</span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a>                    out<span class="op">=</span>decode_output,</span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>                    fa_version<span class="op">=</span><span class="va">self</span>.fa_version,</span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Use flash_attn_with_kvcache for normal decoding.</span></span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>                (</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>                    seq_lens_arg,</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a>                    _,</span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a>                    block_tables_arg,</span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a>                ) <span class="op">=</span> get_seq_len_block_table_args(decode_meta, <span class="va">False</span>, attn_type)</span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a>                flash_attn_with_kvcache(</span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>                    q<span class="op">=</span>decode_query.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>                    k_cache<span class="op">=</span>key_cache,</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a>                    v_cache<span class="op">=</span>value_cache,</span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a>                    block_table<span class="op">=</span>block_tables_arg,</span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a>                    cache_seqlens<span class="op">=</span>seq_lens_arg,</span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a>                    softmax_scale<span class="op">=</span>softmax_scale,</span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>                    causal<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a>                    window_size<span class="op">=</span>window_size,</span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>                    alibi_slopes<span class="op">=</span>alibi_slopes,</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a>                    softcap<span class="op">=</span>logits_soft_cap,</span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>                    out<span class="op">=</span>decode_output.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a>                    fa_version<span class="op">=</span><span class="va">self</span>.fa_version,</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>他上面实际上就是分三部分，首先是kv的reshape:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> kv_cache.numel() <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>            key_cache <span class="op">=</span> kv_cache[<span class="dv">0</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>            value_cache <span class="op">=</span> kv_cache[<span class="dv">1</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We skip updating the KV cache under two conditions:</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>            <span class="co">#  a. When the Attention Type is ENCODER. In this phase, we compute</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>            <span class="co">#     only the encoder attention without updating the cache.</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>            <span class="co">#  b. When both Key and Value are None. This occurs during</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>            <span class="co">#     cross-attention computation in the decoding phase, where the</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>            <span class="co">#     KV cache is already populated with the cross-attention</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>            <span class="co">#     tensor. Thus, we skip cache updates during this time.</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (attn_type <span class="op">!=</span> AttentionType.ENCODER) <span class="kw">and</span> (key <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>) <span class="kw">and</span> (</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>                    value <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>):</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> attn_type <span class="op">==</span> AttentionType.ENCODER_DECODER:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Update cross-attention KV cache (prefill-only)</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>                    updated_slot_mapping <span class="op">=</span> attn_metadata.cross_slot_mapping</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Update self-attention KV cache (prefill/decode)</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>                    updated_slot_mapping <span class="op">=</span> attn_metadata.slot_mapping</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Reshape the input keys and values and store them in the cache.</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>                <span class="co"># If kv_cache is not provided, the new key and value tensors are</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>                <span class="co"># not cached. This happens during the initial memory</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>                <span class="co"># profiling run.</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>                torch.ops._C_cache_ops.reshape_and_cache_flash(</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>                    key,</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>                    value,</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>                    kv_cache[<span class="dv">0</span>],</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>                    kv_cache[<span class="dv">1</span>],</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>                    updated_slot_mapping.flatten(),  <span class="co"># type: ignore[union-attr]</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>                    kv_cache_dtype,</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>                    layer._k_scale,</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>                    layer._v_scale,</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        (num_prefill_query_tokens, num_prefill_kv_tokens,</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        num_decode_query_tokens) <span class="op">=</span> <span class="op">\</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>            get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        decode_query <span class="op">=</span> query[num_prefill_query_tokens:]</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>        decode_output <span class="op">=</span> output[num_prefill_query_tokens:]</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># QKV for prefill.</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> query[:num_prefill_query_tokens]</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        prefill_output <span class="op">=</span> output[:num_prefill_query_tokens]</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> query.shape[<span class="dv">0</span>] <span class="op">==</span> num_prefill_query_tokens</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> decode_query.shape[<span class="dv">0</span>] <span class="op">==</span> num_decode_query_tokens</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后根据场景走prefill和decode的flash attn,这里我主要关注decode部分。 在reshape kv的过程中，他的updated_slot_mapping是<code>[54]</code>，而实际上当前的seq_lens_arg是<code>[55]</code>, block_tables_arg为<code>[1,2,3,4]</code>。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> decode_meta <span class="op">:=</span> attn_metadata.decode_metadata:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Decoding run.</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use flash_attn_varlen_func kernel for speculative decoding</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>            <span class="co"># because different queries might have different lengths.</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> decode_meta.max_decode_query_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># use only for actual varlen decoding</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> decode_meta.max_decode_query_len <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> attn_type <span class="op">==</span> AttentionType.DECODER, (</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"Only decoder-only models support max_decode_query_len &gt; 1"</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>                flash_attn_varlen_func(</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>                    q<span class="op">=</span>decode_query,</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>                    k<span class="op">=</span>key_cache,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>                    v<span class="op">=</span>value_cache,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>                    cu_seqlens_q<span class="op">=</span>decode_meta.query_start_loc,</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>                    max_seqlen_q<span class="op">=</span>decode_meta.max_decode_query_len,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>                    seqused_k<span class="op">=</span>decode_meta.seq_lens_tensor,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>                    max_seqlen_k<span class="op">=</span>decode_meta.max_decode_seq_len,</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>                    softmax_scale<span class="op">=</span>softmax_scale,</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>                    causal<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>                    window_size<span class="op">=</span>window_size,</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>                    alibi_slopes<span class="op">=</span>alibi_slopes,</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>                    softcap<span class="op">=</span>logits_soft_cap,</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>                    block_table<span class="op">=</span>decode_meta.block_tables,</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>                    out<span class="op">=</span>decode_output,</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>                    fa_version<span class="op">=</span><span class="va">self</span>.fa_version,</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Use flash_attn_with_kvcache for normal decoding.</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>                (</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>                    seq_lens_arg,</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>                    _,</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>                    block_tables_arg,</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>                ) <span class="op">=</span> get_seq_len_block_table_args(decode_meta, <span class="va">False</span>, attn_type)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>                flash_attn_with_kvcache(</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>                    q<span class="op">=</span>decode_query.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>                    k_cache<span class="op">=</span>key_cache,</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>                    v_cache<span class="op">=</span>value_cache,</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>                    block_table<span class="op">=</span>block_tables_arg,</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>                    cache_seqlens<span class="op">=</span>seq_lens_arg,</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>                    softmax_scale<span class="op">=</span>softmax_scale,</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>                    causal<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>                    window_size<span class="op">=</span>window_size,</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>                    alibi_slopes<span class="op">=</span>alibi_slopes,</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>                    softcap<span class="op">=</span>logits_soft_cap,</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>                    out<span class="op">=</span>decode_output.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>                    fa_version<span class="op">=</span><span class="va">self</span>.fa_version,</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>目前调用的是flash attn 2:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flash_attn_with_kvcache(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    q,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    k_cache,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    v_cache,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    v<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    rotary_cos<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    rotary_sin<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    cache_seqlens: Optional[Union[(<span class="bu">int</span>, torch.Tensor)]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    cache_batch_idx: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    cache_leftpad: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    block_table: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    softmax_scale<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    causal<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    window_size<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>),  <span class="co"># -1 means infinite context window</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    softcap<span class="op">=</span><span class="fl">0.0</span>, <span class="co"># 0.0 means deactivated</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    rotary_interleaved<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    alibi_slopes<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    num_splits<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    return_softmax_lse<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="op">*</span>,</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    fa_version: <span class="bu">int</span> <span class="op">=</span> DEFAULT_FA_VERSION,</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="co">    If k and v are not None, k_cache and v_cache will be updated *inplace* with the new values from</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co">    k and v. This is useful for incremental decoding: you can pass in the cached keys/values from</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="co">    the previous step, and update them with the new keys/values from the current step, and do</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co">    attention with the updated cache, all in 1 kernel.</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="co">    If you pass in k / v, you must make sure that the cache is large enough to hold the new values.</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co">    For example, the KV cache could be pre-allocated with the max sequence length, and you can use</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co">    cache_seqlens to keep track of the current sequence lengths of each sequence in the batch.</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co">    Also apply rotary embedding if rotary_cos and rotary_sin are passed in. The key @k will be</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co">    rotated by rotary_cos and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co">    If causal or local (i.e., window_size != (-1, -1)), the query @q will be rotated by rotary_cos</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="co">    and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co">    If not causal and not local, the query @q will be rotated by rotary_cos and rotary_sin at</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="co">    indices cache_seqlens only (i.e. we consider all tokens in @q to be at position cache_seqlens).</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="co">    See tests/test_flash_attn.py::test_flash_attn_kvcache for examples of how to use this function.</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co">    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="co">    For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="co">    0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="co">    If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a><span class="co">    For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a><span class="co">        1 1 1 1 0</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="co">        1 1 1 1 1</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="co">    If seqlen_q = 5 and seqlen_k = 2, the causal mask is:</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="co">        0 0</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="co">        0 0</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="co">        0 0</span></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="co">        1 0</span></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a><span class="co">        1 1</span></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="co">    If the row of the mask is all zero, the output will be zero.</span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a><span class="co">    If window_size != (-1, -1), implements sliding window local attention. Query at position i</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a><span class="co">    will only attend to keys between</span></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="co">    [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a><span class="co">    Note: Does not support backward pass.</span></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a><span class="co">        q: (batch_size, seqlen, nheads, headdim)</span></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a><span class="co">        k_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a><span class="co">            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)</span></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a><span class="co">            page_block_size must be a multiple of 256.</span></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a><span class="co">        v_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,</span></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a><span class="co">            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a><span class="co">        k [optional]: (batch_size, seqlen_new, nheads_k, headdim). If not None, we concatenate</span></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a><span class="co">            k with k_cache, starting at the indices specified by cache_seqlens.</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a><span class="co">        v [optional]: (batch_size, seqlen_new, nheads_k, headdim). Similar to k.</span></span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a><span class="co">        rotary_cos [optional]: (seqlen_ro, rotary_dim / 2). If not None, we apply rotary embedding</span></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a><span class="co">            to k and q. Only applicable if k and v are passed in. rotary_dim must be divisible by 16.</span></span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a><span class="co">        rotary_sin [optional]: (seqlen_ro, rotary_dim / 2). Similar to rotary_cos.</span></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a><span class="co">        cache_seqlens: int, or (batch_size,), dtype torch.int32. The sequence lengths of the</span></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a><span class="co">            KV cache.</span></span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a><span class="co">        block_table [optional]: (batch_size, max_num_blocks_per_seq), dtype torch.int32.</span></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a><span class="co">        cache_batch_idx: (batch_size,), dtype torch.int32. The indices used to index into the KV cache.</span></span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a><span class="co">            If None, we assume that the batch indices are [0, 1, 2, ..., batch_size - 1].</span></span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a><span class="co">            If the indices are not distinct, and k and v are provided, the values updated in the cache</span></span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a><span class="co">                 might come from any of the duplicate indices.</span></span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a><span class="co">        softmax_scale: float. The scaling of QK^T before applying softmax.</span></span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a><span class="co">            Default to 1 / sqrt(headdim).</span></span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a><span class="co">        causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).</span></span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a><span class="co">        window_size: (left, right). If not (-1, -1), implements sliding window local attention.</span></span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a><span class="co">        softcap: float. Anything &gt; 0 activates softcapping attention.</span></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a><span class="co">        rotary_interleaved: bool. Only applicable if rotary_cos and rotary_sin are passed in.</span></span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a><span class="co">            If True, rotary embedding will combine dimensions 0 &amp; 1, 2 &amp; 3, etc. If False,</span></span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a><span class="co">            rotary embedding will combine dimensions 0 &amp; rotary_dim / 2, 1 &amp; rotary_dim / 2 + 1</span></span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a><span class="co">            (i.e. GPT-NeoX style).</span></span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a><span class="co">        alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of</span></span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a><span class="co">            (-alibi_slope * |i + seqlen_k - seqlen_q - j|)</span></span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a><span class="co">            is added to the attention score of query i and key j.</span></span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a><span class="co">        num_splits: int. If &gt; 1, split the key/value into this many chunks along the sequence.</span></span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a><span class="co">           If num_splits == 1, we don't split the key/value. If num_splits == 0, we use a heuristic</span></span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a><span class="co">           to automatically determine the number of splits.</span></span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a><span class="co">           Don't change this unless you know what you are doing.</span></span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a><span class="co">        return_softmax_lse: bool. Whether to return the logsumexp of the attention scores.</span></span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a><span class="co">    Return:</span></span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a><span class="co">        out: (batch_size, seqlen, nheads, headdim).</span></span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a><span class="co">        softmax_lse [optional, if return_softmax_lse=True]: (batch_size, nheads, seqlen). The</span></span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a><span class="co">            logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax</span></span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a><span class="co">            normalization factor).</span></span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> k_cache.stride(<span class="op">-</span><span class="dv">1</span>) <span class="op">==</span> <span class="dv">1</span>, <span class="st">"k_cache must have contiguous last dimension"</span></span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> v_cache.stride(<span class="op">-</span><span class="dv">1</span>) <span class="op">==</span> <span class="dv">1</span>, <span class="st">"v_cache must have contiguous last dimension"</span></span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>    q, k, v <span class="op">=</span> [maybe_contiguous(x) <span class="cf">for</span> x <span class="kw">in</span> (q, k, v)]</span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> softmax_scale <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a>        softmax_scale <span class="op">=</span> q.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">**</span> (<span class="op">-</span><span class="fl">0.5</span>)</span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cache_seqlens <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="bu">isinstance</span>(cache_seqlens, <span class="bu">int</span>):</span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a>        cache_seqlens <span class="op">=</span> torch.full(</span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a>            (k_cache.shape[<span class="dv">0</span>],), cache_seqlens, dtype<span class="op">=</span>torch.int32, device<span class="op">=</span>k_cache.device</span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a>        cache_seqlens <span class="op">=</span> maybe_contiguous(cache_seqlens)</span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a>    cache_batch_idx <span class="op">=</span> maybe_contiguous(cache_batch_idx)</span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a>    block_table <span class="op">=</span> maybe_contiguous(block_table)</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> fa_version <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>        out, softmax_lse <span class="op">=</span> torch.ops._vllm_fa2_C.fwd_kvcache(</span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a>            q, k_cache, v_cache,</span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>            k, v,             <span class="co"># k_new, v_new</span></span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>            cache_seqlens,</span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a>            rotary_cos,</span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a>            rotary_sin,</span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a>            cache_batch_idx,</span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a>            cache_leftpad,</span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a>            block_table,</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a>            alibi_slopes,</span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a>            out,</span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a>            softmax_scale,</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a>            causal,</span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a>            window_size[<span class="dv">0</span>],</span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a>            window_size[<span class="dv">1</span>],</span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a>            softcap,</span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a>            rotary_interleaved,</span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a>            num_splits,</span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (out, softmax_lse) <span class="cf">if</span> return_softmax_lse <span class="cf">else</span> out</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>其中这个<code>torch.ops._vllm_fa2_C.fwd_kvcache</code>实际上位于vllm的flash attention的<code>csrc/flash_attn/flash_api_torch_lib.cpp</code>。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">std::</span>vector<span class="op">&lt;</span>at<span class="op">::</span>Tensor<span class="op">&gt;</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>mha_fwd_kvcache<span class="op">(</span>at<span class="op">::</span>Tensor <span class="op">&amp;</span>q<span class="op">,</span>                 <span class="co">// batch_size x seqlen_q x num_heads x head_size</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">const</span> at<span class="op">::</span>Tensor <span class="op">&amp;</span>kcache<span class="op">,</span>            <span class="co">// batch_size_c x seqlen_k x num_heads_k x head_size or num_blocks x page_block_size x num_heads_k x head_size if there's a block_table.</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">const</span> at<span class="op">::</span>Tensor <span class="op">&amp;</span>vcache<span class="op">,</span>            <span class="co">// batch_size_c x seqlen_k x num_heads_k x head_size or num_blocks x page_block_size x num_heads_k x head_size if there's a block_table.</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span><span class="at">const</span> at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">k_</span><span class="op">,</span> <span class="co">// batch_size x seqlen_knew x num_heads_k x head_size</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span><span class="at">const</span> at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">v_</span><span class="op">,</span> <span class="co">// batch_size x seqlen_knew x num_heads_k x head_size</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span><span class="at">const</span> at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">seqlens_k_</span><span class="op">,</span> <span class="co">// batch_size</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span><span class="at">const</span> at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">rotary_cos_</span><span class="op">,</span> <span class="co">// seqlen_ro x (rotary_dim / 2)</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span><span class="at">const</span> at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">rotary_sin_</span><span class="op">,</span> <span class="co">// seqlen_ro x (rotary_dim / 2)</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span><span class="at">const</span> at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">cache_batch_idx_</span><span class="op">,</span> <span class="co">// indices to index into the KV cache</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span><span class="at">const</span> at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">leftpad_k_</span><span class="op">,</span> <span class="co">// batch_size</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span>at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">block_table_</span><span class="op">,</span> <span class="co">// batch_size x max_num_blocks_per_seq</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span>at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">alibi_slopes_</span><span class="op">,</span> <span class="co">// num_heads or batch_size x num_heads</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>                <span class="bu">std::</span>optional<span class="op">&lt;</span>at<span class="op">::</span>Tensor<span class="op">&gt;</span> <span class="op">&amp;</span><span class="va">out_</span><span class="op">,</span>             <span class="co">// batch_size x seqlen_q x num_heads x head_size</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>                <span class="at">const</span> <span class="dt">float</span> softmax_scale<span class="op">,</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>                <span class="dt">bool</span> is_causal<span class="op">,</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>                <span class="dt">int</span> window_size_left<span class="op">,</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>                <span class="dt">int</span> window_size_right<span class="op">,</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>                <span class="at">const</span> <span class="dt">float</span> softcap<span class="op">,</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>                <span class="dt">bool</span> is_rotary_interleaved<span class="op">,</span>   <span class="co">// if true, rotary combines indices 0 &amp; 1, else indices 0 &amp; rotary_dim / 2</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>                <span class="dt">int</span> num_splits</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>                <span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Otherwise the kernel will be launched from cuda:0 device</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    at<span class="op">::</span>cuda<span class="op">::</span>CUDAGuard device_guard<span class="op">{</span>q<span class="op">.</span>device<span class="op">()};</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> <span class="op">[</span>cc_major<span class="op">,</span> cc_minor<span class="op">]</span> <span class="op">=</span> get_compute_capability<span class="op">(</span>get_current_device<span class="op">());</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="dt">bool</span> is_sm8x_min <span class="op">=</span> cc_major <span class="op">&gt;=</span> <span class="dv">8</span><span class="op">;</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>is_sm8x_min<span class="op">,</span> <span class="st">"FlashAttention only supports Ampere GPUs or newer."</span><span class="op">);</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> q_dtype <span class="op">=</span> q<span class="op">.</span>dtype<span class="op">();</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>q_dtype <span class="op">==</span> torch<span class="op">::</span>kFloat16 <span class="op">||</span> q_dtype <span class="op">==</span> torch<span class="op">::</span>kBFloat16<span class="op">,</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>                <span class="st">"FlashAttention only support fp16 and bf16 data type"</span><span class="op">);</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>kcache<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> q_dtype<span class="op">,</span> <span class="st">"query and key must have the same dtype"</span><span class="op">);</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>vcache<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> q_dtype<span class="op">,</span> <span class="st">"query and value must have the same dtype"</span><span class="op">);</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>    CHECK_DEVICE<span class="op">(</span>q<span class="op">);</span> CHECK_DEVICE<span class="op">(</span>kcache<span class="op">);</span> CHECK_DEVICE<span class="op">(</span>vcache<span class="op">);</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>q<span class="op">.</span>stride<span class="op">(-</span><span class="dv">1</span><span class="op">)</span> <span class="op">==</span> <span class="dv">1</span><span class="op">,</span> <span class="st">"Input tensor must have contiguous last dimension"</span><span class="op">);</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>kcache<span class="op">.</span>stride<span class="op">(-</span><span class="dv">1</span><span class="op">)</span> <span class="op">==</span> <span class="dv">1</span><span class="op">,</span> <span class="st">"Input tensor must have contiguous last dimension"</span><span class="op">);</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>vcache<span class="op">.</span>stride<span class="op">(-</span><span class="dv">1</span><span class="op">)</span> <span class="op">==</span> <span class="dv">1</span><span class="op">,</span> <span class="st">"Input tensor must have contiguous last dimension"</span><span class="op">);</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    at<span class="op">::</span>Tensor block_table<span class="op">;</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">bool</span> paged_KV <span class="op">=</span> <span class="va">block_table_</span><span class="op">.</span>has_value<span class="op">();</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>paged_KV<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(!</span><span class="va">cache_batch_idx_</span><span class="op">.</span>has_value<span class="op">(),</span> <span class="st">"Paged KVcache does not support cache_batch_idx"</span><span class="op">);</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>        block_table <span class="op">=</span> <span class="va">block_table_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>        CHECK_DEVICE<span class="op">(</span>block_table<span class="op">);</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>block_table<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> torch<span class="op">::</span>kInt32<span class="op">,</span> <span class="st">"block_table must have dtype torch.int32"</span><span class="op">);</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>block_table<span class="op">.</span>stride<span class="op">(-</span><span class="dv">1</span><span class="op">)</span> <span class="op">==</span> <span class="dv">1</span><span class="op">,</span> <span class="st">"block_table must have contiguous last dimension"</span><span class="op">);</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="kw">auto</span> sizes <span class="op">=</span> q<span class="op">.</span>sizes<span class="op">();</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> batch_size <span class="op">=</span> sizes<span class="op">[</span><span class="dv">0</span><span class="op">];</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> seqlen_q <span class="op">=</span> sizes<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> num_heads <span class="op">=</span> sizes<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> head_size_og <span class="op">=</span> sizes<span class="op">[</span><span class="dv">3</span><span class="op">];</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> seqlen_q_og <span class="op">=</span> seqlen_q<span class="op">;</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> num_heads_og <span class="op">=</span> num_heads<span class="op">;</span></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> max_num_blocks_per_seq <span class="op">=</span> <span class="op">!</span>paged_KV <span class="op">?</span> <span class="dv">0</span> <span class="op">:</span> block_table<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> num_blocks <span class="op">=</span> <span class="op">!</span>paged_KV <span class="op">?</span> <span class="dv">0</span> <span class="op">:</span> kcache<span class="op">.</span>size<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> page_block_size <span class="op">=</span> <span class="op">!</span>paged_KV <span class="op">?</span> <span class="dv">1</span> <span class="op">:</span> kcache<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(!</span>paged_KV <span class="op">||</span> page_block_size <span class="op">%</span> <span class="dv">16</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">"Paged KV cache block size must be divisible by 16"</span><span class="op">);</span></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> seqlen_k <span class="op">=</span> <span class="op">!</span>paged_KV <span class="op">?</span> kcache<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">)</span> <span class="op">:</span> max_num_blocks_per_seq <span class="op">*</span> page_block_size<span class="op">;</span></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> num_heads_k <span class="op">=</span> kcache<span class="op">.</span>size<span class="op">(</span><span class="dv">2</span><span class="op">);</span></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> batch_size_c <span class="op">=</span> <span class="op">!</span>paged_KV <span class="op">?</span> kcache<span class="op">.</span>size<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">:</span> batch_size<span class="op">;</span></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>batch_size <span class="op">&gt;</span> <span class="dv">0</span><span class="op">,</span> <span class="st">"batch size must be positive"</span><span class="op">);</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>head_size_og <span class="op">&lt;=</span> <span class="dv">256</span><span class="op">,</span> <span class="st">"FlashAttention forward only supports head dimension at most 256"</span><span class="op">);</span></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>    TORCH_CHECK<span class="op">(</span>num_heads <span class="op">%</span> num_heads_k <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">"Number of heads in key/value must divide number of heads in query"</span><span class="op">);</span></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>    <span class="co">// causal=true is the same as causal=false in this case</span></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>seqlen_q <span class="op">==</span> <span class="dv">1</span> <span class="op">&amp;&amp;</span> <span class="op">!</span><span class="va">alibi_slopes_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span> is_causal <span class="op">=</span> <span class="kw">false</span><span class="op">;</span> <span class="op">}</span></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>is_causal<span class="op">)</span> <span class="op">{</span> window_size_right <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> <span class="op">}</span></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Faster to transpose q from (b, 1, (nheads_kv ngroups), d) to (b, ngroups, nheads_kv, d) in this case</span></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>    <span class="co">// H/t Daniel Haziza</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> seqlenq_ngroups_swapped <span class="op">=</span> seqlen_q <span class="op">==</span> <span class="dv">1</span> <span class="op">&amp;&amp;</span> num_heads <span class="op">&gt;</span> num_heads_k <span class="op">&amp;&amp;</span> window_size_left <span class="op">&lt;</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> window_size_right <span class="op">&lt;</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> head_size_og <span class="op">%</span> <span class="dv">8</span> <span class="op">==</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> <span class="op">!</span><span class="va">alibi_slopes_</span><span class="op">.</span>has_value<span class="op">();</span></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>seqlenq_ngroups_swapped<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>        <span class="at">const</span> <span class="dt">int</span> ngroups <span class="op">=</span> num_heads <span class="op">/</span> num_heads_k<span class="op">;</span></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q<span class="op">.</span>reshape<span class="op">({</span>batch_size<span class="op">,</span> num_heads_k<span class="op">,</span> ngroups<span class="op">,</span> head_size_og<span class="op">}).</span>transpose<span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>        seqlen_q <span class="op">=</span> ngroups<span class="op">;</span></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>        num_heads <span class="op">=</span> num_heads_k<span class="op">;</span></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>window_size_left <span class="op">&gt;=</span> seqlen_k<span class="op">)</span> <span class="op">{</span> window_size_left <span class="op">=</span> <span class="op">-</span><span class="dv">1</span><span class="op">;</span> <span class="op">}</span></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>window_size_right <span class="op">&gt;=</span> seqlen_k<span class="op">)</span> <span class="op">{</span> window_size_right <span class="op">=</span> <span class="op">-</span><span class="dv">1</span><span class="op">;</span> <span class="op">}</span></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>    CHECK_SHAPE<span class="op">(</span>q<span class="op">,</span> batch_size<span class="op">,</span> seqlen_q<span class="op">,</span> num_heads<span class="op">,</span> head_size_og<span class="op">);</span></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(!</span>paged_KV<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>kcache<span class="op">,</span> batch_size_c<span class="op">,</span> seqlen_k<span class="op">,</span> num_heads_k<span class="op">,</span> head_size_og<span class="op">);</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>vcache<span class="op">,</span> batch_size_c<span class="op">,</span> seqlen_k<span class="op">,</span> num_heads_k<span class="op">,</span> head_size_og<span class="op">);</span></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>kcache<span class="op">,</span> num_blocks<span class="op">,</span> page_block_size<span class="op">,</span> num_heads_k<span class="op">,</span> head_size_og<span class="op">);</span></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>vcache<span class="op">,</span> num_blocks<span class="op">,</span> page_block_size<span class="op">,</span> num_heads_k<span class="op">,</span> head_size_og<span class="op">);</span></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>block_table<span class="op">,</span> batch_size<span class="op">,</span> max_num_blocks_per_seq<span class="op">);</span></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>    at<span class="op">::</span>Tensor q_padded<span class="op">,</span> kcache_padded<span class="op">,</span> vcache_padded<span class="op">;</span></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>head_size_og <span class="op">%</span> <span class="dv">8</span> <span class="op">!=</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>        q_padded <span class="op">=</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>pad<span class="op">(</span>q<span class="op">,</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>PadFuncOptions<span class="op">({</span><span class="dv">0</span><span class="op">,</span> <span class="dv">8</span> <span class="op">-</span> head_size_og <span class="op">%</span> <span class="dv">8</span><span class="op">}));</span></span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a>        kcache_padded <span class="op">=</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>pad<span class="op">(</span>kcache<span class="op">,</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>PadFuncOptions<span class="op">({</span><span class="dv">0</span><span class="op">,</span> <span class="dv">8</span> <span class="op">-</span> head_size_og <span class="op">%</span> <span class="dv">8</span><span class="op">}));</span></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>        vcache_padded <span class="op">=</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>pad<span class="op">(</span>vcache<span class="op">,</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>PadFuncOptions<span class="op">({</span><span class="dv">0</span><span class="op">,</span> <span class="dv">8</span> <span class="op">-</span> head_size_og <span class="op">%</span> <span class="dv">8</span><span class="op">}));</span></span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>        q_padded <span class="op">=</span> q<span class="op">;</span></span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a>        kcache_padded <span class="op">=</span> kcache<span class="op">;</span></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>        vcache_padded <span class="op">=</span> vcache<span class="op">;</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a>    at<span class="op">::</span>Tensor out<span class="op">;</span></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="va">out_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">out_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>out<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> q_dtype<span class="op">,</span> <span class="st">"Output must have the same dtype as inputs"</span><span class="op">);</span></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a>        CHECK_DEVICE<span class="op">(</span>out<span class="op">);</span></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>out<span class="op">.</span>stride<span class="op">(-</span><span class="dv">1</span><span class="op">)</span> <span class="op">==</span> <span class="dv">1</span><span class="op">,</span> <span class="st">"Output tensor must have contiguous last dimension"</span><span class="op">);</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>out<span class="op">,</span> batch_size<span class="op">,</span> seqlen_q_og<span class="op">,</span> num_heads_og<span class="op">,</span> head_size_og<span class="op">);</span></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>head_size_og <span class="op">%</span> <span class="dv">8</span> <span class="op">!=</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> torch<span class="op">::</span>empty_like<span class="op">(</span>q_padded<span class="op">);</span></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span> <span class="cf">else</span> <span class="cf">if</span> <span class="op">(</span>seqlenq_ngroups_swapped<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out<span class="op">.</span>reshape<span class="op">({</span>batch_size<span class="op">,</span> num_heads<span class="op">,</span> seqlen_q<span class="op">,</span> head_size_og<span class="op">}).</span>transpose<span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch<span class="op">::</span>empty_like<span class="op">(</span>q_padded<span class="op">);</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> round_multiple <span class="op">=</span> <span class="op">[](</span><span class="dt">int</span> x<span class="op">,</span> <span class="dt">int</span> m<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> <span class="op">(</span>x <span class="op">+</span> m <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> m <span class="op">*</span> m<span class="op">;</span> <span class="op">};</span></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> head_size <span class="op">=</span> round_multiple<span class="op">(</span>head_size_og<span class="op">,</span> <span class="dv">8</span><span class="op">);</span></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> head_size_rounded <span class="op">=</span> head_size <span class="op">&lt;=</span> <span class="dv">192</span> <span class="op">?</span> round_multiple<span class="op">(</span>head_size<span class="op">,</span> <span class="dv">32</span><span class="op">)</span> <span class="op">:</span> <span class="dv">256</span><span class="op">;</span></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> seqlen_q_rounded <span class="op">=</span> round_multiple<span class="op">(</span>seqlen_q<span class="op">,</span> <span class="dv">128</span><span class="op">);</span></span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> seqlen_k_rounded <span class="op">=</span> round_multiple<span class="op">(</span>seqlen_k<span class="op">,</span> <span class="dv">128</span><span class="op">);</span></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> opts <span class="op">=</span> q<span class="op">.</span>options<span class="op">();</span></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> softmax_lse <span class="op">=</span> torch<span class="op">::</span>empty<span class="op">({</span>batch_size<span class="op">,</span> num_heads<span class="op">,</span> seqlen_q<span class="op">},</span> opts<span class="op">.</span>dtype<span class="op">(</span>at<span class="op">::</span>kFloat<span class="op">));</span></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a>    Flash_fwd_params params<span class="op">;</span></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a>    set_params_fprop<span class="op">(</span>params<span class="op">,</span></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a>                     batch_size<span class="op">,</span></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a>                     seqlen_q<span class="op">,</span> seqlen_k<span class="op">,</span></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a>                     seqlen_q_rounded<span class="op">,</span> seqlen_k_rounded<span class="op">,</span></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a>                     num_heads<span class="op">,</span> num_heads_k<span class="op">,</span></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a>                     head_size<span class="op">,</span> head_size_rounded<span class="op">,</span></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a>                     q_padded<span class="op">,</span> kcache_padded<span class="op">,</span> vcache_padded<span class="op">,</span> out<span class="op">,</span></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a>                     <span class="co">/*cu_seqlens_q_d=*/</span><span class="kw">nullptr</span><span class="op">,</span></span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a>                     <span class="co">/*cu_seqlens_k_d=*/</span><span class="kw">nullptr</span><span class="op">,</span></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a>                     <span class="co">/*seqused_k=*/</span><span class="kw">nullptr</span><span class="op">,</span></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a>                     <span class="co">/*p_ptr=*/</span><span class="kw">nullptr</span><span class="op">,</span></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a>                     softmax_lse<span class="op">.</span>data_ptr<span class="op">(),</span></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a>                     <span class="co">/*p_dropout=*/</span><span class="fl">0.</span><span class="bu">f</span><span class="op">,</span></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a>                     softmax_scale<span class="op">,</span></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a>                     window_size_left<span class="op">,</span></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>                     window_size_right<span class="op">,</span></span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a>                     softcap</span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>                     <span class="op">);</span></span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a>    at<span class="op">::</span>Tensor k<span class="op">,</span> v<span class="op">,</span> k_padded<span class="op">,</span> v_padded<span class="op">;</span></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="va">k_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span></span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span><span class="va">v_</span><span class="op">.</span>has_value<span class="op">(),</span> <span class="st">"If key is supplied, value must also be passed in"</span><span class="op">);</span></span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span><span class="va">seqlens_k_</span><span class="op">.</span>has_value<span class="op">(),</span> <span class="st">"If key is supplied, seqlens_k must also be passed in"</span><span class="op">);</span></span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>seqlen_q <span class="op">&lt;=</span> seqlen_k<span class="op">,</span> <span class="st">"If key is supplied, it must have seqlen &lt;= the seqlen of the KV cache"</span><span class="op">);</span></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">k_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">v_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>k<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> q_dtype<span class="op">,</span> <span class="st">"Key must have the same dtype as query"</span><span class="op">);</span></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>v<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> q_dtype<span class="op">,</span> <span class="st">"Value must have the same dtype as query"</span><span class="op">);</span></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a>        CHECK_DEVICE<span class="op">(</span>k<span class="op">);</span> CHECK_DEVICE<span class="op">(</span>v<span class="op">);</span></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>k<span class="op">.</span>stride<span class="op">(-</span><span class="dv">1</span><span class="op">)</span> <span class="op">==</span> <span class="dv">1</span><span class="op">,</span> <span class="st">"Key tensor must have contiguous last dimension"</span><span class="op">);</span></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>v<span class="op">.</span>stride<span class="op">(-</span><span class="dv">1</span><span class="op">)</span> <span class="op">==</span> <span class="dv">1</span><span class="op">,</span> <span class="st">"Value tensor must have contiguous last dimension"</span><span class="op">);</span></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> seqlen_knew <span class="op">=</span> k<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>k<span class="op">,</span> batch_size<span class="op">,</span> seqlen_knew<span class="op">,</span> num_heads_k<span class="op">,</span> head_size_og<span class="op">);</span></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>v<span class="op">,</span> batch_size<span class="op">,</span> seqlen_knew<span class="op">,</span> num_heads_k<span class="op">,</span> head_size_og<span class="op">);</span></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>head_size_og <span class="op">%</span> <span class="dv">8</span> <span class="op">!=</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a>            k_padded <span class="op">=</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>pad<span class="op">(</span>k<span class="op">,</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>PadFuncOptions<span class="op">({</span><span class="dv">0</span><span class="op">,</span> <span class="dv">8</span> <span class="op">-</span> head_size_og <span class="op">%</span> <span class="dv">8</span><span class="op">}));</span></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a>            v_padded <span class="op">=</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>pad<span class="op">(</span>v<span class="op">,</span> torch<span class="op">::</span>nn<span class="op">::</span>functional<span class="op">::</span>PadFuncOptions<span class="op">({</span><span class="dv">0</span><span class="op">,</span> <span class="dv">8</span> <span class="op">-</span> head_size_og <span class="op">%</span> <span class="dv">8</span><span class="op">}));</span></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a>            k_padded <span class="op">=</span> k<span class="op">;</span></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a>            v_padded <span class="op">=</span> v<span class="op">;</span></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>seqlen_knew <span class="op">=</span> seqlen_knew<span class="op">;</span></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>knew_ptr <span class="op">=</span> k_padded<span class="op">.</span>data_ptr<span class="op">();</span></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>vnew_ptr <span class="op">=</span> v_padded<span class="op">.</span>data_ptr<span class="op">();</span></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a>        <span class="co">// All stride are in elements, not bytes.</span></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>knew_batch_stride <span class="op">=</span> k_padded<span class="op">.</span>stride<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>vnew_batch_stride <span class="op">=</span> v_padded<span class="op">.</span>stride<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>knew_row_stride <span class="op">=</span> k_padded<span class="op">.</span>stride<span class="op">(-</span><span class="dv">3</span><span class="op">);</span></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>vnew_row_stride <span class="op">=</span> v_padded<span class="op">.</span>stride<span class="op">(-</span><span class="dv">3</span><span class="op">);</span></span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>knew_head_stride <span class="op">=</span> k_padded<span class="op">.</span>stride<span class="op">(-</span><span class="dv">2</span><span class="op">);</span></span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>vnew_head_stride <span class="op">=</span> v_padded<span class="op">.</span>stride<span class="op">(-</span><span class="dv">2</span><span class="op">);</span></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="va">seqlens_k_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a>        <span class="kw">auto</span> seqlens_k <span class="op">=</span> <span class="va">seqlens_k_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>seqlens_k<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> torch<span class="op">::</span>kInt32<span class="op">,</span> <span class="st">"seqlens_k must have dtype int32"</span><span class="op">);</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a>        CHECK_DEVICE<span class="op">(</span>seqlens_k<span class="op">);</span></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a>        CHECK_CONTIGUOUS<span class="op">(</span>seqlens_k<span class="op">);</span></span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>seqlens_k<span class="op">,</span> batch_size<span class="op">);</span></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>cu_seqlens_k <span class="op">=</span> <span class="kw">static_cast</span><span class="op">&lt;</span><span class="dt">int</span> <span class="op">*&gt;(</span>seqlens_k<span class="op">.</span>data_ptr<span class="op">());</span></span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>    params<span class="op">.</span>is_seqlens_k_cumulative <span class="op">=</span> <span class="op">!(</span><span class="va">seqlens_k_</span><span class="op">.</span>has_value<span class="op">());</span></span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="va">leftpad_k_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(!</span>paged_KV<span class="op">,</span> <span class="st">"We don't support Paged KV and leftpad_k running at the same time yet"</span><span class="op">);</span></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>        <span class="kw">auto</span> leftpad_k <span class="op">=</span> <span class="va">leftpad_k_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>leftpad_k<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> torch<span class="op">::</span>kInt32<span class="op">,</span> <span class="st">"leftpad_k must have dtype int32"</span><span class="op">);</span></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>        CHECK_DEVICE<span class="op">(</span>leftpad_k<span class="op">);</span></span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a>        CHECK_CONTIGUOUS<span class="op">(</span>leftpad_k<span class="op">);</span></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>leftpad_k<span class="op">,</span> batch_size<span class="op">);</span></span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>leftpad_k <span class="op">=</span> <span class="kw">static_cast</span><span class="op">&lt;</span><span class="dt">int</span> <span class="op">*&gt;(</span>leftpad_k<span class="op">.</span>data_ptr<span class="op">());</span></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="va">rotary_cos_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span><span class="va">k_</span><span class="op">.</span>has_value<span class="op">(),</span> <span class="st">"If rotary cos/sin are provided, new key / value to be appended to KV cache must also be provided"</span><span class="op">);</span></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>        <span class="kw">auto</span> rotary_cos <span class="op">=</span> <span class="va">rotary_cos_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a>        CHECK_DEVICE<span class="op">(</span>rotary_cos<span class="op">);</span></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>rotary_dim <span class="op">=</span> rotary_cos<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">)</span> <span class="op">*</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>params<span class="op">.</span>rotary_dim <span class="op">&lt;=</span> head_size<span class="op">,</span> <span class="st">"rotary_dim must be &lt;= headdim"</span><span class="op">);</span></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>params<span class="op">.</span>rotary_dim <span class="op">%</span> <span class="dv">16</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">"Only rotary dimensions divisible by 16 are currently supported"</span><span class="op">);</span></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a>        <span class="at">const</span> <span class="dt">int</span> seqlen_ro <span class="op">=</span> rotary_cos<span class="op">.</span>size<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>seqlen_ro <span class="op">&gt;=</span> seqlen_k<span class="op">,</span> <span class="st">"cos/sin seqlen must be at least the seqlen of KV cache"</span><span class="op">);</span></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>rotary_cos<span class="op">,</span> seqlen_ro<span class="op">,</span> params<span class="op">.</span>rotary_dim <span class="op">/</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a>        CHECK_CONTIGUOUS<span class="op">(</span>rotary_cos<span class="op">);</span></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>rotary_cos<span class="op">.</span><span class="dt">scalar_type</span><span class="op">()</span> <span class="op">==</span> q_dtype<span class="op">,</span> <span class="st">"rotary_cos must have the same dtype as query"</span><span class="op">);</span></span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span><span class="va">rotary_sin_</span><span class="op">.</span>has_value<span class="op">(),</span> <span class="st">"If rotary cos is provided, rotary sin must also be provided"</span><span class="op">);</span></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a>        <span class="kw">auto</span> rotary_sin <span class="op">=</span> <span class="va">rotary_sin_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a>        CHECK_DEVICE<span class="op">(</span>rotary_sin<span class="op">);</span></span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a>        CHECK_SHAPE<span class="op">(</span>rotary_sin<span class="op">,</span> seqlen_ro<span class="op">,</span> params<span class="op">.</span>rotary_dim <span class="op">/</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a>        CHECK_CONTIGUOUS<span class="op">(</span>rotary_sin<span class="op">);</span></span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>rotary_sin<span class="op">.</span><span class="dt">scalar_type</span><span class="op">()</span> <span class="op">==</span> q_dtype<span class="op">,</span> <span class="st">"rotary_cos must have the same dtype as query"</span><span class="op">);</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>rotary_cos_ptr <span class="op">=</span> rotary_cos<span class="op">.</span>data_ptr<span class="op">();</span></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>rotary_sin_ptr <span class="op">=</span> rotary_sin<span class="op">.</span>data_ptr<span class="op">();</span></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>is_rotary_interleaved <span class="op">=</span> is_rotary_interleaved<span class="op">;</span></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>rotary_dim <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="va">cache_batch_idx_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span></span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a>        <span class="kw">auto</span> cache_batch_idx <span class="op">=</span> <span class="va">cache_batch_idx_</span><span class="op">.</span>value<span class="op">();</span></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a>        CHECK_DEVICE<span class="op">(</span>cache_batch_idx<span class="op">);</span></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a>        CHECK_CONTIGUOUS<span class="op">(</span>cache_batch_idx<span class="op">);</span></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a>        TORCH_CHECK<span class="op">(</span>cache_batch_idx<span class="op">.</span><span class="dt">scalar_type</span><span class="op">()</span> <span class="op">==</span> torch<span class="op">::</span>kInt32<span class="op">,</span> <span class="st">"cache_batch_idx must have dtype int32"</span><span class="op">);</span></span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>cache_batch_idx <span class="op">=</span> <span class="kw">reinterpret_cast</span><span class="op">&lt;</span><span class="dt">int</span> <span class="op">*&gt;(</span>cache_batch_idx<span class="op">.</span>data_ptr<span class="op">());</span></span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Keep references to these tensors to extend their lifetime</span></span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a>    at<span class="op">::</span>Tensor softmax_lse_accum<span class="op">,</span> out_accum<span class="op">;</span></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a>    <span class="bu">std::</span>tie<span class="op">(</span>softmax_lse_accum<span class="op">,</span> out_accum<span class="op">)</span> <span class="op">=</span> set_params_splitkv<span class="op">(</span></span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a>        params<span class="op">,</span> batch_size<span class="op">,</span> num_heads<span class="op">,</span> head_size<span class="op">,</span> seqlen_k<span class="op">,</span> seqlen_q<span class="op">,</span></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a>        head_size_rounded<span class="op">,</span> <span class="co">/*dropout*/</span> <span class="fl">0.</span><span class="bu">f</span><span class="op">,</span> num_splits<span class="op">,</span> get_num_sm<span class="op">(</span>get_current_device<span class="op">()),</span> opts<span class="op">);</span></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>paged_KV<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>block_table <span class="op">=</span> block_table<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">int</span><span class="op">&gt;();</span></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a>        params<span class="op">.</span>block_table_batch_stride <span class="op">=</span> block_table<span class="op">.</span>stride<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a>    params<span class="op">.</span>page_block_size <span class="op">=</span> page_block_size<span class="op">;</span></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a>    set_params_alibi<span class="op">(</span>params<span class="op">,</span> <span class="va">alibi_slopes_</span><span class="op">,</span> batch_size<span class="op">,</span> num_heads<span class="op">);</span></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> stream <span class="op">=</span> at<span class="op">::</span>cuda<span class="op">::</span>getCurrentCUDAStream<span class="op">().</span>stream<span class="op">();</span></span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Only split kernel supports appending to KV cache, or indexing to the cache with cache_batch_idx,</span></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a>    <span class="co">// or paged KV cache</span></span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a>    run_mha_fwd<span class="op">(</span>params<span class="op">,</span> stream<span class="op">,</span> <span class="co">/*force_split_kernel=*/</span><span class="va">k_</span><span class="op">.</span>has_value<span class="op">()</span> <span class="op">||</span> <span class="va">cache_batch_idx_</span><span class="op">.</span>has_value<span class="op">()</span> <span class="op">||</span> paged_KV<span class="op">);</span></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>head_size_og <span class="op">%</span> <span class="dv">8</span> <span class="op">!=</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out<span class="op">.</span>index<span class="op">({</span><span class="st">"..."</span><span class="op">,</span> torch<span class="op">::</span>indexing<span class="op">::</span>Slice<span class="op">(</span>torch<span class="op">::</span>indexing<span class="op">::</span>None<span class="op">,</span> head_size_og<span class="op">)});</span></span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span><span class="va">out_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span> <span class="va">out_</span><span class="op">.</span>value<span class="op">().</span><span class="va">copy_</span><span class="op">(</span>out<span class="op">);</span> <span class="op">}</span></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span><span class="va">k_</span><span class="op">.</span>has_value<span class="op">())</span> <span class="op">{</span></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a>            <span class="co">// It's expensive to copy the KV cache here for the case where head size not divisible by 8,</span></span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a>            <span class="co">// but we don't expect to get this case in practice. This is just so that the code works for that case.</span></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a>            kcache<span class="op">.</span><span class="va">copy_</span><span class="op">(</span>kcache_padded<span class="op">.</span>index<span class="op">({</span><span class="st">"..."</span><span class="op">,</span> torch<span class="op">::</span>indexing<span class="op">::</span>Slice<span class="op">(</span>torch<span class="op">::</span>indexing<span class="op">::</span>None<span class="op">,</span> head_size_og<span class="op">)}));</span></span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>            vcache<span class="op">.</span><span class="va">copy_</span><span class="op">(</span>vcache_padded<span class="op">.</span>index<span class="op">({</span><span class="st">"..."</span><span class="op">,</span> torch<span class="op">::</span>indexing<span class="op">::</span>Slice<span class="op">(</span>torch<span class="op">::</span>indexing<span class="op">::</span>None<span class="op">,</span> head_size_og<span class="op">)}));</span></span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>seqlenq_ngroups_swapped<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out<span class="op">.</span>transpose<span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="dv">2</span><span class="op">).</span>reshape<span class="op">({</span>batch_size<span class="op">,</span> <span class="dv">1</span><span class="op">,</span> num_heads_k <span class="op">*</span> seqlen_q<span class="op">,</span> head_size_og<span class="op">});</span></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a>        softmax_lse <span class="op">=</span> softmax_lse<span class="op">.</span>reshape<span class="op">({</span>batch_size<span class="op">,</span> num_heads_k <span class="op">*</span> seqlen_q<span class="op">,</span> <span class="dv">1</span><span class="op">});</span></span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">{</span>out<span class="op">,</span> softmax_lse<span class="op">};</span></span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>实际上他这里也是有padding的。当k存在时，强行走split k:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> run_mha_fwd<span class="op">(</span>Flash_fwd_params <span class="op">&amp;</span>params<span class="op">,</span> <span class="dt">cudaStream_t</span> stream<span class="op">,</span> <span class="dt">bool</span> force_split_kernel<span class="op">=</span><span class="kw">false</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    FP16_SWITCH<span class="op">(!</span>params<span class="op">.</span>is_bf16<span class="op">,</span> <span class="op">[&amp;]</span> <span class="op">{</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        HEADDIM_SWITCH<span class="op">(</span>params<span class="op">.</span>d<span class="op">,</span> <span class="op">[&amp;]</span> <span class="op">{</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>            BOOL_SWITCH<span class="op">(</span>params<span class="op">.</span>is_causal<span class="op">,</span> Is_causal<span class="op">,</span> <span class="op">[&amp;]</span> <span class="op">{</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="op">(</span>params<span class="op">.</span>num_splits <span class="op">&lt;=</span> <span class="dv">1</span> <span class="op">&amp;&amp;</span> <span class="op">!</span>force_split_kernel<span class="op">)</span> <span class="op">{</span>  <span class="co">// If we don't set it num_splits == 0</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>                    <span class="va">run_mha_fwd_</span><span class="op">&lt;</span><span class="dt">elem_type</span><span class="op">,</span> kHeadDim<span class="op">,</span> Is_causal<span class="op">&gt;(</span>params<span class="op">,</span> stream<span class="op">);</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>                    run_mha_fwd_splitkv_dispatch<span class="op">&lt;</span><span class="dt">elem_type</span><span class="op">,</span> kHeadDim<span class="op">,</span> Is_causal<span class="op">&gt;(</span>params<span class="op">,</span> stream<span class="op">);</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>                <span class="op">}</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>            <span class="op">});</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="op">});</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">});</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后在这个dispatch中还有许多变体，不过重要的就是tile的kv都是从远端读取过来的。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>block_table <span class="op">!=</span> <span class="kw">nullptr</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>        <span class="kw">auto</span> final_block_size <span class="op">=</span> binfo<span class="op">.</span>actual_seqlen_k <span class="op">-</span> <span class="op">(</span>n_block_max <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">*</span> kBlockN<span class="op">;</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        tKgK<span class="op">.</span>data<span class="op">()</span> <span class="op">=</span> gK<span class="op">.</span>data<span class="op">()</span> <span class="op">+</span> flash<span class="op">::</span>resolve_thread_kv_page_slice_offset<span class="op">&lt;</span>Kernel_traits<span class="op">&gt;(</span>tidx<span class="op">,</span> n_block_max <span class="op">-</span> <span class="dv">1</span><span class="op">,</span> params<span class="op">.</span>page_block_size<span class="op">,</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>            block_table<span class="op">,</span> params<span class="op">.</span>k_batch_stride<span class="op">,</span> params<span class="op">.</span>k_row_stride<span class="op">,</span> final_block_size<span class="op">);</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        tVgV<span class="op">.</span>data<span class="op">()</span> <span class="op">=</span> gV<span class="op">.</span>data<span class="op">()</span> <span class="op">+</span> flash<span class="op">::</span>resolve_thread_kv_page_slice_offset<span class="op">&lt;</span>Kernel_traits<span class="op">&gt;(</span>tidx<span class="op">,</span> n_block_max <span class="op">-</span> <span class="dv">1</span><span class="op">,</span> params<span class="op">.</span>page_block_size<span class="op">,</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>            block_table<span class="op">,</span> params<span class="op">.</span>v_batch_stride<span class="op">,</span> params<span class="op">.</span>v_row_stride<span class="op">,</span> final_block_size<span class="op">);</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="vllm-fused-moe" class="level2">
<h2 class="anchored" data-anchor-id="vllm-fused-moe">vllm fused moe</h2>
<p>要理解vllm fused moe，首先从最naive的huggingface版来理解moe的执行流程，首先整体流程如下：</p>
<p><img src="vllm/moe_ep.png" class="img-fluid"></p>
<p>实际上就是一共64个expert，假设4个设备，ep=4，那么每个设备放8个expert。到gating时的输入都需要是broadcast的，然后gating计算hidden states对应每个expert的概率，将概率排序后进行all to all后使用当前设备中expert的计算outputs。注意他这里的all to all之后实际上每个设备还是算64个expert，只不过是8个expert重复了4次。 等计算完outpus之后再all to all就可以恢复到每个节点64个expert的输出。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> moe_infer(<span class="va">self</span>, x, topk_ids, topk_weight):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        cnts <span class="op">=</span> topk_ids.new_zeros((topk_ids.shape[<span class="dv">0</span>], <span class="bu">len</span>(<span class="va">self</span>.experts))) <span class="co"># [global batch, n_experts]</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        cnts.scatter_(<span class="dv">1</span>, topk_ids, <span class="dv">1</span>) <span class="co"># assgin activated experts to 1</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        tokens_per_expert <span class="op">=</span> cnts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># 每个token会选择不同的，那么一个专家会处理多个token，统计每个专家要处理的token数量</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        idxs <span class="op">=</span> topk_ids.view(<span class="op">-</span><span class="dv">1</span>).argsort() <span class="co"># 先sort topk id, 这样idx 为 [global batch * n_experts], 其中expert的索引从小到大排序</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        sorted_tokens <span class="op">=</span> x[idxs <span class="op">//</span> topk_ids.shape[<span class="dv">1</span>]] <span class="co"># 因为他前面是先view再argsort，除shape[1]是为了保证只选当前token对应的expert。</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        sorted_tokens_shape <span class="op">=</span> sorted_tokens.shape</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.ep_size <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>            tokens_per_ep_rank <span class="op">=</span> tokens_per_expert.view(<span class="va">self</span>.ep_size, <span class="op">-</span><span class="dv">1</span>).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># [ep_size, n_experts/ep_size] -&gt; [ep_size]</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>            tokens_per_expert_group <span class="op">=</span> tokens_per_expert.new_empty(</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>                tokens_per_expert.shape[<span class="dv">0</span>]</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>            dist.all_to_all_single(tokens_per_expert_group, tokens_per_expert) <span class="co"># 先all to all拿到当前的节点上每个expert要处理的token数量</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>            output_splits <span class="op">=</span> (</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>                tokens_per_expert_group.view(<span class="va">self</span>.ep_size, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>                .<span class="bu">sum</span>(<span class="dv">1</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>                .cpu()</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>                .numpy()</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>                .tolist()</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>            gathered_tokens <span class="op">=</span> sorted_tokens.new_empty(</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>                tokens_per_expert_group.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>).cpu().item(), sorted_tokens.shape[<span class="dv">1</span>]</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>            input_split_sizes <span class="op">=</span> tokens_per_ep_rank.cpu().numpy().tolist()</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>            dist.all_to_all(</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>                <span class="bu">list</span>(gathered_tokens.split(output_splits)),</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>                <span class="bu">list</span>(sorted_tokens.split(input_split_sizes)),</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>            ) <span class="co"># all to all拿到当前的节点上每个expert要处理的token</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>            tokens_per_expert_post_gather <span class="op">=</span> tokens_per_expert_group.view(</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.ep_size, <span class="va">self</span>.experts_per_rank</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>            ).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>            gatherd_idxs <span class="op">=</span> np.zeros(shape<span class="op">=</span>(gathered_tokens.shape[<span class="dv">0</span>],), dtype<span class="op">=</span>np.int32)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>            s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, k <span class="kw">in</span> <span class="bu">enumerate</span>(tokens_per_expert_group.cpu().numpy()):</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>                gatherd_idxs[s : s <span class="op">+</span> k] <span class="op">=</span> i <span class="op">%</span> <span class="va">self</span>.experts_per_rank</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>                s <span class="op">+=</span> k</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>            gatherd_idxs <span class="op">=</span> gatherd_idxs.argsort()</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>            sorted_tokens <span class="op">=</span> gathered_tokens[gatherd_idxs]</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>            tokens_per_expert <span class="op">=</span> tokens_per_expert_post_gather</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>        tokens_per_expert <span class="op">=</span> tokens_per_expert.cpu().numpy()</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> []</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>        start_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, num_tokens <span class="kw">in</span> <span class="bu">enumerate</span>(tokens_per_expert):</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>            end_idx <span class="op">=</span> start_idx <span class="op">+</span> num_tokens</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> num_tokens <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>            expert <span class="op">=</span> <span class="va">self</span>.experts[i <span class="op">+</span> <span class="va">self</span>.ep_rank <span class="op">*</span> <span class="va">self</span>.experts_per_rank]</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>            tokens_for_this_expert <span class="op">=</span> sorted_tokens[start_idx:end_idx] <span class="co"># 由于sort之后，分到同一个expert上token已经都排好了，所以直接取</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>            expert_out <span class="op">=</span> expert(tokens_for_this_expert)</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>            outputs.append(expert_out)</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>            start_idx <span class="op">=</span> end_idx</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>        outs <span class="op">=</span> torch.cat(outputs, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">if</span> <span class="bu">len</span>(outputs) <span class="cf">else</span> sorted_tokens.new_empty(<span class="dv">0</span>) <span class="co"># [global batch*n_act_experts, hidden_size]</span></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.ep_size <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>            new_x <span class="op">=</span> torch.empty_like(outs)</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>            new_x[gatherd_idxs] <span class="op">=</span> outs</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>            gathered_tokens <span class="op">=</span> new_x.new_empty(<span class="op">*</span>sorted_tokens_shape)</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>            dist.all_to_all(</span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>                <span class="bu">list</span>(gathered_tokens.split(input_split_sizes)),</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>                <span class="bu">list</span>(new_x.split(output_splits)),</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>            outs <span class="op">=</span> gathered_tokens <span class="co"># 再来一次all to all拿到当前节点处理好的gathered tokens</span></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>        new_x <span class="op">=</span> torch.empty_like(outs)</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>        new_x[idxs] <span class="op">=</span> outs</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>        final_out <span class="op">=</span> (</span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>            new_x.view(<span class="op">*</span>topk_ids.shape, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>            .<span class="bu">type</span>(topk_weight.dtype)</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>            .mul_(topk_weight.unsqueeze(dim<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>            .<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>            .<span class="bu">type</span>(new_x.dtype)</span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> final_out</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="vllm并行模式" class="level2">
<h2 class="anchored" data-anchor-id="vllm并行模式">vllm并行模式</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 31%">
<col style="width: 19%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Attention</strong></th>
<th><strong>MoE - Shared Experts</strong></th>
<th><strong>MoE - Routed Experts</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>DP</strong></td>
<td>权重复制 <br> <em>可在推理框架外部处理</em></td>
<td>权重复制</td>
<td><strong>EP关闭:</strong> 权重复制<br><strong>EP开启:</strong> 按EP(=DP)数量切分</td>
</tr>
<tr class="even">
<td><strong>TP</strong></td>
<td>按head切分<br><em>需同步LM head</em><br><em>需支持广播meta data</em></td>
<td>切分n和k维度<br><em>需做all-reduce</em></td>
<td><strong>EP关闭:</strong> 权重复制<br><strong>EP开启:</strong> 按EP(=TP)数量切分</td>
</tr>
<tr class="odd">
<td><strong>TP+DP</strong></td>
<td>TP组内切分<br>DP组内复制</td>
<td>TP组内切分<br>DP组内复制</td>
<td><strong>EP关闭:</strong> 权重复制<br><strong>EP开启:</strong> 按EP(=TP*DP)总数切分</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>EP是依赖于DP/TP的MoE专用切分模式，EP = TP * DP</strong>，主要影响Routed Experts的分布</li>
</ul>
<p><img src="vllm/dp_and_tp.png" class="img-fluid"></p>
<p>同时开启DP + TP时，对于权重分布如上图所示，在group 0/group 1间为权重复制，在每个group内为权重切分。此时需要主要attn metadata是group间不同，group内相同。</p>
</section>
</section>
<section id="trt-llm" class="level1">
<h1>trt llm</h1>
<p>trt llm的整体流程其实还蛮像一个端到端的AI编译器的。他可以直接吃整个模型，然后直接生成c代码，并且替换已有的plugin算子。</p>
<section id="plugin" class="level2">
<h2 class="anchored" data-anchor-id="plugin">plugin</h2>
<p>先看看他如何定义plugin的，首先他在python端提供了一些辅助函数用于定义plugin算子的输入输出信息。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_fmha_kernel_meta_data():</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> KernelMetaData(</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        kernel_name<span class="op">=</span><span class="st">'fused_attention_kernel'</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        ios<span class="op">=</span>[</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>            <span class="co"># outputs</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>            OutputArg(<span class="st">'Out'</span>, Type(<span class="st">'tensor[fp16]'</span>), hints<span class="op">=</span>[<span class="st">'16'</span>, <span class="st">'16'</span>]),</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>            OutputArg(<span class="st">'L'</span>, Type(<span class="st">'tensor[fp32]'</span>), hints<span class="op">=</span>[<span class="st">'16'</span>, <span class="st">'16'</span>]),</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            OutputArg(<span class="st">'M'</span>, Type(<span class="st">'tensor[fp16]'</span>), hints<span class="op">=</span>[<span class="st">'16'</span>, <span class="st">'16'</span>]),</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># inputs</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>            InputArg(<span class="st">'Q'</span>, Type(<span class="st">'tensor[fp16]'</span>), hints<span class="op">=</span>[<span class="st">'16'</span>, <span class="st">'16'</span>]),</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>            InputArg(<span class="st">'K'</span>, Type(<span class="st">'tensor[fp16]'</span>), hints<span class="op">=</span>[<span class="st">'16'</span>, <span class="st">'16'</span>]),</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>            InputArg(<span class="st">'V'</span>, Type(<span class="st">'tensor[fp16]'</span>), hints<span class="op">=</span>[<span class="st">'16'</span>, <span class="st">'16'</span>]),</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>            ParamArg(<span class="st">'sm_scale'</span>, Type(<span class="st">'fp32'</span>)),</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>            DimSizeArg(<span class="st">'batch_size'</span>),</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>            ParamArg(<span class="st">'num_heads'</span>, Type(<span class="st">'i32'</span>)),</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>            DimSizeArg(<span class="st">'seq_len'</span>, hints<span class="op">=</span>[<span class="st">''</span>, <span class="st">'16'</span>]),</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># constexprs</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>            Constexpr(<span class="dv">128</span>),</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>            Constexpr(<span class="dv">64</span>),</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>            Constexpr(<span class="dv">128</span>),</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        shape_infer_rules<span class="op">=</span>[</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The following rules helps to deduce the shapes of the output tensors</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Q[*] -&gt; Out[*]"</span>,</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Q[m,n,k,*] -&gt; L[m,n,k]"</span>,</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Q[m,n,k,*] -&gt; M[m,n,k]"</span>,</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The following rules helps to deduce both DimSizeArgs: batch_size and seq_len</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Q[m,n,k,*] : m -&gt; batch_size"</span>,</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Q[m,n,k,*] : k -&gt; seq_len"</span>,</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        version<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        kernel_file<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>openai_triton_example_root<span class="sc">}</span><span class="ss">/fmha_triton.py'</span>,</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        num_warps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        grid_dims<span class="op">=</span>(<span class="st">"(seq_len + 127) / 128"</span>, <span class="st">"batch_size * num_heads"</span>, <span class="st">"1"</span>))</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>KERNELS <span class="op">=</span> [</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>    get_fmha_kernel_meta_data(),</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>甚至还支持了一套读取复杂计算表达式参数的转换器，用于把算子的计算逻辑转换成c接口代码。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="at">@pytest.mark.parametrize</span>(<span class="st">'expr, target'</span>, [</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"a[m,n,k]:m*2+k+(n+1) -&gt; b"</span>,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>     <span class="st">"((inputDesc[0].dims.d[0] * 2) + (inputDesc[0].dims.d[2] + (inputDesc[0].dims.d[1] + 1)))"</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>     ),</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"a[m,n,k]:m*(2+k)+n+1 -&gt; b"</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>     <span class="st">"((inputDesc[0].dims.d[0] * (2 + inputDesc[0].dims.d[2])) + (inputDesc[0].dims.d[1] + 1))"</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>     ),</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"a[m,n,k] -&gt; b[m*((((n+1))))]"</span>, <span class="st">"""</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="st">if (outputIndex == 0) {</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="st">  outputDims.nbDims = 1;</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="st">  outputDims.d[0] = (inputDims[0].d[0] * (inputDims[0].d[1] + 1));</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="st">     """</span>),</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"a[m,n,k] -&gt; b[m*(n+k), 2*n, k+3]"</span>, <span class="st">"""</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="st">nvinfer1::DimsExprs outputDims;</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="st">if (outputIndex == 0) {</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="st">  outputDims.nbDims = 3;</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="st">  outputDims.d[0] = (inputDims[0].d[0] * (inputDims[0].d[1] + inputDims[0].d[2]));</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="st">  outputDims.d[1] = (2 * inputDims[0].d[1]);</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="st">  outputDims.d[2] = (inputDims[0].d[2] + 3);</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="st">}"""</span>)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_CppCodeTranspiler(expr: <span class="bu">str</span>, target: <span class="bu">str</span>):</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    args <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        a<span class="op">=</span>InputArg(<span class="st">'a'</span>, Type(<span class="st">'fp16'</span>)),</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        b<span class="op">=</span>InputArg(<span class="st">'b'</span>, Type(<span class="st">'fp16'</span>)),</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    target <span class="op">=</span> target.strip()</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    transpiler <span class="op">=</span> CppCodeTranspiler(args)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    shape_infer_code, dim_infer_code <span class="op">=</span> transpiler([expr])</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we don't check the correctness of the code since the lark produces unstable ast tree</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># refer to https://github.com/lark-parser/lark/issues/324</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> shape_infer_code <span class="kw">or</span> dim_infer_code</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="auto-parallel" class="level2">
<h2 class="anchored" data-anchor-id="auto-parallel">auto parallel</h2>
<p>首先是切分描述，trt llm和shardy类似，是对一个维度来描述切分，如果为空，那么当前维度就是复制，否则在对应的mesh上进行切分：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DimSpec:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, shard_list):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.is_replica <span class="op">=</span> <span class="bu">len</span>(shard_list) <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.shard_list <span class="op">=</span> shard_list</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.is_replica:</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="st">'R'</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> <span class="ss">f"S(</span><span class="sc">{</span><span class="st">','</span><span class="sc">.</span>join(<span class="bu">str</span>(dim) <span class="cf">for</span> dim <span class="kw">in</span> <span class="va">self</span>.shard_list)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> target</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后一个tensor的切分信息由多个dim spec组成:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ShardingSpec:</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, entire_shape, sharding_sequence, device_mesh):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>      ...</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> <span class="st">"DistSpec("</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        res <span class="op">+=</span> <span class="ss">f"shape=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>entire_shape<span class="sc">}</span><span class="ss">,"</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        res <span class="op">+=</span> <span class="ss">f"shard=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>sharding_sequence<span class="sc">}</span><span class="ss">,"</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        res <span class="op">+=</span> <span class="ss">f"mesh=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>device_mesh<span class="sc">.</span>mesh_shape<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        res <span class="op">+=</span> <span class="st">")"</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> res</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后构造一个分布式切分搜索图：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_cost_graph(<span class="va">self</span>, lmesh):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>        leaf_strategies <span class="op">=</span> []</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> node <span class="kw">in</span> <span class="va">self</span>.nodes:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> node.is_replicated:</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                node.set_strategy(<span class="va">None</span>, lmesh)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                node.collect_strategies(lmesh)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> node <span class="kw">in</span> <span class="va">self</span>.nodes:</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>            strategies_vector <span class="op">=</span> node.update_resharding_cost()</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(strategies_vector) <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>                leaf_strategies.append(strategies_vector)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        cost_graph <span class="op">=</span> CostGraph(leaf_strategies)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cost_graph</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>首先是遍历所有节点，如果这个是<code>is_replicated</code>，也就是提前标记了不可分布式，那么直接设置为None，否则遍历所有可能的切分策略，然后计算这个节点的cost（包含了通信和计算）。其中<code>collect_strategies</code>核心代码如下, 他似乎是只考虑在2维拓扑以下进行分布式，但是这里加的策略还是全的，不过好像只添加考虑输出节点的切分，并不是SBP的基于推导的方法。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _collect_strategies(<span class="va">self</span>, device_mesh):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>        dim_partition_list <span class="op">=</span> []</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        dim_size <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.op_data[<span class="st">'output0'</span>].shape)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        dim_partition_list.append({})</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        dim_partition_list.extend(</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._enumerate_all_possible_1d_sharding([<span class="dv">0</span>, <span class="dv">1</span>], dim_size))</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        dim_partition_list.extend(</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._enumerate_all_possible_2d_sharding([<span class="dv">0</span>], [<span class="dv">1</span>], dim_size))</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        dim_partition_list.extend(</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._enumerate_all_possible_1d_sharding([<span class="dv">0</span>], dim_size))</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        dim_partition_list.extend(</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._enumerate_all_possible_1d_sharding([<span class="dv">1</span>], dim_size))</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        strategies_vector <span class="op">=</span> StrategiesVector(<span class="va">self</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> dim_partition_dict <span class="kw">in</span> dim_partition_list:</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>            dim_partition_dict_mapping <span class="op">=</span> {<span class="st">'output0'</span>: dim_partition_dict}</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>            sharding_spec_mapping <span class="op">=</span> <span class="va">self</span>._to_sharding_spec_mapping(</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>                dim_partition_dict_mapping, device_mesh)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="dv">0</span> <span class="op">==</span> <span class="bu">len</span>(sharding_spec_mapping):</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>            sharding_seq <span class="op">=</span> sharding_spec_mapping[<span class="st">'output0'</span>].sharding_sequence</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>            sharding_strategy <span class="op">=</span> <span class="va">self</span>._get_sharding_strategy(</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="ss">f'constant-op </span><span class="sc">{</span>sharding_seq<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>                sharding_spec_mapping<span class="op">=</span>sharding_spec_mapping,</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>                communication_action_mapping<span class="op">=</span>{})</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>            strategies_vector.append(sharding_strategy)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> strategies_vector</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后发现实际上这个方法是可以被override的，对于matmul来说有自己的collect方法：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _collect_strategies(<span class="va">self</span>, device_mesh):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>        strategies_vector <span class="op">=</span> StrategiesVector(<span class="va">self</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        dp_strategies <span class="op">=</span> <span class="va">self</span>._dp_strategies(device_mesh)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        tp_strategies <span class="op">=</span> <span class="va">self</span>._tp_strategies(device_mesh)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        mix_strategies <span class="op">=</span> <span class="va">self</span>._mix_strategies(device_mesh)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        bmm_strategies <span class="op">=</span> <span class="va">self</span>._bmm_strategies(device_mesh)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        strategies_vector.extend(dp_strategies)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        strategies_vector.extend(tp_strategies)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        strategies_vector.extend(mix_strategies)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        strategies_vector.extend(bmm_strategies)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> strategies_vector</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>这是matmul所collect出来的切分方式：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>RR <span class="op">=</span> RS(<span class="dv">0</span>) x S(<span class="dv">0</span>)R_allreduceS(<span class="dv">0</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>RR <span class="op">=</span> RS(<span class="dv">1</span>) x S(<span class="dv">1</span>)R_allreduceS(<span class="dv">1</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>RR <span class="op">=</span> RS(<span class="dv">0</span>,<span class="dv">1</span>) x S(<span class="dv">0</span>,<span class="dv">1</span>)R_allreduceS(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>[R, S(<span class="dv">0</span>)] <span class="op">=</span> [R, S(<span class="dv">0</span>)] x [S(<span class="dv">0</span>), R]_reducescatter(<span class="dv">1</span>, S(<span class="dv">0</span>))</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>[R, S(<span class="dv">1</span>)] <span class="op">=</span> [R, S(<span class="dv">1</span>)] x [S(<span class="dv">1</span>), R]_reducescatter(<span class="dv">1</span>, S(<span class="dv">1</span>))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>[R, S(<span class="dv">0</span>,<span class="dv">1</span>)] <span class="op">=</span> [R, S(<span class="dv">0</span>,<span class="dv">1</span>)] x [S(<span class="dv">0</span>,<span class="dv">1</span>), R]_reducescatter(<span class="dv">1</span>, S(<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>RS(<span class="dv">0</span>) <span class="op">=</span> RR x RS(<span class="dv">0</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>RS(<span class="dv">1</span>) <span class="op">=</span> RR x RS(<span class="dv">1</span>)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>RS(<span class="dv">0</span>,<span class="dv">1</span>) <span class="op">=</span> RR x RS(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>RS(<span class="dv">1</span>) <span class="op">=</span> RS(<span class="dv">0</span>) x S(<span class="dv">0</span>)S(<span class="dv">1</span>)_allreduceS(<span class="dv">0</span>)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>RS(<span class="dv">0</span>) <span class="op">=</span> RS(<span class="dv">1</span>) x S(<span class="dv">1</span>)S(<span class="dv">0</span>)_allreduceS(<span class="dv">1</span>)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>RR <span class="op">=</span> RR x RR</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>同样attention或其他节点也有自己的分布式切分收集函数，这里和oneflow不同的是，他并不会传播partial的切分，我的理解是拆分的越细那么灵活性更高，可以后续做自动的通算融合，大粒度后面还是走匹配替换来优化。</p>
<p>每个算子的分布式策略收集好之后，还需要构建reshard cost，因为可能上一个节点的切分策略并不是下一个节点所需要的切分策略，所以需要计算reshard cost。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _update_resharding_cost(<span class="va">self</span>, strategies):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> strategy <span class="kw">in</span> strategies:</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>            resharding_costs <span class="op">=</span> {}</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> pre_node, out_index <span class="kw">in</span> <span class="va">self</span>.predecessor_nodes_out_index.items():</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> pre_node <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>                pre_node_out_data_name <span class="op">=</span> pre_node.get_output(out_index).name</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>                pre_node_out_data_lname <span class="op">=</span> pre_node.global_to_local_op_name[</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>                    pre_node_out_data_name]</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> pre_node_out_data_name <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.global_to_local_op_name:</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"pre_node_out_data_name = </span><span class="sc">{</span>pre_node_out_data_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>                cur_node_inp_data_lname <span class="op">=</span> <span class="va">self</span>.global_to_local_op_name[</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>                    pre_node_out_data_name]</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>                cur_sharding_spec <span class="op">=</span> strategy.sharding_specs[</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>                    cur_node_inp_data_lname]</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>                pre_node_out_sharding_specs <span class="op">=</span> []</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> pre_strategy <span class="kw">in</span> pre_node.strategies_vector:</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>                    pre_node_out_sharding_specs.append(</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>                        pre_strategy.sharding_specs[pre_node_out_data_lname])</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> pre_node <span class="kw">not</span> <span class="kw">in</span> resharding_costs:</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>                    resharding_costs[pre_node.node_name] <span class="op">=</span> []</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> prev_sharding_spec <span class="kw">in</span> pre_node_out_sharding_specs:</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>                    resharding_cost <span class="op">=</span> <span class="va">self</span>._compute_resharding_cost(</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>                        prev_sharding_spec, cur_sharding_spec,</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.op_data[cur_node_inp_data_lname]) <span class="co"># 话说为什么他要限制这个op的切分类型，本来这个gather就可以切输入啊</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>                    resharding_costs[pre_node.node_name].append(resharding_cost)</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>            strategy.resharding_costs <span class="op">=</span> resharding_costs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="pyexector" class="level2">
<h2 class="anchored" data-anchor-id="pyexector">PyExector</h2>
<p>trt llm之前通过编译的方式发现使用起来不方便，因此他也模仿vllm，提供了一个python的executor，直接动态执行大模型。并且他新的接口就做了相当的精简：</p>
<p><img src="vllm/trt_attn.png" class="img-fluid"></p>
</section>
</section>
<section id="mlc-llm" class="level1">
<h1>mlc llm</h1>
<p>mlc这里就不看调度的过程了，只看kv cache，attention是如何和编译器集成的。</p>
<p><img src="vllm/tvm_attn.png" class="img-fluid"></p>
<p>tvm中首先是在vm中定义了基础的kv cache state接口，然后实现了具体的kv cache管理逻辑。然后AttentionKVCacheObj中包含了一个最重要的函数AttentionWithFusedQKV。tvm为了在python端可以集成不同的 attention实现，是在python中调用vm的kv cache的create，在create中将不同的attention计算函数作为packed func传入。最终执行时是vm调用AttentionWithFusedQKV，AttentionWithFusedQKV中调用自身的tir_attention_decode_cpu/tir_attention_decode_gpu等等函数。</p>
<p>他这套逻辑还是比较复杂的，需要在python和cpp中转换多次。</p>
</section>
<section id="sglang" class="level1">
<h1>sglang</h1>
<p>这是sglang在attention部分的类图： <img src="vllm/sglang_attn.png" class="img-fluid"></p>
</section>
<section id="问题汇总" class="level1">
<h1>问题汇总</h1>
<section id="vllm问题" class="level2">
<h2 class="anchored" data-anchor-id="vllm问题">vllm问题</h2>
<ol type="1">
<li>TypeError: must be called with a dataclass type or instance</li>
</ol>
<p>发现是torch 2.5.1 cu118不能用triton 3.2，需要降级到3.1</p>
<ol start="2" type="1">
<li>RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)</li>
</ol>
<p>检查detail发现是runtime的nccl是cu12的</p>
<ol start="3" type="1">
<li>json.decoder.JSONDecodeError: Extra data: line 5298 column 2 (char 479924)</li>
</ol>
<p>发现是之前下载的模型的json有问题，重新下载就行了</p>
<ol start="4" type="1">
<li>RuntimeError: Triton Error [CUDA]: device kernel image is invalid</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[rank0]:</span> torch._dynamo.exc.BackendCompilerFailed: backend=<span class="st">'inductor'</span> raised:</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="ex">[rank0]:</span> RuntimeError: Triton Error <span class="pp">[</span><span class="ss">CUDA</span><span class="pp">]</span>: device kernel image is invalid</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="ex">[rank0]:</span> Set TORCH_LOGS=<span class="st">"+dynamo"</span> and TORCHDYNAMO_VERBOSE=1 for more information</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="ex">[rank0]:</span> You can suppress this exception and fall back to eager by setting:</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="ex">[rank0]:</span>     import torch._dynamo</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="ex">[rank0]:</span>     torch._dynamo.config.suppress_errors = True</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>这个应该是triton默认只能编译出最新的cuda版本的代码,把本地的ptxas替换掉triton中的：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cp</span> /usr/local/cuda/bin/ptxas /root/miniconda3/envs/vllm/lib/python3.10/site-packages/triton/backends/nvidia/bin/ptxas</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol start="5" type="1">
<li><code>NCCL WARN NCCL cannot be captured in a graph</code></li>
</ol>
<p>好像是安装的nccl版本和cuda 11.8还是兼容？</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="ex">misc/strongstream.cc:53</span> NCCL WARN NCCL cannot be captured in a graph if either it wasn<span class="st">'t built with CUDA runtime &gt;= 11.3 or if the installed CUDA driver &lt; R465.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>我现在的版本是<code>nvidia-nccl-cu11-2.21.5</code>,卸载之后重装老的版本</p>
<pre><code>❯ pip install nvidia-nccl-cu11==2.19.3 -i https://download.pytorch.org/whl/cu118</code></pre>
<p>好像也不行，可能是因为老的版本的nccl就没有支持capture graph的功能，需要重新编译nccl，但是实际上可以<code>--enforce-eager</code>跳过这个问题。</p>
</section>
<section id="trt-llm问题" class="level2">
<h2 class="anchored" data-anchor-id="trt-llm问题">trt llm问题</h2>
<ol type="1">
<li>cuda版本问题</li>
</ol>
<p>trt llm和cuda版本是强绑定的，所以如果cuda 版本不到，就直接没法运行，所以只能安装老版本的trt llm。</p>
<ol start="2" type="1">
<li>tensorrt bindings的问题</li>
</ol>
<p>我发现使用pip安装的tensorrt llm中的tensorrt 调用的是tensorrt bindings，然后我安装他得到的是8.6.1版本，这个不是我想要的。解决方法是pip卸载tensorrt，然后用trt llm中的shell脚本安装tensorrt。</p>
<ol start="3" type="1">
<li>mpi版本问题</li>
</ol>
<p>pip安装trt llm的时候总是报错编译mpi4py失败，然后我查看了一下，发现是mpi版本的问题，默认apt安装openmpi得到40+版本了，但是trt llm中需要的是3.1.5，所以解决方案是不要apt装openmpi，通过conda-forge安装3.1.5版本</p>
</section>
<section id="scaled-dot-product-attention-sdpa-精度问题" class="level2">
<h2 class="anchored" data-anchor-id="scaled-dot-product-attention-sdpa-精度问题">scaled dot product attention (SDPA) 精度问题</h2>
<p>我发现在mac上使用 <code>torch.nn.functional.scaled_dot_product_attention</code> 与自己实现的 <code>scaled_dot_product_attention</code> 得到的结果精度有差异。但是如果设定了SDPA的实现为MATH就不会有精度差异，并且是否设定MATH的backend也会导致精度问题，我就很难理解默认的backend是什么。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.attention <span class="im">import</span> SDPBackend, sdpa_kernel</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>, dropout_p: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span>, is_causal: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>, scale: Optional[<span class="bu">float</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    L, S <span class="op">=</span> query.size(<span class="op">-</span><span class="dv">2</span>), key.size(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    scale_factor <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> math.sqrt(query.size(<span class="op">-</span><span class="dv">1</span>)) <span class="cf">if</span> scale <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> scale</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    attn_bias <span class="op">=</span> torch.zeros(L, S, dtype<span class="op">=</span>query.dtype, device<span class="op">=</span>query.device)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_causal:</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> attn_mask <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        temp_mask <span class="op">=</span> torch.ones(L, S, dtype<span class="op">=</span>torch.<span class="bu">bool</span>).tril(diagonal<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        attn_bias.masked_fill_(temp_mask.logical_not(), <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        attn_bias.to(query.dtype)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> attn_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> attn_mask.dtype <span class="op">==</span> torch.<span class="bu">bool</span>:</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>            attn_bias.masked_fill_(attn_mask.logical_not(), <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>            attn_bias <span class="op">=</span> attn_mask <span class="op">+</span> attn_bias</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if enable_gqa:</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> query <span class="op">@</span> key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> scale_factor</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">+=</span> attn_bias</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> torch.softmax(attn_weight, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> torch.dropout(attn_weight, dropout_p, train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attn_weight <span class="op">@</span> value</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">101</span>, <span class="dv">64</span>)</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">64</span>)</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">64</span>)</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">101</span>, <span class="dv">101</span>)</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> scaled_dot_product_attention(</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>    q, k, v, attn_mask<span class="op">=</span>mask, dropout_p<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, is_causal<span class="op">=</span>mask <span class="kw">is</span> <span class="va">None</span>)</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>q2 <span class="op">=</span> q.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">64</span>)</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>k2 <span class="op">=</span> torch.repeat_interleave(k, <span class="dv">7</span>, <span class="dv">2</span>).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">64</span>)</span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>v2 <span class="op">=</span> torch.repeat_interleave(v, <span class="dv">7</span>, <span class="dv">2</span>).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">64</span>)</span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>mask2 <span class="op">=</span> mask.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">101</span>)</span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> scaled_dot_product_attention(</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a>    q2, k2, v2, attn_mask<span class="op">=</span>mask2, dropout_p<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, is_causal<span class="op">=</span>mask2 <span class="kw">is</span> <span class="va">None</span>)</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(y1, y2.reshape(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">101</span>, <span class="dv">64</span>))</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> sdpa_kernel([SDPBackend.MATH]):</span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>  y3 <span class="op">=</span> torch.nn.functional.scaled_dot_product_attention(</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>      q2, k2, v2, attn_mask<span class="op">=</span>mask2, dropout_p<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, is_causal<span class="op">=</span>mask2 <span class="kw">is</span> <span class="va">None</span>)</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(y2, y3)</span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a>y3_1 <span class="op">=</span> torch.nn.functional.scaled_dot_product_attention(</span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>    q2, k2, v2, attn_mask<span class="op">=</span>mask2, dropout_p<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, is_causal<span class="op">=</span>mask2 <span class="kw">is</span> <span class="va">None</span>)</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(y3, y3_1) <span class="co"># will fail</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/zhen8838\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>