{
 "cells": [
  {
   "cell_type": "raw",
   "id": "31121eea",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers\"\n",
    "mathjax: true\n",
    "toc: true\n",
    "categories:\n",
    "- 编译器\n",
    "date: 2026-02-08 18:49:45\n",
    "tags:\n",
    "- Layout\n",
    "---\n",
    "\n",
    "这篇论文是陈天奇团队的成果，提出一个统一的硬件感知抽象（Axe Layout），将逻辑张量坐标映射到多维物理空间，并设计基于此的多粒度、分布式感知的编译器DSL。今天就来解析一下Axe Layout的设计思路和实现细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345f286",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 核心公式\n",
    "\n",
    "Axe Layout将逻辑张量索引映射到物理坐标集合：\n",
    "\n",
    "$$L(x) = \\{ D(x) + r + O \\mid r \\in R \\}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "### D (Shard) - 分片映射\n",
    "- 是一个**有序的iter列表**\n",
    "- 每个iter = (extent, stride, @axis)\n",
    "- **extent**: 硬件维度的逻辑大小\n",
    "- **stride**: 相邻逻辑元素在硬件维度上的距离\n",
    "- **@axis**: 物理轴的标签（device, warp, lane等）\n",
    "- 作用：将**逻辑索引**转换为**物理坐标**\n",
    "\n",
    "### R (Replica) - 复制维度\n",
    "- 是一个**集合**（无序），独立于逻辑索引\n",
    "- 格式：{axis_name: replica_count, ...}\n",
    "- 作用：为**并行执行**添加额外维度\n",
    "- 例如：4个线程独立执行相同的计算\n",
    "\n",
    "### O (Offset) - 偏移\n",
    "- 固定的**基地址**或**资源保留**\n",
    "- 格式：{axis_name: offset_value, ...}\n",
    "- 例如：数据在内存中的起始位置、执行器件偏移\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa038435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pycute as cute\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Iter:\n",
    "  extent: int\n",
    "  stride: int\n",
    "  axis: str\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"({self.extent}, {self.stride}@{self.axis})\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AxeLayout:\n",
    "  D: List[Iter]\n",
    "  R: List[Iter]\n",
    "  O: Dict[str, int]\n",
    "\n",
    "  def __repr__(self):\n",
    "    d_str = \" × \".join(map(str, self.D))\n",
    "    r_str = \" × \".join(map(str, self.R)) if self.R else \"∅\"\n",
    "    o_str = \", \".join([f\"{v}@{k}\" for k, v in self.O.items()]) if self.O else \"∅\"\n",
    "    return f\"D: {d_str} | R: {r_str} | O: {o_str}\"\n",
    "\n",
    "  def print(self, name=\"\"):\n",
    "    if name:\n",
    "      print(f\"\\n{name}:\")\n",
    "\n",
    "    if not self.D:\n",
    "      d_extent_line = \"( )\"\n",
    "      d_stride_line = \"( )\"\n",
    "    else:\n",
    "      col_widths = []\n",
    "      for iter_obj in self.D:\n",
    "        extent_str = str(iter_obj.extent)\n",
    "        stride_str = f\"{iter_obj.stride}@{iter_obj.axis}\"\n",
    "        col_width = max(len(extent_str), len(stride_str))\n",
    "        col_widths.append(col_width)\n",
    "\n",
    "      extent_parts = []\n",
    "      for i, iter_obj in enumerate(self.D):\n",
    "        extent_str = str(iter_obj.extent)\n",
    "        padded = extent_str.center(col_widths[i])\n",
    "        extent_parts.append(padded)\n",
    "      d_extent_line = \"( \" + \"  \".join(extent_parts) + \" )\"\n",
    "\n",
    "      stride_parts = []\n",
    "      for i, iter_obj in enumerate(self.D):\n",
    "        stride_str = f\"{iter_obj.stride}@{iter_obj.axis}\"\n",
    "        padded = stride_str.rjust(col_widths[i])\n",
    "        stride_parts.append(padded)\n",
    "      d_stride_line = \"( \" + \", \".join(stride_parts) + \" )\"\n",
    "\n",
    "    if self.R:\n",
    "      r_col_widths = []\n",
    "      for iter_obj in self.R:\n",
    "        extent_str = str(iter_obj.extent)\n",
    "        stride_str = f\"{iter_obj.stride}@{iter_obj.axis}\"\n",
    "        col_width = max(len(extent_str), len(stride_str))\n",
    "        r_col_widths.append(col_width)\n",
    "\n",
    "      r_extent_parts = []\n",
    "      for i, iter_obj in enumerate(self.R):\n",
    "        extent_str = str(iter_obj.extent)\n",
    "        padded = extent_str.center(r_col_widths[i])\n",
    "        r_extent_parts.append(padded)\n",
    "      r_extent_line = \"( \" + \"  \".join(r_extent_parts) + \" )\"\n",
    "\n",
    "      r_stride_parts = []\n",
    "      for i, iter_obj in enumerate(self.R):\n",
    "        stride_str = f\"{iter_obj.stride}@{iter_obj.axis}\"\n",
    "        padded = stride_str.rjust(r_col_widths[i])\n",
    "        r_stride_parts.append(padded)\n",
    "      r_stride_line = \"( \" + \", \".join(r_stride_parts) + \" )\"\n",
    "\n",
    "      print(d_extent_line + \"   \" + r_extent_line)\n",
    "      print(d_stride_line + \" + \" + r_stride_line, end=\"\")\n",
    "    else:\n",
    "      print(d_extent_line)\n",
    "      print(d_stride_line, end=\"\")\n",
    "\n",
    "    if self.O:\n",
    "      o_items = [f\"{offset}@{axis}\" for axis, offset in self.O.items()]\n",
    "      o_str = \" + \".join(o_items)\n",
    "      print(\" + \" + o_str, end=\"\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37dc8b",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### NVIDIA Tensor Core tile\n",
    "\n",
    "有了基础的定义后， 我们可以尝试实现论文中的第一个例子， 映射逻辑 $8×16$ tile到GPU的2个warp（各32 lane）+ 2个寄存器：\n",
    "\n",
    "$$\\begin{pmatrix}8 & 2 & 4 & 2 \\\\ 4@\\texttt{lane} & 1@\\texttt{warp} & 1@\\texttt{lane} & 1@\\texttt{reg}\\end{pmatrix} + \\begin{bmatrix}2 \\\\ 4@\\texttt{warp}\\end{bmatrix} + 5@\\texttt{warp}$$\n",
    "\n",
    "![](axe-0/example_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e6c398a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layout_a:\n",
      "(   8       2       4       2   )   (   2    )\n",
      "( 4@lane, 1@warp, 1@lane, 1@reg ) + ( 4@warp ) + 5@warp\n"
     ]
    }
   ],
   "source": [
    "layout_a = AxeLayout(D=[\n",
    "    Iter(8, 4, \"lane\"),\n",
    "    Iter(2, 1, \"warp\"),\n",
    "    Iter(4, 1, \"lane\"),\n",
    "    Iter(2, 1, \"reg\"),\n",
    "],\n",
    "    R=[Iter(2, 4, \"warp\")],\n",
    "    O={\"warp\": 5})\n",
    "\n",
    "layout_a.print(\"layout_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c013c",
   "metadata": {},
   "source": [
    "### Distributed sharding on a 2×2 GPU mesh\n",
    "\n",
    "假设一个$64×128$张量在4个GPU上的分片+复制混合：\n",
    "\n",
    "![](axe-0/example_2.png)\n",
    "\n",
    "**完全切分**（行按GPU行，列按GPU列分）：\n",
    "$$\\begin{pmatrix}2 & 32 & 2 & 64 \\\\ 1@\\texttt{gpuid} & 128@\\texttt{m} & 2@\\texttt{gpuid} & 1@\\texttt{m}\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "这里的`gpuid`表示设备维度，这里的`m`表示内存维度，如果把内存维度的Iter单独抽取出来，就可以计算在每个设备中Local Tensor的Layout。把gpuid的Iter出来，可以用stride来隐式体现gpu mesh的分布方式。\n",
    "\n",
    "**行切分+列复制**（行切分，每行shard复制到行内两GPU）：\n",
    "$$\\begin{pmatrix}2 & 32 & 128 \\\\ 1@\\texttt{gpuid} & 128@\\texttt{m} & 1@\\texttt{m}\\end{pmatrix} + \\begin{bmatrix}2 \\\\ 2@\\texttt{gpuid}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "实际上这两个就是经典的分布式张量切分方式，对应到SBP里面分别为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    (split(0), split(1)) \\\\\n",
    "    (split(0), broadcast)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cfd8d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layout_b:\n",
      "(    2       32      2      64 )\n",
      "( 1@gpuid, 128@m, 2@gpuid, 1@m )\n"
     ]
    }
   ],
   "source": [
    "layout_b = AxeLayout(D=[\n",
    "    Iter(2, 1, \"gpuid\"),\n",
    "    Iter(32, 128, \"m\"),\n",
    "    Iter(2, 2, \"gpuid\"),\n",
    "    Iter(64, 1, \"m\")],\n",
    "    R=[],\n",
    "    O={})\n",
    "layout_b.print(\"layout_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32c4f5",
   "metadata": {},
   "source": [
    "### Native multidimensional memory in Accelerators\n",
    "\n",
    "\n",
    "![](axe-0/example_3.png)\n",
    "\n",
    "这里是对于物理内存硬件的映射，认为P是`memory bank partitions`, F为`free dimensions`。主要体现的是Axe Layout是同时支持并行硬件维度和存储硬件的表达，所以当不考虑并行维度时，可以完成传统layout的功能。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "24739207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layout_c:\n",
      "(   2    128  512 )\n",
      "( 512@F, 1@P, 1@F )\n",
      "\n",
      "layout_d:\n",
      "(    2      128     112  )\n",
      "( 112@Col, 1@Lane, 1@Col )\n"
     ]
    }
   ],
   "source": [
    "layout_c = AxeLayout(D=[\n",
    "    Iter(2, 512, \"F\"),\n",
    "    Iter(128, 1, \"P\"),\n",
    "    Iter(512, 1, \"F\")],\n",
    "    R=[],\n",
    "    O={})\n",
    "\n",
    "layout_d = AxeLayout(D=[\n",
    "    Iter(2, 112, \"Col\"),\n",
    "    Iter(128, 1, \"Lane\"),\n",
    "    Iter(112, 1, \"Col\")],\n",
    "    R=[],\n",
    "    O={})\n",
    "\n",
    "layout_c.print(\"layout_c\")\n",
    "layout_d.print(\"layout_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09948d90",
   "metadata": {},
   "source": [
    "# Forward Mapping\n",
    "\n",
    "AxeLayout实际上是定义了logical index与hardware index的relation，基于此所实现的 logical index -> parallel index的映射称为forward mapping。\n",
    "\n",
    "我这里拿第一个例子，具体计算前向过程。首先`shard`部分负责分解原始的logical index，而他的stride是应用于hardware axes的。而后`replica`则会扩展映射集合，和`offset`部分一起将偏移作用到hardware axes上。\n",
    "\n",
    "    logical coord: (i=2, j=9)\n",
    "              ▼\n",
    "    shape: (8, 16)\n",
    "              ▼\n",
    "    linear index: 2 * 16 + 9 = 41\n",
    "              ▼\n",
    "    decomposed index as:\n",
    "        > 41 ÷ 16 = 2\n",
    "        > (41 % 16) ÷ 8 = 1\n",
    "        > ((41 % 16) % 8) // 2 = 0\n",
    "        > ((41 % 16) % 8) % 2 = 1\n",
    "              ▼\n",
    "    shard coord:\n",
    "        (  2   ,   1   ,   0   ,   1   )\n",
    "              ▼\n",
    "        (  8       2       4       2   )             (  2   )  \n",
    "    D = (4@lane, 1@warp, 1@lane, 1@reg)     +    R = (4@warp)    +    O = (5@warp)\n",
    "              ▼\n",
    "        > lane = 2*4 + 0*1 = 8                > warp0 = 0*4 = 0       >  warp = 5\n",
    "        > warp = 1*1 = 1                      > warp1 = 1*4 = 4     \n",
    "        > reg = 1*1 = 1                              \n",
    "              ▼\n",
    "      {lane: 8, warp: 1, reg: 1}            +  [{warp: 0}, {warp: 4}]  +   {warp: 5}\n",
    "              ▼\n",
    "    offset = [\n",
    "        {warp: 1+0+5 = 6 , lane: 8, reg: 1},\n",
    "        {warp: 1+4+5 = 10, lane: 8, reg: 1}\n",
    "    ]\n",
    "\n",
    "\n",
    "上述流程也许有些抽象，我们可以通过代码来更直观地理解Axe Layout的前向计算过程。 这里我复用了cute中的一些函数来辅助计算映射过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "806b53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward(self: AxeLayout, coord: Tuple[int, ...], shape: Tuple[int, ...]):\n",
    "  # 1. map layout to AxeLayout's shard\n",
    "  shard_extents = tuple(i.extent for i in self.D)\n",
    "  org_idx = cute.crd2idx(coord, shape, cute.suffix_product(shape))\n",
    "  shard_crd = cute.idx2crd(org_idx, shard_extents[::-1])[::-1]  # reverse for correct order\n",
    "  shards = [(idx * it.stride, it.axis) for (idx, it) in zip(shard_crd, self.D)]\n",
    "  rep_set = []\n",
    "  replica = self.R if len(self.R) else [Iter(1, 0, None)]\n",
    "  for reps in itertools.product([(i * it.stride, it.axis) for it in replica for i in range(it.extent)]):\n",
    "    d = dict()\n",
    "    for val, axis in shards:  # D\n",
    "      d[axis] = d.get(axis, 0) + val\n",
    "    for val, axis in reps:  # R\n",
    "      if axis is not None:\n",
    "        d[axis] = d.get(axis, 0) + val\n",
    "    for axis, val in self.O.items():  # O\n",
    "      d[axis] = d.get(axis, 0) + val\n",
    "    rep_set.append(d)\n",
    "\n",
    "  return rep_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7494b",
   "metadata": {},
   "source": [
    "接下来我们给定一个具体的logical coordinate (2, 9)，并通过Axe Layout的forward函数来计算映射到的并行轴坐标集合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "83b13ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'warp': 6, 'lane': 8, 'reg': 1}, {'warp': 10, 'lane': 8, 'reg': 1}]\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(crd: List[Dict[str, int]], **kwargs):\n",
    "  key_order = dict()\n",
    "  for k,v in kwargs.items():\n",
    "    key_order[k] = v\n",
    "  print([{k: d[k] for k in sorted(d, key=lambda k: key_order.get(k, 99))} for d in crd])\n",
    "\n",
    "AxeLayout.forward = forward\n",
    "hardware_coords = layout_a.forward((2, 9), (8, 16))\n",
    "\n",
    "pretty_print(hardware_coords, warp=0, lane=1, reg=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9973a6f",
   "metadata": {},
   "source": [
    "# Backward Mapping\n",
    "\n",
    "Axe Layout同样支持从 hardware index -> logical index的映射过程，称为backward mapping。这个过程就会比之前稍微简单一些，因为不需要考虑replica的扩展，只需要将offset和shard的过程反过来即可。具体流程我就不赘述，直接给出代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "96da7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self: AxeLayout, indices: List[Dict[str, int]], shape: Tuple[int, ...]):\n",
    "  index = indices[0].copy()\n",
    "  # 1. remove O\n",
    "  for axis, offset in self.O.items():\n",
    "    index[axis] = index.get(axis, 0) - offset\n",
    "  # 2. skip R\n",
    "  # 3. remove D\n",
    "  shard_domains: Dict[str, List[Tuple[int, int, int]]] = {}\n",
    "  for i, it in enumerate(self.D):\n",
    "    sdomain = shard_domains.get(it.axis, ([]))\n",
    "    sdomain.append((i, it.extent, it.stride))\n",
    "    shard_domains[it.axis] = sdomain\n",
    "  \n",
    "  scoords = [-1] * len(self.D) \n",
    "  for axis, tps in shard_domains.items():\n",
    "    tps.sort(key=lambda it: it[0])\n",
    "    slayout = cute.Layout(tuple(tp[1] for tp in tps), tuple(tp[2] for tp in tps))\n",
    "    sindex = index[axis]\n",
    "    scrd = cute.idx2crd(sindex, slayout.shape, slayout.stride)\n",
    "    for i, (idx, _, _) in enumerate(tps):\n",
    "      scoords[idx] = scrd[i]\n",
    "  \n",
    "  shard_extents = tuple(it.extent for it in self.D)\n",
    "  linear_idx = cute.crd2idx(tuple(scoords), shard_extents, cute.suffix_product(shard_extents))\n",
    "  logical_coord = cute.idx2crd(linear_idx, shape, cute.suffix_product(shape))\n",
    "\n",
    "  return logical_coord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd894bdb",
   "metadata": {},
   "source": [
    "我们逆推之前layout a的backward结果，可以检查backward的结果和forward给的输入是一致的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a80ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AxeLayout.backward = backward\n",
    "\n",
    "logical_coord = layout_a.backward(hardware_coords, (8, 16))\n",
    "assert logical_coord == (2, 9)\n",
    "print(logical_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68af408",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "这篇论文里面还提到支持Tile，Slice等操作，实际上也都是可以复用cute Layout Algorithm来实现的，这里就不进一步展开了。 同时还有一些编译器语法的前端支持，比如如何定义Tensor，Execution scopes， 这些内容不是我关注的重点，也就不展开了。 总的来说Axe Layout还是一个比较简洁且强大的张量布局抽象，他察觉到了在sharding的时候，实际上需要绑定tensor logical shape和hardware shape，因此可以把两部分放到一个`Shard`结构中进行统一处理，这样把parallel resource和memory resource都可以纳入同一个layout framework下进行处理，能在兼容原始layout的同时方便的扩展分布式能力。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
