<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2019-08-01">

<title>人脸识别方法总结 – Zheng’s Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-8b4baf804e461d9b72633f0de59a0cac.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7072389654d23eff08f359f9aa0d1ee7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Zheng’s Notes</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#deep-face-recognition-with-keras-dlib-and-opencv" id="toc-deep-face-recognition-with-keras-dlib-and-opencv" class="nav-link active" data-scroll-target="#deep-face-recognition-with-keras-dlib-and-opencv">Deep face recognition with Keras, Dlib and OpenCV</a>
  <ul class="collapse">
  <li><a href="#environment-setup-环境设置" id="toc-environment-setup-环境设置" class="nav-link" data-scroll-target="#environment-setup-环境设置">Environment setup 环境设置</a></li>
  <li><a href="#cnn-architecture-and-training" id="toc-cnn-architecture-and-training" class="nav-link" data-scroll-target="#cnn-architecture-and-training">CNN architecture and training</a></li>
  <li><a href="#custom-dataset-自定义数据集" id="toc-custom-dataset-自定义数据集" class="nav-link" data-scroll-target="#custom-dataset-自定义数据集">Custom dataset 自定义数据集</a></li>
  <li><a href="#face-alignment-面部对齐" id="toc-face-alignment-面部对齐" class="nav-link" data-scroll-target="#face-alignment-面部对齐">Face alignment 面部对齐</a></li>
  <li><a href="#embedding-vectors-嵌入向量" id="toc-embedding-vectors-嵌入向量" class="nav-link" data-scroll-target="#embedding-vectors-嵌入向量">Embedding vectors 嵌入向量</a></li>
  <li><a href="#distance-threshold-距离阈值" id="toc-distance-threshold-距离阈值" class="nav-link" data-scroll-target="#distance-threshold-距离阈值">Distance threshold 距离阈值</a></li>
  <li><a href="#face-recognition-人脸识别" id="toc-face-recognition-人脸识别" class="nav-link" data-scroll-target="#face-recognition-人脸识别">Face recognition 人脸识别</a></li>
  <li><a href="#dataset-visualization-数据集可视化" id="toc-dataset-visualization-数据集可视化" class="nav-link" data-scroll-target="#dataset-visualization-数据集可视化">Dataset visualization 数据集可视化</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">人脸识别方法总结</h1>
  <div class="quarto-categories">
    <div class="quarto-category">深度学习</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 1, 2019</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>要搞个人脸识别的应用，花了半天时间浏览一下，准备基于<code>open face</code>的模型来做移植。下面是对开源库<a href="https://github.com/krasserm/face-recognition">face-recognition</a>的使用指南进行一个翻译，看了一下基本知道了大致流程。不过我记得上次写过<a href="https://zhen8838.github.io/2019/06/03/l-softmax/">L softmx -&gt; A softmx -&gt; AM softmax</a>的这些<code>loss</code>都是用在人脸识别里面的，但是如果基于<code>softmax loss</code>的话，每加一个人脸不都是要重新训练一波吗？不知道是不是这个情况，目前还没看到别的方式。</p>
<!--more-->
<section id="deep-face-recognition-with-keras-dlib-and-opencv" class="level2">
<h2 class="anchored" data-anchor-id="deep-face-recognition-with-keras-dlib-and-opencv">Deep face recognition with Keras, Dlib and OpenCV</h2>
<p>面部识别识别面部图像或视频帧上的人。简而言之，人脸识别系统从输入人脸图像中提取特征，并将其与数据库中标记人脸的特征进行比较。比较基于特征相似性度量，并且最相似的数据库条目的标签用于标记输入图像。如果相似度值低于某个阈值，则输入图像标记为<em>unknown</em>。比较两个面部图像以确定它们是否显示同一个人被称为面部验证。</p>
<p>该笔记本使用深度卷积神经网络（CNN）从输入图像中提取特征。它遵循<a href="https://arxiv.org/abs/1503.03832">1</a>中描述的方法，其修改受<a href="http://cmusatyalab.github.io/openface/">OpenFace</a>项目的启发。 <a href="https://keras.io/">Keras</a>用于实现CNN，<a href="http://dlib.net/">Dlib</a>和<a href="https://opencv.org/">OpenCV</a>用于对齐面部在输入图像上。在<a href="http://vis-www.cs.umass.edu/lfw/">LFW</a>数据集的一小部分上评估面部识别性能，您可以将其替换为您自己的自定义数据集，例如：如果你想进一步试验这款笔记本，请附上你的家人和朋友的照片。在概述了CNN架构以及如何训练模型之后，将演示如何：</p>
<ul>
<li>在输入图像上检测，变换和裁剪面部。这可确保面部在进入CNN之前对齐。该预处理步骤对于神经网络的性能非常重要。</li>
<li>使用CNN从对齐的输入图像中提取面部的128维表示或<em>嵌入</em>。在嵌入空间中，欧几里德距离直接对应于面部相似性的度量。</li>
<li>将输入嵌入向量与数据库中标记的嵌入向量进行比较。这里，支持向量机（SVM）和KNN分类器，在标记的嵌入向量上训练，起到数据库的作用。在此上下文中的面部识别意味着使用这些分类器来预测标签，即新输入的身份。</li>
</ul>
<section id="environment-setup-环境设置" class="level3">
<h3 class="anchored" data-anchor-id="environment-setup-环境设置">Environment setup 环境设置</h3>
<p>For running this notebook, create and activate a new <a href="https://docs.python.org/3/tutorial/venv.html">virtual environment</a> and install the packages listed in <a href="requirements.txt">requirements.txt</a> with <code>pip install -r requirements.txt</code>. Furthermore, you’ll need a local copy of Dlib’s face landmarks data file for running face alignment:</p>
</section>
<section id="cnn-architecture-and-training" class="level3">
<h3 class="anchored" data-anchor-id="cnn-architecture-and-training">CNN architecture and training</h3>
<p>这里使用的CNN架构是初始架构<a href="https://arxiv.org/abs/1409.4842">2</a>的变体。更确切地说，它是<a href="https://arxiv.org/abs/1503.03832">1</a>中描述的NN4体系结构的变体，并标识为<a href="https://cmusatyalab.github.io/openface/models-and-accuracies/#model-definitions">nn4.small2</a>。这个笔记本使用该模型的Keras实现，其定义取自<a href="https://github.com/iwantooxxoox/Keras-OpenFace">Keras-OpenFace</a>项目。这里的体系结构细节并不太重要，只知道有一个完全连接的层，其中有128个隐藏单元，后面是卷积基础顶部的L2规范化层。这两个顶层被称为<em>嵌入层</em>，从中可以获得128维嵌入向量。完整模型在[model.py]（model.py）中定义，图形概述在[model.png]（model.png）中给出。可以使用<code>create_model（）</code>创建nn4.small2模型的Keras版本。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> model <span class="im">import</span> create_model</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>nn4_small2 <span class="op">=</span> create_model()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>W0801 21:29:26.376736 140043235366720 deprecation.py:506] From /home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor</code></pre>
<p>模型训练旨在学习嵌入<span class="math inline">\(f(x)\)</span>图像<span class="math inline">\(x\)</span>，使得相同身份的所有面部之间的平方L2距离较小，并且来自不同身份的一对面部之间的距离较大。当嵌入空间中的锚图像<span class="math inline">\(x^a_i\)</span>和正图像<span class="math inline">\(x^p_i\)</span>（相同身份）之间的距离小于两者之间的距离时，可以实现<em>三元组损失</em> <span class="math inline">\(L\)</span>。锚图像和负图像<span class="math inline">\(x^n_i\)</span>（不同的身份）至少有一个边缘<span class="math inline">\(\alpha\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}
L = \sum^{m}_{i=1} \large[ \small {\mid \mid f(x_{i}^{a}) - f(x_{i}^{p})) \mid \mid_2^2} - {\mid \mid f(x_{i}^{a}) - f(x_{i}^{n})) \mid \mid_2^2} + \alpha \large ] \small_+
\end{aligned}
\]</span></p>
<p><span class="math inline">\([z]_+\)</span>表示<span class="math inline">\(\max(z，0)\)</span>和<span class="math inline">\(m\)</span>是训练集中三元组的数量。 Keras中的三重态损失最好用自定义层实现，因为损失函数不遵循通常的“损失（输入，目标）”模式。该层调用<code>self.add_loss</code>来安装三元组丢失：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.python.keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.python.keras.models <span class="im">import</span> Model</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.python.keras.layers <span class="im">import</span> Input, Layer</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow.python <span class="im">as</span> tf</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> tf.ConfigProto()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>config.gpu_options.allow_growth <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>K.set_session(tf.Session(config<span class="op">=</span>config))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Input for anchor, positive and negative images</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>in_a <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">96</span>, <span class="dv">96</span>, <span class="dv">3</span>))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>in_p <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">96</span>, <span class="dv">96</span>, <span class="dv">3</span>))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>in_n <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">96</span>, <span class="dv">96</span>, <span class="dv">3</span>))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Output for anchor, positive and negative embedding vectors</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># The nn4_small model instance is shared (Siamese network)</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>emb_a <span class="op">=</span> nn4_small2(in_a)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>emb_p <span class="op">=</span> nn4_small2(in_p)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>emb_n <span class="op">=</span> nn4_small2(in_n)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TripletLossLayer(Layer):</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha, <span class="op">**</span>kwargs):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TripletLossLayer, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> triplet_loss(<span class="va">self</span>, inputs):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        a, p, n <span class="op">=</span> inputs</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        p_dist <span class="op">=</span> K.<span class="bu">sum</span>(K.square(a<span class="op">-</span>p), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        n_dist <span class="op">=</span> K.<span class="bu">sum</span>(K.square(a<span class="op">-</span>n), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> K.<span class="bu">sum</span>(K.maximum(p_dist <span class="op">-</span> n_dist <span class="op">+</span> <span class="va">self</span>.alpha, <span class="dv">0</span>), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.triplet_loss(inputs)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add_loss(loss)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer that computes the triplet loss from anchor, positive and negative embedding vectors</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>triplet_loss_layer <span class="op">=</span> TripletLossLayer(alpha<span class="op">=</span><span class="fl">0.2</span>, name<span class="op">=</span><span class="st">'triplet_loss_layer'</span>)([emb_a, emb_p, emb_n])</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Model that can be trained with anchor, positive negative images</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>nn4_small2_train <span class="op">=</span> Model([in_a, in_p, in_n], triplet_loss_layer)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>在训练期间，选择正对<span class="math inline">\((x^a_i,x^p_i)\)</span>和负对<span class="math inline">\((x^a_i，x^n_i)\)</span>难以区分的三元组是很重要的，即它们在嵌入空间中的距离差异应该是低于间距<span class="math inline">\(\alpha\)</span>，否则，网络无法学习有用的嵌入。因此，每次训练迭代应该基于在前一次迭代中学习的嵌入来选择一批新的三元组。假设从<code>triplet_generator（）</code>调用返回的生成器可以在这些约束下生成三元组，可以通过以下方式训练网络：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> data <span class="im">import</span> triplet_generator</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># triplet_generator() creates a generator that continuously returns </span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch </span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and n_batch are batches of anchor, positive and negative RGB images </span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># each having a shape of (batch_size, 96, 96, 3).</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> triplet_generator() </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>nn4_small2_train.<span class="bu">compile</span>(loss<span class="op">=</span><span class="va">None</span>, optimizer<span class="op">=</span><span class="st">'adam'</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>nn4_small2_train.fit_generator(generator, epochs<span class="op">=</span><span class="dv">10</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Please note that the current implementation of the generator only generates </span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># random image data. The main goal of this code snippet is to demonstrate </span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># the general setup for model training. In the following, we will anyway </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># use a pre-trained model so we don't need a generator here that operates </span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># on real training data. I'll maybe provide a fully functional generator</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># later.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>W0801 21:29:38.732154 140043235366720 training_utils.py:1101] Output triplet_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_loss_layer.
W0801 21:29:38.856654 140043235366720 deprecation.py:323] From /home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where


Epoch 1/10
100/100 [==============================] - 19s 191ms/step - loss: 0.8117
Epoch 2/10
100/100 [==============================] - 5s 46ms/step - loss: 0.7971
Epoch 3/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8035
Epoch 4/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8018
Epoch 5/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8049
Epoch 6/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8009
Epoch 7/10
100/100 [==============================] - 5s 47ms/step - loss: 0.8003
Epoch 8/10
100/100 [==============================] - 5s 48ms/step - loss: 0.7995
Epoch 9/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8004
Epoch 10/10
100/100 [==============================] - 5s 46ms/step - loss: 0.7998





&lt;tensorflow.python.keras.callbacks.History at 0x7f5cda4385c0&gt;</code></pre>
<p>上面的代码片段应该只演示如何设置模型训练。但是，我们不是从头开始实际训练模型，而是使用预先训练的模型，因为从头开始的训练非常昂贵，并且需要庞大的数据集来实现良好的泛化性能。例如，[1]（https://arxiv.org/abs/1503.03832）使用包含大约8M身份的200M图像的数据集。</p>
<p>OpenFace项目提供了<a href="https://cmusatyalab.github.io/openface/models-and-accuracies/#pre-trained-models">预训练模型</a>，这些模型使用公共人脸识别数据集<a href="http://vintage.winklerbros.net/facescrub.html">FaceScrub</a>进行训练,和<a href="http://arxiv.org/abs/1411.7923">CASIA-WebFace</a>。 Keras-OpenFace项目将预先训练的nn4.small2.v1模型的权重转换为<a href="https://github.com/iwantooxxoox/Keras-OpenFace/tree/master/weights">CSV文件</a>，然后进行<a href="http://vintage.winklerbros.net/facescrub.html">转换</a>这里x为一个二进制格式，可由Keras用<code>load_weights</code>加载：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>nn4_small2_pretrained <span class="op">=</span> create_model()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>nn4_small2_pretrained.load_weights(<span class="st">'weights/nn4.small2.v1.h5'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="custom-dataset-自定义数据集" class="level3">
<h3 class="anchored" data-anchor-id="custom-dataset-自定义数据集">Custom dataset 自定义数据集</h3>
<p>为了演示自定义数据集上的人脸识别，使用了<a href="http://vis-www.cs.umass.edu/lfw/">LFW</a>数据集的一小部分。它由<a href="images">10个身份</a>的100个面部图像组成。每个图像的元数据（文件和身份名称）被加载到内存中以供以后处理。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os.path</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> IdentityMetadata():</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, base, name, <span class="bu">file</span>):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dataset base directory</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base <span class="op">=</span> base</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># identity name</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># image file name</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span> <span class="op">=</span> <span class="bu">file</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.image_path()</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> image_path(<span class="va">self</span>):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> os.path.join(<span class="va">self</span>.base, <span class="va">self</span>.name, <span class="va">self</span>.<span class="bu">file</span>) </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_metadata(path):</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    metadata <span class="op">=</span> []</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">sorted</span>(os.listdir(path)):</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> f <span class="kw">in</span> <span class="bu">sorted</span>(os.listdir(os.path.join(path, i))):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Check file extension. Allow only jpg/jpeg' files.</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            ext <span class="op">=</span> os.path.splitext(f)[<span class="dv">1</span>]</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ext <span class="op">==</span> <span class="st">'.jpg'</span> <span class="kw">or</span> ext <span class="op">==</span> <span class="st">'.jpeg'</span>:</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                metadata.append(IdentityMetadata(path, i, f))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(metadata)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>metadata <span class="op">=</span> load_metadata(<span class="st">'images'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="face-alignment-面部对齐" class="level3">
<h3 class="anchored" data-anchor-id="face-alignment-面部对齐">Face alignment 面部对齐</h3>
<p>nn4.small2.v1模型使用对齐的面部图像进行训练，因此，自定义数据集中的面部图像也必须对齐。在这里，我们使用<a href="http://dlib.net/">Dlib</a>进行人脸检测，使用<a href="https://opencv.org/">OpenCV</a>进行图像变换和裁剪，以生成对齐的96x96 RGB人脸图像。通过使用OpenFace项目中的<a href="align.py">AlignDlib</a>实用程序，这很简单：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> align <span class="im">import</span> AlignDlib</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_image(path):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> cv2.imread(path, <span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># OpenCV loads images with color channels</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># in BGR order. So we need to reverse them</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img[...,::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the OpenFace face alignment utility</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>alignment <span class="op">=</span> AlignDlib(<span class="st">'models/landmarks.dat'</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Load an image of Jacques Chirac</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>jc_orig <span class="op">=</span> load_image(metadata[<span class="dv">77</span>].image_path())</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Detect face and return bounding box</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>bb <span class="op">=</span> alignment.getLargestFaceBoundingBox(jc_orig)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform image using specified face landmark indices and crop image to 96x96</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>jc_aligned <span class="op">=</span> alignment.align(<span class="dv">96</span>, jc_orig, bb, landmarkIndices<span class="op">=</span>AlignDlib.OUTER_EYES_AND_NOSE)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Show original image</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">131</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>plt.imshow(jc_orig)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Show original image with bounding box</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">132</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>plt.imshow(jc_orig)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>))</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Show aligned image</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">133</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>plt.imshow(jc_aligned)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><img src="face-recognition/output_14_0.png" class="img-fluid"></p>
<p>如OpenFace <a href="https://cmusatyalab.github.io/openface/models-and-accuracies/#pre-trained-models">预训练模型</a>中所述部分,模型nn4.small2.v1需要地标索引<code>OUTER_EYES_AND_NOSE</code>。让我们将面部检测，转换和裁剪实现为<code>align_image</code>函数，以便以后重用。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> align_image(img):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alignment.align(<span class="dv">96</span>, img, alignment.getLargestFaceBoundingBox(img), </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                           landmarkIndices<span class="op">=</span>AlignDlib.OUTER_EYES_AND_NOSE)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="embedding-vectors-嵌入向量" class="level3">
<h3 class="anchored" data-anchor-id="embedding-vectors-嵌入向量">Embedding vectors 嵌入向量</h3>
<p>现在可以通过将对齐和缩放的图像馈送到预训练的网络中来计算嵌入向量。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>embedded <span class="op">=</span> np.zeros((metadata.shape[<span class="dv">0</span>], <span class="dv">128</span>))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, m <span class="kw">in</span> <span class="bu">enumerate</span>(metadata):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> load_image(m.image_path())</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> align_image(img)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># scale RGB values to interval [0,1]</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> (img <span class="op">/</span> <span class="fl">255.</span>).astype(np.float32)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># obtain embedding vector for image</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    embedded[i] <span class="op">=</span> nn4_small2_pretrained.predict(np.expand_dims(img, axis<span class="op">=</span><span class="dv">0</span>))[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Let’s verify on a single triplet example that the squared L2 distance between its anchor-positive pair is smaller than the distance between its anchor-negative pair.</p>
<p>让我们在单个三元组示例上验证其锚定正对之间的平方L2距离小于其锚定负对之间的距离。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> distance(emb1, emb2):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(np.square(emb1 <span class="op">-</span> emb2))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_pair(idx1, idx2):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">3</span>))</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(<span class="ss">f'Distance = </span><span class="sc">{</span>distance(embedded[idx1], embedded[idx2])<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">121</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    plt.imshow(load_image(metadata[idx1].image_path()))</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">122</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    plt.imshow(load_image(metadata[idx2].image_path()))    </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>show_pair(<span class="dv">77</span>, <span class="dv">78</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>show_pair(<span class="dv">77</span>, <span class="dv">50</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><img src="face-recognition/output_21_0.png" class="img-fluid"></p>
<p><img src="face-recognition/output_21_1.png" class="img-fluid"></p>
<p>正如预期的那样，Jacques Chirac的两幅图像之间的距离小于Jacques Chirac图像与GerhardSchröder图像之间的距离（0.30 &lt;1.12）。但是我们仍然不知道距离阈值<span class="math inline">\(\tau\)</span>是在<em>相同身份</em>和<em>不同身份</em>之间作出决定的最佳边界。</p>
</section>
<section id="distance-threshold-距离阈值" class="level3">
<h3 class="anchored" data-anchor-id="distance-threshold-距离阈值">Distance threshold 距离阈值</h3>
<p>要查找$ $的最佳值，必须在一系列距离阈值上评估面部验证性能。在给定阈值处，所有可能的嵌入向量对被分类为<em>相同的身份</em>或<em>不同的身份</em>并且与基础事实进行比较。因为我们正在处理偏斜的类（比正对更多的负对），我们使用<a href="https://en.wikipedia.org/wiki/F1_score">F1得分</a>作为评估指标而不是<a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">准确度</a></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score, accuracy_score</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>distances <span class="op">=</span> [] <span class="co"># squared L2 distance between pairs</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>identical <span class="op">=</span> [] <span class="co"># 1 if same identity, 0 otherwise</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> <span class="bu">len</span>(metadata)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        distances.append(distance(embedded[i], embedded[j]))</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        identical.append(<span class="dv">1</span> <span class="cf">if</span> metadata[i].name <span class="op">==</span> metadata[j].name <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>distances <span class="op">=</span> np.array(distances)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>identical <span class="op">=</span> np.array(identical)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>thresholds <span class="op">=</span> np.arange(<span class="fl">0.3</span>, <span class="fl">1.0</span>, <span class="fl">0.01</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> [f1_score(identical, distances <span class="op">&lt;</span> t) <span class="cf">for</span> t <span class="kw">in</span> thresholds]</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>acc_scores <span class="op">=</span> [accuracy_score(identical, distances <span class="op">&lt;</span> t) <span class="cf">for</span> t <span class="kw">in</span> thresholds]</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>opt_idx <span class="op">=</span> np.argmax(f1_scores)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Threshold at maximal F1 score</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>opt_tau <span class="op">=</span> thresholds[opt_idx]</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy at maximal F1 score</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>opt_acc <span class="op">=</span> accuracy_score(identical, distances <span class="op">&lt;</span> opt_tau)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot F1 score and accuracy as function of distance threshold</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>plt.plot(thresholds, f1_scores, label<span class="op">=</span><span class="st">'F1 score'</span>)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>plt.plot(thresholds, acc_scores, label<span class="op">=</span><span class="st">'Accuracy'</span>)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>opt_tau, linestyle<span class="op">=</span><span class="st">'--'</span>, lw<span class="op">=</span><span class="dv">1</span>, c<span class="op">=</span><span class="st">'lightgrey'</span>, label<span class="op">=</span><span class="st">'Threshold'</span>)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Accuracy at threshold </span><span class="sc">{</span>opt_tau<span class="sc">:.2f}</span><span class="ss"> = </span><span class="sc">{</span>opt_acc<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Distance threshold'</span>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192
  return f(*args, **kwds)
/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
  return f(*args, **kwds)
/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
  return f(*args, **kwds)
/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192
  return f(*args, **kwds)</code></pre>
<p><img src="face-recognition/output_25_1.png" class="img-fluid"></p>
<p><span class="math inline">\(\tau\)</span> = 0.56的面部验证准确率为95.7％。对于总是预测<em>不同身份</em>（有980个pos。对和8821个neg。对）的分类器的基线为89％，这也不错，但由于nn4.small2.v1是一个相对较小的模型，它仍然小于最先进的模型（&gt; 99％）。</p>
<p>以下两个直方图显示了正负对的距离分布和决策边界的位置。这些分布明显分开，这解释了网络的辨别性能。人们也可以发现正对中的一些强异常值，但这里不再进一步分析。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>dist_pos <span class="op">=</span> distances[identical <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>dist_neg <span class="op">=</span> distances[identical <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">4</span>))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.hist(dist_pos)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>opt_tau, linestyle<span class="op">=</span><span class="st">'--'</span>, lw<span class="op">=</span><span class="dv">1</span>, c<span class="op">=</span><span class="st">'lightgrey'</span>, label<span class="op">=</span><span class="st">'Threshold'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Distances (pos. pairs)'</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>plt.hist(dist_neg)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>opt_tau, linestyle<span class="op">=</span><span class="st">'--'</span>, lw<span class="op">=</span><span class="dv">1</span>, c<span class="op">=</span><span class="st">'lightgrey'</span>, label<span class="op">=</span><span class="st">'Threshold'</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Distances (neg. pairs)'</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><img src="face-recognition/output_27_0.png" class="img-fluid"></p>
</section>
<section id="face-recognition-人脸识别" class="level3">
<h3 class="anchored" data-anchor-id="face-recognition-人脸识别">Face recognition 人脸识别</h3>
<p>给定距离阈值$ <span class="math inline">\(的估计，人脸识别现在就像计算输入嵌入向量与数据库中所有嵌入向量之间的距离一样简单。如果输入小于\)</span> $或标签<em>unknown</em>，则为输入分配具有最小距离的数据库条目的标签（即标识）。此过程还可以扩展到大型数据库，因为它可以轻松并行化。它还支持一次性学习，因为仅添加新标识的单个条目可能足以识别该标识的新示例。</p>
<p>更稳健的方法是使用数据库中的前$ k $评分条目标记输入，该条目基本上是<a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">KNN分类</a>，具有欧几里德距离度量。或者，线性<a href="https://en.wikipedia.org/wiki/Support_vector_machine">支持向量机</a>可以用数据库条目训练并用于分类，即识别新输入。为了训练这些分类器，我们使用50％的数据集，用于评估其他50％。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> np.array([m.name <span class="cf">for</span> m <span class="kw">in</span> metadata])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>encoder.fit(targets)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Numerical encoding of identities</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> encoder.transform(targets)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>train_idx <span class="op">=</span> np.arange(metadata.shape[<span class="dv">0</span>]) <span class="op">%</span> <span class="dv">2</span> <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>test_idx <span class="op">=</span> np.arange(metadata.shape[<span class="dv">0</span>]) <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 50 train examples of 10 identities (5 examples each)</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> embedded[train_idx]</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 50 test examples of 10 identities (5 examples each)</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> embedded[test_idx]</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y[train_idx]</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y[test_idx]</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">1</span>, metric<span class="op">=</span><span class="st">'euclidean'</span>)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>svc <span class="op">=</span> LinearSVC()</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>knn.fit(X_train, y_train)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>svc.fit(X_train, y_train)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>acc_knn <span class="op">=</span> accuracy_score(y_test, knn.predict(X_test))</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>acc_svc <span class="op">=</span> accuracy_score(y_test, svc.predict(X_test))</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'KNN accuracy = </span><span class="sc">{</span>acc_knn<span class="sc">}</span><span class="ss">, SVM accuracy = </span><span class="sc">{</span>acc_svc<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>KNN accuracy = 0.96, SVM accuracy = 0.98</code></pre>
<p>KNN分类器在测试集上实现了96％的准确度，SVM分类器为98％。让我们使用SVM分类器来说明单个示例中的人脸识别。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppress LabelEncoder warning</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>example_idx <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>example_image <span class="op">=</span> load_image(metadata[test_idx][example_idx].image_path())</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>example_prediction <span class="op">=</span> svc.predict([embedded[test_idx][example_idx]])</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>example_identity <span class="op">=</span> encoder.inverse_transform(example_prediction)[<span class="dv">0</span>]</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>plt.imshow(example_image)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Recognized as </span><span class="sc">{</span>example_identity<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><img src="face-recognition/output_32_0.png" class="img-fluid"></p>
<p>似乎合理:-)实际上应该检查分类结果是否（预测身份的数据库条目的一个子集）的距离小于<span class="math inline">\(\tau\)</span>，否则应该分配一个<em>未知标签</em>。此处跳过此步骤，但可以轻松添加。</p>
</section>
<section id="dataset-visualization-数据集可视化" class="level3">
<h3 class="anchored" data-anchor-id="dataset-visualization-数据集可视化">Dataset visualization 数据集可视化</h3>
<p>为了将数据集嵌入到2D空间中以显示身份聚类，将<a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-distributed Stochastic Neighbor Embedding</a>（t-SNE）应用于128维嵌入向量。除了一些异常值，身份集群很好地分开。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>X_embedded <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>).fit_transform(embedded)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">set</span>(targets)):</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> targets <span class="op">==</span> t</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X_embedded[idx, <span class="dv">0</span>], X_embedded[idx, <span class="dv">1</span>], label<span class="op">=</span>t)   </span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><img src="face-recognition/output_36_0.png" class="img-fluid"></p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ul>
<li>[1] <a href="https://arxiv.org/abs/1503.03832">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></li>
<li>[2] <a href="https://arxiv.org/abs/1409.4842">Going Deeper with Convolutions</a></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/zhen8838\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>