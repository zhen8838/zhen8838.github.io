---
title: tensorflowä¸pytorchä»£ç å·®å¼‚
mathjax: true
toc: true
categories:
  - æ·±åº¦å­¦ä¹ 
date: 2020-07-05 20:40:33
tags:
- Tensorflow
- Pytorch
---
å¯èƒ½ä¼šé•¿æœŸæ›´æ–°,å› ä¸ºç»å¸¸éœ€è¦ä»`pytorch`å·ä»£ç ç¿»è¯‘æˆ`tensorflow`ğŸ˜‘å› æ­¤è®°å½•ä¸€ä¸‹å·®å¼‚çš„åœ°æ–¹.

<!--more-->

####   1. `torch`ä¸­`nn.Conv2d`çš„`groups`å‚æ•°

`torch`ä¸­`groups`æ§åˆ¶è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„è¿æ¥,`in_channels`å’Œ`out_channels`å¿…é¡»éƒ½å¯ä»¥è¢«ç»„æ•´é™¤.
- `groups=1` ä¼ ç»Ÿçš„å·ç§¯æ–¹å¼.
- `groups=2` ç­‰æ•ˆäºå¹¶æ’è®¾ç½®ä¸¤ä¸ª`conv`å±‚ï¼Œæ¯ä¸ª`conv`å±‚çœ‹åˆ°ä¸€åŠçš„è¾“å…¥é€šé“ï¼Œå¹¶äº§ç”Ÿä¸€åŠçš„è¾“å‡ºé€šé“ï¼Œå¹¶ä¸”éšåå°†å®ƒä»¬éƒ½è¿æ¥åœ¨ä¸€èµ·.
- `groups=in_channels` æ¯ä¸ªè¾“å…¥é€šé“éƒ½æœ‰è‡ªå·±çš„æ»¤æ³¢å™¨.


ç­‰ä»·å†™æ³•:
```python
nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, 
          stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)

kl.DepthwiseConv2D(kernel_size=kernel_size,
                  strides=stride, padding='same', use_bias=False)
```

NOTE:

è¿™é‡Œ`pytorch`ç”Ÿæˆçš„å·ç§¯æ ¸`shape = [out_channel, 1, kh, kw]`
è¿™é‡Œ`tflite`ç”Ÿæˆçš„å·ç§¯æ ¸`shape = [1, kh, kw, out_channel]`


#### 2. `nn.AdaptiveAvgPool2d`ä¸`kl.GlobalAveragePooling2D`

å½“`nn.AdaptiveAvgPool2d(1)`æ—¶å’Œ`kl.GlobalAveragePooling2D()`ç›¸åŒ,ä½†æ˜¯æ³¨æ„`torch`çš„è¾“å‡ºæ˜¯ä¿æŒ`4`ç»´çš„,è€Œ`tensorflow`ä¸ä¿æŒç»´åº¦.

ç­‰ä»·å†™æ³•:
```python
x=nn.AdaptiveAvgPool2d(1)(x)
# -----------------------------
pool=kl.GlobalAveragePooling2D()
x=k.backend.expand_dims(k.backend.expand_dims(pool(x),1),1)
```

å½“ç„¶ç›´æ¥ä¿®æ”¹`GlobalAveragePooling2D`é‡Œ,æ·»åŠ `keepdims=true`å‚æ•°ä¹Ÿå¯ä»¥.


#### tf.contrib.layers.layer_normä¸tf.keras.LayerNormä¸nn.LayerNorm

##### `tf.contrib.layers.layer_norm`

tfä»¥å‰é—ç•™ä»£ç è¿˜æ˜¯æŒºè›‹ç–¼çš„ã€‚åœ¨`tf.contrib.layers.layer_norm`ä¸­ï¼Œå¯¹äºè¾“å…¥ä¸º`(4, 10, 10, 3)`çš„å¼ é‡ï¼Œæ˜¯å¯¹`(h,w,c)`è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä½†æ˜¯ä»–çš„ä»¿å°„ç³»æ•°é»˜è®¤åªå¯¹`c`æœ‰æ•ˆï¼š
```python
x = tf.reshape(tf.range(4 * 3 * 10 * 10, dtype=tf.float32), (4, 10, 10, 3))
xout = tf_contrib.layers.layer_norm(x,
                                    center=True, scale=True,
                                    scope='layer_norm')
mean.shape = (4, 1, 1, 1) 
gamma.shape = (3,)
```


##### `tf.keras.LayerNorm`

`tf.keras.LayerNorm`æˆ‘å°±å±å®ä¸æ‡‚äº†ï¼Œè®²é“ç†ä»–çš„å½’ä¸€åŒ–æ˜¯å¯¹`(h,w,c)`è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä»¿å°„ç³»æ•°å¯¹`c`æœ‰æ•ˆï¼Œä½†æ˜¯è¾“å‡ºå½’ä¸€åŒ–ç»“æœæ˜¯`400=4Ã—10x10`ï¼Œè¿™å°±å¾ˆå¥‡æ€ªäº†ï¼Œä»–é»˜è®¤çš„ç‰¹å¾ç»´åº¦æ˜¯`-1`ï¼Œä½†æ˜¯çœ‹èµ·æ¥å´æ²¡æœ‰å¹²`LayerNorm`åº”è¯¥åšçš„äº‹æƒ…ï¼Œåè€ŒæŠŠ`batch`ç»´åº¦ä¹Ÿå½’ä¸€åŒ–äº†ï¼Œ**ä½†æ˜¯**åœ¨æœ€ç»ˆæµ‹è¯•è¾“å‡ºçš„æ—¶å€™å‘ç°ç»“æœæ˜¯ç¬¦åˆé¢„æœŸçš„ã€‚ã€‚å±å®ä¸ç†è§£ã€‚

```python
inputs_np = tf.convert_to_tensor(
    np.arange(4 * 3 * 10 * 10).reshape((4, 10, 10, 3)), dtype=tf.float32)
inputs = k.Input((10, 10, 3), batch_size=None)
lm = k.layers.LayerNormalization()
lm.weights
lm_out = lm(inputs)
md = k.Model(inputs, lm_out) 
scale.shape # (3,)
mean.shape # (400,1)

lm_out_np = md(inputs_np)
lm_out_np = lm_out_np.numpy()
np.mean(lm_out_np[0, ...]) # -3.8146972e-08
np.var(lm_out_np[0, ...]) # 0.9985023
```



##### `nn.LayerNorm`

`nn.LayerNorm`æ˜¯å¯¹`(c,h,w)`è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä»¿å°„ç³»æ•°å¯¹`c,h,w`æœ‰æ•ˆï¼Œä½†æœ‰ä¸ªéå¸¸è›‹ç–¼çš„é—®é¢˜å°±æ˜¯ï¼Œä»–æ²¡æœ‰åŠæ³•å¤ç°è€ç‰ˆæœ¬`tf`çš„è¡Œä¸ºï¼Œå³åªç”¨`c`ä½œä¸ºä»¿å°„ç³»æ•°ï¼Œå¦‚æœå¼€å¯ä»¿å°„ä¼šå¯¼è‡´å‚æ•°éå¸¸å¤§ã€‚ã€‚ã€‚


```python
inputs = torch.tensor(np.arange(4 * 3 * 10 * 10).reshape((4, 3, 10, 10)), dtype=torch.float32)
lm = nn.LayerNorm([3, 10, 10], elementwise_affine=True)
ln_out = lm(inputs)
lm.weight.shape # torch.Size([3, 10, 10])
```

æˆ‘ç»§ç»­æ£€æŸ¥ä»–çš„æºç ,åœ¨`aten/src/ATen/native/layer_norm.h`ä¸­ï¼Œå°†è¾“å…¥ç»´åº¦åˆ†ä¸º`M*N`ï¼ŒæŒ‰ç…§æˆ‘ä»¬ä¸Šé¢çš„åšæ³•å³`M=4,N=3*10*10`ã€‚
ç„¶åè¿›å…¥cudaä»£ç `aten/src/ATen/native/cuda/layer_norm_kernel.cu`åˆ©ç”¨`RowwiseMomentsCUDAKernel`è®¡ç®—å‡å€¼ä¸æ–¹å·®ï¼š
```cpp
template <typename T>
void LayerNormKernelImplInternal(
    const Tensor& X,
    const Tensor& gamma,
    const Tensor& beta,
    int64_t M,
    int64_t N,
    T eps,
    Tensor* Y,
    Tensor* mean,
    Tensor* rstd) {
  DCHECK_EQ(X.numel(), M * N);
  DCHECK(!gamma.defined() || gamma.numel() == N);
  DCHECK(!beta.defined() || beta.numel() == N);
  const T* X_data = X.data_ptr<T>();
  const T* gamma_data = gamma.defined() ? gamma.data_ptr<T>() : nullptr;
  const T* beta_data = beta.defined() ? beta.data_ptr<T>() : nullptr;
  T* Y_data = Y->data_ptr<T>();
  T* mean_data = mean->data_ptr<T>();
  T* rstd_data = rstd->data_ptr<T>();
  cudaStream_t cuda_stream = at::cuda::getCurrentCUDAStream();
  RowwiseMomentsCUDAKernel<T>
      <<<M, cuda_utils::kCUDABlockReduceNumThreads, 0, cuda_stream>>>(
          N, eps, X_data, mean_data, rstd_data);
  LayerNormForwardCUDAKernel<T><<<M, kCUDANumThreads, 0, cuda_stream>>>(
      N, X_data, mean_data, rstd_data, gamma_data, beta_data, Y_data);
  AT_CUDA_CHECK(cudaGetLastError());
}
```

æ¥ä¸‹æ¥æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹`group norm`ï¼Œé¦–å…ˆç»™å®š`group`ï¼Œä»–å°†æ¨¡å‹è¾“å…¥åˆ†ä¸º`N,C,HxW`ã€‚åœ¨`aten/src/ATen/native/cuda/group_norm_kernel.cu`ä¸­ï¼Œå½“`group=1`çš„æ—¶å€™ï¼Œ`D=C/G=C`ï¼Œ`NÃ—G=N`,ä¹Ÿå°±æ˜¯`group=1`çš„æ˜¯ç­‰åŒäº`layer norm`ï¼Œå¹¶ä¸”æ­¤æ—¶ä»–çš„å¯å˜åŒ–å‚æ•°ä¸º`C`ï¼Œå¯ä»¥ç”¨æ¥ç­‰æ•ˆ`tf.contrib.layers.layer_norm`ã€‚

```cpp
template <typename T>
void GroupNormKernelImplInternal(
    const Tensor& X,
    const Tensor& gamma,
    const Tensor& beta,
    int64_t N,
    int64_t C,
    int64_t HxW,
    int64_t group,
    T eps,
    Tensor* Y,
    Tensor* mean,
    Tensor* rstd) {
  using T_ACC = acc_type<T, true>;
  TORCH_CHECK(X.numel() == N * C * HxW);
  TORCH_CHECK(!gamma.defined() || gamma.numel() == C);
  TORCH_CHECK(!beta.defined() || beta.numel() == C);
  if (N == 0) {
    return;
  }
  const int64_t G = group;
  const int64_t D = C / G;
  const T* X_data = X.data_ptr<T>();
  const T* gamma_data = gamma.defined() ? gamma.data_ptr<T>() : nullptr;
  const T* beta_data = beta.defined() ? beta.data_ptr<T>() : nullptr;
  T* Y_data = Y->data_ptr<T>();
  T* mean_data = mean->data_ptr<T>();
  T* rstd_data = rstd->data_ptr<T>();
  const auto kAccType = X.scalar_type() == kHalf ? kFloat : X.scalar_type();
  Tensor a = at::empty({N, C}, X.options().dtype(kAccType));
  Tensor b = at::empty({N, C}, X.options().dtype(kAccType));
  T_ACC* a_data = a.data_ptr<T_ACC>();
  T_ACC* b_data = b.data_ptr<T_ACC>();
  cudaStream_t cuda_stream = at::cuda::getCurrentCUDAStream();
  RowwiseMomentsCUDAKernel<T>
      <<<N * G, cuda_utils::kCUDABlockReduceNumThreads, 0, cuda_stream>>>(
          D * HxW, eps, X_data, mean_data, rstd_data);
  int64_t B = (N * C + kCUDANumThreads - 1) / kCUDANumThreads;
  ComputeFusedParamsCUDAKernel<T><<<B, kCUDANumThreads, 0, cuda_stream>>>(
      N, C, G, mean_data, rstd_data, gamma_data, beta_data, a_data, b_data);
  if (HxW < kCUDANumThreads) {
    B = (N * C * HxW + kCUDANumThreads - 1) / kCUDANumThreads;
    GroupNormForwardSimpleCUDAKernel<T><<<B, kCUDANumThreads, 0, cuda_stream>>>(
        N, C, HxW, X_data, a_data, b_data, Y_data);
  } else {
    GroupNormForwardCUDAKernel<T><<<N * C, kCUDANumThreads, 0, cuda_stream>>>(
        HxW, X_data, a_data, b_data, Y_data);
  }
  AT_CUDA_CHECK(cudaGetLastError());
}
```