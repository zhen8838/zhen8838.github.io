---
title: L softmx -> A softmx -> AM softmax
mathjax: true
toc: true
categories:
  - æ·±åº¦å­¦ä¹ 
date: 2019-06-03 12:44:43
tags:
-   æŸå¤±å‡½æ•°
-   Tensorflow
---

æœ¬ç¯‡æ–‡ç« æ˜¯å¯¹`Large Margin Softmax loss`,`Angular Margin to Softmax Loss`,` Additive Margin Softmax Loss`çš„å­¦ä¹ è®°å½•ã€‚å…¬å¼æˆ‘å°½é‡æŒ‰ç…§åŸæ–‡æ¥å†™ï¼Œå¹¶åŠ å…¥ä¸€ç‚¹æ³¨é‡Šã€‚

<!--more-->

# åŸå§‹çš„softmax

é¦–å…ˆå®šä¹‰ç¬¬`i`ä¸ªè¾“å…¥ç‰¹å¾$x_i$å’Œæ ‡ç­¾$y_i$ã€‚ä¼ ç»Ÿçš„`softmax`å®šä¹‰ä¸ºï¼š

$$ 
\begin{aligned}
    Loss=\frac{1}{N}\sum_i Loss_i= -\frac{1}{N}log(P_{y_i})= -\frac{1}{N}log(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}})
\end{aligned} 
$$

**æ³¨ï¼š** $f_{y_i}$åº”è¯¥æ˜¯æŒ‡çš„æ˜¯è¾“å‡ºä¸­å¯¹åº”labelåˆ†ç±»çš„é‚£ä¸ªä½ç½®ï¼Œ$f_j$å¯¹åº”ç¬¬$j$ä¸ªå…ƒç´ ($j \in [1,K)$,$K$å¯¹åº”ç±»åˆ«æ•°é‡)çš„è¾“å‡º.$N$ä¸ºè¾“å…¥æ ·æœ¬æ•°é‡.

å› ä¸ºåœ¨`softmax loss`ä¸­æœ€åéƒ½ä½¿ç”¨å…¨è¿æ¥å±‚æ¥å®ç°åˆ†ç±»,æ‰€ä»¥$f_{y_i}=\boldsymbol{W}_{y_i}^Tx_i$,$\boldsymbol{W}_{y_i}$æ˜¯$\boldsymbol{W}$çš„ç¬¬$y_i$åˆ—.(ä¸€èˆ¬æ¥è¯´æ˜¯å…¨è¿æ¥å±‚è¿˜éœ€è¦åŠ ä¸Šåç½®,ä½†æ˜¯ä¸ºäº†å…¬å¼æ¨å¯¼çš„æ–¹ä¾¿è¿™é‡Œä¸åŠ ,åœ¨å®é™…ä¸­ç›´æ¥åŠ åç½®å³å¯).å†å› ä¸ºè¿™é‡Œçš„$f_j$æ˜¯$\boldsymbol{W}_j$å’Œ$x_i$çš„å…§ç§¯,æ‰€ä»¥$f_j=\boldsymbol{W}_jx_i=\parallel \boldsymbol{W}_j\parallel \parallel x_i\parallel cos(\theta_j)$,$\theta_j(0\leq\theta_j\leq\pi)$æ˜¯å‘é‡$\boldsymbol{W}_i$å’Œ$x_i$çš„å¤¹è§’.æœ€ç»ˆæŸå¤±å‡½æ•°å®šä¹‰ä¸º:

$$ \begin{aligned}
    Loss_i=-log(\frac{e^{\parallel \boldsymbol{W}_{j_i}\parallel \parallel x_i\parallel cos(\theta_{y_i})}}{\sum_j e^{\parallel \boldsymbol{W}_j\parallel \parallel x_i\parallel cos(\theta_j)}})
\end{aligned} $$

# Large-Margin Softmax Loss

# åŠ¨æœº

è€ƒè™‘ä¸€ä¸ªäºŒåˆ†ç±»çš„`softmax loss`,å®ƒçš„ç›®æ ‡æ˜¯ä½¿$\boldsymbol{W}_{1}^Tx>\boldsymbol{W}_{1}^Tx$æˆ–$\parallel \boldsymbol{W}_{1}\parallel \parallel x\parallel cos(\theta_{1})>\parallel \boldsymbol{W}_{2}\parallel \parallel x\parallel cos(\theta_{2})$æ¥æ­£ç¡®åˆ†ç±»$x$. ç°åœ¨ä½œè€…æƒ³åˆ°æ„å»ºä¸€ä¸ªå†³ç­–ä½™é‡æ¥æ›´åŠ ä¸¥æ ¼åœ°çº¦æŸå†³ç­–é—´è·,æ‰€ä»¥è¦æ±‚$\parallel \boldsymbol{W}_{1}\parallel \parallel x\parallel cos(m\theta_{1})>\parallel \boldsymbol{W}_{2}\parallel \parallel x\parallel cos(\theta_{2}) (0\leq \theta_1 \leq \frac{\pi}{m})$,$m$æ˜¯ä¸€ä¸ªæ­£æ•´æ•°.ä»è€Œä½¿ä»¥ä¸‹ä¸ç­‰å¼æˆç«‹:
$$ \begin{aligned}
    \parallel \boldsymbol{W}_{1}\parallel \parallel x\parallel cos(\theta_{1})\geq\parallel \boldsymbol{W}_{1}\parallel \parallel x\parallel cos(m\theta_{1})>\parallel \boldsymbol{W}_{2}\parallel \parallel x\parallel cos(\theta_{2})
\end{aligned} $$

è¿™æ ·å¯¹å­¦ä¹ $\boldsymbol{W}_1\boldsymbol{W}_2$éƒ½æå‡ºäº†æ›´é«˜çš„è¦æ±‚.

**æ³¨:** å®é™…ä¸Šè¿™é‡Œæ˜¯å¯¹ç±»å†…çš„é—´è·æœ‰ä¸ªé™åˆ¶,å‡è®¾$m=6$æ—¶,$cos(\theta)\geq cos(m\theta)$çš„æ¡ä»¶ä¸‹,$\theta$è¢«å‹ç¼©åˆ°ä¸€ä¸ªç‰¹å®šçš„èŒƒå›´ä¸­,å¦‚ä¸‹å›¾æ‰€ç¤º,åªæœ‰è“çº¿å¤§äºçº¢çº¿çš„æ—¶å€™çš„$\theta$å–å€¼æ‰æ˜¯ç¬¦åˆæ¡ä»¶çš„,è¿™ç›¸å½“äºå˜ç›¸çš„å¢åŠ äº†$\boldsymbol{W}_1$ä¸$x$çš„æ–¹å‘é™åˆ¶,ä¹Ÿå°±æ˜¯å­¦ä¹ éš¾åº¦æ›´å¤§,ç±»å†…é—´è·æ›´å°.ä¸è¿‡æˆ‘æ„Ÿè§‰è¿˜å¾—é™åˆ¶ä¸€ä¸‹$\theta$çš„èŒƒå›´,å› ä¸ºç¬¦åˆæ¡ä»¶çš„$\theta$èŒƒå›´å¹¶ä¸æ­¢ä¸€ä¸ª:

![](l-softmax/1.png)

# å®šä¹‰

ä¸‹é¢ç»™å‡º`L softmax`çš„å®šä¹‰:

$$ \begin{align}
Loss_{i}=-\log \left(\frac{e^{\left\|\boldsymbol{W}_{y_{i}}\right\| \boldsymbol{x}_{i} \| \psi\left(\theta_{y_{i}}\right)}}{e^{\left\|\boldsymbol{W}_{y_{i}}\right\| \boldsymbol{x}_{i} \| \psi\left(\theta_{y_{i}}\right)}+\sum_{j \neq y_{i}} e^{\left\|\boldsymbol{W}_{j}\right\|\left\|\boldsymbol{x}_{i}\right\| \cos \left(\theta_{j}\right)} )}\right)
\end{align}
$$

å¹¶:

$$ \begin{align}
\psi(\theta)=\left\{\begin{array}{l}{\cos (m \theta), \quad 0 \leq \theta \leq \frac{\pi}{m}} \\ {\mathcal{D}(\theta), \quad \frac{\pi}{m}<\theta \leq \pi}\end{array}\right.
\end{align}
$$

$m$æ˜¯ä¸åˆ†ç±»è¾¹ç•Œå¯†åˆ‡ç›¸å…³çš„å‚æ•°,$m$è¶Šå¤§åˆ†ç±»å­¦ä¹ è¶Šéš¾.åŒæ—¶$\mathcal{D}(\theta)$éœ€è¦æ˜¯ä¸€ä¸ªå•è°ƒé€’å¢å‡½æ•°,ä¸”$\mathcal{D}(\frac{\pi}{m})==cas(\frac{\pi}{m})$(ä¿è¯ä»–æ˜¯è¿ç»­å‡½æ•°æ‰å¯ä»¥æ±‚å¯¼).ä¸‹å›¾æ˜¾ç¤ºäº†ä¸åŒ$\theta$å€¼ä¸‹çš„ä¸¤ä¸ªæŸå¤±å‡½æ•°çš„ç»“æœ,å¤¹è§’$\theta$è¶Šå¤§`L softmax loss`è¶Šå¤§.

![](l-softmax/2.png)

ä¸ºäº†ç®€å•èµ·è§,æ–‡ç« ä¸­ç‰¹åŒ–äº†$\psi(\theta)$å‡½æ•°ä¸º:
$$ \begin{align}
\psi(\theta)=(-1)^{k} \cos (m \theta)-2 k, \quad \theta \in\left[\frac{k \pi}{m}, \frac{(k+1) \pi}{m}\right]
\end{align}
$$

å…¶ä¸­$k\in[0,m-1]$ä¸”ä¸ºæ­£æ•´æ•°.

åœ¨å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­ä½œè€…å°†$cos(\theta_j)$æ›¿æ¢æˆ$\frac{\boldsymbol{W}_{j}^{T} \boldsymbol{x}_{i}}{\left\|\boldsymbol{W}_{j}\right\|\left\|\boldsymbol{x}_{i}\right\|}$,$cos(m\theta_{y_i})$æ›¿æ¢ä¸º:
$$  \begin{align}
\begin{aligned} \cos \left(m \theta_{y_{i}}\right) &=C_{m}^{0} \cos ^{m}\left(\theta_{y_{i}}\right)-C_{m}^{2} \cos ^{m-2}\left(\theta_{y_{i}}\right)\left(1-\cos ^{2}\left(\theta_{y_{i}}\right)\right) \\ &+C_{m}^{4} \cos ^{m-4}\left(\theta_{y_{i}}\right)\left(1-\cos ^{2}\left(\theta_{y_{i}}\right)\right)^{2}+\cdots \\ &(-1)^{n} C_{m}^{2 n} \cos ^{m-2 n}\left(\theta_{y_{y_{i}}}\right)\left(1-\cos ^{2}\left(\theta_{y_{i}}\right)\right)^{n}+\cdots \end{aligned}
\end{align}
$$

$n$æ˜¯æ­£æ•´æ•°ä¸”$2n\leq m$.ç„¶åå°†è¿™äº›å‡½æ•°å¸¦å…¥`L softmax loss`å…¬å¼ä¸­è®¡ç®—å³å¯æ‹¿æ¥åšæŸå¤±.ä½†æ˜¯è¿™ä¸ªå…¬å¼è¿˜æ˜¯å¤ªé•¿äº†,æ‰€ä»¥æˆ‘å†³å®šè¿˜æ˜¯ä¸ç”¨è¿™ä¸ªæŸå¤±å‡½æ•°.

# A softmax

è¿™é‡Œè¿˜æ˜¯ä»‹ç»ä¼ ç»Ÿçš„`softmax`,è¿™é‡Œå°±ä¸èµ˜è¿°äº†.

## modified softmax 

è¿™ä¸ªå°±æ˜¯ä½œè€…è®©ä¼ ç»Ÿçš„`softmax`çš„æƒé‡$\boldsymbol{W}$å½’ä¸€åŒ–:$\parallel \boldsymbol{W}_j\parallel =1$(å¿…é¡»æ˜¯æ²¡æœ‰biasçš„),å¾—åˆ°äº†`modified softmax loss`:
$$ \begin{align}
Loss=\frac{1}{N} \sum_{i}-\log \left(\frac{e^{\left\|\boldsymbol{x}_{i}\right\| \cos \left(\theta_{y_{i}, i}\right)}}{\sum_{j} e^{\left\|\boldsymbol{x}_{i}\right\| \cos \left(\theta_{j, i}\right)}}\right)
\end{align}
$$

å½“ç„¶æˆ‘ä¸çŸ¥é“è¿™ä¸ªå½’ä¸€åŒ–æœ‰ä»€ä¹ˆå¥½å¤„,ä»ä½œè€…ç»™å‡ºçš„ç»“æœä¸Šæ¥çœ‹å‡†ç¡®ç‡æé«˜äº†1%.

## Angular Margin to Softmax Loss

å¯¹äºä¸Šé¢çš„`modified softmax loss`åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­,å½“$\cos(\theta_1)>\cos(\theta_2)$å¯ä»¥ç¡®å®šç±»åˆ«ä¸º1,ä½†æ˜¯ä¸¤ä¸ªç±»åˆ«çš„å†³ç­–é¢æ˜¯$\cos(\theta_1)=\cos(\theta_2)$,è¿™æ ·çš„å†³ç­–é¢é—´éš”å¤ªå°,ä¸ºäº†è®©å†³ç­–é¢ä¹‹é—´çš„é—´è·æ›´å¤§ä¸€äº›,ä½œè€…æå‡ºåšä¸¤ä¸ªå†³ç­–é¢:
    
ç±»åˆ«1çš„å†³ç­–é¢ä¸º$\cos(m\theta_1)=\cos(\theta_2)$;
ç±»åˆ«2çš„å†³ç­–é¢ä¸º$\cos(\theta_1)=\cos(m\theta_2)$;
å…¶ä¸­$m\geq2,m\in N$,$m$å–æ•´æ•°å¯ä»¥åˆ©ç”¨å€è§’å…¬å¼;

è¿™æ ·çš„è¯,ä¹Ÿå°±æ˜¯è¯´é¢„æµ‹å‡ºæ¥çš„$\boldsymbol{x}$ä¸ä»–å¯¹åº”çš„ç±»çš„å¤¹è§’å¿…é¡»è¦å°äºä»–ä¸å…¶ä»–ç±»**æœ€å°**çš„å¤¹è§’çš„$m$å€,æ¯”å¦‚2åˆ†ç±»é—®é¢˜,å…¶å®å°±æœ‰ä¸‰ä¸ªå†³ç­–é¢,ä¸­é—´ä¸¤ä¸ªå†³ç­–é¢ä¹‹é—´çš„å°±æ˜¯å†³ç­–é—´è·.ç„¶åæ¨å¯¼å‡º`A softmax loss`:
$$ \begin{aligned}
Loss = \frac{1}{N}\sum_i-\log(\frac{\exp(\|x_i\|\cos(m\theta_{yi,i}))}{\exp(\|x_i\|\cos(m\theta_{yi,i}))+\sum_{j\neq y_i}\exp(\|x_i\|\cos(\theta_{j,i}))})
\end{aligned} $$

å…¶ä¸­$\theta_{yi,i}\in[0, \frac{\pi}{m}]$,è¿™å°±æ˜¯å› ä¸º$cos$çš„æ€§è´¨å†³å®š,æˆ‘åœ¨ä¸Šé¢ä¹Ÿæåˆ°äº†,å½“$m\theta_{yi,i}>\pi$æ—¶,ä¼šä½¿å¾—$m\theta_{yi,i}>\theta_{j,i}\ ,j\neq y_i$,ä½†$cos(m\theta_{1})>cos(\theta_2)$ä¹Ÿä¼šæˆç«‹,è¿™å°±ä¸ä¹‹å‰çš„å‡è®¾ç›¸å.

ä¸ºäº†é¿å…$cos$çš„é—®é¢˜,å°±è®¾è®¡
$$
\begin{aligned}
\psi(\theta_{y_i,i})=(-1)^k\cos(m\theta_{y_i,i})-2k\ \ \ \theta_{y_i,i}\in[\frac{k\pi}{m},\frac{(k+1)\pi}{m}],ä¸”k\in[0,m-1]    
\end{aligned}$$

æ¥ä»£æ›¿.è¿™æ ·ä½¿å¾—$\psi$éšç€$\theta_{y_i,i}$å•è°ƒé€’å‡,å¦‚æœ$m\theta_{y_i,i}>\theta_{j,i},j\neq y_i$é‚£ä¹ˆå¿…æœ‰$\psi(\theta_{y_i,i})<\cos(\theta_{j,i})$,è¿™é‡Œæˆ‘ä»¬çœ‹ä¸€ä¸‹$\psi(\theta)$å‡½æ•°çš„å›¾åƒ(å®Œå…¨ç¬¦åˆå•ç‹¬é€’å‡çš„è¦æ±‚,å¹¶ä¸”æ˜¯è¿ç»­å‡½æ•°å¯å¯¼):

![](l-softmax/psi.png)

æœ€ç»ˆå¾—å‡ºæŸå¤±å‡½æ•°å¦‚ä¸‹:
$$ \begin{aligned}
L_{ang} = \frac{1}{N}\sum_i-\log(\frac{\exp(\|x_i\|\psi(\theta_{yi,i}))}{\exp(\|x_i\|\psi(\theta_{yi,i}))+\sum_{j\neq y_i}\exp(\|x_i\|\cos(\theta_{j,i}))})
\end{aligned} $$


è¿™é‡Œå¯¹æ¯”ä¸€ä¸‹ä¸‰ä¸ªlossçš„ä¸åŒå†³ç­–ç•Œ:


|       æŸå¤±å‡½æ•°        |                                                                                                     å†³ç­–é¢                                                                                                     |
| :-------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|     Softmax Loss      |                                                                                  $\boldsymbol{W}_1-\boldsymbol{W}_2+b1-b2=0$                                                                                   |
| Modified Softmax Loss |                                                                       $\parallel\boldsymbol{x}(cos(\theta_1)-cos(\theta_2))\parallel$=0                                                                        |
|    A Softmax Loss     | $$\begin{aligned}   \parallel\boldsymbol{x}\parallel(cos(m\theta_1)-cos(\theta_2)=0\ \text{for class 1}\\ \parallel\boldsymbol{x}\parallel(cos(\theta_1)-cos(m\theta_2)=0\ \text{for class 2} \end{aligned} $$ |


**æ³¨:** å†™åˆ°è¿™é‡Œæˆ‘å‘ç°è¿™tmå°±æ˜¯ä¸€ä¸ªä½œè€…çš„ä¸¤ç¯‡æ–‡ç« ,åˆ°è¿™é‡Œ`A softmax`è¿˜æ˜¯ä¸`L softmax`çš„åŒºåˆ«å°±åœ¨äºæ˜¯å¦å¯¹$\parallel\boldsymbol{W}\parallel$è¿›è¡Œå½’ä¸€åŒ–.è¿™æ ·çš„è¯å¯¹äºåˆ†ç±»æ¥è¯´å¯ä»¥è§‚å¯Ÿåˆ°`A softmax`çš„åˆ†ç±»ç»“æœéƒ½æ˜¯é•¿åº¦ä¼šè¶‹è¿‘ç›¸åŒ,`L softmax`çš„åˆ†ç±»é•¿åº¦ä¼šä¸ç›¸åŒ.

| A softmax                        | L softmax                        |
| -------------------------------- | -------------------------------- |
| ![](l-softmax/a_softmax_res.png) | ![](l-softmax/l_softmax_res.png) |


# Additive Margin Softmax 

è¿™é‡Œè¿™ä¸ªä½œè€…åœ¨2018å¹´å‘è¡¨çš„æ–‡ç« ,è¿™é‡Œä¹Ÿä¸€å¹¶å­¦ä¹ äº†.è¿™ä¸ªæ˜¯æ•ˆæœå¥½å¹¶ä¸”å®ç°ç®€å•çš„ä¸€ç§æ–¹æ¡ˆ.

## å®šä¹‰ 

å®é™…ä¸Šçœ‹äº†ä¸Šé¢çš„`Loss`å‡½æ•°,æ‰€æœ‰çš„å˜åŒ–ç‚¹å…¶å®å°±åœ¨$e^{?}$åšæ–‡ç« .è¿™é‡Œé¦–å…ˆæ›¿æ¢äº†$\psi(\theta)$å‡½æ•°($m$æ˜¯åç§»):
$$ \begin{aligned}
    \psi(\theta)=cos(\theta)-m
\end{aligned} $$

ç„¶ååˆæŠŠ$W,x$éƒ½å½’ä¸€åŒ–:
$$ \begin{aligned}
    Loss_i=-log(\frac{e^{\parallel \boldsymbol{W}_{j_i}\parallel \parallel x_i\parallel cos(\theta_{y_i})}}{\sum_j e^{\parallel \boldsymbol{W}_j\parallel \parallel x_i\parallel cos(\theta_j)}})\ \ \ \ \parallel\boldsymbol{W}\parallel=1,\parallel x\parallel=1
\end{aligned} $$

è¿™æ ·å…§ç§¯ç»“æœå°±æ˜¯:
$$ \begin{aligned}
    <\boldsymbol{W_{y_i}},x>=cos(\theta_{y_i})
\end{aligned} $$

æ¥ç€å†æ¥ä¸€ä¸ªåç§»$m,\ m>0$ä¸ç¼©æ”¾å› å­$s$,å¾—åˆ°æœ€åçš„æŸå¤±å‡½æ•°:
$$ \begin{aligned}
    Loss_i = - \log \frac{e^{s\cdot(\cos\theta_{y_i} -m)}}{e^{s\cdot (\cos\theta_{y_i} -m)}+\sum^c_{j=1,i\neq t}  e^{s\cdot\cos\theta_j }}
\end{aligned} $$

æŒ‰ç…§åŸè®ºæ–‡,å–$m=0.35$,$s=30$.ç„¶åå°±ç»“æŸäº†... ğŸ˜„

å†è¡¥å¼ å›¾(`AM softmax`æ²¡æœ‰ä¹˜$s$):
![](l-softmax/psi2.png)

# ç¼–ç¨‹å®ç°

å› ä¸º`A softmax loss`æ˜¯å‡çº§ç‰ˆ,æ‰€ä»¥å°±å®ç°è¿™ä¸ª.

## A softmax loss

é¦–å…ˆæ˜¯å‡ ä¸ªä»£ç è¦æ³¨æ„çš„ç‚¹,$cos(\theta)$å¯ä»¥é€šè¿‡å‘é‡é™¤è®¡ç®—,$cos(m\theta)$å¯ä»¥é€šè¿‡å€è§’å…¬å¼è®¡ç®—.:
$$ 
\begin{split}
\cos \theta_{i,j} &= \frac{\vec{x_i}\cdot\vec{W_j}}{\|\vec{x_i}\|\cdot\|\vec{W_j}\|} \frac{\vec{x_i}\cdot\vec{W_{norm_j}}}{\|\vec{x_i}\|} \cr
\cos 2\theta &= 2\cos^2 \theta -1 \cr
\cos 3\theta &= 4\cos^2 \theta -3 \cos \theta \cr
\cos 4\theta &= 8\cos^4 \theta -8\cos^2 \theta + 1 \cr
\end{split}
$$

ç„¶åè¿˜æœ‰$k$çš„å–å€¼,å…ˆåˆ©ç”¨$sign$å‡½æ•°åˆ¤æ–­$cos(\theta)$å±äºå“ªä¸€ä¸ªåŒºé—´,å†ç¡®å®š$k$çš„å€¼:
$$ \begin{aligned}
    sign_0&=sign(cos(\theta))\\
    sign_3&=sign(cos(2\theta))*sign_0\\
    sign_4&=2*sign_0+sign_3-3 \\
    \psi(\theta)&=sign_3*cos(4\theta)+sign_4 \\
    &=sign_3*(8\cos^4 \theta -8\cos^2 \theta + 1)+sign_4
\end{aligned} $$

ä¸‹é¢æ˜¯$m=4$æ—¶çš„$Loss$è®¡ç®—å‡½æ•°.

```python
import tensorflow.python as tf
from tensorflow import contrib
def Angular_Softmax_Loss(embeddings, labels, margin=4):
        """
        Note:(about the value of margin)
        as for binary-class case, the minimal value of margin is 2+sqrt(3)
        as for multi-class  case, the minimal value of margin is 3

        the value of margin proposed by the author of paper is 4.
        here the margin value is 4.
        """
        l = 0.
        embeddings = tf.random_normal((2, 10))
        labels = tf.convert_to_tensor([[1], [2]], dtype=tf.int64)
        x_norm = tf.norm(embeddings, axis=1)

        with tf.variable_scope("softmax"):
            weights = tf.get_variable(name='embedding_weights',
                                      shape=[embeddings.get_shape().as_list()[-1], 10],
                                      initializer=contrib.layers.xavier_initializer())
            W = tf.nn.l2_normalize(weights, axis=0)
            # cacualting the cos value of angles between embeddings and W
            orgina_logits = tf.matmul(embeddings, W)
            N = embeddings.get_shape()[0]  # get batch_size
            single_sample_label_index = tf.concat([tf.constant(list(range(N)), tf.int64, shape=(N, 1)), labels], axis=1)
            # N = 128, labels = [1,0,...,9]
            # single_sample_label_index:
            # [ [0,1],
            #   [1,0],
            #   ....
            #   [128,9]]
            # è¿™é‡Œå°±æ˜¯F_y_i,æ ¹æ®æœ‰ç›®æ ‡çš„ä½ç½®æ¥é€‰å–éœ€è¦è®¡ç®—çš„lossä½ç½®.
            f_y_i = tf.gather_nd(orgina_logits, single_sample_label_index)
            # NOTE å› ä¸º \parallel W\parallel =1 æ‰€ä»¥ cos(theta)=f_y_i/x_norm
            cos_theta = tf.div(f_y_i, x_norm)
            cos_theta_2 = tf.pow(cos_theta, 2)
            cos_theta_4 = tf.pow(cos_theta, 4)

            sign0 = tf.sign(cos_theta)
            sign3 = tf.multiply(tf.sign(2 * cos_theta_2 - 1), sign0)
            sign4 = 2 * sign0 + sign3 - 3
            result = sign3 * (8 * cos_theta_4 - 8 * cos_theta_2 + 1) + sign4

            margin_logits = tf.multiply(result, x_norm)
            f = 1.0 / (1.0 + l)
            ff = 1.0 - f
            combined_logits = tf.add(orgina_logits,
                                     tf.scatter_nd(single_sample_label_index,
                                                   tf.subtract(margin_logits, f_y_i),
                                                   orgina_logits.get_shape()))
            updated_logits = ff * orgina_logits + f * combined_logits
            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits=updated_logits, labels=tf.reshape(labels, (-1,))))
            pred_prob = tf.nn.softmax(logits=updated_logits)
            return pred_prob, loss
```


## Additive Margin Softmax

è¿™ä¸ªæ¯”å‰é¢é‚£ä¸ªç®€å•å¤šäº†:


```python
from tensorflow.python import keras
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import *
import tensorflow.python.keras.backend as K
from tensorflow.python.keras.constraints import unit_norm

(train_x, train_y), (test_x, test_y) = keras.datasets.fashion_mnist.load_data()

train_x = K.reshape(train_x, (-1, 784))
train_y = keras.utils.to_categorical(train_y, 10)


model = keras.Sequential([Input(shape=(784,)),
                          Dense(512, keras.activations.relu),
                          Dense(256, keras.activations.relu),
                          Dense(128, keras.activations.relu),
                          Lambda(lambda x: K.l2_normalize(x, 1)),
                          Dense(10, use_bias=False, kernel_constraint=unit_norm())])


def am_softmax_loss(y_true, y_pred, scale=30, margin=0.35):
    # NOTE é¢„æµ‹å‡ºæ¥çš„xå°±æ˜¯å½’ä¸€åŒ–åçš„,å¹¶ä¸”Wä¹Ÿæ˜¯å½’ä¸€åŒ–åçš„,æ‰€ä»¥y_predå°±æ˜¯cos(ğœƒ)
    y_pred = (y_true * (y_pred - margin) + (1 - y_true) * y_pred) * scale
    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)


model.compile(loss=am_softmax_loss, optimizer=keras.optimizers.Adam(),
              metrics=[keras.metrics.CategoricalAccuracy()])

model.fit(x=train_x, y=train_y, batch_size=128, epochs=5)
```


å¯¹äºäººè„¸è¯†åˆ«é—®é¢˜ï¼Œè¿˜æ˜¯éœ€è¦ç¨€ç–ç‰ˆæœ¬çš„`Additive Margin Softmax`å®ç°ä»¥èŠ‚çœæ˜¾å­˜ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªå®ç°ï¼š




```python
class Sparse_AmsoftmaxLoss(kls.Loss):

  def __init__(self,
               batch_size: int,
               scale: int = 30,
               margin: int = 0.35,
               reduction='auto',
               name=None):
    """ sparse addivate margin softmax

        Parameters
        ----------

        scale : int, optional

            by default 30

        margin : int, optional

            by default 0.35

        """
    super().__init__(reduction=reduction, name=name)
    self.scale = scale
    self.margin = margin
    self.batch_idxs = tf.expand_dims(tf.range(0, batch_size, dtype=tf.int32),
                                     1)  # shape [batch,1]

  def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
    """ loss calc

        Parameters
        ----------
        y_true : tf.Tensor

            shape = [batch,1] type = tf.int32

        y_pred : tf.Tensor

            shape = [batch,class num] type = tf.float32

        Returns
        -------

        tf.Tensor

            loss
        """
    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], 1)
    y_true_pred = tf.gather_nd(y_pred, idxs)
    y_true_pred = tf.expand_dims(y_true_pred, 1)
    y_true_pred_margin = y_true_pred - self.margin
    _Z = tf.concat([y_pred, y_true_pred_margin], 1)
    _Z = _Z * self.scale
    logZ = tf.math.reduce_logsumexp(_Z, 1, keepdims=True)
    logZ = logZ + tf.math.log(1 - tf.math.exp(self.scale * y_true_pred - logZ))
    return -y_true_pred_margin * self.scale + logZ
```


ç¨€ç–ç‰ˆæœ¬çš„`Additive Margin Softmax`ä»£ç ä¸­æœ€åä¸¤æ­¥çš„æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ï¼š
$$
\begin{aligned}
    \text{Let}\ \ p&=y_{pred}\\
    log_Z&=\log\left[e^{s*p_0}+\ldots+e^{s*p_c}+\ldots+e^{s*p_{y_i}}+e^{s*(p_{y_i}-m)}\right]\\
    log_Z&=log_Z+\log\left[1-e^{s*p_{y_i}-log_Z}  \right]\\
    &=log_Z+\log\left[1-\frac{e^{s*p_{y_i}}}{e^{log_Z}}  \right]\\
    &=log_Z+\log\left[1-\frac{e^{s*p_{y_i}}}{e^{\log\left[e^{s*p_0}+\ldots+e^{s*p_c}+\ldots+e^{s*p_{y_i}}+e^{s*(p_{y_i}-m)}\right]}}  \right]\\
    &=\log\left[e^{s*p_0}+\ldots+e^{s*p_c}+e^{s*(p_{y_i}-m)}\right]\\
    \mathcal{L}&=-\log\left[\frac{e^{s*(p_{y_i}-m)}}{e^{s*p_0}+\ldots+e^{s*p_c}+e^{s*(p_{y_i}-m)}} \right]\\
    &=-\log e^{s*(p_{y_i}-m)}+log_Z\\
    &=-s*(p_{y_i}-m)+log_Z\\
\end{aligned}
$$

# æ€»ç»“

çœ‹äº†ä¸‰ä¸ªæ–‡ç« ,éƒ½æ˜¯é€šè¿‡å‡å°å†…ç±»é—´è·æ¥è¾¾åˆ°æ•ˆæœ.å‡å°å†…ç±»é—´è·çš„é€”å¾„éƒ½æ˜¯æ„å»º$\psi(\theta)$ä»£æ›¿$cos(\theta)$,å½“ç„¶è¦ä¿è¯$\psi(\theta)  < \cos\theta$.

ä¹‹å‰çš„`L softmax`å’Œ`A softmax`ä¸ºäº†ä¿è¯$\psi(\theta)=cos(m\theta)  < \cos\theta$è¿˜ä½¿ç”¨äº†åˆ†æ®µå‡½æ•°,æ¯”è¾ƒéº»çƒ¦.ç„¶å`AM softmax`å°±æ¯”è¾ƒç®€å•ç²—æš´äº†.