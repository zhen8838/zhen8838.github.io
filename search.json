[
  {
    "objectID": "posts/numpy-subcalss-ndarray.html",
    "href": "posts/numpy-subcalss-ndarray.html",
    "title": "numpy中继承ndarray",
    "section": "",
    "text": "关于如何更好的利用numpy中ndarray的特性来提升编程舒适度.\nCode\nimport numpy as np"
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#array_ufunc__",
    "href": "posts/numpy-subcalss-ndarray.html#array_ufunc__",
    "title": "numpy中继承ndarray",
    "section": "__array_ufunc__",
    "text": "__array_ufunc__\n__array_ufunc__是一个unary操作函数的一个接口,即调用ufunc是对数组元素进行elemwise的操作,比如add\\subtract\\multiply\\log\\sin等等.\n每个__array_ufunc__接收参数如下: - ufunc, ufunc函数对象,比如numpy.xxx\n\nmethod, 方法名,因为每个ufunc函数对象都有四个方法,所以还得选方法\ninputs, 输入对象\nkwargs, ufunc的可选参数\n\n对于每个ufunc都有相同的输入参数、属性,这个可以去文档中看,主要是每个函数还对应了4个method: |name | description| |-|-| |ufunc.reduce(array[, axis, dtype, out, …])|Reduces array’s dimension by one, by applying ufunc along one axis.| |ufunc.accumulate(array[, axis, dtype, out])|Accumulate the result of applying the operator to all elements.| |ufunc.reduceat(array, indices[, axis, …])|Performs a (local) reduce with specified slices over a single axis.| |ufunc.outer(A, B, /, **kwargs)|Apply the ufunc op to all pairs (a, b) with a in A and b in B.| |ufunc.at(a, indices[, b])|Performs unbuffered in place operation on operand ‘a’ for elements specified by ‘indices’. |\n接下来我们适配一个__call__方法,也就是直接调用的方法:\n\n\nCode\nfrom numbers import Number\n\n\nclass PEArray:\n  def __init__(self, height, width, spad_size, pe=None):\n    self._h = height\n    self._w = width\n    self._spad = spad_size\n    if pe is not None:\n      self._pe = pe\n    else:\n      self._pe = np.random.rand(self._h, self._w)\n\n  def __repr__(self):\n    return f\"{self.__class__.__name__}(h={self._h}, w={self._w})\"\n\n  def __array__(self, dtype=None):\n    return self._pe\n\n  def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n    if method == '__call__':\n      scalars = []\n      objects = []\n      for input in inputs:\n        if isinstance(input, Number):\n          scalars.append(input)\n        elif isinstance(input, self.__class__):\n          if input._pe.shape != self._pe.shape:\n            raise ValueError(\"inconsistent shape\")\n          objects.append(input._pe)\n        else:\n          return NotImplementedError(\"not support the other type\")\n      return self.__class__(self._h, self._w, self._spad, ufunc(*objects, *scalars, **kwargs))\n    else:\n      return NotImplementedError(\"now only support __call__!\")\n\n\n在编写以上代码时需要注意类内的array也会被传入到input里面的,所以不要手动再传入self._pe了. 还有就是要给自己类写一个合适的构造函数,以便于直接传入数组重新构造,接下来可以看到可以输出的正确的对象了.\n\n\nCode\na = PEArray(3,4,5)\nb = 3.\nc = PEArray(3,4,6)\nprint(np.add(a,b))\nprint(np.multiply(a,c))\n\n\nPEArray(h=3, w=4)\nPEArray(h=3, w=4)\n\n\n但是还有个问题,我们此时没有继承python内部的操作符号:\n\n\nCode\na + b\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-bd58363a63fc&gt; in &lt;module&gt;\n----&gt; 1 a + b\n\nTypeError: unsupported operand type(s) for +: 'PEArray' and 'float'\n\n\n\n如果一个个继承比较麻烦,我们可以继承numpy内置的脚手架类numpy.lib.mixins.NDArrayOperatorsMixin\n\n\nCode\nfrom numpy.lib.mixins import NDArrayOperatorsMixin\n\n\nclass PEArray(NDArrayOperatorsMixin):\n  def __init__(self, height, width, spad_size, pe=None):\n    self._h = height\n    self._w = width\n    self._spad = spad_size\n    if pe is not None:\n      self._pe = pe\n    else:\n      self._pe = np.random.rand(self._h, self._w)\n\n  def __repr__(self):\n    return f\"{self.__class__.__name__}(h={self._h}, w={self._w})\"\n\n  def __array__(self, dtype=None):\n    return self._pe\n\n  def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n    if method == '__call__':\n      scalars = []\n      objects = []\n      for input in inputs:\n        if isinstance(input, Number):\n          scalars.append(input)\n        elif isinstance(input, self.__class__):\n          if input._pe.shape != self._pe.shape:\n            raise ValueError(\"inconsistent shape\")\n          objects.append(input._pe)\n        else:\n          return NotImplementedError(\"not support the other type\")\n      return self.__class__(self._h, self._w, self._spad, ufunc(*objects, *scalars, **kwargs))\n    else:\n      return NotImplementedError(\"now only support __call__!\")\n\n\n\n\nCode\na = PEArray(1,2,3)\nb = 10\na + b\n\n\nPEArray(h=1, w=2)"
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#array_function__",
    "href": "posts/numpy-subcalss-ndarray.html#array_function__",
    "title": "numpy中继承ndarray",
    "section": "__array_function__",
    "text": "__array_function__\n之前方式我们支持了ufunc,其实按那种方式也可以支持一些非ufunc,比如np.sum,其实他默认是调用的reduce方法,那么只需要在__array_ufunc__中添加对reduce也是可以的. 不过还有一种更加方便的方式,那就是直接在整个函数级别进行overwrite,比如我们要使用w w\n\n\nCode\nnp.sum(a)\n\n\nNotImplementedError('now only support __call__!')\n\n\n\n\nCode\nfrom typing import List\nHANDLED_FUNCTIONS = {}\n\n\ndef register(np_function):\n  def decorator(func):\n    HANDLED_FUNCTIONS[np_function] = func\n    return func\n  return decorator\n\n\nclass PEArray(NDArrayOperatorsMixin):\n  def __init__(self, height, width, spad_size, pe=None):\n    self._h = height\n    self._w = width\n    self._spad = spad_size\n    if pe is not None:\n      self._pe = pe\n    else:\n      self._pe = np.random.rand(self._h, self._w)\n\n  def __repr__(self):\n    return f\"{self.__class__.__name__}(h={self._h}, w={self._w})\"\n\n  def __array__(self, dtype=None):\n    return self._pe\n\n  def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n    if method == '__call__':\n      scalars = []\n      objects = []\n      for input in inputs:\n        if isinstance(input, Number):\n          scalars.append(input)\n        elif isinstance(input, self.__class__):\n          if input._pe.shape != self._pe.shape:\n            raise ValueError(\"inconsistent shape\")\n          objects.append(input._pe)\n        else:\n          return NotImplementedError(\"not support the other type\")\n      return self.__class__(self._h, self._w, self._spad, ufunc(*objects, *scalars, **kwargs))\n    else:\n      return NotImplementedError(\"now only support __call__!\")\n\n  def __array_function__(self, func, types, args, kwargs):\n    if func not in HANDLED_FUNCTIONS:\n      return NotImplemented\n    # Note: this allows subclasses that don't override\n    # __array_function__ to handle DiagonalArray objects.\n    if not all(issubclass(t, self.__class__) for t in types):\n      return NotImplemented\n    return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\n\n@register(np.sum)\ndef pe_sum(arr: PEArray) -&gt; np.ndarray:\n  return arr._pe.sum()\n\n\n@register(np.concatenate)\ndef pe_concat(arrs: List[PEArray], axis: int = 0):\n  assert(len(arrs) &gt; 1)\n  assert(axis &lt; 2)\n  assert((arrs[0]._spad == np.array([arr._spad for arr in arrs[1:]])).all())\n  new_pe = np.concatenate([arr._pe for arr in arrs], axis=axis)\n  return PEArray(new_pe.shape[0], new_pe.shape[1], arrs[0]._spad, new_pe)\n\n\n\n\nCode\na = PEArray(2, 4, 1)\nb = PEArray(2, 3, 1)\nc = PEArray(3, 4, 1)\n\nnp.sum(a)\n\n\n5.520550933404442\n\n\n\n\nCode\nnp.concatenate([a, b], axis=1)\n\n\nPEArray(h=2, w=7)\n\n\n\n\nCode\nnp.concatenate([a, c], axis=0)\n\n\nPEArray(h=5, w=4)"
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#总结",
    "href": "posts/numpy-subcalss-ndarray.html#总结",
    "title": "numpy中继承ndarray",
    "section": "总结",
    "text": "总结\n自定义数组容器的方法还是比较方便的,同时可以在两个层次上最大程度的复用numpy内置的接口,提高抽象的一致性."
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#view转换类似于c中的dynamic-cast.",
    "href": "posts/numpy-subcalss-ndarray.html#view转换类似于c中的dynamic-cast.",
    "title": "numpy中继承ndarray",
    "section": "2. view转换,类似于c++中的dynamic cast.",
    "text": "2. view转换,类似于c++中的dynamic cast.\n\n\nCode\nimport numpy as np\n# create a completely useless ndarray subclass\nclass C(np.ndarray): pass\n# create a standard ndarray\narr = np.zeros((3,))\n# take a view of it, as our useless subclass\nc_arr = arr.view(C)\ntype(c_arr)\n\n\n__main__.C"
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#from-template-比如copysliceufunc都会生成",
    "href": "posts/numpy-subcalss-ndarray.html#from-template-比如copysliceufunc都会生成",
    "title": "numpy中继承ndarray",
    "section": "3. from template, 比如copy,slice,ufunc都会生成",
    "text": "3. from template, 比如copy,slice,ufunc都会生成\n\n\nCode\nv = c_arr[1:]\nprint(type(v)) # 切片后还是老类别,那是因为切片只是原始数组中的一个数组投影.\n\n\n&lt;class '__main__.C'&gt;"
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#view-cast和from-template的关系",
    "href": "posts/numpy-subcalss-ndarray.html#view-cast和from-template的关系",
    "title": "numpy中继承ndarray",
    "section": "view cast和from template的关系",
    "text": "view cast和from template的关系\nview cast主要是当有了一个完整的ndarry的时候,创建子类类型的新对象.from template主要是从已有的对象中创建新对象, 这时候我们子类的属性通常就要复制过去."
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#继承的问题",
    "href": "posts/numpy-subcalss-ndarray.html#继承的问题",
    "title": "numpy中继承ndarray",
    "section": "继承的问题",
    "text": "继承的问题\n继承的问题在于我们编写合适的处理方法对应以上三种情况的,否则你编写的子类很容易就变成了ndarray类型,导致后续调用出错."
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#new__方法",
    "href": "posts/numpy-subcalss-ndarray.html#new__方法",
    "title": "numpy中继承ndarray",
    "section": "1. __new__方法",
    "text": "1. __new__方法\n首先我们不能从__init__方法开始,因为ndarray是从__new__方法就开始构造了的.__new__是可以返回任意的值的,同时__init__方法的self参数其实是从__new__返回的.\n一个类构造的流程其实这样的,从__new__中创建特定类型的对象,然后返回值传入到__init__方法中对对象的属性等进行修改,最后这个对象返回给用户. 也就是我们从pe=PEArray()中获取的对象就是从new中返回的.\n通过重载__new__方法,我们可以做到对一个类返回不同类型的对象,下面这个例子就是从初始化D返回一个C对象(因为他返回对象类型不是自身类型,所以不会触发__init__):\n\n\nCode\nclass C:\n    def __new__(cls, *args):\n        print('Cls in __new__:', cls)\n        print('Args in __new__:', args)\n        # The `object` type __new__ method takes a single argument.\n        return object.__new__(cls)\n\n    def __init__(self, *args):\n        print('type(self) in __init__:', type(self))\n        print('Args in __init__:', args)\n\nclass D(C):\n    def __new__(cls, *args):\n        print('D cls is:', cls)\n        print('D args in __new__:', args)\n        return C.__new__(C, *args)\n\n    def __init__(self, *args):\n        # we never get here\n        print('In D __init__')\nD()\n\n\nD cls is: &lt;class '__main__.D'&gt;\nD args in __new__: ()\nCls in __new__: &lt;class '__main__.C'&gt;\nArgs in __new__: ()\n\n\n&lt;__main__.C at 0x10fd0cb50&gt;\n\n\n在view cast的时候其实就是使用__new__方法,通过obj = ndarray.__new__(subtype, shape, ...返回了一个子类的对象,保证了子类在切片等时候返回对象的一致性."
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#array_finalize__方法",
    "href": "posts/numpy-subcalss-ndarray.html#array_finalize__方法",
    "title": "numpy中继承ndarray",
    "section": "2. __array_finalize__方法",
    "text": "2. __array_finalize__方法\narray_finalize 是 numpy 提供的机制，允许子类处理创建新实例的各种方式。因为上面的__new__只有在显式构建的时候才会被调用,所以需要这个方法对别的创建方法进行处理\n\n\nCode\n\nclass C(np.ndarray):\n  def __new__(cls, *args, **kwargs):\n    print('In __new__ with class %s' % cls)\n    return super().__new__(cls, *args, **kwargs)\n\n  def __init__(self, *args, **kwargs):\n    # in practice you probably will not need or want an __init__\n    # method for your subclass\n    print('In __init__ with class %s' % self.__class__)\n\n  def __array_finalize__(self, obj):\n    print('In array_finalize:')\n    print('   self type is %s' % type(self))\n    print('   obj type is %s' % type(obj))\nprint(\"\\nmethod 1 \\n\")\nc = C((1,2,3))\nprint(\"\\nmethod 2 \\n\")\nnp.arange(10).view(C)\nprint(\"\\nmethod 3 \\n\")\ncc = c[1:]\n\n\n\nmethod 1 \n\nIn __new__ with class &lt;class '__main__.C'&gt;\nIn array_finalize:\n   self type is &lt;class '__main__.C'&gt;\n   obj type is &lt;class 'NoneType'&gt;\nIn __init__ with class &lt;class '__main__.C'&gt;\n\nmethod 2 \n\nIn array_finalize:\n   self type is &lt;class '__main__.C'&gt;\n   obj type is &lt;class 'numpy.ndarray'&gt;\n\nmethod 3 \n\nIn array_finalize:\n   self type is &lt;class '__main__.C'&gt;\n   obj type is &lt;class '__main__.C'&gt;\n\n\n上述的例子中,可以看出array_finalize方法是可以在不同的构造方式中被调用的,在不同的构造方法中,他所接收的参数也是不同的:\n\n显式构造的时候obj是None\nview cast时,obj是ndarray的任意子类类型\nfrom template时,obj是当前子类的一个对象,我们可以用这个对象来更新self这个对象.\n\n所以在array_finalize中对self设置一系列属性是比较合适的."
  },
  {
    "objectID": "posts/numpy-subcalss-ndarray.html#例子1-向ndarray添加额外属性",
    "href": "posts/numpy-subcalss-ndarray.html#例子1-向ndarray添加额外属性",
    "title": "numpy中继承ndarray",
    "section": "例子1 向ndarray添加额外属性",
    "text": "例子1 向ndarray添加额外属性\n\n\nCode\nclass PEArray(np.ndarray):\n  def __new__(subtype, height, width, spad_size, max_height=12, max_width=14, dtype=float, buffer=None, offset=0,\n              strides=None, order=None):\n    obj = super().__new__(subtype, (height, width, spad_size), dtype=dtype, buffer=buffer,\n                          offset=offset, strides=strides, order=order)\n    obj.h = height\n    obj.w = width\n    obj.spad = spad_size\n    obj.mh = max_height\n    obj.mw = max_width\n    return obj\n\n  def __array_finalize__(self, obj):\n    # 1. 显示构造函数 obj=none\n    if obj is None:\n      return\n    # 2. view cast, type(obj) == np.ndarray\n    if type(obj) == np.ndarray:\n      self.h = self.shape[0]\n      self.w = self.shape[1]\n      self.spad = self.shape[2]\n      self.mh = getattr(obj, 'mh', 0)\n      self.mw = getattr(obj, 'mw', 0)\n    # 3. from template, type(obj) == PEArray\n    if type(obj) == PEArray:\n      self.h = self.shape[0]\n      self.w = self.shape[1]\n      self.spad = self.shape[2]\n      self.mh = getattr(obj, 'mh')\n      self.mw = getattr(obj, 'mw')\n\n\nprint('\\nmethod 1:\\n')\npearr = PEArray(2, 3, 8)\nprint(type(pearr))\nprint(pearr.h, pearr.w, pearr.spad, pearr.mh, pearr.mw)\n\nprint('\\nmethod 2:\\n')\nr = np.random.rand(3, 4, 6)\nrr = r.view(PEArray)\nprint(type(rr))\nprint(rr.h, rr.w, rr.spad, rr.mh, rr.mw)\n\nprint('\\nmethod 3:\\n')\npearr_sub = pearr[2:]\nprint(type(pearr_sub))\nprint(pearr_sub.h, pearr_sub.w, pearr_sub.spad, pearr_sub.mh, pearr_sub.mw)\n\n\n\nmethod 1:\n\n&lt;class '__main__.PEArray'&gt;\n2 3 8 12 14\n\nmethod 2:\n\n&lt;class '__main__.PEArray'&gt;\n3 4 6 0 0\n\nmethod 3:\n\n&lt;class '__main__.PEArray'&gt;\n0 3 8 12 14\n\n\n其实对于view cast,我们可以不做支持.然后对于from template,其中self就是已经被切分的数组部分,但是他的一些属性还是在obj中,所以需要取出. 实际我感觉对于带大量额外参数的子类,是需要禁止view cast构造的,但是不知道会不会造成一些问题."
  },
  {
    "objectID": "posts/Axe-0.html",
    "href": "posts/Axe-0.html",
    "title": "Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers",
    "section": "",
    "text": "这篇论文是陈天奇团队的成果，提出一个统一的硬件感知抽象（Axe Layout），将逻辑张量坐标映射到多维物理空间，并设计基于此的多粒度、分布式感知的编译器DSL。今天就来解析一下Axe Layout的设计思路和实现细节。"
  },
  {
    "objectID": "posts/Axe-0.html#核心公式",
    "href": "posts/Axe-0.html#核心公式",
    "title": "Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers",
    "section": "核心公式",
    "text": "核心公式\nAxe Layout将逻辑张量索引映射到物理坐标集合：\n\\[L(x) = \\{ D(x) + r + O \\mid r \\in R \\}\\]\n其中：\n\nD (Shard) - 分片映射\n\n是一个有序的iter列表\n每个iter = (extent, stride, @axis)\nextent: 硬件维度的逻辑大小\nstride: 相邻逻辑元素在硬件维度上的距离\n@axis: 物理轴的标签（device, warp, lane等）\n作用：将逻辑索引转换为物理坐标\n\n\n\nR (Replica) - 复制维度\n\n是一个集合（无序），独立于逻辑索引\n格式：{axis_name: replica_count, …}\n作用：为并行执行添加额外维度\n例如：4个线程独立执行相同的计算\n\n\n\nO (Offset) - 偏移\n\n固定的基地址或资源保留\n格式：{axis_name: offset_value, …}\n例如：数据在内存中的起始位置、执行器件偏移\n\n\n\nCode\nimport numpy as np\nimport itertools\nimport pycute as cute\nfrom typing import List, Tuple, Dict\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Iter:\n  extent: int\n  stride: int\n  axis: str\n\n  def __repr__(self):\n    return f\"({self.extent}, {self.stride}@{self.axis})\"\n\n\n@dataclass\nclass AxeLayout:\n  D: List[Iter]\n  R: List[Iter]\n  O: Dict[str, int]\n\n  def __repr__(self):\n    d_str = \" × \".join(map(str, self.D))\n    r_str = \" × \".join(map(str, self.R)) if self.R else \"∅\"\n    o_str = \", \".join([f\"{v}@{k}\" for k, v in self.O.items()]) if self.O else \"∅\"\n    return f\"D: {d_str} | R: {r_str} | O: {o_str}\"\n\n  def print(self, name=\"\"):\n    if name:\n      print(f\"\\n{name}:\")\n\n    if not self.D:\n      d_extent_line = \"( )\"\n      d_stride_line = \"( )\"\n    else:\n      col_widths = []\n      for iter_obj in self.D:\n        extent_str = str(iter_obj.extent)\n        stride_str = f\"{iter_obj.stride}@{iter_obj.axis}\"\n        col_width = max(len(extent_str), len(stride_str))\n        col_widths.append(col_width)\n\n      extent_parts = []\n      for i, iter_obj in enumerate(self.D):\n        extent_str = str(iter_obj.extent)\n        padded = extent_str.center(col_widths[i])\n        extent_parts.append(padded)\n      d_extent_line = \"( \" + \"  \".join(extent_parts) + \" )\"\n\n      stride_parts = []\n      for i, iter_obj in enumerate(self.D):\n        stride_str = f\"{iter_obj.stride}@{iter_obj.axis}\"\n        padded = stride_str.rjust(col_widths[i])\n        stride_parts.append(padded)\n      d_stride_line = \"( \" + \", \".join(stride_parts) + \" )\"\n\n    if self.R:\n      r_col_widths = []\n      for iter_obj in self.R:\n        extent_str = str(iter_obj.extent)\n        stride_str = f\"{iter_obj.stride}@{iter_obj.axis}\"\n        col_width = max(len(extent_str), len(stride_str))\n        r_col_widths.append(col_width)\n\n      r_extent_parts = []\n      for i, iter_obj in enumerate(self.R):\n        extent_str = str(iter_obj.extent)\n        padded = extent_str.center(r_col_widths[i])\n        r_extent_parts.append(padded)\n      r_extent_line = \"( \" + \"  \".join(r_extent_parts) + \" )\"\n\n      r_stride_parts = []\n      for i, iter_obj in enumerate(self.R):\n        stride_str = f\"{iter_obj.stride}@{iter_obj.axis}\"\n        padded = stride_str.rjust(r_col_widths[i])\n        r_stride_parts.append(padded)\n      r_stride_line = \"( \" + \", \".join(r_stride_parts) + \" )\"\n\n      print(d_extent_line + \"   \" + r_extent_line)\n      print(d_stride_line + \" + \" + r_stride_line, end=\"\")\n    else:\n      print(d_extent_line)\n      print(d_stride_line, end=\"\")\n\n    if self.O:\n      o_items = [f\"{offset}@{axis}\" for axis, offset in self.O.items()]\n      o_str = \" + \".join(o_items)\n      print(\" + \" + o_str, end=\"\")\n\n    print()"
  },
  {
    "objectID": "posts/Axe-0.html#examples",
    "href": "posts/Axe-0.html#examples",
    "title": "Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers",
    "section": "Examples",
    "text": "Examples\n\nNVIDIA Tensor Core tile\n有了基础的定义后， 我们可以尝试实现论文中的第一个例子， 映射逻辑 \\(8×16\\) tile到GPU的2个warp（各32 lane）+ 2个寄存器：\n\\[\\begin{pmatrix}8 & 2 & 4 & 2 \\\\ 4@\\texttt{lane} & 1@\\texttt{warp} & 1@\\texttt{lane} & 1@\\texttt{reg}\\end{pmatrix} + \\begin{bmatrix}2 \\\\ 4@\\texttt{warp}\\end{bmatrix} + 5@\\texttt{warp}\\]\n\n\n\nCode\nlayout_a = AxeLayout(D=[\n    Iter(8, 4, \"lane\"),\n    Iter(2, 1, \"warp\"),\n    Iter(4, 1, \"lane\"),\n    Iter(2, 1, \"reg\"),\n],\n    R=[Iter(2, 4, \"warp\")],\n    O={\"warp\": 5})\n\nlayout_a.print(\"layout_a\")\n\n\n\nlayout_a:\n(   8       2       4       2   )   (   2    )\n( 4@lane, 1@warp, 1@lane, 1@reg ) + ( 4@warp ) + 5@warp\n\n\n\n\nDistributed sharding on a 2×2 GPU mesh\n假设一个\\(64×128\\)张量在4个GPU上的分片+复制混合：\n\n完全切分（行按GPU行，列按GPU列分）： \\[\\begin{pmatrix}2 & 32 & 2 & 64 \\\\ 1@\\texttt{gpuid} & 128@\\texttt{m} & 2@\\texttt{gpuid} & 1@\\texttt{m}\\end{pmatrix}\\]\n这里的gpuid表示设备维度，这里的m表示内存维度，如果把内存维度的Iter单独抽取出来，就可以计算在每个设备中Local Tensor的Layout。把gpuid的Iter出来，可以用stride来隐式体现gpu mesh的分布方式。\n行切分+列复制（行切分，每行shard复制到行内两GPU）： \\[\\begin{pmatrix}2 & 32 & 128 \\\\ 1@\\texttt{gpuid} & 128@\\texttt{m} & 1@\\texttt{m}\\end{pmatrix} + \\begin{bmatrix}2 \\\\ 2@\\texttt{gpuid}\\end{bmatrix}\\]\n实际上这两个就是经典的分布式张量切分方式，对应到SBP里面分别为：\n\\[\n\\begin{aligned}\n    (split(0), split(1)) \\\\\n    (split(0), broadcast)\n\\end{aligned}\n\\]\n\n\nCode\nlayout_b = AxeLayout(D=[\n    Iter(2, 1, \"gpuid\"),\n    Iter(32, 128, \"m\"),\n    Iter(2, 2, \"gpuid\"),\n    Iter(64, 1, \"m\")],\n    R=[],\n    O={})\nlayout_b.print(\"layout_b\")\n\n\n\nlayout_b:\n(    2       32      2      64 )\n( 1@gpuid, 128@m, 2@gpuid, 1@m )\n\n\n\n\nNative multidimensional memory in Accelerators\n\n这里是对于物理内存硬件的映射，认为P是memory bank partitions, F为free dimensions。主要体现的是Axe Layout是同时支持并行硬件维度和存储硬件的表达，所以当不考虑并行维度时，可以完成传统layout的功能。\n\n\nCode\nlayout_c = AxeLayout(D=[\n    Iter(2, 512, \"F\"),\n    Iter(128, 1, \"P\"),\n    Iter(512, 1, \"F\")],\n    R=[],\n    O={})\n\nlayout_d = AxeLayout(D=[\n    Iter(2, 112, \"Col\"),\n    Iter(128, 1, \"Lane\"),\n    Iter(112, 1, \"Col\")],\n    R=[],\n    O={})\n\nlayout_c.print(\"layout_c\")\nlayout_d.print(\"layout_d\")\n\n\n\nlayout_c:\n(   2    128  512 )\n( 512@F, 1@P, 1@F )\n\nlayout_d:\n(    2      128     112  )\n( 112@Col, 1@Lane, 1@Col )"
  },
  {
    "objectID": "posts/编译静态库.html",
    "href": "posts/编译静态库.html",
    "title": "CMake编译静态库",
    "section": "",
    "text": "安装cmake  我的系统是ubuntu16，这一步就不赘述了，apt或者源代码安装都没问题。\n\n\n\n源代码  我是想在系统中学习好linux应用层编程，所以我买了Linux/Uinx系统编程手册。 在学习过程中发现他的代码都依赖于作者所写的几个头文件，所以我产生了将其将其编译成静态库的想法，虽然文件不多，但是姑且也算是学习到了一些东西。需要的可以自行百度搜索下载。  源代码分布如下： ```sh zqh@linux:~/system_program$ tree . . ├── build ├── CMakeLists.txt ├── lib │ ├── alt_functions.c │ ├── alt_functions.h │ ├── CMakeLists.txt │ ├── ename.c.inc │ ├── error_functions.c │ ├── error_functions.h │ ├── get_num.c │ ├── get_num.h │ └── tlpi_hdr.h └── 编译静态库.md\n2 directories, 11 files ```\n编写CMakeLists\n\nlib目录下的CMakeLists\n\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)  \n\nSET(LIB_SRC alt_functions.c error_functions.c get_num.c)  #添加源文件\n\n#添加静态库  \nADD_LIBRARY(tpli_static STATIC ${LIB_SRC})  \n\n#将静态库重新命名为Libtpli\nSET_TARGET_PROPERTIES(tpli_static PROPERTIES OUTPUT_NAME \"tpli\")\n\n当前目录下的CMakeLists\n\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)  \nPROJECT(TestLIB)  #工程名\nADD_SUBDIRECTORY(lib) #添加子目录\n编译 sh     cd build/     cmake ..     make 查看结果 sh     zqh@linux:~/system_program/build$ ls     CMakeCache.txt  CMakeFiles  cmake_install.cmake  lib  Makefile     zqh@linux:~/system_program/build$ cd lib/     zqh@linux:~/system_program/build/lib$ ls     CMakeFiles  cmake_install.cmake  libtpli.a  Makefile 现在生成了静态库libtpli.a。\n测试\n\n准备 新建了一个目录test，将头文件加入其中，内容如下 ```sh zqh@linux:~/system_program/test$ tree . ├── build ├── CMakeLists.txt ├── inc │ ├── alt_functions.h │ ├── ename.c.inc │ ├── error_functions.h │ ├── get_num.h │ └── tlpi_hdr.h └── test.c\n2 directories, 7 files ``` test.c（复制文件内容到另一个文件）:\n    /*************************************************************************\\\n*                  Copyright (C) Michael Kerrisk, 2017.                   *\n*                                                                         *\n* This program is free software. You may use, modify, and redistribute it *\n* under the terms of the GNU General Public License as published by the   *\n* Free Software Foundation, either version 3 or (at your option) any      *\n* later version. This program is distributed without any warranty.  See   *\n* the file COPYING.gpl-v3 for details.                                    *\n\\*************************************************************************/\n\n/* Listing 4-1 */\n\n/* copy.c\n\nCopy the file named argv[1] to a new file named in argv[2].\n*/\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\n#include \"tlpi_hdr.h\"\n\n#ifndef BUF_SIZE        /* Allow \"cc -D\" to override definition */\n#define BUF_SIZE 1024\n#endif\n\nint\nmain(int argc, char *argv[])\n{\n    int inputFd, outputFd, openFlags;\n    mode_t filePerms;\n    ssize_t numRead;\n    char buf[BUF_SIZE];\n\n    if (argc != 3 || strcmp(argv[1], \"--help\") == 0)\n        usageErr(\"%s old-file new-file\\n\", argv[0]);\n\n    /* Open input and output files */\n\n    inputFd = open(argv[1], O_RDONLY);\n    if (inputFd == -1)\n        errExit(\"opening file %s\", argv[1]);\n\n    openFlags = O_CREAT | O_WRONLY | O_TRUNC;\n    filePerms = S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP |\n                S_IROTH | S_IWOTH;      /* rw-rw-rw- */\n    outputFd = open(argv[2], openFlags, filePerms);\n    if (outputFd == -1)\n        errExit(\"opening file %s\", argv[2]);\n\n    /* Transfer data until we encounter end of input or an error */\n\n    while ((numRead = read(inputFd, buf, BUF_SIZE)) &gt; 0)\n        if (write(outputFd, buf, numRead) != numRead)\n            fatal(\"couldn't write whole buffer\");\n    if (numRead == -1)\n        errExit(\"read\");\n\n    if (close(inputFd) == -1)\n        errExit(\"close input\");\n    if (close(outputFd) == -1)\n        errExit(\"close output\");\n\n    exit(EXIT_SUCCESS);\n}\nCMakeLists.txt:\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)  \nproject (Tutorial)  #工程名\n\n# 添加头文件目录\ninclude_directories(${PROJECT_SOURCE_DIR}/inc )\n\n\nlink_libraries(/home/zqh/system_program/build/lib/libtpli.a)#添加静态库\n\nadd_executable (Tutorial test.c) #创建可执行文件\n\ntarget_link_libraries(Tutorial /home/zqh/system_program/build/lib/libtpli.a)# 连接静态库库\n编译 sh       cd build/       cmake ..       make\n运行 sh       ./Tutorial       Usage: ./Tutorial old-file new-file       cat 1.txt       编译静态库成功       ./Tutorial 1.txt 2.txt       cat 2.txt       编译静态库成功"
  },
  {
    "objectID": "posts/编译静态库.html#准备工作",
    "href": "posts/编译静态库.html#准备工作",
    "title": "CMake编译静态库",
    "section": "",
    "text": "安装cmake  我的系统是ubuntu16，这一步就不赘述了，apt或者源代码安装都没问题。\n\n\n\n源代码  我是想在系统中学习好linux应用层编程，所以我买了Linux/Uinx系统编程手册。 在学习过程中发现他的代码都依赖于作者所写的几个头文件，所以我产生了将其将其编译成静态库的想法，虽然文件不多，但是姑且也算是学习到了一些东西。需要的可以自行百度搜索下载。  源代码分布如下： ```sh zqh@linux:~/system_program$ tree . . ├── build ├── CMakeLists.txt ├── lib │ ├── alt_functions.c │ ├── alt_functions.h │ ├── CMakeLists.txt │ ├── ename.c.inc │ ├── error_functions.c │ ├── error_functions.h │ ├── get_num.c │ ├── get_num.h │ └── tlpi_hdr.h └── 编译静态库.md\n2 directories, 11 files ```\n编写CMakeLists\n\nlib目录下的CMakeLists\n\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)  \n\nSET(LIB_SRC alt_functions.c error_functions.c get_num.c)  #添加源文件\n\n#添加静态库  \nADD_LIBRARY(tpli_static STATIC ${LIB_SRC})  \n\n#将静态库重新命名为Libtpli\nSET_TARGET_PROPERTIES(tpli_static PROPERTIES OUTPUT_NAME \"tpli\")\n\n当前目录下的CMakeLists\n\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)  \nPROJECT(TestLIB)  #工程名\nADD_SUBDIRECTORY(lib) #添加子目录\n编译 sh     cd build/     cmake ..     make 查看结果 sh     zqh@linux:~/system_program/build$ ls     CMakeCache.txt  CMakeFiles  cmake_install.cmake  lib  Makefile     zqh@linux:~/system_program/build$ cd lib/     zqh@linux:~/system_program/build/lib$ ls     CMakeFiles  cmake_install.cmake  libtpli.a  Makefile 现在生成了静态库libtpli.a。\n测试\n\n准备 新建了一个目录test，将头文件加入其中，内容如下 ```sh zqh@linux:~/system_program/test$ tree . ├── build ├── CMakeLists.txt ├── inc │ ├── alt_functions.h │ ├── ename.c.inc │ ├── error_functions.h │ ├── get_num.h │ └── tlpi_hdr.h └── test.c\n2 directories, 7 files ``` test.c（复制文件内容到另一个文件）:\n    /*************************************************************************\\\n*                  Copyright (C) Michael Kerrisk, 2017.                   *\n*                                                                         *\n* This program is free software. You may use, modify, and redistribute it *\n* under the terms of the GNU General Public License as published by the   *\n* Free Software Foundation, either version 3 or (at your option) any      *\n* later version. This program is distributed without any warranty.  See   *\n* the file COPYING.gpl-v3 for details.                                    *\n\\*************************************************************************/\n\n/* Listing 4-1 */\n\n/* copy.c\n\nCopy the file named argv[1] to a new file named in argv[2].\n*/\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\n#include \"tlpi_hdr.h\"\n\n#ifndef BUF_SIZE        /* Allow \"cc -D\" to override definition */\n#define BUF_SIZE 1024\n#endif\n\nint\nmain(int argc, char *argv[])\n{\n    int inputFd, outputFd, openFlags;\n    mode_t filePerms;\n    ssize_t numRead;\n    char buf[BUF_SIZE];\n\n    if (argc != 3 || strcmp(argv[1], \"--help\") == 0)\n        usageErr(\"%s old-file new-file\\n\", argv[0]);\n\n    /* Open input and output files */\n\n    inputFd = open(argv[1], O_RDONLY);\n    if (inputFd == -1)\n        errExit(\"opening file %s\", argv[1]);\n\n    openFlags = O_CREAT | O_WRONLY | O_TRUNC;\n    filePerms = S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP |\n                S_IROTH | S_IWOTH;      /* rw-rw-rw- */\n    outputFd = open(argv[2], openFlags, filePerms);\n    if (outputFd == -1)\n        errExit(\"opening file %s\", argv[2]);\n\n    /* Transfer data until we encounter end of input or an error */\n\n    while ((numRead = read(inputFd, buf, BUF_SIZE)) &gt; 0)\n        if (write(outputFd, buf, numRead) != numRead)\n            fatal(\"couldn't write whole buffer\");\n    if (numRead == -1)\n        errExit(\"read\");\n\n    if (close(inputFd) == -1)\n        errExit(\"close input\");\n    if (close(outputFd) == -1)\n        errExit(\"close output\");\n\n    exit(EXIT_SUCCESS);\n}\nCMakeLists.txt:\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)  \nproject (Tutorial)  #工程名\n\n# 添加头文件目录\ninclude_directories(${PROJECT_SOURCE_DIR}/inc )\n\n\nlink_libraries(/home/zqh/system_program/build/lib/libtpli.a)#添加静态库\n\nadd_executable (Tutorial test.c) #创建可执行文件\n\ntarget_link_libraries(Tutorial /home/zqh/system_program/build/lib/libtpli.a)# 连接静态库库\n编译 sh       cd build/       cmake ..       make\n运行 sh       ./Tutorial       Usage: ./Tutorial old-file new-file       cat 1.txt       编译静态库成功       ./Tutorial 1.txt 2.txt       cat 2.txt       编译静态库成功"
  },
  {
    "objectID": "posts/使用git下载项目数据.html",
    "href": "posts/使用git下载项目数据.html",
    "title": "使用git下载项目数据",
    "section": "",
    "text": "为了避免大家需要测试数据，但是需要一个人不停的分发的烦恼。所以我决定使用git的方式让大家获取测试数据。\n\n\n注册git\n你所需要做的第一件事是创建一个免费账户。 直接访问 git官网，选择一个未被占用的用户名，提供一个电子邮件地址和密码，点击写着Sign up for GitHub的绿色大按钮即可。 \n\n\n安装git\n\n首先从github官网下载Windows版的git。然后按默认选项安装即可。\n安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，蹦出一个类似命令行窗口的东西，就说明Git安装成功！ \n安装完成后，还需要最后一步设置，在命令行输入： sh     git config --global user.name \"Your Name\"     git config --global user.email \"email@example.com\" 因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。你也许会担心，如果有人故意冒充别人怎么办？这个不必担心，首先我们相信大家都是善良无知的群众，其次，真的有冒充的也是有办法可查的。（这里的名字和邮箱是你的git账户的名字和邮箱）\n\n\n\n创建ssh秘钥\n\n生成秘钥 大多数 Git 服务器都会选择使用 SSH 公钥来进行授权(本服务器也不例外)，系统中的每个用户都必须提供一个公钥用于授权，没有的话就要生成一个。打开git cmd使用以下命令： 输入ssh-keygen命令后连续回车，即可在默认位置生成秘钥。 sh     $ ssh-keygen     Generating public/private rsa key pair.     Enter file in which to save the key (/Users/schacon/.ssh/id_rsa):     Enter passphrase (empty for no passphrase):     Enter same passphrase again:     Your identification has been saved in /Users/schacon/.ssh/id_rsa.     Your public key has been saved in /Users/schacon/.ssh/id_rsa.pub.     The key fingerprint is:     43:c5:5b:5f:b1:f1:50:43:ad:20:a6:92:6a:1f:9a:3a schacon@agadorlaptop.local\n分发公钥 生成公钥的过程在所有操作系统上都差不多。 现在去确认一下是否已经有一个公钥了。SSH 公钥默认储存在账户的主目录下的 ~/.ssh 目录。进去看看： 输入命令： sh     cd ~/.ssh     ls  id_rsa.pub就是公钥，将其中的内容复制发送给我即可。命令如下： cat ~/.ssh/id_rsa.pub之后将下面的内容复制发送给我即可。 \n\n\n\n下载数据\n到这一步就可以开始数据的下载了。在想要的位置右键打开git cmd：  输入git clone git@202.182.101.4:AllData就可以把云端文件下载到本地了。  接下来就可以查看文件了，当然现在还是空文件夹。"
  },
  {
    "objectID": "posts/yolo-loss.html",
    "href": "posts/yolo-loss.html",
    "title": "Yolo中loss函数分析",
    "section": "",
    "text": "今天又回顾了一下yolo中的loss函数,我对比了keras_yolo3,keras-yolo2,以及yolo作者的实现.又有了一些新发现.\n\n\n总结\n这里我首先总结一下,在做yolo的box回归的时候,都是要考虑到预测出box与真实box的iou,计算出iou score,并通过阈值来判断出ignore mask去除一些不需要的noobj loss.并且做loss的时候都是使用以cell大小为尺度的数值进行计算.但是对于哪一个效果好，我是无法评价的.\n\n\nkeras_yolo3中的实现\n在这个项目中,作者使用while来计算pred box与true box的iou score.他会把其他的所有pred box与true box计算iou.\ndef loop_body(b, ignore_mask):\n            true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0])\n            iou = box_iou(pred_box[b], true_box)\n            best_iou = K.max(iou, axis=-1)\n            ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box)))\n            return b+1, ignore_mask\n我的实现类似这种方式,但是我使用的是矩阵的形式去计算的.\n\n\nkeras_yolo2中的实现\n他是首先把true box和对应位置的pred box计算出iou score,这个时候把true box的置信度乘上iou score,因为iou score是在[0-1]之间的,所以在计算置信度误差的时候计算与pred_box_conf的平方差就会更大,相当于加大惩罚力度:\ntrue_box_conf = iou_scores * y_true[..., 4]\n.\n.\n.\nloss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n接下来分析conf_mask,他是把pred box和所有的true box计算iou score,然后获得对应的pred box的best iou.然后best iou小于阈值的pred box则是需要进行惩罚.其他的格子不需要进行惩罚.\nconf_mask  = tf.zeros(mask_shape)\n.\n.\n.\n\nconf_mask = conf_mask + tf.to_float(best_ious &lt; 0.6) * (1 - y_true[..., 4]) * self.no_object_scale\n\n# penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\nconf_mask = conf_mask + y_true[..., 4] * self.object_scale\n总体来说这个loss的实现多了一个点,调整了true_box_conf的值,但是我使用的误差计算是sigmoid cross所以无法使用.\n\n\n原版yolo\n原版的loss在box confidence也做了一些调整,作者首先也是计算每个pred box与所有的true box的iou score然后获得best iou,当大于阈值时去除对应的noobj loss.\n接下来就开始有所变化了,作者遍历所有的true box,将每个true box与对应cell的5个pred box计算iou score然后获得best iou以及best_n(bast iou对应的anchor),接下来计算obj loss、coordinate loss、class loss就是单独针对那个anchor与true box进行计算的.\n下面是我写的部分代码注释:\nvoid forward_region_layer(const layer l, network net)\n{\n    int i,j,b,t,n;\n    memcpy(l.output, net.input, l.outputs*l.batch*sizeof(float));\n\n#ifndef GPU\n    for (b = 0; b &lt; l.batch; ++b){\n        for(n = 0; n &lt; l.n; ++n){\n            int index = entry_index(l, b, n*l.w*l.h, 0);\n            activate_array(l.output + index, 2*l.w*l.h, LOGISTIC);\n            index = entry_index(l, b, n*l.w*l.h, l.coords);\n            if(!l.background) activate_array(l.output + index,   l.w*l.h, LOGISTIC);\n            index = entry_index(l, b, n*l.w*l.h, l.coords + 1);\n            if(!l.softmax && !l.softmax_tree) activate_array(l.output + index, l.classes*l.w*l.h, LOGISTIC);\n        }\n    }\n    if (l.softmax_tree){\n        int i;\n        int count = l.coords + 1;\n        for (i = 0; i &lt; l.softmax_tree-&gt;groups; ++i) {\n            int group_size = l.softmax_tree-&gt;group_size[i];\n            softmax_cpu(net.input + count, group_size, l.batch, l.inputs, l.n*l.w*l.h, 1, l.n*l.w*l.h, l.temperature, l.output + count);\n            count += group_size;\n        }\n    } else if (l.softmax){\n        int index = entry_index(l, 0, 0, l.coords + !l.background);\n        softmax_cpu(net.input + index, l.classes + l.background, l.batch*l.n, l.inputs/l.n, l.w*l.h, 1, l.w*l.h, 1, l.output + index);\n    }\n#endif\n\n    memset(l.delta, 0, l.outputs * l.batch * sizeof(float));\n    if(!net.train) return;\n    float avg_iou = 0;\n    float recall = 0;\n    float avg_cat = 0;\n    float avg_obj = 0;\n    float avg_anyobj = 0;\n    int count = 0;\n    int class_count = 0;\n    *(l.cost) = 0;\n    for (b = 0; b &lt; l.batch; ++b) {\n        if(l.softmax_tree){\n            int onlyclass = 0;\n            for(t = 0; t &lt; 30; ++t){\n                box truth = float_to_box(net.truth + t*(l.coords + 1) + b*l.truths, 1);\n                if(!truth.x) break;\n                int class = net.truth[t*(l.coords + 1) + b*l.truths + l.coords];\n                float maxp = 0;\n                int maxi = 0;\n                if(truth.x &gt; 100000 && truth.y &gt; 100000){\n                    for(n = 0; n &lt; l.n*l.w*l.h; ++n){\n                        int class_index = entry_index(l, b, n, l.coords + 1);\n                        int obj_index = entry_index(l, b, n, l.coords);\n                        float scale =  l.output[obj_index];\n                        l.delta[obj_index] = l.noobject_scale * (0 - l.output[obj_index]);\n                        float p = scale*get_hierarchy_probability(l.output + class_index, l.softmax_tree, class, l.w*l.h);\n                        if(p &gt; maxp){\n                            maxp = p;\n                            maxi = n;\n                        }\n                    }\n                    int class_index = entry_index(l, b, maxi, l.coords + 1);\n                    int obj_index = entry_index(l, b, maxi, l.coords);\n                    delta_region_class(l.output, l.delta, class_index, class, l.classes, l.softmax_tree, l.class_scale, l.w*l.h, &avg_cat, !l.softmax);\n                    if(l.output[obj_index] &lt; .3) l.delta[obj_index] = l.object_scale * (.3 - l.output[obj_index]);\n                    else  l.delta[obj_index] = 0;\n                    l.delta[obj_index] = 0;\n                    ++class_count;\n                    onlyclass = 1;\n                    break;\n                }\n            }\n            if(onlyclass) continue;\n        }\n        //* 遍历所有格子和box,计算每个格子和真实box的iou\n        for (j = 0; j &lt; l.h; ++j) {\n            for (i = 0; i &lt; l.w; ++i) {\n                for (n = 0; n &lt; l.n; ++n) {\n                    int box_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, 0);\n                    box pred = get_region_box(l.output, l.biases, n, box_index, i, j, l.w, l.h, l.w*l.h);\n                    float best_iou = 0;\n                    // * 找到每个ground truth与这个pred box最大的iou\n                    for(t = 0; t &lt; 30; ++t){\n                        box truth = float_to_box(net.truth + t*(l.coords + 1) + b*l.truths, 1);\n                        if(!truth.x) break;\n                        float iou = box_iou(pred, truth);\n                        if (iou &gt; best_iou) {\n                            best_iou = iou;\n                        }\n                    }\n                    //* 首先先把所有的格子当成没有目标来计算损失\n                    int obj_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, l.coords);\n                    avg_anyobj += l.output[obj_index]; //*误差累积 \n                    // * 计算loss\n                    l.delta[obj_index] = l.noobject_scale * (0 - l.output[obj_index]);\n                    if(l.background) l.delta[obj_index] = l.noobject_scale * (1 - l.output[obj_index]);\n                    if (best_iou &gt; l.thresh) { // * 如果最大iou大于阈值,说明预测出有目标\n                        l.delta[obj_index] = 0; //* 删除这个noobj loss\n                    }\n                    // *已经训练12800张图片之后,那么直接把预测值当做真实值\n                    if(*(net.seen) &lt; 12800){\n                        box truth = {0};\n                        truth.x = (i + .5)/l.w;\n                        truth.y = (j + .5)/l.h;\n                        truth.w = l.biases[2*n]/l.w;\n                        truth.h = l.biases[2*n+1]/l.h;\n                        delta_region_box(truth, l.output, l.biases, n, box_index, i, j, l.w, l.h, l.delta, .01, l.w*l.h);\n                    }\n                }\n            }\n        }\n        // * 最多30个真实预测对象\n        for(t = 0; t &lt; 30; ++t){\n            box truth = float_to_box(net.truth + t*(l.coords + 1) + b*l.truths, 1);\n            \n            if(!truth.x) break;//* 没有对象就退出\n            float best_iou = 0;\n            int best_n = 0;\n            // * i,j是真实物体在图片中的位置\n            i = (truth.x * l.w);\n            j = (truth.y * l.h);\n            box truth_shift = truth;\n            truth_shift.x = 0;\n            truth_shift.y = 0;\n            // * l.n是每个格子的anchor box数 找到真实的box对应的5个pred box\n            for(n = 0; n &lt; l.n; ++n){\n                int box_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, 0);\n                box pred = get_region_box(l.output, l.biases, n, box_index, i, j, l.w, l.h, l.w*l.h);\n                // * 预测出来的对象边框= 预测出来的值 * anchor box 大小/ 图像的大小\n                if(l.bias_match){\n                    pred.w = l.biases[2*n]/l.w;\n                    pred.h = l.biases[2*n+1]/l.h;\n                }\n                pred.x = 0;\n                pred.y = 0;\n                // * 计算预测框与真实框的iou NOTE 为了便于计算,\n                // * 他把对应的预测的x,y都设成0,只看w h,因为他们都是同一个\n                float iou = box_iou(pred, truth_shift);\n                if (iou &gt; best_iou){\n                    best_iou = iou; //*找到最大iou\n                    best_n = n;     //*与其所在的\n                }\n            }\n            // *找到best_n对应box\n            int box_index = entry_index(l, b, best_n*l.w*l.h + j*l.w + i, 0);\n            // * 计算best_n 和 truth的iou\n            float iou = delta_region_box(truth, l.output, l.biases, best_n, box_index, i, j, l.w, l.h, l.delta, l.coord_scale *  (2 - truth.w*truth.h), l.w*l.h);\n            // ? 这里\n            if(l.coords &gt; 4){\n                int mask_index = entry_index(l, b, best_n*l.w*l.h + j*l.w + i, 4);\n                delta_region_mask(net.truth + t*(l.coords + 1) + b*l.truths + 5, l.output, l.coords - 4, mask_index, l.delta, l.w*l.h, l.mask_scale);\n            }\n            // *iou 大于 0.5 召回数加1\n            if(iou &gt; .5) recall += 1;\n            avg_iou += iou;\n            // * 获得目标位置\n            int obj_index = entry_index(l, b, best_n*l.w*l.h + j*l.w + i, l.coords);\n            avg_obj += l.output[obj_index];\n            // * 置信度误差= 尺度 * (1 - 输出置信度)\n            l.delta[obj_index] = l.object_scale * (1 - l.output[obj_index]);\n            if (l.rescore) {\n                l.delta[obj_index] = l.object_scale * (iou - l.output[obj_index]);\n            }\n            // * 这里是无目标的 置信度误差= 尺度 * (0 - 输出置信度)\n            if(l.background){\n                l.delta[obj_index] = l.object_scale * (0 - l.output[obj_index]);\n            }\n            // * 获得真实的类对象\n            int class = net.truth[t*(l.coords + 1) + b*l.truths + l.coords];\n            if (l.map) class = l.map[class];\n            // * 获得预测的类对象\n            int class_index = entry_index(l, b, best_n*l.w*l.h + j*l.w + i, l.coords + 1);\n            // * 交叉熵 计算类别误差\n            delta_region_class(l.output, l.delta, class_index, class, l.classes, l.softmax_tree, l.class_scale, l.w*l.h, &avg_cat, !l.softmax);\n            ++count;\n            ++class_count;\n        }\n    }\n    *(l.cost) = pow(mag_array(l.delta, l.outputs * l.batch), 2);\n    printf(\"Region Avg IOU: %f, Class: %f, Obj: %f, No Obj: %f, Avg Recall: %f,  count: %d\\n\", avg_iou/count, avg_cat/class_count, avg_obj/count, avg_anyobj/(l.w*l.h*l.n*l.batch), recall/count, count);\n}\n\n\n思考\n现在我觉得我可以采用keras yolov2的方法，计算出pred box与对应位置的true box的iou score，然后根据iou score可以加大对pred box的惩罚～"
  },
  {
    "objectID": "posts/yolo-anchor.html",
    "href": "posts/yolo-anchor.html",
    "title": "yolo中anchor值的解释",
    "section": "",
    "text": "anchor意味着先验思想，是为了给神经网络拟合box减轻负担。下面就来讲解一下anchor的计算。\n\n\n公式\n首先我们得先理解yolo输出boundary box的计算过程： \\[\n\\begin{aligned}\n    b_x&=\\sigma(t_x)+c_x \\\\\n    b_y&=\\sigma(t_y)+c_y \\\\\n    b_w&=p_w e^{t_w} \\\\\n    b_h&=p_h e^{t_h}\n\\end{aligned}\n\\] 解释： \\(t_x,t_y,t_w,t_h\\)为yolo预测输出结果。\n\\(c_x,c_y\\)为当前cell右上角的坐标。\n\\(p_w,p_h\\)是当前anchor的宽高。\n\\(b_x,b_y,b_w,b_h\\)则是最终yolo预测出的boundary box。\n\n\n\n分析\n通过上图我们知道,实际上anchor存在的意义就在于调节网络输出与真实box的比例关系.\n制作label的过程: \\[ \\begin{aligned}\nLabel_w&= \\frac{True_w}{p_w} \\\\\nLabel_h&= \\frac{True_h}{p_h} \\\\\n\\end{aligned} \\]\n然后我们训练的时候就是拟合\\(e^{t_w}\\)到\\(Label_w\\) \\[\\begin{aligned}\n   \\because b_w&=p_w e^{t_w} \\\\\n            b_h&=p_h e^{t_h}  \\\\\n  \\therefore\n            e^{t_w} &\\rightarrow Label_w \\\\\n            e^{t_h} &\\rightarrow Label_h  \\\\\n\\end{aligned} \\]\n所以通过设置合适的\\(p_w,p_h\\)值,我们可以把\\(Label_w,Label_h\\)控制在\\(1\\)左右.这样使得神经网络只需要在预测\\(w,h\\)时只需要接近\\(1\\)就可以取得较好的效果.\n\n\n制作anchor\n知道了原理,我们就可以来选取合适的anchor,我这里是自己写了个kmeans,然后将其中的距离计算改成了iou的函数:\n\n现在我们加载了自己的anchor list,然后测试一下:\ndef test_label_wh():\n    gl = helper.generator(is_make_lable=True, is_training=False)\n    for i in range(20):\n        imgl, label = next(gl)\n        print(\"w_max: {:.3f} ,w_min: {:.3f} ,h_max: {:.3f} ,h_min: {:.3f}\".format(\n            np.max(label[np.where(label[..., 4] &gt; .7)][:, 2]),\n            np.min(label[np.where(label[..., 4] &gt; .7)][:, 2]),\n            np.max(label[np.where(label[..., 4] &gt; .7)][:, 3]),\n            np.min(label[np.where(label[..., 4] &gt; .7)][:, 3])))\n输出:\nw_max: 1.159 ,w_min: 1.159 ,h_max: 0.906 ,h_min: 0.906\nw_max: 0.944 ,w_min: 0.944 ,h_max: 1.223 ,h_min: 1.223\nw_max: 1.299 ,w_min: 1.055 ,h_max: 0.864 ,h_min: 0.751\nw_max: 0.939 ,w_min: 0.939 ,h_max: 1.294 ,h_min: 1.294\nw_max: 0.992 ,w_min: 0.918 ,h_max: 1.289 ,h_min: 1.257\nw_max: 1.346 ,w_min: 1.346 ,h_max: 0.965 ,h_min: 0.965\nw_max: 1.225 ,w_min: 0.986 ,h_max: 0.905 ,h_min: 0.780\nw_max: 1.139 ,w_min: 0.851 ,h_max: 0.961 ,h_min: 0.939\nw_max: 0.979 ,w_min: 0.979 ,h_max: 0.941 ,h_min: 0.941\nw_max: 1.062 ,w_min: 1.062 ,h_max: 0.957 ,h_min: 0.957\nw_max: 1.399 ,w_min: 1.399 ,h_max: 0.831 ,h_min: 0.831\nw_max: 0.945 ,w_min: 0.729 ,h_max: 1.102 ,h_min: 0.744\nw_max: 0.921 ,w_min: 0.921 ,h_max: 0.867 ,h_min: 0.867\nw_max: 0.854 ,w_min: 0.668 ,h_max: 1.298 ,h_min: 1.039\nw_max: 1.060 ,w_min: 1.060 ,h_max: 0.648 ,h_min: 0.648\nw_max: 0.972 ,w_min: 0.972 ,h_max: 0.925 ,h_min: 0.925\nw_max: 0.805 ,w_min: 0.805 ,h_max: 1.126 ,h_min: 1.126\nw_max: 0.809 ,w_min: 0.809 ,h_max: 0.958 ,h_min: 0.958\nw_max: 1.134 ,w_min: 1.027 ,h_max: 1.502 ,h_min: 1.429\nw_max: 1.629 ,w_min: 0.915 ,h_max: 1.208 ,h_min: 0.690\n我们的label中\\(w,h\\)值都在\\(1\\)左右了,说明这个anchor list是合适的~"
  },
  {
    "objectID": "posts/wordcloud.html",
    "href": "posts/wordcloud.html",
    "title": "WordCloud库使用",
    "section": "",
    "text": "最近在学Python，还是比较有意思的，今天试了下WordCloud库写了个小程序\n\n\n程序\nimport wordcloud as wc\nimport os\n\n\n\"\"\"\n需要解析的字符串如下：\ntags: \n-   Linux\ncategories: \n-   学习 \n\"\"\"\n\n\nif __name__ == \"__main__\":\n    dirstr = '/media/zqh/文档/Blog/gitio/source/_posts/'\n    dirls = [name for name in os.listdir(\n        \"/media/zqh/文档/Blog/gitio/source/_posts\") if '.md' in name]\n    # 获取我的所有tag\n    tags = []\n    for filename in dirls:\n        istag = False\n        f = open(dirstr+filename, 'r', encoding='utf-8')\n        for lines in f:\n            if istag == False:\n                if 'tags' in lines:\n                    istag = True\n                else:\n                    continue\n            elif istag == True:\n                if '---' in lines:\n                    istag = False\n                elif '-' in lines:\n                    tags.append(list(lines.strip().split())[1])\n                else:\n                    continue\n\n    c = wc.WordCloud(width=600, height=600,\n                     font_path=\"/usr/share/fonts/deepin-font-install/YaHei Consolas Hybrid/YaHei.Consolas.1.11b.ttf\")\n    c.generate(' '.join(tags))\n    c.to_file('/media/zqh/文档/Blog/gitio/source/_posts/wordcloud/1.png')\n\n\n效果\n\n\n\n效果图\n\n\n\n\n实例2\nimport wordcloud\nimport jieba\n\nif __name__ == \"__main__\":\n    c = wordcloud.WordCloud(width=1000, height=700,\n                            font_path=\"/usr/share/fonts/deepin-font-install/YaHei Consolas Hybrid/YaHei.Consolas.1.11b.ttf\", background_color='white')\n    f = open('关于实施乡村振兴战略的意见.txt')\n    s = f.read()\n    f.close()\n    words = jieba.lcut(s)\n    c.generate(' '.join(words))\n    c.to_file('./2.png')\n\n\n效果2\n\n\n\n效果2"
  },
  {
    "objectID": "posts/wenda-week8.html",
    "href": "posts/wenda-week8.html",
    "title": "机器学习作业第八周",
    "section": "",
    "text": "终于到了最后一周接下来就是学习深度学习相关内容,一个是吴恩达老师的深度学习课程,他的深度学习前面一些内容学过机器学习就不需要看了.然后还可以看斯坦福大学的UFLDL教程,也有翻译中文版."
  },
  {
    "objectID": "posts/wenda-week8.html#效果",
    "href": "posts/wenda-week8.html#效果",
    "title": "机器学习作业第八周",
    "section": "效果",
    "text": "效果\nVisualizing example dataset for outlier detection.\n\nProgram paused. Press enter to continue.\nVisualizing Gaussian fit.\n\nProgram paused. Press enter to continue.\n/media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week8/fucs8.py:113: RuntimeWarning: invalid value encountered in long_scalars\n  prec = tp / (tp + fp)\nBest epsilon found using cross-validation: 8.990852779269495e-05\nBest F1 on Cross Validation Set:  0.8750000000000001\n   (you should see a value epsilon of about 8.99e-05)\n   (you should see a Best F1 value of  0.875000)\n\nProgram paused. Press enter to continue.\nBest epsilon found using cross-validation: 1.377228890761358e-18\nBest F1 on Cross Validation Set:  0.6153846153846154\n   (you should see a value epsilon of about 1.38e-18)\n   (you should see a Best F1 value of 0.615385)\n# Outliers found: 117"
  },
  {
    "objectID": "posts/wenda-week8.html#效果-1",
    "href": "posts/wenda-week8.html#效果-1",
    "title": "机器学习作业第八周",
    "section": "效果",
    "text": "效果\nLoading movie ratings dataset.\n\nAverage rating for movie 1 (Toy Story): 3.8783185840707963 / 5\n\nProgram paused. Press enter to continue.\nCost at loaded parameters: 22.22460372568567\n(this value should be about 22.22)\nProgram paused. Press enter to continue.\n\nChecking Gradients (without regularization) ...\n[  1.43203539  -0.63274492  -1.00531856   0.89769276  -0.28926118\n  -0.65993552  10.17121834  10.43486571 -12.40615625   0.35938134\n  -0.02758239  -0.38856257  -5.66399174 -10.80568736  18.42971531\n  -3.17534552   1.28489352  10.89656357  -0.23222024  -0.66648527\n   0.9995297   -0.0185099    0.65721699   0.33866012   0.\n   0.           0.        ] [  1.43203539  -0.63274492  -1.00531856   0.89769276  -0.28926118\n  -0.65993552  10.17121834  10.43486571 -12.40615625   0.35938134\n  -0.02758239  -0.38856257  -5.66399174 -10.80568736  18.42971531\n  -3.17534552   1.28489352  10.89656357  -0.23222024  -0.66648527\n   0.9995297   -0.0185099    0.65721699   0.33866012   0.\n   0.           0.        ]\nThe above two columns you get should be very similar.\n(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\nIf your cost function implementation is correct, then\nthe relative difference will be small (less than 1e-9).\n\nRelative Difference: 1.3943940851783395e-12\n\n\nProgram paused. Press enter to continue.\nCost at loaded parameters (lambda = 1.5): 31.344056244274213\n(this value should be about 31.34)\n\nProgram paused. Press enter to continue.\n\nChecking Gradients (with regularization) ...\n[-0.3960421  -3.42481695 -0.52898699  0.05550482  1.45849918 -1.46259522\n  2.19529571  2.05102836  0.22049044  0.37247193  1.01557475 -5.3171257\n -2.28357796 -1.12923924  1.67314559 -1.21504222 -0.7500923  -2.03542911\n -0.24564074  1.62362884 -4.8857543   1.13231058 -0.71201535 -1.5937657\n  2.36345388  3.61334864  1.43292096] [-0.3960421  -3.42481695 -0.52898699  0.05550482  1.45849918 -1.46259522\n  2.19529571  2.05102836  0.22049044  0.37247193  1.01557475 -5.3171257\n -2.28357796 -1.12923924  1.67314559 -1.21504222 -0.7500923  -2.03542911\n -0.24564074  1.62362884 -4.8857543   1.13231058 -0.71201535 -1.5937657\n  2.36345388  3.61334864  1.43292096]\nThe above two columns you get should be very similar.\n(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\nIf your cost function implementation is correct, then\nthe relative difference will be small (less than 1e-9).\n\nRelative Difference: 2.3144279833807326e-12\n\n\nProgram paused. Press enter to continue.\n\nNew user ratings:\nRated 4.0 for Toy Story (1995)\nRated 3.0 for Twelve Monkeys (1995)\nRated 5.0 for Usual Suspects, The (1995)\nRated 4.0 for Outbreak (1995)\nRated 5.0 for Shawshank Redemption, The (1994)\nRated 3.0 for While You Were Sleeping (1995)\nRated 5.0 for Forrest Gump (1994)\nRated 2.0 for Silence of the Lambs, The (1991)\nRated 4.0 for Alien (1979)\nRated 5.0 for Die Hard 2 (1990)\nRated 5.0 for Sphere (1998)\n\nProgram paused. Press enter to continue.\n\nTraining collaborative filtering...\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 38967.455063\n         Iterations: 50\n         Function evaluations: 85\n         Gradient evaluations: 85\nRecommender system learning completed.\n\nProgram paused. Press enter to continue.\n\nTop recommendations for you:\nPredicting rating 5.0 for movie Entertaining Angels: The Dorothy Day Story (1996)\nPredicting rating 5.0 for movie Santa with Muscles (1996)\nPredicting rating 5.0 for movie Someone Else's America (1995)\nPredicting rating 5.0 for movie Prefontaine (1997)\nPredicting rating 5.0 for movie Marlene Dietrich: Shadow and Light (1996)\nPredicting rating 5.0 for movie They Made Me a Criminal (1939)\nPredicting rating 5.0 for movie Great Day in Harlem, A (1994)\nPredicting rating 5.0 for movie Saint of Fort Washington, The (1993)\nPredicting rating 5.0 for movie Star Kid (1997)\nPredicting rating 5.0 for movie Aiqing wansui (1994)\n\nOriginal ratings provided:\nRated 4.0 for Toy Story (1995)\nRated 3.0 for Twelve Monkeys (1995)\nRated 5.0 for Usual Suspects, The (1995)\nRated 4.0 for Outbreak (1995)\nRated 5.0 for Shawshank Redemption, The (1994)\nRated 3.0 for While You Were Sleeping (1995)\nRated 5.0 for Forrest Gump (1994)\nRated 2.0 for Silence of the Lambs, The (1991)\nRated 4.0 for Alien (1979)\nRated 5.0 for Die Hard 2 (1990)\nRated 5.0 for Sphere (1998)"
  },
  {
    "objectID": "posts/wenda-week6.html",
    "href": "posts/wenda-week6.html",
    "title": "机器学习作业第六周",
    "section": "",
    "text": "这周是SVM,其实SVM部分是没有什么难点,主要问题是在于自定义核函数在sklean中比较蛋疼."
  },
  {
    "objectID": "posts/wenda-week6.html#效果",
    "href": "posts/wenda-week6.html#效果",
    "title": "机器学习作业第六周",
    "section": "效果",
    "text": "效果\n     \n➜  week6 python3 ex6.py\nLoading and Visualizing Data ...\nProgram paused. Press enter to continue.\nTraining Linear SVM ...\n/home/zqh/.local/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n  % self.max_iter, ConvergenceWarning)\nProgram paused. Press enter to continue.\nEvaluating the Gaussian Kernel ...\nGaussian Kernel between x1 = [1; 2; 1], x2 = [0; 4; -1], sigma = 2 :\n        [[0.32465247]]\n(for sigma = 2, this value should be about 0.324652)\nProgram paused. Press enter to continue.\nLoading and Visualizing Data ...\nProgram paused. Press enter to continue.\nTraining SVM with RBF Kernel (this may take 1 to 2 minutes) ...\nProgram paused. Press enter to continue.\nLoading and Visualizing Data ...\nProgram paused. Press enter to continue.\nProgram paused. Press enter to continue."
  },
  {
    "objectID": "posts/wenda-week6.html#效果-1",
    "href": "posts/wenda-week6.html#效果-1",
    "title": "机器学习作业第六周",
    "section": "效果",
    "text": "效果\n➜  week6 python3 ex6_spam.py\nPreprocessing sample email (emailSample1.txt)\n\n==== Processed Email ====\nanyon know how much it cost to host a web portal it depend on how mani visitor you re expect can be anywher from less than number buck a month to a coupl of dollarnumb should checkout httpaddr or perhap amazon ecnumb your run someth big unsubscrib yourself from thi mail list send an email to\n\n=======================\nWord Indices:\n[86, 916, 794, 1077, 883, 370, 1699, 790, 1822, 883, 431, 1171, 794, 1002, 1893, 1364, 592, 238, 162, 89, 688, 945, 1663, 1120, 1062, 1699, 375, 1162, 479, 1510, 799, 1182, 1237, 1895, 1440, 1547, 181, 1758, 1896, 688, 1676, 992, 961, 1477, 71, 530, 1699]\n\nProgram paused. Press enter to continue.\nExtracting features from sample email (emailSample1.txt)\n\n==== Processed Email ====\nanyon know how much it cost to host a web portal it depend on how mani visitor you re expect can be anywher from less than number buck a month to a coupl of dollarnumb should checkout httpaddr or perhap amazon ecnumb your run someth big unsubscrib yourself from thi mail list send an email to\n\n=======================\nLength of feature vector: 1899\nNumber of non-zero entries: 42\nProgram paused. Press enter to continue.\nTraining Linear SVM (Spam Classification)\n(this may take 1 to 2 minutes) ...\nTraining Accuracy: 56.56122500000001%\nEvaluating the trained Linear SVM on a test set ...\nTest Accuracy: 57.1808%\nTop predictors of spam:\n       our              1190\n      click             297\n      remov             1397\n    guarante            738\n      visit             1795\n    basenumb            155\n     dollar             476\n      will              1851\n      price             1298\n      pleas             1263\n      most              1066\n      nbsp              1088\n       lo               965\n       ga               698\n      hour              791\n\nProgram paused. Press enter to continue.\n\n==== Processed Email ====\ndo you want to make dollarnumb or more per week you are a motiv and qualifi individu i person demonstr to you a system that will you dollarnumb number per week or more thi is not mlm our number hour pre record number to get the number need peopl who want to make seriou money make call and get the fact number minut in yourself now number forward to your call and i will introduc youpeopl like yourself current make dollarnumb number plu per week number numberleannumberlrmsnumbnumberwxhonumberqiytnumb numberrjuvnumberhqcfnumb\n\n=======================\nProcessed spamSample1.txt\n\nSpam Classification: [1]\n(1 indicates spam, 0 indicates not spam)"
  },
  {
    "objectID": "posts/wenda-week4.html",
    "href": "posts/wenda-week4.html",
    "title": "机器学习作业第四周",
    "section": "",
    "text": "第四周的作业是神经网络的训练和预测.这个和我之前写的神经网络有点不一样,吴恩达老师这里所有的都是加上bias节点以及正则化的.主要注意一下梯度函数.\n以下是正则化的梯度函数(代码矩阵形式),假设一共3层: \\[ \\begin{aligned}\n    temp^{(2)}&=\\Theta^{(2)} \\\\\n    temp^{(1)}&=\\Theta^{(1)} \\\\\n    temp^{(2)}[:,1:]&=0 \\\\\n    temp^{(1)}[:,1:]&=0 \\\\\n    \\Delta^{(2)} &= (a^{(3)}-y)^T*a^{(2)} \\\\\n    \\Delta^{(1)} &= ((a^{(3)}-y)[:,1:]\\cdot g'(z^{(2)}))^T*a^{(1)} \\\\\n    Grad^{(2)} &=\\frac{\\Delta^{(2)}+\\lambda*temp^{(2)} }{m} \\\\\n    Grad^{(1)} &=\\frac{\\Delta^{(1)}+\\lambda*temp^{(1)} }{m}\n\\end{aligned} \\]"
  },
  {
    "objectID": "posts/wenda-week4.html#效果",
    "href": "posts/wenda-week4.html#效果",
    "title": "机器学习作业第四周",
    "section": "效果",
    "text": "效果\n➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week4/ex4.py\nLoading and Visualizing Data ...\nProgram paused. Press enter to continue.\nLoading Saved Neural Network Parameters ...\nFeedforward Using Neural Network ...\nCost at parameters (loaded from ex4weights): 0.2876291651613189\n    (this value should be about 0.287629)\nProgram paused. Press enter to continue.\nChecking Cost Function (w/ Regularization) ...\nCost at parameters (loaded from ex4weights): 0.38376985909092365\n    this value should be about 0.383770)\nProgram paused. Press enter to continue.\nEvaluating sigmoid gradient...\nSigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n[[0.19661193]\n [0.23500371]\n [0.25      ]\n [0.23500371]\n [0.19661193]]\nProgram paused. Press enter to continue.\nInitializing Neural Network Parameters ...\nChecking Backpropagation...\n[[ 1.23162247e-02]\n .\n .\n .\n [ 5.02929547e-02]]\nThe above two columns you get should be very similar.\n(Left-Your Numerical Gradient, Right-Analytical Gradient)\nIf your backpropagation implementation is correct, then\nthe relative difference will be small (less than 1e-9).\n\nRelative Difference: 1.848611973407009e-11\nProgram paused. Press enter to continue.\nChecking Backpropagation (w/ Regularization) ...\n[[ 0.01231622]\n .\n .\n .\n [ 0.00523372]]\nThe above two columns you get should be very similar.\n(Left-Your Numerical Gradient, Right-Analytical Gradient)\nIf your backpropagation implementation is correct, then\nthe relative difference will be small (less than 1e-9).\n\nRelative Difference: 1.8083382559674107e-11\nCost at (fixed) debugging parameters (w/ lamda = 3): 0.5760512469501331\n (for lamda = 3, this value should be about 0.576051)\nProgram paused. Press enter to continue.\nTraining Neural Network...\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.483014\n         Iterations: 50\n         Function evaluations: 116\n         Gradient evaluations: 116\nProgram paused. Press enter to continue.\nVisualizing Neural Network...\nProgram paused. Press enter to continue.\nTraining Set Accuracy: 95.88%"
  },
  {
    "objectID": "posts/wenda-week2.html",
    "href": "posts/wenda-week2.html",
    "title": "机器学习作业第二周",
    "section": "",
    "text": "这是第二周逻辑回归的作业，经过上一次的作业，我对ocatve和numpy的语法有了一个感觉，所以转换起来也比较方便了。我这次就像他的原版作业一样，两个执行文件，另外把所有需要自己实现的函数都放在func.py里面了。\n这次没有什么需要特别记录的坑点。"
  },
  {
    "objectID": "posts/wenda-week2.html#执行效果",
    "href": "posts/wenda-week2.html#执行效果",
    "title": "机器学习作业第二周",
    "section": "执行效果",
    "text": "执行效果\n➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week2/ex2.py\nPlotting data with + indicating (y = 1)     examples and o indicating (y = 0) examples.\n\nCost at initial theta (zeros): 0.6931471805599453\nExpected cost (approx): 0.693\nGradient at initial theta (zeros):\n[[ -0.1       ]\n [-12.00921659]\n [-11.26284221]]\nExpected gradients (approx):\n -0.1000\n -12.0092\n -11.2628\n\nCost at test theta: 0.21833019382659774\n\nExpected cost (approx): 0.218\n\nGradient at test theta:\n\n[[0.04290299]\n [2.56623412]\n [2.64679737]]\nExpected gradients (approx):\n 0.043\n 2.566\n 2.647\nCost at theta found by fminunc: 0.20349770158947478\n\nExpected cost (approx): 0.203\n\ntheta:\n\n[[-25.16131857]\n [  0.20623159]\n [  0.20147149]]\nExpected theta (approx):\n -25.161\n 0.206\n 0.201\nFor a student with scores 45 and 85, we predict an admission probability of [0.77629062]\nExpected value: 0.775 +/- 0.002\nTrain Accuracy: 89.0%\nExpected accuracy (approx): 89.0"
  },
  {
    "objectID": "posts/wenda-week2.html#正则化逻辑回归",
    "href": "posts/wenda-week2.html#正则化逻辑回归",
    "title": "机器学习作业第二周",
    "section": "正则化逻辑回归",
    "text": "正则化逻辑回归\n这个程序不但使用了正则化，并且还将数据特征的维度进行了扩展，最终出现曲线\nfrom numpy.core import *\nimport matplotlib.pyplot as plt\nfrom fucs import costFuc, costFunction, gradFuc, load, minimize, plotData, plotDecisionBoundary, predict, mapFeature, costFunctionReg\n\n\nif __name__ == \"__main__\":\n\n    # Machine Learning Online Class - Exercise 2: Logistic Regression\n    #\n    #  Instructions\n    #  ------------\n    #\n    #  This file contains code that helps you get started on the second part\n    #  of the exercise which covers regularization with logistic regression.\n    #\n    #  You will need to complete the following functions in this exericse:\n    #\n    #     sigmoid.m\n    #     costFunction.m\n    #     predict.m\n    #     costFunctionReg.m\n    #\n    #  For this exercise, you will not need to change any code in this file,\n    #  or any other files other than those mentioned above.\n    #\n\n    # Load Data\n    #  The first two columns contains the X values and the third column\n    #  contains the label (y).\n\n    data = load('machine_learning_exam/week2/ex2data2.txt')\n    X = data[:, :2]\n    y = data[:, 2].reshape(-1, 1)\n\n    plotData(X, y)\n\n    # Labels and Legend\n    plt.xlabel('Microchip Test 1')\n    plt.ylabel('Microchip Test 2')\n\n    # Specified in plot order\n    plt.legend(['y = 1', 'y = 0'], loc='upper right')\n\n    # =========== Part 1: Regularized Logistic Regression ============\n    #  In this part, you are given a dataset with data points that are not\n    #  linearly separable. However, you would still like to use logistic\n    #  regression to classify the data points.\n    #\n    #  To do so, you introduce more features to use -- in particular, you add\n    #  polynomial features to our data matrix (similar to polynomial\n    #  regression).\n    #\n\n    # Add Polynomial Features\n\n    # Note that mapFeature also adds a column of ones for us, so the intercept\n    # term is handled\n    X = mapFeature(X[:, 0], X[:, 1])\n    print(X.shape)\n    # Initialize fitting parameters\n    initial_theta = zeros((size(X, 1), 1))\n    # Set regularization parameter lamda to 1\n    lamda = 1\n\n    # Compute and display initial cost and gradient for regularized logistic\n    # regression\n    [cost, grad] = costFunctionReg(initial_theta, X, y, lamda)\n\n    print('Cost at initial theta (zeros): {}'.format(cost))\n    print('Expected cost (approx): 0.693')\n    print('Gradient at initial theta (zeros) - first five values only:')\n    print(grad[0: 5, 0])\n    print('Expected gradients (approx) - first five values only:')\n    print(' 0.0085\\n 0.0188\\n 0.0001\\n 0.0503\\n 0.0115')\n\n    # Compute and display cost and gradient\n    # with all-ones theta and lamda = 10\n    test_theta = ones((size(X, 1), 1))\n    [cost, grad] = costFunctionReg(test_theta, X, y, 10)\n\n    print('Cost at test theta (with lamda = 10): {}'.format(cost))\n    print('Expected cost (approx): 3.16')\n    print('Gradient at test theta - first five values only:')\n    print(grad[0: 5, 0])\n    print('Expected gradients (approx) - first five values only:')\n    print(' 0.3460\\n 0.1614\\n 0.1948\\n 0.2269\\n 0.0922')\n\n    # ============= Part 2: Regularization and Accuracies =============\n    #  Optional Exercise:\n    #  In this part, you will get to try different values of lamda and\n    #  see how regularization affects the decision coundart\n    #\n    #  Try the following values of lamda (0, 1, 10, 100).\n    #\n    #  How does the decision boundary change when you vary lamda? How does\n    #  the training set accuracy vary?\n    #\n\n    # Initialize fitting parameters\n    initial_theta = zeros((X.shape[1], 1))\n\n    # Set regularization parameter lamda to 1 (you should vary this)\n    lamda = 1\n\n    # Set Options\n    Result = minimize(fun=costFuc, x0=initial_theta,\n                      args=(X, y), method='TNC', jac=gradFuc)\n    theta = Result.x.reshape(-1, 1)\n    cost = Result.fun\n\n    # Plot Boundary\n    plotDecisionBoundary(theta, X, y)\n\n    plt.title('lamda = {}'.format(lamda))\n\n    # Labels and Legend\n    plt.xlabel('Microchip Test 1')\n    plt.ylabel('Microchip Test 2')\n    plt.legend(['y = 1', 'y = 0', 'Decision boundary'])\n\n    # Compute accuracy on our training set\n    p = predict(theta, X)\n\n    print('Train Accuracy: {}%'.format(mean(array(p == y)) * 100))\n    print('Expected accuracy (with lamda = 1): 83.1 (approx)')\n    plt.show()"
  },
  {
    "objectID": "posts/wenda-week2.html#执行结果",
    "href": "posts/wenda-week2.html#执行结果",
    "title": "机器学习作业第二周",
    "section": "执行结果",
    "text": "执行结果\n➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week2/ex2_reg.py\n(118, 28)\nCost at initial theta (zeros): 0.6931471805599454\nExpected cost (approx): 0.693\nGradient at initial theta (zeros) - first five values only:\n[8.47457627e-03 1.87880932e-02 7.77711864e-05 5.03446395e-02\n 1.15013308e-02]\nExpected gradients (approx) - first five values only:\n 0.0085\n 0.0188\n 0.0001\n 0.0503\n 0.0115\nCost at test theta (with lamda = 10): 3.2068822129709416\nExpected cost (approx): 3.16\nGradient at test theta - first five values only:\n[0.34604507 0.16135192 0.19479576 0.22686278 0.09218568]\nExpected gradients (approx) - first five values only:\n 0.3460\n 0.1614\n 0.1948\n 0.2269\n 0.0922\nTrain Accuracy: 87.28813559322035%\nExpected accuracy (with lamda = 1): 83.1 (approx)"
  },
  {
    "objectID": "posts/webdataset.html",
    "href": "posts/webdataset.html",
    "title": "Pytorch Webdataset初体验",
    "section": "",
    "text": "最近都在用pytorch，虽然pytorch很多东西都比tensorflow舒服，但是在data pipeline方面还是tensorflow比较有优势，缺乏一个紧凑压缩的record的读取方法，虽然可以用DALI，但是之前用了一下还是不够灵活。最近在pytorch博客中发现了一个Webdataset，因此就尝试一下。"
  },
  {
    "objectID": "posts/webdataset.html#制作分片的数据集",
    "href": "posts/webdataset.html#制作分片的数据集",
    "title": "Pytorch Webdataset初体验",
    "section": "制作分片的数据集",
    "text": "制作分片的数据集\nfrom pathlib import Path\nimport shutil\nimport os\nimport webdataset as wds\n\nif __name__ == \"__main__\":\n  train = Path('/media/zqh/Documents/facelandmark_dataset/train')\n  test = Path('/media/zqh/Documents/facelandmark_dataset/test')\n  if not train.exists():\n    train.mkdir()\n  if not test.exists():\n    test.mkdir()\n  org1 = Path('/media/zqh/Documents/JOJO_face_crop_big')\n  org2 = Path('/home/zqh/workspace/data512x512')\n\n  test_ids = []\n  train_ids = []\n\n  for org in [org1, org2]:\n    ids = list(set([p.stem for p in org.iterdir()]))\n    n = len(ids)\n    test_n = int(n * 0.1)\n    for id in ids[:test_n]:\n      test_ids.append(org / id)\n\n    for id in ids[test_n:]:\n      train_ids.append(org / id)\n\n  for dst_root, ids in [(test, test_ids), (train, train_ids)]:\n    total = len(ids)\n    pattern = dst_root.as_posix() + f'-{str(total)}-%d.tar'\n    with wds.ShardWriter(pattern, maxcount=5000, encoder=False) as f:\n      for id in ids:\n        with open(id.as_posix() + '.jpg', \"rb\") as stream:\n          image = stream.read()\n        with open(id.as_posix() + '.json', \"rb\") as stream:\n          json = stream.read()\n        key = id.name\n        f.write({'__key__': key, 'jpg': image, 'json': json})"
  },
  {
    "objectID": "posts/webdataset.html#读取分片的数据集",
    "href": "posts/webdataset.html#读取分片的数据集",
    "title": "Pytorch Webdataset初体验",
    "section": "读取分片的数据集",
    "text": "读取分片的数据集\ndef get_pattern_and_total_num(root, stage='train'):\n  root = Path(root)\n  splits = []\n  for s in list(root.glob(f'{stage}*')):\n    name, total, split = s.stem.split('-')\n    splits.append(split)\n  if len(splits) &gt; 1:\n    patten_str = '{' + '..'.join([splits[0], splits[-1]]) + '}'\n  else:\n    patten_str = splits[0]\n  patten = (root / ('-'.join([name, total, patten_str]) + '.tar')).as_posix()\n  return patten, int(total)\n\n\ndef dev_load_shared_dataset():\n  root = '/media/zqh/Documents/facelandmark_dataset'\n  url, total = get_pattern_and_total_num(root, 'train')\n\n  fn = lambda x, **kwarg: x\n  idenity = A.Lambda(image=fn, mask=fn,\n                     keypoint=fn, bbox=fn)\n\n  train_transform = A.Compose([\n      A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2),\n      A.Resize(256, 256),\n  ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n\n  def perar_fn(sample):\n    keypoints = FaceLandMarkDataModule.parser_landmark(sample['json'])\n    return train_transform(image=sample['jpg'], keypoints=keypoints)\n\n  ds: wds.Dataset = wds.Dataset(url, length=total).shuffle(5000).decode(\n      'rgb8').map(perar_fn).to_tuple('image', 'keypoints').batched(8)\n  # Read ！\n  for sampe in ds:\n    pass"
  },
  {
    "objectID": "posts/vllm.html",
    "href": "posts/vllm.html",
    "title": "推理框架调研",
    "section": "",
    "text": "记录一下学习vllm/trt llm等框架的内容。"
  },
  {
    "objectID": "posts/vllm.html#安装-开发",
    "href": "posts/vllm.html#安装-开发",
    "title": "推理框架调研",
    "section": "安装 & 开发",
    "text": "安装 & 开发\n为了构建一个方便调试的开发环境，可以采用官方镜像, 并且需要自己修改entrypoint：\ndocker run -d -it \\\n  --gpus all \\\n  --name vllm_dev \\\n  --cap-add=NET_ADMIN \\\n  --network=host \\\n  --privileged=true \\\n  --shm-size 50g \\\n  --entrypoint /bin/bash \\\n  vllm/vllm-openai:latest \\\n  -c \"while true; do sleep 10; done\"\n进入官方镜像之后，他自带一个python3的环境，并且安装好了所有依赖，所以直接clone最新的vllm的并只安装python部分即可：\ngit clone https://gitee.com/mirrors/vllm.git\ncd vllm\nexport VLLM_TARGET_DEVICE=cuda\nexport VLLM_USE_PRECOMPILED=1\npython3 use_existing_torch.py\npip install --no-build-isolation -e ."
  },
  {
    "objectID": "posts/vllm.html#cuda-graph",
    "href": "posts/vllm.html#cuda-graph",
    "title": "推理框架调研",
    "section": "cuda graph",
    "text": "cuda graph\n基于qwen 2.5 0.5b， 在vllm/worker/model_runner.py中1915行，注意他这里capture并不是一次，而是类似shape bucket, 迭代这些batch size[256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1]，他们也只会在batch上进行capture。\n        self._graph = torch.cuda.CUDAGraph()\n        self._graph.enable_debug_mode()\n        with torch.cuda.graph(self._graph, pool=memory_pool, stream=stream):\n            output_hidden_or_intermediate_states = self.model(\n                input_ids=input_ids, # [256]\n                positions=positions, # [256]\n                kv_caches=kv_caches, # [[2, 103168, 16, 2, 64], [2, 103168, 16, 2, 64],...] 24个\n                attn_metadata=attn_metadata,\n                intermediate_tensors=intermediate_inputs,\n                **kwargs,\n            )\n\n            if isinstance(output_hidden_or_intermediate_states, torch.Tensor):\n                hidden_or_intermediate_states = weak_ref_tensor(\n                    output_hidden_or_intermediate_states)\n            elif isinstance(output_hidden_or_intermediate_states,\n                            IntermediateTensors):\n                hidden_or_intermediate_states = IntermediateTensors(\n                    tensors={\n                        key: weak_ref_tensor(value)\n                        for key, value in\n                        output_hidden_or_intermediate_states.tensors.items()\n                    })\n\n            del output_hidden_or_intermediate_states\n            # make sure `output_hidden_or_intermediate_states` is deleted\n            # in the graph's memory pool\n            gc.collect()\n        torch.cuda.synchronize()\n        self._graph.debug_dump(\"/root/vllm_learn/graph.dot\")\n不过他这里生成的cuda graph是这样的，并没有shape。"
  },
  {
    "objectID": "posts/vllm.html#attention-layer-size-trace",
    "href": "posts/vllm.html#attention-layer-size-trace",
    "title": "推理框架调研",
    "section": "attention layer size trace",
    "text": "attention layer size trace\n在模型初始化的时候，会用最大的batch size trace一次， 这是每一个att的打印代码：\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n    ) -&gt; torch.Tensor:\n        print(\"hidden_states\", hidden_states.shape) # [1, 896]\n        qkv, _ = self.qkv_proj(hidden_states) \n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        print(\"q\", q.shape,\"k\", k.shape,\"v\", v.shape)\n        q, k = self.rotary_emb(positions, q, k)\n        print(\"ro q\", q.shape,\"ro k\", k.shape)\n        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n        print(\"attn_output\", attn_output.shape)\n        output, _ = self.o_proj(attn_output)\n        print(\"output\", output.shape)\n        return output\n\nhidden_states torch.Size([32768, 896])\nq torch.Size([32768, 896]) k torch.Size([32768, 128]) v torch.Size([32768, 128])\nro q torch.Size([32768, 896]) ro k torch.Size([32768, 128])\nattn_output torch.Size([32768, 896])\noutput torch.Size([32768, 896])\n然后是prefill：\nhidden_states torch.Size([48, 896])\nq torch.Size([48, 896]) k torch.Size([48, 128]) v torch.Size([48, 128])\nro q torch.Size([48, 896]) ro k torch.Size([48, 128])\nattn_output torch.Size([48, 896])\noutput torch.Size([48, 896])\n接下来是decode:\nhidden_states torch.Size([1, 896])\nq torch.Size([1, 896]) k torch.Size([1, 128]) v torch.Size([1, 128])\nro q torch.Size([1, 896]) ro k torch.Size([1, 128])\nattn_output torch.Size([1, 896])\noutput torch.Size([1, 896])\n所以最重要的就是看decode的时候是如何适配seq len增长的。"
  },
  {
    "objectID": "posts/vllm.html#vllm-attention-detail",
    "href": "posts/vllm.html#vllm-attention-detail",
    "title": "推理框架调研",
    "section": "vllm attention detail",
    "text": "vllm attention detail\n这是vllm中对于attention类的的设计： \n这是vllm调度出来的请求与attention的meta data的对应关系图： \n在decode的时候，会调用vllm/attention/layer.py的attention：\ndef forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n    ) -&gt; torch.Tensor:\n        # NOTE: please avoid accessing `kv_cache` and `attn_metadata` arguments\n        # directly, use `self.kv_cache` and\n        # `get_forward_context().attn_metadata` instead.\n        if self.calculate_kv_scales:\n            ctx_attn_metadata = get_forward_context().attn_metadata\n            if ctx_attn_metadata.enable_kv_scales_calculation:\n                self.calc_kv_scales(key, value)\n        if self.use_output:\n            output = torch.empty_like(query)\n            hidden_size = query.size(-1)\n            # Reshape the query, key, and value tensors.\n            # NOTE(woosuk): We do this outside the custom op to minimize the\n            # CPU overheads from the non-CUDA-graph regions.\n            query = query.view(-1, self.num_heads, self.head_size)\n            output = output.view(-1, self.num_heads, self.head_size)\n            if key is not None:\n                key = key.view(-1, self.num_kv_heads, self.head_size)\n            if value is not None:\n                value = value.view(-1, self.num_kv_heads, self.head_size)\n            if self.use_direct_call:\n                forward_context: ForwardContext = get_forward_context()\n                ctx_attn_metadata = forward_context.attn_metadata\n                self_kv_cache = self.kv_cache[forward_context.virtual_engine]\n                self.impl.forward(self,\n                                  query,\n                                  key,\n                                  value,\n                                  self_kv_cache,\n                                  ctx_attn_metadata,\n                                  output=output)\n            else:\n                # note 上面把q都reshape到 [batch,num_heads,head_size]， kv转换为[num_kv_heads, head_size]\n                # q [1,1,64] , k,v [1,2,64]\n                torch.ops.vllm.unified_attention_with_output(\n                    query, key, value, output, self.layer_name)\n            return output.view(-1, hidden_size)\n        else:\n            if self.use_direct_call:\n                forward_context = get_forward_context()\n                ctx_attn_metadata = forward_context.attn_metadata\n                self_kv_cache = self.kv_cache[forward_context.virtual_engine]\n                return self.impl.forward(self, query, key, value,\n                                         self_kv_cache, ctx_attn_metadata)\n            else:\n                return torch.ops.vllm.unified_attention(\n                    query, key, value, self.layer_name)\n# 虽然torch.ops.vllm.unified_attention_with_output看起来没有使用kv cache，但是实际上是使用了的。\ndef unified_attention_with_output(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    output: torch.Tensor,\n    layer_name: str,\n) -&gt; None:\n    forward_context: ForwardContext = get_forward_context()\n    attn_metadata = forward_context.attn_metadata\n    self = forward_context.attn_layers[layer_name]\n    kv_cache = self.kv_cache[forward_context.virtual_engine]\n    self.impl.forward(self,\n                      query,\n                      key,\n                      value,\n                      kv_cache,\n                      attn_metadata,\n                      output=output)\n检查当前的attn metadata：\nFlashAttentionMetadata(num_prefills=0, num_prefill_tokens=0, num_decode_tokens=1, slot_mapping=tensor([54], device='cuda:0'), multi_modal_placeholder_index_maps={}, enable_kv_scales_calculation=True, seq_lens=[55], seq_lens_tensor=tensor([55], device='cuda:0', dtype=torch.int32), max_prefill_seq_len=0, max_decode_seq_len=55, context_lens_tensor=tensor([54], device='cuda:0', dtype=torch.int32), block_tables=tensor([[0, 1, 2, 3]], device='cuda:0', dtype=torch.int32), use_cuda_graph=False, max_query_len=1, max_decode_query_len=1, query_start_loc=tensor([0, 1], device='cuda:0', dtype=torch.int32), seq_start_loc=tensor([ 0, 55], device='cuda:0', dtype=torch.int32), _cached_prefill_metadata=None, _cached_decode_metadata=FlashAttentionMetadata(num_prefills=0, num_prefill_tokens=0, num_decode_tokens=1, slot_mapping=tensor([54], device='cuda:0'), multi_modal_placeholder_index_maps=None, enable_kv_scales_calculation=True, seq_lens=None, seq_lens_tensor=tensor([55], device='cuda:0', dtype=torch.int32), max_prefill_seq_len=0, max_decode_seq_len=55, context_lens_tensor=None, block_tables=tensor([[0, 1, 2, 3]], device='cuda:0', dtype=torch.int32), use_cuda_graph=False, max_query_len=1, max_decode_query_len=1, query_start_loc=tensor([0, 1], device='cuda:0', dtype=torch.int32), seq_start_loc=tensor([ 0, 55], device='cuda:0', dtype=torch.int32), _cached_prefill_metadata=None, _cached_decode_metadata=FlashAttentionMetadata(num_prefills=0, num_prefill_tokens=0, num_decode_tokens=1, slot_mapping=tensor([54], device='cuda:0'), multi_modal_placeholder_index_maps=None, enable_kv_scales_calculation=True, seq_lens=None, seq_lens_tensor=tensor([55], device='cuda:0', dtype=torch.int32), max_prefill_seq_len=0, max_decode_seq_len=55, context_lens_tensor=None, block_tables=tensor([[0, 1, 2, 3]], device='cuda:0', dtype=torch.int32), use_cuda_graph=False, max_query_len=1, max_decode_query_len=1, query_start_loc=tensor([0, 1], device='cuda:0', dtype=torch.int32), seq_start_loc=tensor([ 0, 55], device='cuda:0', dtype=torch.int32), _cached_prefill_metadata=None, _cached_decode_metadata=None, encoder_seq_lens=None, encoder_seq_lens_tensor=None, encoder_seq_start_loc=None, max_encoder_seq_len=None, num_encoder_tokens=None, cross_slot_mapping=None, cross_block_tables=None), encoder_seq_lens=None, encoder_seq_lens_tensor=None, encoder_seq_start_loc=None, max_encoder_seq_len=None, num_encoder_tokens=None, cross_slot_mapping=None, cross_block_tables=None), encoder_seq_lens=None, encoder_seq_lens_tensor=None, encoder_seq_start_loc=None, max_encoder_seq_len=None, num_encoder_tokens=None, cross_slot_mapping=None, cross_block_tables=None)\n\nkv_cache: torch.Size([2, 101291, 16, 2, 64])\n然后走到了FlashAttentionImpl.\n注意这里在启用chunked prefill之后，一批token里面会同时存在prefill和decode，因此需要拆分为两个部分分别执行prefill的decode的attention。 同时这个只是在动态的情况下会被执行到，如果开启了cuda graph，那么decode阶段会直接走cuda graph replay，同时\n    \"\"\"\n    If the input tensors contain prompt tokens, the layout is as follows:\n    |&lt;--------------- num_prefill_tokens -----------------&gt;|    \n    |&lt;--prefill_0--&gt;|&lt;--prefill_1--&gt;|...|&lt;--prefill_N-1---&gt;|\n\n    Otherwise, the layout is as follows:    \n    |&lt;----------------- num_decode_tokens ------------------&gt;|  \n    |&lt;--decode_0--&gt;|..........|&lt;--decode_M-1--&gt;|&lt;--padding--&gt;|\n\n    Generation tokens can contain padding when cuda-graph is used.\n    Currently, prompt tokens don't contain any padding.\n\n    The prompts might have different lengths, while the generation tokens\n    always have length 1.\n\n    If chunked prefill is enabled, prefill tokens and decode tokens can be\n    batched together in a flattened 1D query.\n\n    |&lt;----- num_prefill_tokens ----&gt;|&lt;------- num_decode_tokens ---------&gt;|\n    |&lt;-prefill_0-&gt;|...|&lt;-prefill_N-1-&gt;|&lt;--decode_0--&gt;|...|&lt;--decode_M-1--&gt;|\n\n    Currently, cuda graph is disabled for chunked prefill, meaning there's no\n    padding between prefill and decode tokens.\n    \"\"\"\n    def forward(\n        self,\n        layer: AttentionLayer,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: FlashAttentionMetadata,\n        output: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass with FlashAttention.\n\n        Args:\n            query: shape = [num_tokens, num_heads, head_size]\n            key: shape = [num_tokens, num_kv_heads, head_size]\n            value: shape = [num_tokens, num_kv_heads, head_size]\n            output: shape = [num_tokens, num_heads, head_size]\n            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]\n                NOTE: kv_cache will be an empty tensor with shape [0]\n                for profiling run.\n            attn_metadata: Metadata for attention.\n        NOTE: It in-place updates the output tensor.\n        \"\"\"\n        # NOTE(woosuk): FlashAttention does not support FP8 KV cache.\n\n        (num_prefill_query_tokens, num_prefill_kv_tokens,\n        num_decode_query_tokens) = \\\n            get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)\n        decode_query = query[num_prefill_query_tokens:]\n        decode_output = output[num_prefill_query_tokens:]\n        # QKV for prefill.\n        query = query[:num_prefill_query_tokens]\n        prefill_output = output[:num_prefill_query_tokens]\n        assert query.shape[0] == num_prefill_query_tokens\n        assert decode_query.shape[0] == num_decode_query_tokens\n        \n        if prefill_meta := attn_metadata.prefill_metadata:\n            # Prompt run.\n            if (kv_cache.numel() == 0 or prefill_meta.block_tables is None\n                    or prefill_meta.block_tables.numel() == 0):\n                # normal attention\n                # When block_tables are not filled, it means q and k are the\n                # prompt, and they have the same length.\n                q_seq_start_loc, q_seq_len, k_seq_start_loc, k_seq_len = \\\n                    _get_query_key_seq_metadata(prefill_meta, True, attn_type)\n\n                key = key[:num_prefill_kv_tokens]\n                value = value[:num_prefill_kv_tokens]\n\n                flash_attn_varlen_func(\n                    q=query,\n                    k=key,\n                    v=value,\n                    cu_seqlens_q=q_seq_start_loc,\n                    cu_seqlens_k=k_seq_start_loc,\n                    max_seqlen_q=q_seq_len,\n                    max_seqlen_k=k_seq_len,\n                    softmax_scale=softmax_scale,\n                    causal=_get_causal_option(attn_type),\n                    window_size=window_size,\n                    alibi_slopes=alibi_slopes,\n                    softcap=logits_soft_cap,\n                    out=prefill_output,\n                    fa_version=self.fa_version,\n                )\n            else:\n                # prefix-enabled attention\n                assert attn_type == AttentionType.DECODER, (\n                    \"Only decoder-only models support prefix caching\")\n                assert prefill_meta.seq_lens is not None\n                max_seq_len = max(prefill_meta.seq_lens)\n                flash_attn_varlen_func(  # noqa\n                    q=query,\n                    k=key_cache,\n                    v=value_cache,\n                    cu_seqlens_q=prefill_meta.query_start_loc,\n                    max_seqlen_q=prefill_meta.max_query_len,\n                    seqused_k=prefill_meta.seq_lens_tensor,\n                    max_seqlen_k=max_seq_len,\n                    softmax_scale=softmax_scale,\n                    causal=True,\n                    window_size=window_size,\n                    alibi_slopes=alibi_slopes,\n                    block_table=prefill_meta.block_tables,\n                    softcap=logits_soft_cap,\n                    out=prefill_output,\n                    fa_version=self.fa_version,\n                )\n\n        if decode_meta := attn_metadata.decode_metadata:\n            # Decoding run.\n            # Use flash_attn_varlen_func kernel for speculative decoding\n            # because different queries might have different lengths.\n\n            assert decode_meta.max_decode_query_len is not None\n            # use only for actual varlen decoding\n            if decode_meta.max_decode_query_len &gt; 1:\n                assert attn_type == AttentionType.DECODER, (\n                    \"Only decoder-only models support max_decode_query_len &gt; 1\"\n                )\n                flash_attn_varlen_func(\n                    q=decode_query,\n                    k=key_cache,\n                    v=value_cache,\n                    cu_seqlens_q=decode_meta.query_start_loc,\n                    max_seqlen_q=decode_meta.max_decode_query_len,\n                    seqused_k=decode_meta.seq_lens_tensor,\n                    max_seqlen_k=decode_meta.max_decode_seq_len,\n                    softmax_scale=softmax_scale,\n                    causal=True,\n                    window_size=window_size,\n                    alibi_slopes=alibi_slopes,\n                    softcap=logits_soft_cap,\n                    block_table=decode_meta.block_tables,\n                    out=decode_output,\n                    fa_version=self.fa_version,\n                )\n            else:\n                # Use flash_attn_with_kvcache for normal decoding.\n                (\n                    seq_lens_arg,\n                    _,\n                    block_tables_arg,\n                ) = get_seq_len_block_table_args(decode_meta, False, attn_type)\n                flash_attn_with_kvcache(\n                    q=decode_query.unsqueeze(1),\n                    k_cache=key_cache,\n                    v_cache=value_cache,\n                    block_table=block_tables_arg,\n                    cache_seqlens=seq_lens_arg,\n                    softmax_scale=softmax_scale,\n                    causal=True,\n                    window_size=window_size,\n                    alibi_slopes=alibi_slopes,\n                    softcap=logits_soft_cap,\n                    out=decode_output.unsqueeze(1),\n                    fa_version=self.fa_version,\n                )\n        return output\n他上面实际上就是分三部分，首先是kv的reshape:\n            if kv_cache.numel() &gt; 0:\n            key_cache = kv_cache[0]\n            value_cache = kv_cache[1]\n            # We skip updating the KV cache under two conditions:\n            #  a. When the Attention Type is ENCODER. In this phase, we compute\n            #     only the encoder attention without updating the cache.\n            #  b. When both Key and Value are None. This occurs during\n            #     cross-attention computation in the decoding phase, where the\n            #     KV cache is already populated with the cross-attention\n            #     tensor. Thus, we skip cache updates during this time.\n            if (attn_type != AttentionType.ENCODER) and (key is not None) and (\n                    value is not None):\n                if attn_type == AttentionType.ENCODER_DECODER:\n                    # Update cross-attention KV cache (prefill-only)\n                    updated_slot_mapping = attn_metadata.cross_slot_mapping\n                else:\n                    # Update self-attention KV cache (prefill/decode)\n                    updated_slot_mapping = attn_metadata.slot_mapping\n\n                # Reshape the input keys and values and store them in the cache.\n                # If kv_cache is not provided, the new key and value tensors are\n                # not cached. This happens during the initial memory\n                # profiling run.\n                torch.ops._C_cache_ops.reshape_and_cache_flash(\n                    key,\n                    value,\n                    kv_cache[0],\n                    kv_cache[1],\n                    updated_slot_mapping.flatten(),  # type: ignore[union-attr]\n                    kv_cache_dtype,\n                    layer._k_scale,\n                    layer._v_scale,\n                )\n\n        (num_prefill_query_tokens, num_prefill_kv_tokens,\n        num_decode_query_tokens) = \\\n            get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)\n        decode_query = query[num_prefill_query_tokens:]\n        decode_output = output[num_prefill_query_tokens:]\n        # QKV for prefill.\n        query = query[:num_prefill_query_tokens]\n        prefill_output = output[:num_prefill_query_tokens]\n        assert query.shape[0] == num_prefill_query_tokens\n        assert decode_query.shape[0] == num_decode_query_tokens\n然后根据场景走prefill和decode的flash attn,这里我主要关注decode部分。 在reshape kv的过程中，他的updated_slot_mapping是[54]，而实际上当前的seq_lens_arg是[55], block_tables_arg为[1,2,3,4]。\n        if decode_meta := attn_metadata.decode_metadata:\n            # Decoding run.\n            # Use flash_attn_varlen_func kernel for speculative decoding\n            # because different queries might have different lengths.\n\n            assert decode_meta.max_decode_query_len is not None\n            # use only for actual varlen decoding\n            if decode_meta.max_decode_query_len &gt; 1:\n                assert attn_type == AttentionType.DECODER, (\n                    \"Only decoder-only models support max_decode_query_len &gt; 1\"\n                )\n                flash_attn_varlen_func(\n                    q=decode_query,\n                    k=key_cache,\n                    v=value_cache,\n                    cu_seqlens_q=decode_meta.query_start_loc,\n                    max_seqlen_q=decode_meta.max_decode_query_len,\n                    seqused_k=decode_meta.seq_lens_tensor,\n                    max_seqlen_k=decode_meta.max_decode_seq_len,\n                    softmax_scale=softmax_scale,\n                    causal=True,\n                    window_size=window_size,\n                    alibi_slopes=alibi_slopes,\n                    softcap=logits_soft_cap,\n                    block_table=decode_meta.block_tables,\n                    out=decode_output,\n                    fa_version=self.fa_version,\n                )\n            else:\n                # Use flash_attn_with_kvcache for normal decoding.\n                (\n                    seq_lens_arg,\n                    _,\n                    block_tables_arg,\n                ) = get_seq_len_block_table_args(decode_meta, False, attn_type)\n                flash_attn_with_kvcache(\n                    q=decode_query.unsqueeze(1),\n                    k_cache=key_cache,\n                    v_cache=value_cache,\n                    block_table=block_tables_arg,\n                    cache_seqlens=seq_lens_arg,\n                    softmax_scale=softmax_scale,\n                    causal=True,\n                    window_size=window_size,\n                    alibi_slopes=alibi_slopes,\n                    softcap=logits_soft_cap,\n                    out=decode_output.unsqueeze(1),\n                    fa_version=self.fa_version,\n                )\n        return output\n目前调用的是flash attn 2:\ndef flash_attn_with_kvcache(\n    q,\n    k_cache,\n    v_cache,\n    k=None,\n    v=None,\n    rotary_cos=None,\n    rotary_sin=None,\n    cache_seqlens: Optional[Union[(int, torch.Tensor)]] = None,\n    cache_batch_idx: Optional[torch.Tensor] = None,\n    cache_leftpad: Optional[torch.Tensor] = None,\n    block_table: Optional[torch.Tensor] = None,\n    softmax_scale=None,\n    causal=False,\n    window_size=(-1, -1),  # -1 means infinite context window\n    softcap=0.0, # 0.0 means deactivated\n    rotary_interleaved=True,\n    alibi_slopes=None,\n    num_splits=0,\n    return_softmax_lse=False,\n    *,\n    out=None,\n    fa_version: int = DEFAULT_FA_VERSION,\n):\n    \"\"\"\n    If k and v are not None, k_cache and v_cache will be updated *inplace* with the new values from\n    k and v. This is useful for incremental decoding: you can pass in the cached keys/values from\n    the previous step, and update them with the new keys/values from the current step, and do\n    attention with the updated cache, all in 1 kernel.\n\n    If you pass in k / v, you must make sure that the cache is large enough to hold the new values.\n    For example, the KV cache could be pre-allocated with the max sequence length, and you can use\n    cache_seqlens to keep track of the current sequence lengths of each sequence in the batch.\n\n    Also apply rotary embedding if rotary_cos and rotary_sin are passed in. The key @k will be\n    rotated by rotary_cos and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.\n    If causal or local (i.e., window_size != (-1, -1)), the query @q will be rotated by rotary_cos\n    and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.\n    If not causal and not local, the query @q will be rotated by rotary_cos and rotary_sin at\n    indices cache_seqlens only (i.e. we consider all tokens in @q to be at position cache_seqlens).\n\n    See tests/test_flash_attn.py::test_flash_attn_kvcache for examples of how to use this function.\n\n    Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads\n    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\n    For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head\n    0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.\n\n    If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.\n    For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:\n        1 1 1 1 0\n        1 1 1 1 1\n    If seqlen_q = 5 and seqlen_k = 2, the causal mask is:\n        0 0\n        0 0\n        0 0\n        1 0\n        1 1\n    If the row of the mask is all zero, the output will be zero.\n\n    If window_size != (-1, -1), implements sliding window local attention. Query at position i\n    will only attend to keys between\n    [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.\n\n    Note: Does not support backward pass.\n\n    Arguments:\n        q: (batch_size, seqlen, nheads, headdim)\n        k_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,\n            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)\n            page_block_size must be a multiple of 256.\n        v_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,\n            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)\n        k [optional]: (batch_size, seqlen_new, nheads_k, headdim). If not None, we concatenate\n            k with k_cache, starting at the indices specified by cache_seqlens.\n        v [optional]: (batch_size, seqlen_new, nheads_k, headdim). Similar to k.\n        rotary_cos [optional]: (seqlen_ro, rotary_dim / 2). If not None, we apply rotary embedding\n            to k and q. Only applicable if k and v are passed in. rotary_dim must be divisible by 16.\n        rotary_sin [optional]: (seqlen_ro, rotary_dim / 2). Similar to rotary_cos.\n        cache_seqlens: int, or (batch_size,), dtype torch.int32. The sequence lengths of the\n            KV cache.\n        block_table [optional]: (batch_size, max_num_blocks_per_seq), dtype torch.int32.\n        cache_batch_idx: (batch_size,), dtype torch.int32. The indices used to index into the KV cache.\n            If None, we assume that the batch indices are [0, 1, 2, ..., batch_size - 1].\n            If the indices are not distinct, and k and v are provided, the values updated in the cache\n                 might come from any of the duplicate indices.\n        softmax_scale: float. The scaling of QK^T before applying softmax.\n            Default to 1 / sqrt(headdim).\n        causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).\n        window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n        softcap: float. Anything &gt; 0 activates softcapping attention.\n        rotary_interleaved: bool. Only applicable if rotary_cos and rotary_sin are passed in.\n            If True, rotary embedding will combine dimensions 0 & 1, 2 & 3, etc. If False,\n            rotary embedding will combine dimensions 0 & rotary_dim / 2, 1 & rotary_dim / 2 + 1\n            (i.e. GPT-NeoX style).\n        alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of\n            (-alibi_slope * |i + seqlen_k - seqlen_q - j|)\n            is added to the attention score of query i and key j.\n        num_splits: int. If &gt; 1, split the key/value into this many chunks along the sequence.\n           If num_splits == 1, we don't split the key/value. If num_splits == 0, we use a heuristic\n           to automatically determine the number of splits.\n           Don't change this unless you know what you are doing.\n        return_softmax_lse: bool. Whether to return the logsumexp of the attention scores.\n\n    Return:\n        out: (batch_size, seqlen, nheads, headdim).\n        softmax_lse [optional, if return_softmax_lse=True]: (batch_size, nheads, seqlen). The\n            logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax\n            normalization factor).\n    \"\"\"\n    assert k_cache.stride(-1) == 1, \"k_cache must have contiguous last dimension\"\n    assert v_cache.stride(-1) == 1, \"v_cache must have contiguous last dimension\"\n    q, k, v = [maybe_contiguous(x) for x in (q, k, v)]\n    if softmax_scale is None:\n        softmax_scale = q.shape[-1] ** (-0.5)\n    if cache_seqlens is not None and isinstance(cache_seqlens, int):\n        cache_seqlens = torch.full(\n            (k_cache.shape[0],), cache_seqlens, dtype=torch.int32, device=k_cache.device\n        )\n        cache_seqlens = maybe_contiguous(cache_seqlens)\n    cache_batch_idx = maybe_contiguous(cache_batch_idx)\n    block_table = maybe_contiguous(block_table)\n    \n    if fa_version == 2:\n        out, softmax_lse = torch.ops._vllm_fa2_C.fwd_kvcache(\n            q, k_cache, v_cache,\n            k, v,             # k_new, v_new\n            cache_seqlens,\n            rotary_cos,\n            rotary_sin,\n            cache_batch_idx,\n            cache_leftpad,\n            block_table,\n            alibi_slopes,\n            out,\n            softmax_scale,\n            causal,\n            window_size[0],\n            window_size[1],\n            softcap,\n            rotary_interleaved,\n            num_splits,\n        )\n    return (out, softmax_lse) if return_softmax_lse else out\n其中这个torch.ops._vllm_fa2_C.fwd_kvcache实际上位于vllm的flash attention的csrc/flash_attn/flash_api_torch_lib.cpp。\n\nstd::vector&lt;at::Tensor&gt;\nmha_fwd_kvcache(at::Tensor &q,                 // batch_size x seqlen_q x num_heads x head_size\n                const at::Tensor &kcache,            // batch_size_c x seqlen_k x num_heads_k x head_size or num_blocks x page_block_size x num_heads_k x head_size if there's a block_table.\n                const at::Tensor &vcache,            // batch_size_c x seqlen_k x num_heads_k x head_size or num_blocks x page_block_size x num_heads_k x head_size if there's a block_table.\n                std::optional&lt;const at::Tensor&gt; &k_, // batch_size x seqlen_knew x num_heads_k x head_size\n                std::optional&lt;const at::Tensor&gt; &v_, // batch_size x seqlen_knew x num_heads_k x head_size\n                std::optional&lt;const at::Tensor&gt; &seqlens_k_, // batch_size\n                std::optional&lt;const at::Tensor&gt; &rotary_cos_, // seqlen_ro x (rotary_dim / 2)\n                std::optional&lt;const at::Tensor&gt; &rotary_sin_, // seqlen_ro x (rotary_dim / 2)\n                std::optional&lt;const at::Tensor&gt; &cache_batch_idx_, // indices to index into the KV cache\n                std::optional&lt;const at::Tensor&gt; &leftpad_k_, // batch_size\n                std::optional&lt;at::Tensor&gt; &block_table_, // batch_size x max_num_blocks_per_seq\n                std::optional&lt;at::Tensor&gt; &alibi_slopes_, // num_heads or batch_size x num_heads\n                std::optional&lt;at::Tensor&gt; &out_,             // batch_size x seqlen_q x num_heads x head_size\n                const float softmax_scale,\n                bool is_causal,\n                int window_size_left,\n                int window_size_right,\n                const float softcap,\n                bool is_rotary_interleaved,   // if true, rotary combines indices 0 & 1, else indices 0 & rotary_dim / 2\n                int num_splits\n                ) {\n\n    // Otherwise the kernel will be launched from cuda:0 device\n    at::cuda::CUDAGuard device_guard{q.device()};\n\n    auto [cc_major, cc_minor] = get_compute_capability(get_current_device());\n    bool is_sm8x_min = cc_major &gt;= 8;\n    TORCH_CHECK(is_sm8x_min, \"FlashAttention only supports Ampere GPUs or newer.\");\n\n    auto q_dtype = q.dtype();\n    TORCH_CHECK(q_dtype == torch::kFloat16 || q_dtype == torch::kBFloat16,\n                \"FlashAttention only support fp16 and bf16 data type\");\n    TORCH_CHECK(kcache.dtype() == q_dtype, \"query and key must have the same dtype\");\n    TORCH_CHECK(vcache.dtype() == q_dtype, \"query and value must have the same dtype\");\n\n    CHECK_DEVICE(q); CHECK_DEVICE(kcache); CHECK_DEVICE(vcache);\n\n    TORCH_CHECK(q.stride(-1) == 1, \"Input tensor must have contiguous last dimension\");\n    TORCH_CHECK(kcache.stride(-1) == 1, \"Input tensor must have contiguous last dimension\");\n    TORCH_CHECK(vcache.stride(-1) == 1, \"Input tensor must have contiguous last dimension\");\n\n    at::Tensor block_table;\n    const bool paged_KV = block_table_.has_value();\n    if (paged_KV) {\n        TORCH_CHECK(!cache_batch_idx_.has_value(), \"Paged KVcache does not support cache_batch_idx\");\n        block_table = block_table_.value();\n        CHECK_DEVICE(block_table);\n        TORCH_CHECK(block_table.dtype() == torch::kInt32, \"block_table must have dtype torch.int32\");\n        TORCH_CHECK(block_table.stride(-1) == 1, \"block_table must have contiguous last dimension\");\n    }\n\n    const auto sizes = q.sizes();\n\n    const int batch_size = sizes[0];\n    int seqlen_q = sizes[1];\n    int num_heads = sizes[2];\n    const int head_size_og = sizes[3];\n    const int seqlen_q_og = seqlen_q;\n    const int num_heads_og = num_heads;\n\n    const int max_num_blocks_per_seq = !paged_KV ? 0 : block_table.size(1);\n    const int num_blocks = !paged_KV ? 0 : kcache.size(0);\n    const int page_block_size = !paged_KV ? 1 : kcache.size(1);\n    TORCH_CHECK(!paged_KV || page_block_size % 16 == 0, \"Paged KV cache block size must be divisible by 16\");\n    const int seqlen_k = !paged_KV ? kcache.size(1) : max_num_blocks_per_seq * page_block_size;\n    const int num_heads_k = kcache.size(2);\n    const int batch_size_c = !paged_KV ? kcache.size(0) : batch_size;\n    TORCH_CHECK(batch_size &gt; 0, \"batch size must be positive\");\n    TORCH_CHECK(head_size_og &lt;= 256, \"FlashAttention forward only supports head dimension at most 256\");\n    TORCH_CHECK(num_heads % num_heads_k == 0, \"Number of heads in key/value must divide number of heads in query\");\n\n    // causal=true is the same as causal=false in this case\n    if (seqlen_q == 1 && !alibi_slopes_.has_value()) { is_causal = false; }\n    if (is_causal) { window_size_right = 0; }\n\n    // Faster to transpose q from (b, 1, (nheads_kv ngroups), d) to (b, ngroups, nheads_kv, d) in this case\n    // H/t Daniel Haziza\n    const int seqlenq_ngroups_swapped = seqlen_q == 1 && num_heads &gt; num_heads_k && window_size_left &lt; 0 && window_size_right &lt; 0 && head_size_og % 8 == 0 && !alibi_slopes_.has_value();\n    if (seqlenq_ngroups_swapped) {\n        const int ngroups = num_heads / num_heads_k;\n        q = q.reshape({batch_size, num_heads_k, ngroups, head_size_og}).transpose(1, 2);\n        seqlen_q = ngroups;\n        num_heads = num_heads_k;\n    }\n\n    if (window_size_left &gt;= seqlen_k) { window_size_left = -1; }\n    if (window_size_right &gt;= seqlen_k) { window_size_right = -1; }\n\n    CHECK_SHAPE(q, batch_size, seqlen_q, num_heads, head_size_og);\n    if (!paged_KV) {\n        CHECK_SHAPE(kcache, batch_size_c, seqlen_k, num_heads_k, head_size_og);\n        CHECK_SHAPE(vcache, batch_size_c, seqlen_k, num_heads_k, head_size_og);\n    } else {\n        CHECK_SHAPE(kcache, num_blocks, page_block_size, num_heads_k, head_size_og);\n        CHECK_SHAPE(vcache, num_blocks, page_block_size, num_heads_k, head_size_og);\n        CHECK_SHAPE(block_table, batch_size, max_num_blocks_per_seq);\n    }\n\n    at::Tensor q_padded, kcache_padded, vcache_padded;\n    if (head_size_og % 8 != 0) {\n        q_padded = torch::nn::functional::pad(q, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));\n        kcache_padded = torch::nn::functional::pad(kcache, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));\n        vcache_padded = torch::nn::functional::pad(vcache, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));\n    } else {\n        q_padded = q;\n        kcache_padded = kcache;\n        vcache_padded = vcache;\n    }\n\n    at::Tensor out;\n    if (out_.has_value()) {\n        out = out_.value();\n        TORCH_CHECK(out.dtype() == q_dtype, \"Output must have the same dtype as inputs\");\n        CHECK_DEVICE(out);\n        TORCH_CHECK(out.stride(-1) == 1, \"Output tensor must have contiguous last dimension\");\n        CHECK_SHAPE(out, batch_size, seqlen_q_og, num_heads_og, head_size_og);\n        if (head_size_og % 8 != 0) {\n            out = torch::empty_like(q_padded);\n        } else if (seqlenq_ngroups_swapped) {\n            out = out.reshape({batch_size, num_heads, seqlen_q, head_size_og}).transpose(1, 2);\n        }\n    } else {\n        out = torch::empty_like(q_padded);\n    }\n\n    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };\n    const int head_size = round_multiple(head_size_og, 8);\n    const int head_size_rounded = head_size &lt;= 192 ? round_multiple(head_size, 32) : 256;\n    const int seqlen_q_rounded = round_multiple(seqlen_q, 128);\n    const int seqlen_k_rounded = round_multiple(seqlen_k, 128);\n\n    auto opts = q.options();\n\n    auto softmax_lse = torch::empty({batch_size, num_heads, seqlen_q}, opts.dtype(at::kFloat));\n\n    Flash_fwd_params params;\n    set_params_fprop(params,\n                     batch_size,\n                     seqlen_q, seqlen_k,\n                     seqlen_q_rounded, seqlen_k_rounded,\n                     num_heads, num_heads_k,\n                     head_size, head_size_rounded,\n                     q_padded, kcache_padded, vcache_padded, out,\n                     /*cu_seqlens_q_d=*/nullptr,\n                     /*cu_seqlens_k_d=*/nullptr,\n                     /*seqused_k=*/nullptr,\n                     /*p_ptr=*/nullptr,\n                     softmax_lse.data_ptr(),\n                     /*p_dropout=*/0.f,\n                     softmax_scale,\n                     window_size_left,\n                     window_size_right,\n                     softcap\n                     );\n\n    at::Tensor k, v, k_padded, v_padded;\n    if (k_.has_value()) {\n        TORCH_CHECK(v_.has_value(), \"If key is supplied, value must also be passed in\");\n        TORCH_CHECK(seqlens_k_.has_value(), \"If key is supplied, seqlens_k must also be passed in\");\n        TORCH_CHECK(seqlen_q &lt;= seqlen_k, \"If key is supplied, it must have seqlen &lt;= the seqlen of the KV cache\");\n        k = k_.value();\n        v = v_.value();\n        TORCH_CHECK(k.dtype() == q_dtype, \"Key must have the same dtype as query\");\n        TORCH_CHECK(v.dtype() == q_dtype, \"Value must have the same dtype as query\");\n        CHECK_DEVICE(k); CHECK_DEVICE(v);\n        TORCH_CHECK(k.stride(-1) == 1, \"Key tensor must have contiguous last dimension\");\n        TORCH_CHECK(v.stride(-1) == 1, \"Value tensor must have contiguous last dimension\");\n        int seqlen_knew = k.size(1);\n        CHECK_SHAPE(k, batch_size, seqlen_knew, num_heads_k, head_size_og);\n        CHECK_SHAPE(v, batch_size, seqlen_knew, num_heads_k, head_size_og);\n        if (head_size_og % 8 != 0) {\n            k_padded = torch::nn::functional::pad(k, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));\n            v_padded = torch::nn::functional::pad(v, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));\n        } else {\n            k_padded = k;\n            v_padded = v;\n        }\n        params.seqlen_knew = seqlen_knew;\n        params.knew_ptr = k_padded.data_ptr();\n        params.vnew_ptr = v_padded.data_ptr();\n        // All stride are in elements, not bytes.\n        params.knew_batch_stride = k_padded.stride(0);\n        params.vnew_batch_stride = v_padded.stride(0);\n        params.knew_row_stride = k_padded.stride(-3);\n        params.vnew_row_stride = v_padded.stride(-3);\n        params.knew_head_stride = k_padded.stride(-2);\n        params.vnew_head_stride = v_padded.stride(-2);\n    }\n\n    if (seqlens_k_.has_value()) {\n        auto seqlens_k = seqlens_k_.value();\n        TORCH_CHECK(seqlens_k.dtype() == torch::kInt32, \"seqlens_k must have dtype int32\");\n        CHECK_DEVICE(seqlens_k);\n        CHECK_CONTIGUOUS(seqlens_k);\n        CHECK_SHAPE(seqlens_k, batch_size);\n        params.cu_seqlens_k = static_cast&lt;int *&gt;(seqlens_k.data_ptr());\n    }\n    params.is_seqlens_k_cumulative = !(seqlens_k_.has_value());\n    if (leftpad_k_.has_value()) {\n        TORCH_CHECK(!paged_KV, \"We don't support Paged KV and leftpad_k running at the same time yet\");\n        auto leftpad_k = leftpad_k_.value();\n        TORCH_CHECK(leftpad_k.dtype() == torch::kInt32, \"leftpad_k must have dtype int32\");\n        CHECK_DEVICE(leftpad_k);\n        CHECK_CONTIGUOUS(leftpad_k);\n        CHECK_SHAPE(leftpad_k, batch_size);\n        params.leftpad_k = static_cast&lt;int *&gt;(leftpad_k.data_ptr());\n    }\n\n    if (rotary_cos_.has_value()) {\n        TORCH_CHECK(k_.has_value(), \"If rotary cos/sin are provided, new key / value to be appended to KV cache must also be provided\");\n        auto rotary_cos = rotary_cos_.value();\n        CHECK_DEVICE(rotary_cos);\n        params.rotary_dim = rotary_cos.size(1) * 2;\n        TORCH_CHECK(params.rotary_dim &lt;= head_size, \"rotary_dim must be &lt;= headdim\");\n        TORCH_CHECK(params.rotary_dim % 16 == 0, \"Only rotary dimensions divisible by 16 are currently supported\");\n        const int seqlen_ro = rotary_cos.size(0);\n        TORCH_CHECK(seqlen_ro &gt;= seqlen_k, \"cos/sin seqlen must be at least the seqlen of KV cache\");\n        CHECK_SHAPE(rotary_cos, seqlen_ro, params.rotary_dim / 2);\n        CHECK_CONTIGUOUS(rotary_cos);\n        TORCH_CHECK(rotary_cos.scalar_type() == q_dtype, \"rotary_cos must have the same dtype as query\");\n\n        TORCH_CHECK(rotary_sin_.has_value(), \"If rotary cos is provided, rotary sin must also be provided\");\n        auto rotary_sin = rotary_sin_.value();\n        CHECK_DEVICE(rotary_sin);\n        CHECK_SHAPE(rotary_sin, seqlen_ro, params.rotary_dim / 2);\n        CHECK_CONTIGUOUS(rotary_sin);\n        TORCH_CHECK(rotary_sin.scalar_type() == q_dtype, \"rotary_cos must have the same dtype as query\");\n        params.rotary_cos_ptr = rotary_cos.data_ptr();\n        params.rotary_sin_ptr = rotary_sin.data_ptr();\n        params.is_rotary_interleaved = is_rotary_interleaved;\n    } else {\n        params.rotary_dim = 0;\n    }\n\n    if (cache_batch_idx_.has_value()) {\n        auto cache_batch_idx = cache_batch_idx_.value();\n        CHECK_DEVICE(cache_batch_idx);\n        CHECK_CONTIGUOUS(cache_batch_idx);\n        TORCH_CHECK(cache_batch_idx.scalar_type() == torch::kInt32, \"cache_batch_idx must have dtype int32\");\n        params.cache_batch_idx = reinterpret_cast&lt;int *&gt;(cache_batch_idx.data_ptr());\n    }\n\n    // Keep references to these tensors to extend their lifetime\n    at::Tensor softmax_lse_accum, out_accum;\n    std::tie(softmax_lse_accum, out_accum) = set_params_splitkv(\n        params, batch_size, num_heads, head_size, seqlen_k, seqlen_q,\n        head_size_rounded, /*dropout*/ 0.f, num_splits, get_num_sm(get_current_device()), opts);\n\n    if (paged_KV) {\n        params.block_table = block_table.data_ptr&lt;int&gt;();\n        params.block_table_batch_stride = block_table.stride(0);\n    }\n    params.page_block_size = page_block_size;\n\n\n    set_params_alibi(params, alibi_slopes_, batch_size, num_heads);\n\n    auto stream = at::cuda::getCurrentCUDAStream().stream();\n    // Only split kernel supports appending to KV cache, or indexing to the cache with cache_batch_idx,\n    // or paged KV cache\n    run_mha_fwd(params, stream, /*force_split_kernel=*/k_.has_value() || cache_batch_idx_.has_value() || paged_KV);\n\n    if (head_size_og % 8 != 0) {\n        out = out.index({\"...\", torch::indexing::Slice(torch::indexing::None, head_size_og)});\n        if (out_.has_value()) { out_.value().copy_(out); }\n        if (k_.has_value()) {\n            // It's expensive to copy the KV cache here for the case where head size not divisible by 8,\n            // but we don't expect to get this case in practice. This is just so that the code works for that case.\n            kcache.copy_(kcache_padded.index({\"...\", torch::indexing::Slice(torch::indexing::None, head_size_og)}));\n            vcache.copy_(vcache_padded.index({\"...\", torch::indexing::Slice(torch::indexing::None, head_size_og)}));\n        }\n    }\n\n    if (seqlenq_ngroups_swapped) {\n        out = out.transpose(1, 2).reshape({batch_size, 1, num_heads_k * seqlen_q, head_size_og});\n        softmax_lse = softmax_lse.reshape({batch_size, num_heads_k * seqlen_q, 1});\n    }\n    return {out, softmax_lse};\n}\n实际上他这里也是有padding的。当k存在时，强行走split k:\nvoid run_mha_fwd(Flash_fwd_params &params, cudaStream_t stream, bool force_split_kernel=false) {\n    FP16_SWITCH(!params.is_bf16, [&] {\n        HEADDIM_SWITCH(params.d, [&] {\n            BOOL_SWITCH(params.is_causal, Is_causal, [&] {\n                if (params.num_splits &lt;= 1 && !force_split_kernel) {  // If we don't set it num_splits == 0\n                    run_mha_fwd_&lt;elem_type, kHeadDim, Is_causal&gt;(params, stream);\n                } else {\n                    run_mha_fwd_splitkv_dispatch&lt;elem_type, kHeadDim, Is_causal&gt;(params, stream);\n                }\n            });\n        });\n    });\n}\n然后在这个dispatch中还有许多变体，不过重要的就是tile的kv都是从远端读取过来的。\n    if (block_table != nullptr) {\n        auto final_block_size = binfo.actual_seqlen_k - (n_block_max - 1) * kBlockN;\n        tKgK.data() = gK.data() + flash::resolve_thread_kv_page_slice_offset&lt;Kernel_traits&gt;(tidx, n_block_max - 1, params.page_block_size,\n            block_table, params.k_batch_stride, params.k_row_stride, final_block_size);\n        tVgV.data() = gV.data() + flash::resolve_thread_kv_page_slice_offset&lt;Kernel_traits&gt;(tidx, n_block_max - 1, params.page_block_size,\n            block_table, params.v_batch_stride, params.v_row_stride, final_block_size);\n    }"
  },
  {
    "objectID": "posts/vllm.html#vllm-fused-moe",
    "href": "posts/vllm.html#vllm-fused-moe",
    "title": "推理框架调研",
    "section": "vllm fused moe",
    "text": "vllm fused moe\n要理解vllm fused moe，首先从最naive的huggingface版来理解moe的执行流程，首先整体流程如下：\n\n实际上就是一共64个expert，假设4个设备，ep=4，那么每个设备放8个expert。到gating时的输入都需要是broadcast的，然后gating计算hidden states对应每个expert的概率，将概率排序后进行all to all后使用当前设备中expert的计算outputs。注意他这里的all to all之后实际上每个设备还是算64个expert，只不过是8个expert重复了4次。 等计算完outpus之后再all to all就可以恢复到每个节点64个expert的输出。\n    @torch.no_grad()\n    def moe_infer(self, x, topk_ids, topk_weight):\n        cnts = topk_ids.new_zeros((topk_ids.shape[0], len(self.experts))) # [global batch, n_experts]\n        cnts.scatter_(1, topk_ids, 1) # assgin activated experts to 1\n        tokens_per_expert = cnts.sum(dim=0) # 每个token会选择不同的，那么一个专家会处理多个token，统计每个专家要处理的token数量\n        idxs = topk_ids.view(-1).argsort() # 先sort topk id, 这样idx 为 [global batch * n_experts], 其中expert的索引从小到大排序\n        sorted_tokens = x[idxs // topk_ids.shape[1]] # 因为他前面是先view再argsort，除shape[1]是为了保证只选当前token对应的expert。\n        sorted_tokens_shape = sorted_tokens.shape\n        if self.ep_size &gt; 1:\n            tokens_per_ep_rank = tokens_per_expert.view(self.ep_size, -1).sum(dim=1) # [ep_size, n_experts/ep_size] -&gt; [ep_size]\n            tokens_per_expert_group = tokens_per_expert.new_empty(\n                tokens_per_expert.shape[0]\n            )\n            dist.all_to_all_single(tokens_per_expert_group, tokens_per_expert) # 先all to all拿到当前的节点上每个expert要处理的token数量\n            output_splits = (\n                tokens_per_expert_group.view(self.ep_size, -1)\n                .sum(1)\n                .cpu()\n                .numpy()\n                .tolist()\n            )\n            gathered_tokens = sorted_tokens.new_empty(\n                tokens_per_expert_group.sum(dim=0).cpu().item(), sorted_tokens.shape[1]\n            )\n            input_split_sizes = tokens_per_ep_rank.cpu().numpy().tolist()\n            dist.all_to_all(\n                list(gathered_tokens.split(output_splits)),\n                list(sorted_tokens.split(input_split_sizes)),\n            ) # all to all拿到当前的节点上每个expert要处理的token\n            tokens_per_expert_post_gather = tokens_per_expert_group.view(\n                self.ep_size, self.experts_per_rank\n            ).sum(dim=0)\n            gatherd_idxs = np.zeros(shape=(gathered_tokens.shape[0],), dtype=np.int32)\n            s = 0\n            for i, k in enumerate(tokens_per_expert_group.cpu().numpy()):\n                gatherd_idxs[s : s + k] = i % self.experts_per_rank\n                s += k\n            gatherd_idxs = gatherd_idxs.argsort()\n            sorted_tokens = gathered_tokens[gatherd_idxs]\n            tokens_per_expert = tokens_per_expert_post_gather\n        tokens_per_expert = tokens_per_expert.cpu().numpy()\n\n        outputs = []\n        start_idx = 0\n        for i, num_tokens in enumerate(tokens_per_expert):\n            end_idx = start_idx + num_tokens\n            if num_tokens == 0:\n                continue\n            expert = self.experts[i + self.ep_rank * self.experts_per_rank]\n            tokens_for_this_expert = sorted_tokens[start_idx:end_idx] # 由于sort之后，分到同一个expert上token已经都排好了，所以直接取\n            expert_out = expert(tokens_for_this_expert)\n            outputs.append(expert_out)\n            start_idx = end_idx\n\n        outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0) # [global batch*n_act_experts, hidden_size]\n        if self.ep_size &gt; 1:\n            new_x = torch.empty_like(outs)\n            new_x[gatherd_idxs] = outs\n            gathered_tokens = new_x.new_empty(*sorted_tokens_shape)\n            dist.all_to_all(\n                list(gathered_tokens.split(input_split_sizes)),\n                list(new_x.split(output_splits)),\n            )\n            outs = gathered_tokens # 再来一次all to all拿到当前节点处理好的gathered tokens\n\n        new_x = torch.empty_like(outs)\n        new_x[idxs] = outs\n        final_out = (\n            new_x.view(*topk_ids.shape, -1)\n            .type(topk_weight.dtype)\n            .mul_(topk_weight.unsqueeze(dim=-1))\n            .sum(dim=1)\n            .type(new_x.dtype)\n        )\n        return final_out"
  },
  {
    "objectID": "posts/vllm.html#vllm并行模式",
    "href": "posts/vllm.html#vllm并行模式",
    "title": "推理框架调研",
    "section": "vllm并行模式",
    "text": "vllm并行模式\n\n\n\n\n\n\n\n\n\n\nAttention\nMoE - Shared Experts\nMoE - Routed Experts\n\n\n\n\nDP\n权重复制  可在推理框架外部处理\n权重复制\nEP关闭: 权重复制EP开启: 按EP(=DP)数量切分\n\n\nTP\n按head切分需同步LM head需支持广播meta data\n切分n和k维度需做all-reduce\nEP关闭: 权重复制EP开启: 按EP(=TP)数量切分\n\n\nTP+DP\nTP组内切分DP组内复制\nTP组内切分DP组内复制\nEP关闭: 权重复制EP开启: 按EP(=TP*DP)总数切分\n\n\n\n\nEP是依赖于DP/TP的MoE专用切分模式，EP = TP * DP，主要影响Routed Experts的分布\n\n\n同时开启DP + TP时，对于权重分布如上图所示，在group 0/group 1间为权重复制，在每个group内为权重切分。此时需要主要attn metadata是group间不同，group内相同。"
  },
  {
    "objectID": "posts/vllm.html#plugin",
    "href": "posts/vllm.html#plugin",
    "title": "推理框架调研",
    "section": "plugin",
    "text": "plugin\n先看看他如何定义plugin的，首先他在python端提供了一些辅助函数用于定义plugin算子的输入输出信息。\ndef get_fmha_kernel_meta_data():\n    return KernelMetaData(\n        kernel_name='fused_attention_kernel',\n        ios=[\n            # outputs\n            OutputArg('Out', Type('tensor[fp16]'), hints=['16', '16']),\n            OutputArg('L', Type('tensor[fp32]'), hints=['16', '16']),\n            OutputArg('M', Type('tensor[fp16]'), hints=['16', '16']),\n            # inputs\n            InputArg('Q', Type('tensor[fp16]'), hints=['16', '16']),\n            InputArg('K', Type('tensor[fp16]'), hints=['16', '16']),\n            InputArg('V', Type('tensor[fp16]'), hints=['16', '16']),\n            ParamArg('sm_scale', Type('fp32')),\n            DimSizeArg('batch_size'),\n            ParamArg('num_heads', Type('i32')),\n            DimSizeArg('seq_len', hints=['', '16']),\n            # constexprs\n            Constexpr(128),\n            Constexpr(64),\n            Constexpr(128),\n        ],\n        shape_infer_rules=[\n            # The following rules helps to deduce the shapes of the output tensors\n            \"Q[*] -&gt; Out[*]\",\n            \"Q[m,n,k,*] -&gt; L[m,n,k]\",\n            \"Q[m,n,k,*] -&gt; M[m,n,k]\",\n\n            # The following rules helps to deduce both DimSizeArgs: batch_size and seq_len\n            \"Q[m,n,k,*] : m -&gt; batch_size\",\n            \"Q[m,n,k,*] : k -&gt; seq_len\",\n        ],\n        version=0,\n        kernel_file=f'{openai_triton_example_root}/fmha_triton.py',\n        num_warps=1,\n        grid_dims=(\"(seq_len + 127) / 128\", \"batch_size * num_heads\", \"1\"))\n\n\nKERNELS = [\n    get_fmha_kernel_meta_data(),\n]\n甚至还支持了一套读取复杂计算表达式参数的转换器，用于把算子的计算逻辑转换成c接口代码。\n@pytest.mark.parametrize('expr, target', [\n    (\"a[m,n,k]:m*2+k+(n+1) -&gt; b\",\n     \"((inputDesc[0].dims.d[0] * 2) + (inputDesc[0].dims.d[2] + (inputDesc[0].dims.d[1] + 1)))\"\n     ),\n    (\"a[m,n,k]:m*(2+k)+n+1 -&gt; b\",\n     \"((inputDesc[0].dims.d[0] * (2 + inputDesc[0].dims.d[2])) + (inputDesc[0].dims.d[1] + 1))\"\n     ),\n    (\"a[m,n,k] -&gt; b[m*((((n+1))))]\", \"\"\"\nif (outputIndex == 0) {\n  outputDims.nbDims = 1;\n  outputDims.d[0] = (inputDims[0].d[0] * (inputDims[0].d[1] + 1));\n}\n     \"\"\"),\n    (\"a[m,n,k] -&gt; b[m*(n+k), 2*n, k+3]\", \"\"\"\nnvinfer1::DimsExprs outputDims;\nif (outputIndex == 0) {\n  outputDims.nbDims = 3;\n  outputDims.d[0] = (inputDims[0].d[0] * (inputDims[0].d[1] + inputDims[0].d[2]));\n  outputDims.d[1] = (2 * inputDims[0].d[1]);\n  outputDims.d[2] = (inputDims[0].d[2] + 3);\n}\"\"\")\n])\ndef test_CppCodeTranspiler(expr: str, target: str):\n    args = dict(\n        a=InputArg('a', Type('fp16')),\n        b=InputArg('b', Type('fp16')),\n    )\n    target = target.strip()\n\n    transpiler = CppCodeTranspiler(args)\n\n    shape_infer_code, dim_infer_code = transpiler([expr])\n\n    # we don't check the correctness of the code since the lark produces unstable ast tree\n    # refer to https://github.com/lark-parser/lark/issues/324\n    assert shape_infer_code or dim_infer_code"
  },
  {
    "objectID": "posts/vllm.html#auto-parallel",
    "href": "posts/vllm.html#auto-parallel",
    "title": "推理框架调研",
    "section": "auto parallel",
    "text": "auto parallel\n首先是切分描述，trt llm和shardy类似，是对一个维度来描述切分，如果为空，那么当前维度就是复制，否则在对应的mesh上进行切分：\nclass DimSpec:\n    def __init__(self, shard_list):\n        self.is_replica = len(shard_list) == 0\n        self.shard_list = shard_list\n\n    def __repr__(self):\n        if self.is_replica:\n            return 'R'\n        target = f\"S({','.join(str(dim) for dim in self.shard_list)})\"\n        return target\n然后一个tensor的切分信息由多个dim spec组成:\nclass ShardingSpec:\n    def __init__(self, entire_shape, sharding_sequence, device_mesh):\n      ...\n    def __repr__(self):\n        res = \"DistSpec(\"\n        res += f\"shape={self.entire_shape},\"\n        res += f\"shard={self.sharding_sequence},\"\n        res += f\"mesh={self.device_mesh.mesh_shape}\"\n        res += \")\"\n        return res\n然后构造一个分布式切分搜索图：\n    def get_cost_graph(self, lmesh):\n        leaf_strategies = []\n        for node in self.nodes:\n            if node.is_replicated:\n                node.set_strategy(None, lmesh)\n            else:\n                node.collect_strategies(lmesh)\n        for node in self.nodes:\n            strategies_vector = node.update_resharding_cost()\n            if len(strategies_vector) != 0:\n                leaf_strategies.append(strategies_vector)\n        cost_graph = CostGraph(leaf_strategies)\n        return cost_graph\n首先是遍历所有节点，如果这个是is_replicated，也就是提前标记了不可分布式，那么直接设置为None，否则遍历所有可能的切分策略，然后计算这个节点的cost（包含了通信和计算）。其中collect_strategies核心代码如下, 他似乎是只考虑在2维拓扑以下进行分布式，但是这里加的策略还是全的，不过好像只添加考虑输出节点的切分，并不是SBP的基于推导的方法。\n    def _collect_strategies(self, device_mesh):\n        dim_partition_list = []\n        dim_size = len(self.op_data['output0'].shape)\n        dim_partition_list.append({})\n        dim_partition_list.extend(\n            self._enumerate_all_possible_1d_sharding([0, 1], dim_size))\n        dim_partition_list.extend(\n            self._enumerate_all_possible_2d_sharding([0], [1], dim_size))\n        dim_partition_list.extend(\n            self._enumerate_all_possible_1d_sharding([0], dim_size))\n        dim_partition_list.extend(\n            self._enumerate_all_possible_1d_sharding([1], dim_size))\n\n        strategies_vector = StrategiesVector(self)\n        for dim_partition_dict in dim_partition_list:\n            dim_partition_dict_mapping = {'output0': dim_partition_dict}\n            sharding_spec_mapping = self._to_sharding_spec_mapping(\n                dim_partition_dict_mapping, device_mesh)\n            if 0 == len(sharding_spec_mapping):\n                continue\n            sharding_seq = sharding_spec_mapping['output0'].sharding_sequence\n            sharding_strategy = self._get_sharding_strategy(\n                name=f'constant-op {sharding_seq}',\n                sharding_spec_mapping=sharding_spec_mapping,\n                communication_action_mapping={})\n            strategies_vector.append(sharding_strategy)\n\n        return strategies_vector\n然后发现实际上这个方法是可以被override的，对于matmul来说有自己的collect方法：\n    def _collect_strategies(self, device_mesh):\n        strategies_vector = StrategiesVector(self)\n        dp_strategies = self._dp_strategies(device_mesh)\n        tp_strategies = self._tp_strategies(device_mesh)\n        mix_strategies = self._mix_strategies(device_mesh)\n        bmm_strategies = self._bmm_strategies(device_mesh)\n        strategies_vector.extend(dp_strategies)\n        strategies_vector.extend(tp_strategies)\n        strategies_vector.extend(mix_strategies)\n        strategies_vector.extend(bmm_strategies)\n        return strategies_vector\n这是matmul所collect出来的切分方式：\nRR = RS(0) x S(0)R_allreduceS(0)\nRR = RS(1) x S(1)R_allreduceS(1)\nRR = RS(0,1) x S(0,1)R_allreduceS(0,1)\n[R, S(0)] = [R, S(0)] x [S(0), R]_reducescatter(1, S(0))\n[R, S(1)] = [R, S(1)] x [S(1), R]_reducescatter(1, S(1))\n[R, S(0,1)] = [R, S(0,1)] x [S(0,1), R]_reducescatter(1, S(0,1))\nRS(0) = RR x RS(0)\nRS(1) = RR x RS(1)\nRS(0,1) = RR x RS(0,1)\nRS(1) = RS(0) x S(0)S(1)_allreduceS(0)\nRS(0) = RS(1) x S(1)S(0)_allreduceS(1)\nRR = RR x RR\n同样attention或其他节点也有自己的分布式切分收集函数，这里和oneflow不同的是，他并不会传播partial的切分，我的理解是拆分的越细那么灵活性更高，可以后续做自动的通算融合，大粒度后面还是走匹配替换来优化。\n每个算子的分布式策略收集好之后，还需要构建reshard cost，因为可能上一个节点的切分策略并不是下一个节点所需要的切分策略，所以需要计算reshard cost。\n    def _update_resharding_cost(self, strategies):\n        for strategy in strategies:\n            resharding_costs = {}\n            for pre_node, out_index in self.predecessor_nodes_out_index.items():\n                if pre_node is None:\n                    continue\n                pre_node_out_data_name = pre_node.get_output(out_index).name\n                pre_node_out_data_lname = pre_node.global_to_local_op_name[\n                    pre_node_out_data_name]\n                if pre_node_out_data_name not in self.global_to_local_op_name:\n                    print(f\"pre_node_out_data_name = {pre_node_out_data_name}\")\n                    continue\n                cur_node_inp_data_lname = self.global_to_local_op_name[\n                    pre_node_out_data_name]\n                cur_sharding_spec = strategy.sharding_specs[\n                    cur_node_inp_data_lname]\n\n                pre_node_out_sharding_specs = []\n                for pre_strategy in pre_node.strategies_vector:\n                    pre_node_out_sharding_specs.append(\n                        pre_strategy.sharding_specs[pre_node_out_data_lname])\n\n                if pre_node not in resharding_costs:\n                    resharding_costs[pre_node.node_name] = []\n                for prev_sharding_spec in pre_node_out_sharding_specs:\n                    resharding_cost = self._compute_resharding_cost(\n                        prev_sharding_spec, cur_sharding_spec,\n                        self.op_data[cur_node_inp_data_lname]) # 话说为什么他要限制这个op的切分类型，本来这个gather就可以切输入啊\n                    resharding_costs[pre_node.node_name].append(resharding_cost)\n            strategy.resharding_costs = resharding_costs"
  },
  {
    "objectID": "posts/vllm.html#pyexector",
    "href": "posts/vllm.html#pyexector",
    "title": "推理框架调研",
    "section": "PyExector",
    "text": "PyExector\ntrt llm之前通过编译的方式发现使用起来不方便，因此他也模仿vllm，提供了一个python的executor，直接动态执行大模型。并且他新的接口就做了相当的精简："
  },
  {
    "objectID": "posts/vllm.html#vllm问题",
    "href": "posts/vllm.html#vllm问题",
    "title": "推理框架调研",
    "section": "vllm问题",
    "text": "vllm问题\n\nTypeError: must be called with a dataclass type or instance\n\n发现是torch 2.5.1 cu118不能用triton 3.2，需要降级到3.1\n\nRuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\n\n检查detail发现是runtime的nccl是cu12的\n\njson.decoder.JSONDecodeError: Extra data: line 5298 column 2 (char 479924)\n\n发现是之前下载的模型的json有问题，重新下载就行了\n\nRuntimeError: Triton Error [CUDA]: device kernel image is invalid\n\n[rank0]: torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n[rank0]: RuntimeError: Triton Error [CUDA]: device kernel image is invalid\n\n[rank0]: Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\n[rank0]: You can suppress this exception and fall back to eager by setting:\n[rank0]:     import torch._dynamo\n[rank0]:     torch._dynamo.config.suppress_errors = True\n这个应该是triton默认只能编译出最新的cuda版本的代码,把本地的ptxas替换掉triton中的：\ncp /usr/local/cuda/bin/ptxas /root/miniconda3/envs/vllm/lib/python3.10/site-packages/triton/backends/nvidia/bin/ptxas\n\nNCCL WARN NCCL cannot be captured in a graph\n\n好像是安装的nccl版本和cuda 11.8还是兼容？\nmisc/strongstream.cc:53 NCCL WARN NCCL cannot be captured in a graph if either it wasn't built with CUDA runtime &gt;= 11.3 or if the installed CUDA driver &lt; R465.\n我现在的版本是nvidia-nccl-cu11-2.21.5,卸载之后重装老的版本\n❯ pip install nvidia-nccl-cu11==2.19.3 -i https://download.pytorch.org/whl/cu118\n好像也不行，可能是因为老的版本的nccl就没有支持capture graph的功能，需要重新编译nccl，但是实际上可以--enforce-eager跳过这个问题。"
  },
  {
    "objectID": "posts/vllm.html#trt-llm问题",
    "href": "posts/vllm.html#trt-llm问题",
    "title": "推理框架调研",
    "section": "trt llm问题",
    "text": "trt llm问题\n\ncuda版本问题\n\ntrt llm和cuda版本是强绑定的，所以如果cuda 版本不到，就直接没法运行，所以只能安装老版本的trt llm。\n\ntensorrt bindings的问题\n\n我发现使用pip安装的tensorrt llm中的tensorrt 调用的是tensorrt bindings，然后我安装他得到的是8.6.1版本，这个不是我想要的。解决方法是pip卸载tensorrt，然后用trt llm中的shell脚本安装tensorrt。\n\nmpi版本问题\n\npip安装trt llm的时候总是报错编译mpi4py失败，然后我查看了一下，发现是mpi版本的问题，默认apt安装openmpi得到40+版本了，但是trt llm中需要的是3.1.5，所以解决方案是不要apt装openmpi，通过conda-forge安装3.1.5版本"
  },
  {
    "objectID": "posts/vllm.html#scaled-dot-product-attention-sdpa-精度问题",
    "href": "posts/vllm.html#scaled-dot-product-attention-sdpa-精度问题",
    "title": "推理框架调研",
    "section": "scaled dot product attention (SDPA) 精度问题",
    "text": "scaled dot product attention (SDPA) 精度问题\n我发现在mac上使用 torch.nn.functional.scaled_dot_product_attention 与自己实现的 scaled_dot_product_attention 得到的结果精度有差异。但是如果设定了SDPA的实现为MATH就不会有精度差异，并且是否设定MATH的backend也会导致精度问题，我就很难理解默认的backend是什么。\nimport math\nfrom typing import Optional\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\n\ndef scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None, dropout_p: float = 0.0, is_causal: bool = False, scale: Optional[float] = None) -&gt; torch.Tensor:\n    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n        attn_bias.to(query.dtype)\n\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n        else:\n            attn_bias = attn_mask + attn_bias\n\n    # if enable_gqa:\n    #     key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n    #     value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim=-1)\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n    return attn_weight @ value\n\nq = torch.randn(1, 2, 7, 101, 64)\nk = torch.randn(1, 2, 1, 101, 64)\nv = torch.randn(1, 2, 1, 101, 64)\nmask = torch.zeros(1, 2, 7, 101, 101)\n\ny1 = scaled_dot_product_attention(\n    q, k, v, attn_mask=mask, dropout_p=0.0, scale=1.0, is_causal=mask is None)\n\nq2 = q.reshape(1, -1, 101, 64)\nk2 = torch.repeat_interleave(k, 7, 2).reshape(1, -1, 101, 64)\nv2 = torch.repeat_interleave(v, 7, 2).reshape(1, -1, 101, 64)\nmask2 = mask.reshape(1, -1, 101, 101)\ny2 = scaled_dot_product_attention(\n    q2, k2, v2, attn_mask=mask2, dropout_p=0.0, scale=1.0, is_causal=mask2 is None)\n \nassert torch.allclose(y1, y2.reshape(1, 2, 7, 101, 64))\n\nwith sdpa_kernel([SDPBackend.MATH]):\n  y3 = torch.nn.functional.scaled_dot_product_attention(\n      q2, k2, v2, attn_mask=mask2, dropout_p=0.0, scale=1.0, is_causal=mask2 is None)\n \nassert torch.allclose(y2, y3)\n\ny3_1 = torch.nn.functional.scaled_dot_product_attention(\n    q2, k2, v2, attn_mask=mask2, dropout_p=0.0, scale=1.0, is_causal=mask2 is None)\n \nassert torch.allclose(y3, y3_1) # will fail"
  },
  {
    "objectID": "posts/vae.html",
    "href": "posts/vae.html",
    "title": "变分自编码器(VAE)学习",
    "section": "",
    "text": "我看了VAE之后忽然对神经网络的非监督学习以及概率模型很感兴趣,但是无奈概率模型真的好难懂啊. 今天尝试一边描述VAE一边真正的理解他. 参考总结自:https://spaces.ac.cn/archives/5253"
  },
  {
    "objectID": "posts/vae.html#术语表",
    "href": "posts/vae.html#术语表",
    "title": "变分自编码器(VAE)学习",
    "section": "术语表:",
    "text": "术语表:\n\n编码器:\n在概率模型中,编码器称为推理网络,他将参数化潜在特征的后验近似\\(z\\),然后将参数输出到分布\\(q(z|x)\\),也可以说\\(q(z|x)\\)是\\(x\\)的后验分布.\n解码器:\n解码器是根据\\(z\\)重构\\(x\\)的,分布为\\(p(x|z)\\)."
  },
  {
    "objectID": "posts/vae.html#推导",
    "href": "posts/vae.html#推导",
    "title": "变分自编码器(VAE)学习",
    "section": "推导",
    "text": "推导\n在传统的编码器中,解码器\\(p(x|z)\\)是易于训练的,但是\\(q(z|x)\\)是难以学习的.根据贝叶斯定理,我们可以把后验分布转写成另一种形式,但是\\(p(x)\\)也并不知道,所以利用联合概率分布近似代替\\(p(x)\\): \\[ \\begin{aligned}\n    q(z|x)=\\frac{p(x|z)p(z)}{p(x)}=\\frac{p(x|z)p(z)}{\\sum_zp(x|z)p(z)}\n\\end{aligned} \\]\n为了近似求出后验分布,我们还需要一种方法将估计出的分布与真实分布进行比较,所以使用KL分歧来衡量两个概率分布的相似程度.如果相同,则分歧为0,如果大于0,则分布不同. 因此使用如下方式使用KL分歧: 由于我们考虑的是各分量独立的多元正态分布，因此只需要推导一元正态分布的情形即可: \\[ \\begin{aligned}&KL\\Big(N(\\mu,\\sigma^2)\\Big\\Vert N(0,1)\\Big)\\\\\n=&\\int \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2} \\left(\\log \\frac{e^{-(x-\\mu)^2/2\\sigma^2}/\\sqrt{2\\pi\\sigma^2}}{e^{-x^2/2}/\\sqrt{2\\pi}}\\right)dx\\\\\n=&\\int \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2} \\log \\left\\{\\frac{1}{\\sqrt{\\sigma^2}}\\exp\\left\\{\\frac{1}{2}\\big[x^2-(x-\\mu)^2/\\sigma^2\\big]\\right\\} \\right\\}dx\\\\\n=&\\frac{1}{2}\\int \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2} \\Big[-\\log \\sigma^2+x^2-(x-\\mu)^2/\\sigma^2 \\Big] dx \\\\\n=&\\frac{1}{2}[-log\\sigma^2+\\mu^2+\\sigma^2-1]\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/vae.html#重参数",
    "href": "posts/vae.html#重参数",
    "title": "变分自编码器(VAE)学习",
    "section": "重参数",
    "text": "重参数\nVAE可以生成新的\\(X\\)就是因为他从隐空间取\\(z\\)的时候是随机选取的,但是随机选取\\(z\\)会导致无法反向求导,所以就使用一个重参数技巧.\n从\\(N(\\mu,\\sigma^2)\\)中采样一个\\(z\\),就相当于从\\(N(0,1)\\)中采样一个\\(\\epsilon\\),然后令\\(z=\\mu+\\epsilon*\\sigma\\)."
  },
  {
    "objectID": "posts/vae.html#本质结构",
    "href": "posts/vae.html#本质结构",
    "title": "变分自编码器(VAE)学习",
    "section": "本质结构",
    "text": "本质结构\n\n本质上VAE就是训练出一个后验分布\\(q(Z|X)\\),然后生成器从后验分布中随机取元素(相当于加噪声)并解码出对应的\\(\\bar{X}\\).代码上实现起来相当简单,但是背后的数学思想是非常难理解的,现在我还是一知半解."
  },
  {
    "objectID": "posts/use-centernet.html",
    "href": "posts/use-centernet.html",
    "title": "配置CenterNet环境",
    "section": "",
    "text": "昨天我尝试用双cuda的方式来配置换，但是还是遇到了cuda的错误，我不懂pytorch又没办法解决。然后我浏览下issue，看到有同样的问题，大概率是由于显卡是20系列的，老版本的cuda不行，解决方式就是升级pytorch版本用新的cuda。所以我这里把配置环境重新做个记录，免得下次又来。。\n\n\n1. 安装cuda和cudnn\n这个不多说了，cuda 10.0和cudnn 7.5。\n\n\n2. 安装python环境\n\n初始化python环境\n\nconda create --name CenterNet python=3.6\nconda activate CenterNet\nconda install pytorch=1.0 torchvision cudatoolkit=10.0\n\n安装他所使用的库\n\npip install -r requirements.txt\n\n安装cocoapi\n\ngit clone https://github.com/cocodataset/cocoapi.git\ncd cocoapi/PythonAPI\nmake\npython setup.py install --user\n\n安装DCNv2网络\n\n记得编译器要设置为gcc-6\ncd src/lib/models/networks\nrm -rf DCNv2\ngit clone https://github.com/CharlesShang/DCNv2.git\ncd DCNv2\n./make.sh\n\n安装nms\n\ncd src/lib/external\nmake\n\n\n3. 下载数据集\ncd src/tools\n./get_pascal_voc.sh\nmv voc ../../data\n\n\n4. 开始训练\npython main.py ctdet --exp_id pascal_resdcn18_384 --arch resdcn_18 --dataset pascal --num_epochs 70 --lr_step 45,60\n\n\n完成\n我发现在训练模型时gpu使用率居然是100%，这个真的有点强，难道pytorch真的比tensorflow给力吗？"
  },
  {
    "objectID": "posts/train-err.html",
    "href": "posts/train-err.html",
    "title": "解决目标检测任务测试集recall率低",
    "section": "",
    "text": "我把Google官方的mobile-net模型拿来做迁移学习.在训练的过程中发现一个问题,在测试中对于目标的recall率十分低.经过了两天的尝试,大概找到了解决办法"
  },
  {
    "objectID": "posts/train-err.html#分析",
    "href": "posts/train-err.html#分析",
    "title": "解决目标检测任务测试集recall率低",
    "section": "分析",
    "text": "分析\n可以发现我两个模型的测试与训练时的precision都较好,所以暂时不管.\n接着看recall,在训练期间,yolonet明显好于pureconv.但是在测试期间情况却相反了.\n\nBatch norm\n这个原因我很快否决了.因为我上一次就已经解决.\n过拟合\n有的人说这是典型的过拟合,但是我可以确认这完全不是过拟合,因为我才训练一千的step是不应该出现过拟合的情况.当然,我这样说也缺乏证据.必须要绘制出bias-variance曲线才可以证明.(目标检测任务中的bias-variance曲线我还需要去了解如何计算)\n神经元被抑制\n这是我的猜想.不过我确实发现了问题所在."
  },
  {
    "objectID": "posts/torchsharp.html",
    "href": "posts/torchsharp.html",
    "title": "关于如何在M1上使用TorchSharp",
    "section": "",
    "text": "TorchSharp只有x64的,太蛋疼了,所以需要重新安装一遍."
  },
  {
    "objectID": "posts/torchsharp.html#下载源码",
    "href": "posts/torchsharp.html#下载源码",
    "title": "关于如何在M1上使用TorchSharp",
    "section": "1. 下载源码",
    "text": "1. 下载源码\ngit clone https://github.com/dotnet/TorchSharp.git --depth 1"
  },
  {
    "objectID": "posts/torchsharp.html#下载编译libtorch",
    "href": "posts/torchsharp.html#下载编译libtorch",
    "title": "关于如何在M1上使用TorchSharp",
    "section": "2. 下载编译libtorch",
    "text": "2. 下载编译libtorch\ngit clone -b v1.10.0 --recurse-submodule https://github.com/pytorch/pytorch.git --depth 1\nmkdir pytorch-build\ncd pytorch-build\ncmake -DBUILD_SHARED_LIBS:BOOL=ON -DCMAKE_BUILD_TYPE:STRING=Release -DPYTHON_EXECUTABLE:PATH=`which python3` -DCMAKE_INSTALL_PREFIX:PATH=../libtorch ../pytorch -G \"Ninja\"\ncmake --build . --target install\n注意这里默认是不开test的生成的."
  },
  {
    "objectID": "posts/torchsharp.html#修改torchsharp的编译命令",
    "href": "posts/torchsharp.html#修改torchsharp的编译命令",
    "title": "关于如何在M1上使用TorchSharp",
    "section": "3. 修改TorchSharp的编译命令",
    "text": "3. 修改TorchSharp的编译命令\n这里首先是修改libTrochSharp的cmake, 把llvm的路径改成本机的. 然后因为他默认要copy libtorch中的一些test的dylib到nupkg中,我之前编译的libtorch中没有test的输出, 这里需要注释掉. 最后就是把sdk版本改成net6.\ndiff --git a/Directory.Build.props b/Directory.Build.props\nindex 34f28ab..28b977e 100644\n--- a/Directory.Build.props\n+++ b/Directory.Build.props\n@@ -92,7 +92,7 @@\n     &lt;!-- By default only TorchSharp and no libtorch-cpu or libtorch-cuda packages are built.  The CI file controls these via 'BuildLibTorchPackages' --&gt;\n     &lt;!-- This then selectively turns these on over several CI jobs since different pacakges are done in different jobs --&gt;\n     &lt;IncludeTorchSharpPackage&gt;true&lt;/IncludeTorchSharpPackage&gt;\n-    &lt;IncludeLibTorchCpuPackages&gt;false&lt;/IncludeLibTorchCpuPackages&gt;\n+    &lt;IncludeLibTorchCpuPackages&gt;true&lt;/IncludeLibTorchCpuPackages&gt;\n     &lt;IncludeLibTorchCudaPackages&gt;false&lt;/IncludeLibTorchCudaPackages&gt;\n   &lt;/PropertyGroup&gt;\n \ndiff --git a/Directory.Build.targets b/Directory.Build.targets\nindex b79d260..3383de5 100644\n--- a/Directory.Build.targets\n+++ b/Directory.Build.targets\n@@ -68,18 +68,18 @@\n   &lt;ItemGroup Condition=\"'$(NativeTargetArchitecture)' == 'x64'and $([MSBuild]::IsOSPlatform('osx')) and '$(TestUsesLibTorch)' == 'true'  and '$(SkipNative)' != 'true' \"&gt;\n     &lt;NativeAssemblyReference Include=\"c10\" /&gt;\n     &lt;NativeAssemblyReference Include=\"caffe2_detectron_ops\" /&gt;\n-    &lt;NativeAssemblyReference Include=\"caffe2_module_test_dynamic\" /&gt;\n+    &lt;!-- &lt;NativeAssemblyReference Include=\"caffe2_module_test_dynamic\" /&gt; --&gt;\n     &lt;NativeAssemblyReference Include=\"caffe2_observers\" /&gt;\n-    &lt;NativeAssemblyReference Include=\"fbjni\" /&gt;\n-    &lt;NativeAssemblyReference Include=\"iomp5\" /&gt;\n-    &lt;NativeAssemblyReference Include=\"jitbackend_test\" /&gt;\n-    &lt;NativeAssemblyReference Include=\"pytorch_jni\" /&gt;\n+    &lt;!-- &lt;NativeAssemblyReference Include=\"fbjni\" /&gt; --&gt;\n+    &lt;!-- &lt;NativeAssemblyReference Include=\"iomp5\" /&gt; --&gt;\n+    &lt;!-- &lt;NativeAssemblyReference Include=\"jitbackend_test\" /&gt; --&gt;\n+    &lt;!-- &lt;NativeAssemblyReference Include=\"pytorch_jni\" /&gt; --&gt;\n     &lt;NativeAssemblyReference Include=\"shm\" /&gt;\n     &lt;NativeAssemblyReference Include=\"torch\" /&gt;\n     &lt;NativeAssemblyReference Include=\"torch_cpu\" /&gt;\n     &lt;NativeAssemblyReference Include=\"torch_global_deps\" /&gt;\n     &lt;NativeAssemblyReference Include=\"torch_python\" /&gt;\n-    &lt;NativeAssemblyReference Include=\"torchbind_test\" /&gt;\n+    &lt;!-- &lt;NativeAssemblyReference Include=\"torchbind_test\" /&gt; --&gt;\n   &lt;/ItemGroup&gt;\n \n   &lt;!-- Linux CPU libtorch binary list used for examples and testing --&gt;\ndiff --git a/global.json b/global.json\nindex 058794f..15f9eb4 100644\n--- a/global.json\n+++ b/global.json\n@@ -1,6 +1,6 @@\n {\n   \"sdk\": {\n-    \"version\": \"5.0.402\",\n+    \"version\": \"6.0.100\",\n     \"allowPrerelease\": true,\n     \"rollForward\": \"minor\"\n   }\ndiff --git a/src/Native/LibTorchSharp/CMakeLists.txt b/src/Native/LibTorchSharp/CMakeLists.txt\nindex 46b5c32..b1828cc 100644\n--- a/src/Native/LibTorchSharp/CMakeLists.txt\n+++ b/src/Native/LibTorchSharp/CMakeLists.txt\n@@ -1,8 +1,8 @@\n project(LibTorchSharp)\n \n if(APPLE)\n- include_directories(\"/usr/local/include\" \"/usr/local/opt/llvm/include\")\n- link_directories(\"/usr/local/lib\" \"/usr/local/opt/llvm/lib\")\n+ include_directories(\"/Users/lisa/Documents/llvm-project/build/install/include\")\n+ link_directories(\"/Users/lisa/Documents/llvm-project/build/install/lib\")\n endif()\n find_package(Torch REQUIRED PATHS ${LIBTORCH_PATH})\n \ndiff --git a/src/Redist/libtorch-cpu/libtorch-cpu.proj b/src/Redist/libtorch-cpu/libtorch-cpu.proj\nindex 4240fe1..3eeeabe 100644\n--- a/src/Redist/libtorch-cpu/libtorch-cpu.proj\n+++ b/src/Redist/libtorch-cpu/libtorch-cpu.proj\n@@ -41,21 +41,21 @@\n     &lt;File Include=\"libtorch\\lib\\uv.dll\" /&gt;\n   &lt;/ItemGroup&gt;\n   &lt;ItemGroup Condition=\"'$(TargetOS)' == 'mac'\"&gt;\n-    &lt;File Include=\"libtorch\\lib\\libbackend_with_compiler.dylib\" /&gt;\n+    &lt;!-- &lt;File Include=\"libtorch\\lib\\libbackend_with_compiler.dylib\" /&gt; --&gt;\n     &lt;File Include=\"libtorch\\lib\\libc10.dylib\" /&gt;\n     &lt;File Include=\"libtorch\\lib\\libcaffe2_detectron_ops.dylib\" /&gt;\n-    &lt;File Include=\"libtorch\\lib\\libcaffe2_module_test_dynamic.dylib\" /&gt;\n+    &lt;!-- &lt;File Include=\"libtorch\\lib\\libcaffe2_module_test_dynamic.dylib\" /&gt; --&gt;\n     &lt;File Include=\"libtorch\\lib\\libcaffe2_observers.dylib\" /&gt;\n-    &lt;File Include=\"libtorch\\lib\\libfbjni.dylib\" /&gt;\n-    &lt;File Include=\"libtorch\\lib\\libiomp5.dylib\" /&gt;\n-    &lt;File Include=\"libtorch\\lib\\libjitbackend_test.dylib\" /&gt;\n-    &lt;File Include=\"libtorch\\lib\\libpytorch_jni.dylib\" /&gt;\n+    &lt;!-- &lt;File Include=\"libtorch\\lib\\libfbjni.dylib\" /&gt; --&gt;\n+    &lt;!-- &lt;File Include=\"libtorch\\lib\\libiomp5.dylib\" /&gt; --&gt;\n+    &lt;!-- &lt;File Include=\"libtorch\\lib\\libjitbackend_test.dylib\" /&gt; --&gt;\n+    &lt;!-- &lt;File Include=\"libtorch\\lib\\libpytorch_jni.dylib\" /&gt; --&gt;\n     &lt;File Include=\"libtorch\\lib\\libshm.dylib\" /&gt;\n     &lt;File Include=\"libtorch\\lib\\libtorch.dylib\" /&gt;\n     &lt;File Include=\"libtorch\\lib\\libtorch_cpu.dylib\" /&gt;\n     &lt;File Include=\"libtorch\\lib\\libtorch_global_deps.dylib\" /&gt;\n     &lt;File Include=\"libtorch\\lib\\libtorch_python.dylib\" /&gt;\n-    &lt;File Include=\"libtorch\\lib\\libtorchbind_test.dylib\" /&gt;\n+    &lt;!-- &lt;File Include=\"libtorch\\lib\\libtorchbind_test.dylib\" /&gt; --&gt;\n   &lt;/ItemGroup&gt;\n   &lt;ItemGroup Condition=\"'$(TargetOS)' == 'linux'\"&gt;\n     &lt;File Include=\"libtorch\\lib\\libbackend_with_compiler.so\" /&gt;"
  },
  {
    "objectID": "posts/torchsharp.html#开始打包",
    "href": "posts/torchsharp.html#开始打包",
    "title": "关于如何在M1上使用TorchSharp",
    "section": "4. 开始打包",
    "text": "4. 开始打包\n执行dotnet pack --configuration release,注意他这里会自动下载libtorch的release包,但是修改他的下载的脚本又着实麻烦.所以我们要等到他下载好开始编译Native的时候按ctrl+c先中断,然后把我们的libtorch替换他解压出来的libtorch\ncd bin/obj/AnyCPU.Release/libtorch-cpu/libtorch-macos-1.10.0cpu/\nrm -rf libtorch\ncp -r ~/Documents/libtorch .  \n然后再执行dotnet pack --configuration release最后打包出来."
  },
  {
    "objectID": "posts/torchsharp.html#配置nuget配置",
    "href": "posts/torchsharp.html#配置nuget配置",
    "title": "关于如何在M1上使用TorchSharp",
    "section": "5. 配置nuget配置",
    "text": "5. 配置nuget配置\n我们要把编译生成的路径作为nuget的一个源,同时为了避免冲突还得关闭原来的源, 所以在项目路径下添加NuGet.Config文件\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;configuration&gt;\n  &lt;packageSources&gt;\n    &lt;add key=\"nuget.org\" value=\"https://api.nuget.org/v3/index.json\" protocolVersion=\"3\" /&gt;\n    &lt;add key=\"m1\" value=\"/Users/lisa/Documents/TorchSharp/bin/packages/Release\" /&gt;\n  &lt;/packageSources&gt;\n  &lt;activePackageSource&gt;\n    &lt;add key=\"m1\" value=\"/Users/lisa/Documents/TorchSharp/bin/packages/Release\" /&gt;\n  &lt;/activePackageSource&gt;\n  &lt;disabledPackageSources&gt;\n    &lt;add key=\"nuget.org\" value=\"true\" /&gt;\n  &lt;/disabledPackageSources&gt;\n&lt;/configuration&gt;"
  },
  {
    "objectID": "posts/torchsharp.html#编译项目",
    "href": "posts/torchsharp.html#编译项目",
    "title": "关于如何在M1上使用TorchSharp",
    "section": "6. 编译项目",
    "text": "6. 编译项目\n执行dotnet build -v:d &gt; log, 我这里在log 文件中就可以看到如下内容\n正在将文件从“/Users/lisa/.nuget/packages/libtorch-cpu-osx-x64/1.10.0.1/runtimes/osx-x64/native/libcaffe2_observers.dylib”复制到“/Users/lisa/Documents/nncase/bin/Nncase.Tests/net6.0/runtimes/osx-x64/native/libcaffe2_observers.dylib”。\n正在将文件从“/Users/lisa/.nuget/packages/libtorch-cpu-osx-x64/1.10.0.1/runtimes/osx-x64/native/libcaffe2_detectron_ops.dylib”复制到“/Users/lisa/Documents/nncase/bin/Nncase.Tests/net6.0/runtimes/osx-x64/native/libcaffe2_detectron_ops.dylib”。\n正在将文件从“/Users/lisa/.nuget/packages/libtorch-cpu-osx-x64/1.10.0.1/runtimes/osx-x64/native/libc10.dylib”复制到“/Users/lisa/Documents/nncase/bin/Nncase.Tests/net6.0/runtimes/osx-x64/native/libc10.dylib”。\n我们检查dylib的格式, 如果是arm64就说明是ok的. 如果还是x86的话, 是因为替换libtorch的时机晚了,他是在src/Redist/libtorch-cpu/libtorch-cpu.proj中CopyFilesFromArchive执行的.\n❯ file /Users/lisa/Documents/nncase/bin/Nncase.Tests/net6.0/runtimes/osx-x64/native/libcaffe2_observers.dylib\n/Users/lisa/Documents/nncase/bin/Nncase.Tests/net6.0/runtimes/osx-x64/native/libcaffe2_observers.dylib: Mach-O 64-bit dynamically linked shared library arm64"
  },
  {
    "objectID": "posts/torchsharp.html#修复路径问题",
    "href": "posts/torchsharp.html#修复路径问题",
    "title": "关于如何在M1上使用TorchSharp",
    "section": "7. 修复路径问题",
    "text": "7. 修复路径问题\n这里还是有个问题, 那就是TorchSharp默认会从TorchSharp.dll的路径加载libtorch.dylib,但是我生成出来的包,默认是把libtorch.dylib放到net6.0/runtimes/osx-x64/native路径下,导致默认找不到这个libtorch. 手动解决到也简单\ncd bin/Nncase.Tests/net6.0/\nmv runtimes/osx-x64/native/* ."
  },
  {
    "objectID": "posts/tile-flow.html",
    "href": "posts/tile-flow.html",
    "title": "TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis",
    "section": "",
    "text": "学习TileFlow这篇论文中是如何进行多个内存层级的tiling."
  },
  {
    "objectID": "posts/tile-flow.html#arch.yaml",
    "href": "posts/tile-flow.html#arch.yaml",
    "title": "TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis",
    "section": "1.1 arch.yaml",
    "text": "1.1 arch.yaml\n描述了整个芯片的架构层级.\narchitecture: \n  version: 0.2 \n\n  subtree:\n  - name: System\n    \n    local: \n    - name: MainMemory\n      class: DRAM \n      attributes:\n        block-size: 16384\n        depth: 1\n        word-bits: 16\n        read_bandwidth: 4.3\n        write_bandwidth: 2.9\n      \n    subtree: \n    - name: Buffer \n    \n      local:  \n      - name: Cache \n        class: SRAM\n        attributes:\n          word-bits: 16\n          block_size: 16384\n          depth: 3\n          read_bandwidth: 52\n          write_bandwidth: 20 # 16 \n\n\n      subtree:\n      - name: PE\n\n        local: \n        - name: RegFile[0..255] \n          class: regfile\n          attributes:\n            meshX: 16\n            meshY: 16\n            depth: 1\n            block_size: 3\n            word-bits: 16\n            read_bandwidth: 3.2\n            write_bandwidth: 3.2\n\n        - name: mac[0..255] \n          class: intmac \n          attributes: \n            word-bits: 16\n            meshX: 16\n            meshY: 16"
  },
  {
    "objectID": "posts/tile-flow.html#prob.yaml",
    "href": "posts/tile-flow.html#prob.yaml",
    "title": "TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis",
    "section": "1.2 prob.yaml",
    "text": "1.2 prob.yaml\n这里其实类似halide, 需要列出所有共享的迭代变量, 然后下面每个算子都是用这些维度来构建. 具体可以参考这里.\nproblem:\n  io:\n    ins: A, B, D\n    outs: E\n  dimensions: [M,N,K,L]\n  instance:\n    M: M \n    N: N\n    L: L\n    K: K\n\n  ops:\n  - name: GEMM1\n    dimensions: [M,L,K] \n    data-spaces:\n    - name: C \n      projection:\n        - [[M]] \n        - [[L]] \n      read-write: True \n    - name: A \n      projection:\n        - [[M]]\n        - [[K]]\n    - name: B\n      projection:\n        - [[K]]\n        - [[L]]\n    ins: A, B\n    out: C\n    \n  - name: GEMM2 \n    dimensions: [M,L,N]\n    data-spaces:\n    - name: E \n      projection: \n        - [[M]]\n        - [[N]]\n      read-write: True \n    - name: C\n      projection: \n        - [[M]]\n        - [[L]]\n    - name: D \n      projection: \n        - [[L]]\n        - [[N]]\n    ins: C, D\n    out: E \n对应的计算如下所示:\nM = 512\nN = 64\nK = 64 \nL = 512\nC[M,L] = A[M,K] @ B[K,L]\nE[M,N] = C[M,L] @ D[L,N]"
  },
  {
    "objectID": "posts/tile-flow.html#map.yaml",
    "href": "posts/tile-flow.html#map.yaml",
    "title": "TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis",
    "section": "1.3 map.yaml",
    "text": "1.3 map.yaml\nmap是一个比较重要的配置, 这里重点说明一下. type分为temporal表示顺序执行和spatial表示并行执行. factors: M = MO N = NO K= KO表示它将M/N/K维度分别分为MO/NO/KO块.permutation: NMK表示循环从内到外分别为NMK. 这里的factors: M=MM K=KM N=NI表示再次切分这里的三个维度. multicast: true表示多播. split: 1表示映射到硬件xy.\n原始文档参考这里.\nmapping:\n  node-type: Tile \n  type: temporal \n  factors: M=MO \n  target: MainMemory \n\n  subtree: \n  - node-type: Scope \n    type: Sequential \n\n    subtree:\n    - node-type: Tile \n      factors: K=KO L=LO\n      type: temporal \n      bypass: [C]\n      target: MainMemory\n      profile: False\n      tag: op1\n      \n\n      subtree: \n      - node-type: Tile \n        type: temporal \n        factors: K=KM L=LI M=MM\n        permutation: LMK\n        target: Cache \n        tag: op1 \n        \n        subtree:\n        - node-type: Tile\n          type: Spatial \n          factors: M=SX K=SY\n          split: 1\n          permutation: MK\n          target: Cache\n          tag: op1 \n          \n          subtree: \n          - node-type: Tile \n            type: temporal  \n            factors: M=1 L=1 K=1\n            permutation: MLK \n            target: RegFile\n            tag: op1 \n            \n            subtree:\n            - node-type: Op\n              name: GEMM1 \n\n      # A common spatial tile\n    - node-type: Tile\n      type: temporal \n      factors: L=LO N=NO\n      target: MainMemory\n      profile: False\n      bypass: [C]\n      tag: op2\n\n      subtree: \n      - node-type: Tile \n        type: temporal \n        factors: M=MM L=LM N=NI\n        permutation: NML \n        target: Cache\n        tag: op2\n        \n        subtree:\n        - node-type: Tile\n          type: Spatial \n          factors: M=SX L=SY\n          split: 1\n          permutation: ML\n          target: Cache\n          tag: op2\n\n          subtree:\n          - node-type: Tile \n            type: temporal \n            factors: M=1 L=1 N=1 \n            permutation: MLN \n            target: RegFile\n            tag: op2\n            \n            subtree: \n            - node-type: Op\n              name: GEMM2\n之前tile flow, 可以得到初始化时的一些关键信息:\n-----------------Mapping---------------\nroot: 0xa23020\nread: A B E D update: E \nfor M in [0:MO), MainMemory\n  read: A B E D update: E fill: A B E D write-back: E \n  Scope: Sequential{\n    read: A B fill: A B \n    for K in [0:KO), MainMemory\n      for L in [0:LO), MainMemory\n        read: C A B update: C fill: A B \n        for K in [0:KM), Cache\n          for M in [0:MM), Cache\n            for L in [0:LI), Cache\n              read: C A B update: C fill: C A B write-back: C \n              for K in [0:16) (Spatial-Y), Cache\n                for M in [0:16) (Spatial-X), Cache\n                  read: C A B update: C fill: C A B write-back: C \n                  for K in [0:1), RegFile\n                    for L in [0:1), RegFile\n                      for M in [0:1), RegFile\n                        read: C A B update: C fill: C A B write-back: C \n                        Op: GEMM1(A,B,)-&gt;C\n\n    read: E D update: E fill: E D write-back: E \n    for L in [0:LO), MainMemory\n      for N in [0:NO), MainMemory\n        read: C E D update: E fill: E D write-back: E \n        for L in [0:LM), Cache\n          for M in [0:MM), Cache\n            for N in [0:NI), Cache\n              read: C E D update: E fill: C E D write-back: E \n              for L in [0:16) (Spatial-Y), Cache\n                for M in [0:16) (Spatial-X), Cache\n                  read: C E D update: E fill: C E D write-back: E \n                  for N in [0:1), RegFile\n                    for L in [0:1), RegFile\n                      for M in [0:1), RegFile\n                        read: C E D update: E fill: C E D write-back: E \n                        Op: GEMM2(C,D,)-&gt;E\n\n  }\n---------------------------------------\nconstraints:\n        KO*KM==4        #  loopcount constraint for tiling of dim K\n        LO*LI==512      #  loopcount constraint for tiling of dim L\n        MO*MM==32       #  loopcount constraint for tiling of dim M\n        LO*LM==32       #  loopcount constraint for tiling of dim L\n        MO*MM==32       #  loopcount constraint for tiling of dim M\n        NO*NI==64       #  loopcount constraint for tiling of dim N\n        (1*1*1*1+1*1*1*1+1*1*1*1)&lt;=3    # Memory constraint at Tile::op1::RegFile::Temporal\n        (Max(MM*16*1*1,MM*16*1*1)*Max(LO*LI*1*1,LO*LM*16*1*1)+MM*16*1*1*KM*16*1*1+KM*16*1*1*LI*1*1)&lt;=131072     # Memory constraint at Tile::op1::Cache::Temporal\n        (1*1*1*1+1*1*1*1+1*1*1*1)&lt;=3    # Memory constraint at Tile::op2::RegFile::Temporal\n        (Max(MM*16*1*1,MM*16*1*1)*Max(LO*LI*1*1,LO*LM*16*1*1)+MM*16*1*1*NI*1*1+LM*16*1*1*NI*1*1)&lt;=131072        # Memory constraint at Tile::op2::Cache::Temporal\n        (MO*Max(MM*16*1*1,MM*16*1*1)*Max(KO*KM*16*1*1,1)+Max(KO*KM*16*1*1,1)*Max(LO*LI*1*1,LO*LM*16*1*1)+MO*Max(MM*16*1*1,MM*16*1*1)*Max(1,NO*NI*1*1)+Max(LO*LI*1*1,LO*LM*16*1*1)*Max(1,NO*NI*1*1))&lt;=524288   # Memory constraint at Tile::MainMemory::Temporal\n        &lt;16, 16&gt; &lt;= &lt;16, 16&gt;      # Resource constraint at Tile::op1::Cache::Spatial\n        &lt;16, 16&gt; &lt;= &lt;16, 16&gt;      # Resource constraint at Tile::op2::Cache::Spatial\n        Max(&lt;1, 1&gt;,&lt;1, 1&gt;) &lt;= &lt;1, 1&gt;      # Resource constraint at Scope::Sequential\n==============Checker END================\n其实我对于tileflow最好奇的一点是 temporal buffer是在哪个内存层级申请的, 通过上面这个constraints很好的解释了这一点. 他应该是在每个tile node上都会开buffer, 对于op1在cache上的这个node上设定了c为pypass, 因此他的大小计算为C[MM*16,L], 而其他两个buffer就是根据factor来计算的:A[MM*16,KM*16]以及B[KM*16,LI]."
  },
  {
    "objectID": "posts/tile-flow.html#执行结果",
    "href": "posts/tile-flow.html#执行结果",
    "title": "TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis",
    "section": "1.4 执行结果",
    "text": "1.4 执行结果\n这里应该是只搜索了buffer size.\n***Optimal Mapping:\n-----------------Nest Analysis----------------\nTile::MainMemory::Temporal,\nstrides,low,high:MNKL[4]: 128 64 64 512 ,[4]: 0 0 0 0 ,[4]: 511 63 63 511 \nread: A B E D update: E \nfor M in [0:MO(4)), MainMemory\n   read: A B E D update: E fill: A B E D write-back: E \n   Scope: Sequential\n   {\n      Tile::op1::MainMemory::Temporal,\n      strides,low,high:MNKL[4]: 128 1 32 512 ,[4]: 0 0 0 0 ,[4]: 127 0 63 511 \n      read: A B fill: A B \n      for K in [0:KO(2)), MainMemory\n        for L in [0:LO(1)), MainMemory\n            Tile::op1::Cache::Temporal,\n            strides,low,high:MNKL[4]: 128 1 16 512 ,[4]: 0 0 0 0 ,[4]: 127 0 31 511 \n            read: C A B update: C fill: A B \n            for K in [0:KM(2)), Cache\n              for M in [0:MM(8)), Cache\n                for L in [0:LI(512)), Cache\n                     Tile::op1::Cache::Spatial,\n                     strides,low,high:MNKL[4]: 16 1 1 1 ,[4]: 0 0 0 0 ,[4]: 15 0 15 0 \n                     read: C A B update: C fill: C A B write-back: C \n                     for K in [0:16) (Spatial-Y), Cache\n                       for M in [0:16) (Spatial-X), Cache\n                           Tile::op1::RegFile::Temporal,\n                           strides,low,high:MNKL[4]: 1 1 1 1 ,[4]: 0 0 0 0 ,[4]: 0 0 0 0 \n                           read: C A B update: C fill: C A B write-back: C \n                           for K in [0:1), RegFile\n                             for L in [0:1), RegFile\n                               for M in [0:1), RegFile\n                                    read: C A B update: C fill: C A B write-back: C \n                                    Op: GEMM1(A,B,)-&gt;C\n\n                                    repFactor:0\n                                    accesses:0\n                                    expanison:16,16\n      Tile::op2::MainMemory::Temporal,\n      strides,low,high:MNKL[4]: 128 64 1 512 ,[4]: 0 0 0 0 ,[4]: 127 63 0 511 \n      read: E D update: E fill: E D write-back: E \n      for L in [0:LO(1)), MainMemory\n        for N in [0:NO(1)), MainMemory\n            Tile::op2::Cache::Temporal,\n            strides,low,high:MNKL[4]: 128 64 1 16 ,[4]: 0 0 0 0 ,[4]: 127 63 0 511 \n            read: C E D update: E fill: E D write-back: E \n            for L in [0:LM(32)), Cache\n              for M in [0:MM(8)), Cache\n                for N in [0:NI(64)), Cache\n                     Tile::op2::Cache::Spatial,\n                     strides,low,high:MNKL[4]: 16 1 1 1 ,[4]: 0 0 0 0 ,[4]: 15 0 0 15 \n                     read: C E D update: E fill: C E D write-back: E \n                     for L in [0:16) (Spatial-Y), Cache\n                       for M in [0:16) (Spatial-X), Cache\n                           Tile::op2::RegFile::Temporal,\n                           strides,low,high:MNKL[4]: 1 1 1 1 ,[4]: 0 0 0 0 ,[4]: 0 0 0 0 \n                           read: C E D update: E fill: C E D write-back: E \n                           for N in [0:1), RegFile\n                             for L in [0:1), RegFile\n                               for M in [0:1), RegFile\n                                    read: C E D update: E fill: C E D write-back: E \n                                    Op: GEMM2(C,D,)-&gt;E\n\n                                    repFactor:0\n                                    accesses:0\n                                    expanison:16,16\n   }\nCycle: 140288, Energy: 4.18771e+08\n--------------END Nest Analysis---------------"
  },
  {
    "objectID": "posts/tfp-ch3.html",
    "href": "posts/tfp-ch3.html",
    "title": "概率模型第三章 ： MCMC",
    "section": "",
    "text": "Tensorflow 概率模型学习，代码运行于Tensorflow 1.14，文字半机器翻译。"
  },
  {
    "objectID": "posts/tfp-ch3.html#打开mcmc的黑匣子",
    "href": "posts/tfp-ch3.html#打开mcmc的黑匣子",
    "title": "概率模型第三章 ： MCMC",
    "section": "打开MCMC的黑匣子",
    "text": "打开MCMC的黑匣子\n前两章隐藏了TFP的内部机制，更常见的是来自读者的Markov Chain Monte Carlo（MCMC）。包含本章的原因有三个方面。首先，任何关于贝叶斯推理的书都必须讨论MCMC。我不能打这个。责备统计学家。其次，了解MCMC的过程可以让您深入了解您的算法是否已融合（融合到什么？我们将达到这个目标）。第三，我们将理解为什么我们从后验返回了数千个样本作为解决方案，起初认为这可能是奇怪的。\n\n贝叶斯图像\n当我们用\\(N\\)未知数设置贝叶斯推理问题时，我们隐含地为先前的分布创建\\(N\\)维空间。与空间相关联是另一个维度，我们可以将其描述为曲面，或曲线，位于空间的顶部，反映特定点的先验概率。空间的表面由我们先前的分布定义。例如，如果我们有两个未知数\\(p_1\\)和\\(p_2\\)，并且两者的先验都是\\(\\text{Uniform}(0,5)\\)，则创建的空间是长度为5的正方形，表面是平面位于正方形顶部（表示每个点都有可能）。\nx_ = y_ = np.linspace(0., 5., 100., dtype=np.float32)\nX_, Y_ = evaluate(tf.meshgrid(x_, y_))\n\nuni_x_ = evaluate(tfd.Uniform(low=0., high=5.).prob(x_))\nm_ = np.median(uni_x_)\n\nuni_y_ = evaluate(tfd.Uniform(low=0., high=5.).prob(y_))\nM_ = evaluate(tf.matmul(tf.expand_dims(uni_x_, 1), tf.expand_dims(uni_y_, 0)))\n\nplt.figure(figsize(12.5, 6))\njet = plt.cm.jet\nfig = plt.figure()\nplt.subplot(121)\n \nim = plt.imshow(M_, interpolation='none', origin='lower',\n                cmap=jet, vmax=1, vmin=-.15, extent=(0, 5, 0, 5))\n\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.title(\"平均分布先验图像.\")\n \nax = fig.add_subplot(122, projection='3d')\nax.plot_surface(X_, Y_, M_, cmap=plt.cm.jet, vmax=1, vmin=-.15)\nax.view_init(azim=390)\nplt.title(\"平均分布先验图像; 备用视图\");\n&lt;Figure size 900x432 with 0 Axes&gt;\n\n或者，如果两个先验是$ （3）\\(和\\) （10）$，则该空间是2-D平面上的所有正数，并且由先验引起的表面看起来像一个从（0,0）点开始的水落，并流过正数。\n下面的图表显示了这一点。越 红的颜色,表明越大的先验概率被分配到这个位置，反过来越 蓝 的地方表示我们分配的先验概率越小。\nexp_x_ = evaluate(tfd.Exponential(rate=(1./3.)).prob(x_))\nexp_y_ = evaluate(tfd.Exponential(rate=(1./10.)).prob(y_))\n\nM_ = evaluate(tf.matmul(tf.expand_dims(exp_x_, 1), tf.expand_dims(exp_y_, 0)))\n\nplt.figure(figsize(12.5, 6))\njet = plt.cm.jet\nfig = plt.figure()\nplt.subplot(121)\nCS = plt.contour(X_, Y_, M_)\nim = plt.imshow(M_, interpolation='none', origin='lower',\n                cmap=jet, extent=(0, 5, 0, 5))\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.title(r\"$Exp(3), Exp(10)$ 先验图像\")\n\nax = fig.add_subplot(122, projection='3d')\nax.plot_surface(X_, Y_, M_, cmap=plt.cm.jet)\nax.view_init(azim=30)\nplt.title(r\"$Exp(3), Exp(10)$ 先验图像; 备用视角\");\n&lt;Figure size 900x432 with 0 Axes&gt;\n\n这些是2D空间中的简单示例，我们的大脑可以很好地理解表面。在实践中，由我们的先验可以生成的空间或表面等更高维度。\n如果这些表面描述了未知数的先前分布，那么在我们合并观察到的数据\\(X\\)后，我们的空间会发生什么？数据\\(X\\)不会改变空间，但它通过拉动和拉伸先前曲面的结构来改变空间的表面，以反映真实参数可能存在的位置。更多的数据意味着更多的拉伸和拉伸，与新形成的形状相比，我们的原始形状变得严重或微不足道。数据越少，我们的原始形状就越多。无论如何，得到的表面描述了后验分布。\n我必须再次强调，遗憾的是，不可能在大尺寸上将其可视化。对于两个维度，数据基本上向上推原始表面以形成高山。通过先验概率分布检查观测数据推高某些区域中的后验概率的趋势，因此较少的先验概率意味着更大的阻力。因此，在上面的双指数先验案例中，可能在（0,0）角附近爆发的山（或多个山）将比远离（5,5）的山脉高得多，因为那里在（5,5）附近有更大的阻力（低先验概率）。峰值反映了可能找到真实参数的后验概率。重要的是，如果先验已经指定概率为0，则不会在那里分配后验概率。\n假设上面提到的先验代表两个泊松分布的不同参数$ $。我们观察了一些数据点并可视化新的图像：\n\n# 创建观察变量\n# 尝试改变采样数观察结果\nN = 2 #param {type:\"slider\", min:1, max:15, step:1}\n\n# 真正的参数，但当然我们没有看到这些值......\nlambda_1_true = float(1.)\nlambda_2_true = float(3.)\n\n#...依据真实参数生成对应的分布\ndata = tf.concat([\n    tfd.Poisson(rate=lambda_1_true).sample(sample_shape=(N, 1), seed=4),\n    tfd.Poisson(rate=lambda_2_true).sample(sample_shape=(N, 1), seed=8)\n], axis=1)\ndata_ = evaluate(data)\nprint(\"observed (2-dimensional,sample size = %d): \\n\" % N, data_)\n\n# plotting details.\nx_ = y_ = np.linspace(.01, 5, 100)\n\nlikelihood_x = tf.math.reduce_prod(tfd.Poisson(rate=x_).prob(data_[:,0][:,tf.newaxis]),axis=0)\nlikelihood_y = tf.math.reduce_prod(tfd.Poisson(rate=y_).prob(data_[:,1][:,tf.newaxis]),axis=0)\n\nL_ = evaluate(tf.matmul(likelihood_x[:,tf.newaxis],likelihood_y[tf.newaxis,:]))\nobserved (2-dimensional,sample size = 2): \n [[3. 4.]\n [1. 3.]]\nplt.figure(figsize(12.5, 15.0))\n# matplotlib heavy lifting below, beware!\n\n# SUBPLOT for regular Uniform\nplt.subplot(221)\n\nuni_x_ = evaluate(tfd.Uniform(low=0., high=5.).prob(tf.cast(x_,dtype=tf.float32)))\nm = np.median(uni_x_[uni_x_ &gt; 0])\nuni_x_[uni_x_ == 0] = m\nuni_y_ = evaluate(tfd.Uniform(low=0., high=5.).prob(tf.cast(y_,dtype=tf.float32)))\nm = np.median(uni_y_[uni_y_ &gt; 0])\nuni_y_[uni_y_ == 0] = m\n\nM_ = evaluate(tf.matmul(tf.expand_dims(uni_x_, 1), tf.expand_dims(uni_y_, 0)))\n\nim = plt.imshow(M_, interpolation='none', origin='lower',\n                cmap=jet, vmax=1, vmin=-.15, extent=(0, 5, 0, 5))\nplt.scatter(lambda_2_true, lambda_1_true, c=\"k\", s=50, edgecolor=\"none\")\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.title(r\"$p_1, p_2$平均先验图像\")\n\n# SUBPLOT for Uniform + Data point\nplt.subplot(223)\nplt.contour(x_, y_, M_ * L_)\nim = plt.imshow(M_ * L_, interpolation='none', origin='lower',\n                cmap=jet, extent=(0, 5, 0, 5))\nplt.title(\"图像被%d个数据而扭曲;\\n $p_1, p_2$.的平均先验\" % N)\nplt.scatter(lambda_2_true, lambda_1_true, c=\"k\", s=50, edgecolor=\"none\")\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n# SUBPLOT for regular Exponential\nplt.subplot(222)\nexp_x_ = evaluate(tfd.Exponential(rate=.3).prob(tf.to_float(x_)))\nexp_x_[np.isnan(exp_x_)] = exp_x_[1]\nexp_y_ = evaluate(tfd.Exponential(rate=.10).prob(tf.to_float(y_)))\nexp_y_[np.isnan(exp_y_)] = exp_y_[1]\nM_ = evaluate(tf.matmul(tf.expand_dims(exp_x_, 1), tf.expand_dims(exp_y_, 0)))\nplt.contour(x_, y_, M_)\nim = plt.imshow(M_, interpolation='none', origin='lower',\n                cmap=jet, extent=(0, 5, 0, 5))\nplt.scatter(lambda_2_true, lambda_1_true, c=\"k\", s=50, edgecolor=\"none\")\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.title(\"$p_1, p_2$的指数先验图像\")\n\n# SUBPLOT for Exponential + Data point\nplt.subplot(224)\n# This is the likelihood times prior, that results in the posterior.\nplt.contour(x_, y_, M_ * L_)\nim = plt.imshow(M_ * L_, interpolation='none', origin='lower',\n                cmap=jet, extent=(0, 5, 0, 5))\n\nplt.scatter(lambda_2_true, lambda_1_true, c=\"k\", s=50, edgecolor=\"none\")\nplt.title(\"图像被%d个数据扭曲;\\n $p_1, p_2$的指数先验\" % N)\nplt.xlim(0, 5)\nplt.ylim(0, 5);\n\n左边的图是带有$ （0,5）$ 先验的变形图像，右边的图是带有指数先验的变形景观。请注意，后方景观看起来彼此不同，尽管观察到的数据在两种情况下都是相同的。原因如下。注意指数先验的图像，右上图，在图的右上角的值上放置非常小的后验权重：这是因为先验在那里没有太大的权重。另一方面，均匀先验的图像很乐意将后部权重放在右上角，因为之前的重量更重。\n另请注意，对应最暗的红色的最高点在指数情况下偏向于（0,0），这是指数先验在（0,0）角落放置更多先前权重的结果。\n黑点代表真实的参数。即使有1个样本点，山也会尝试包含真实参数。当然，样本大小为“1”的推断非常幼稚，选择如此小的样本量只是说明性的。\n尝试将样本大小更改为其他值（尝试2,5,10,100？…）并观察我们的山峰后验如何变化是一个很好的练习。\n\n\n使用MCMC探索图像\n我们应该探索我们先验表面和产生的变形后图像和观测数据，以找到后验。但是，我们不能天真地搜索空间：任何计算机科学家都会告诉你，在\\(N\\)中遍历\\(N\\)维空间是指数级的难度：当我们增加\\(N\\)时，空间的大小很快就会爆炸（the curse of dimensionality）。我们有什么希望找到这些隐藏的山脉？ MCMC背后的想法是对空间进行智能搜索。说“搜索”意味着我们正在寻找一个特定的点，这可能不准确，因为我们真的在寻找一座宽阔的山峰。\n回想一下，MCMC从后验分布返回样本，而不是分布本身。将我们的山区类比延伸至极限，MCMC执行类似于反复询问“我发现这块鹅卵石有多大可能来自我正在寻找的山脉？”的任务，并通过返回数千个可接受的鹅卵石来完成其任务，以期重建原始的山。\n当我说MCMC智能搜索时，我真的在说MCMC将希望会聚到高后验概率区域。 MCMC通过探索附近的位置并进入更高概率的区域来做到这一点。同样，也许“收敛”并不是描述MCMC进展的准确术语。收敛通常意味着朝向空间中的一个点移动，但是MCMC向空间中的更宽的区域移动并随机地在该区域中行走，从该区域拾取样本。\n\n\n为什么数以千计的样本？\n首先，向用户返回数千个样本可能听起来像是描述后验分布的低效方式。我认为这非常有效。考虑其他可能性：\n\n返回“山脉”的数学公式将涉及描述具有任意峰和谷的N维表面。\n返回图像的“峰值”，虽然在数学上可能并且作为最高点的合理事情对应于未知数的最可能估计，但忽略了景观的形状，我们之前认为这对于确定后验置信度非常重要在未知数。\n\n除了计算原因，返回样本的最有力理由可能是我们可以轻松使用大数定律来解决其他棘手的问题。我推迟了下一章的讨论。通过数千个样本，我们可以通过在直方图中组织它们来重建后表面。\n\n\n执行MCMC的算法\n有一大系列算法可以执行MCMC。这些算法中的大多数可以高级表达如下:(数学细节可以在附录中找到。）\n\n从当前位置开始。\n建议搬到新的位置（调查你附近的鹅卵石）。\n根据位置对数据和先前分布的遵守情况接受/拒绝新位置（询问卵石是否可能来自山区）。\n\n如果您接受：移动到新位置。返回第1步。\n否则：不要搬到新的位置。返回第1步。\n\n经过大量迭代后，返回所有接受的位置。\n\n这样，我们朝向存在后验分布的区域的大致方向移动，并且在旅程中节省地收集样本。一旦我们到达后验分布，我们就可以轻松地收集样本，因为它们可能都属于后验分布。\n如果MCMC算法的当前位置处于极低概率的区域（这通常是算法开始时的情况（通常在空间中的随机位置），则算法将移动到可能不是来自后验但比附近的其他所有更好。因此算法的第一次移动不反映后验。\n在上述算法的伪代码中，请注意只有当前位置很重要（仅在当前位置附近调查新位置）。我们可以将这个属性描述为无记忆，即算法不关心它如何到达当前位置，只是它在那里。\n\n\n后验的其他近似解\n除MCMC外，还有其他程序可用于确定后验分布。拉普拉斯近似是使用简单函数的后验近似。更先进的方法是变分贝叶斯.\n所有三种方法，拉普拉斯近似，变分贝叶斯和经典MCMC都有其优缺点。我们在本书中只关注MCMC。"
  },
  {
    "objectID": "posts/tfp-ch3.html#example-使用混合模型的无监督聚类",
    "href": "posts/tfp-ch3.html#example-使用混合模型的无监督聚类",
    "title": "概率模型第三章 ： MCMC",
    "section": "Example: 使用混合模型的无监督聚类",
    "text": "Example: 使用混合模型的无监督聚类\n假设我们得到以下数据集：\nreset_sess()\n\nimport wget\nurl = 'https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/data/mixture_data.csv'\nfilename = wget.download(url)\nfilename\n'mixture_data.csv'\nplt.figure(figsize(12.5, 6))\ndata_ = np.loadtxt(\"mixture_data.csv\", delimiter=\",\")\n\nplt.hist(data_, bins=20, color=\"k\", histtype=\"stepfilled\", alpha=0.8)\nplt.title(\"数据集的直方图\")\nplt.ylim([0, None]);\nprint(data_[:10], \"...\")\n[115.85679142 152.26153716 178.87449059 162.93500815 107.02820697\n 105.19141146 118.38288501 125.3769803  102.88054011 206.71326136] ...\n\n数据表明了什么？看起来数据具有双峰形式，也就是说，它似乎有两个峰值，一个接近120，另一个接近200。也许在这个数据集中有两个簇。\n该数据集是上一章中数据生成建模技术的一个很好的例子。我们可以提出如何创建数据。我建议使用以下数据生成算法：\n\n对于每个数据点，选择概率为\\(p\\)的集群1，否则选择集群2。\n使用参数$ _i \\(和\\) _i \\(从Normal分布中绘制随机变量，其中\\) i $在步骤1中选择。\n重复.\n\n该算法将产生与观察数据集类似的效果，因此我们选择此作为我们的模型。当然，我们不知道\\(p\\)或Normal分布的参数。因此，我们必须推断或学习这些未知数。\n表示正态分布$ _0 \\(和\\) _1 \\(。两者目前都有未知的均值和标准差，表示为\\) _i \\(和\\) _i，; i = 0,1 \\(。特定数据点可以来自\\) _0 \\(或\\) _1 \\(，我们假设数据点以\\) p \\(的概率分配给\\) _0 $。\n将数据点分配给集群的适当方法是使用TF分类变量它的参数是一个长为$ k \\(概率数组，必须总和为1，它的`value`属性是一个在0和\\)k-1\\(之间的整数，根据精心设计的概率数据随机选择（在我们的例子中为\\) K = 2 \\(）。 对于*先验*，我们不知道分配给集群1的概率是多少，所以我们在\\)（0,1）\\(上形成一个统一变量。我们称之为\\)p_1\\(，因此属于群集2的概率为\\)p_2 = 1 - p_1$。\n幸运的是，我们可以将[p1，p2]赋予我们的Categorical变量。如果需要，我们也可以使用tf.stack（）将$ p_1 \\(和\\) p_2 $组合成一个它能理解的向量。我们将此向量传递给Categorical变量，以便了解从多个分布中进行选择的可能性。\np1 = tfd.Uniform(name='p', low=0., high=1.).sample()  # p1 概率\np2 = 1 - p1 # # p1 概率\np = tf.stack([p1, p2]) # 串联\n\nrv_assignment = tfd.Categorical(name=\"assignment\",probs=p)  #\nassignment = rv_assignment.sample(sample_shape=data_.shape[0])\n\n[p_,assignment_] = evaluate([p,assignment])\n\nprint(\"先验分配, with p = %.2f:\" % p_[0])\nprint (assignment_[:10])\n\n先验分配, with p = 0.16: [0 1 0 0 1 0 1 0 1 1]\n\n看看上面的数据集，我猜想两个法线的标准偏差是不同的。为了保持对标准偏差的未知，我们最初将它们模型化为0到100。我们将使用单行TFP代码在我们的模型中包含两个标准偏差：\nrv_sds = tfd.Uniform(name=\"rv_sds\", low=[0., 0.], high=[100., 100.])\n在这里，我们使用批量形状2，创建两个独立的分布，恰好具有相同的参数。查看 colab on TFP shapes 得到更多信息.\n我们还需要在集群的中心指定先验。这些正常分布中的中心实际上是\\(\\mu\\)参数。他们的先验可以通过正态分布建模。看看这些数据，我知道这两个中心可能在哪里；虽然我对这些看起来的估计并不十分自信，但我猜的分别在120左右和190左右。因此我将设置\\(\\mu_0 = 120,\\mu_1 = 190\\)和\\(\\sigma_0 = \\sigma_1 = 10\\)。\n最后，我们使用了MixtureSameFamily 分布以实现我们的两个正态分布的混合, 使用我们的 Categorical 分布作为我们的选择功能。\nrv_sds = tfd.Uniform(name=\"rv_sds\", low=[0., 0.], high=[100., 100.])\nprint (str(rv_sds))\n\nrv_centers = tfd.Normal(name=\"rv_centers\", loc=[120., 190.], scale=[10., 10.])\n    \nsds = rv_sds.sample()\nprint (\"shape of sds sample:\",sds.shape)\ncenters = rv_centers.sample()\n\nrv_assignments = tfd.Categorical(probs=tf.stack([0.4, 0.6]))\nassignments = rv_assignments.sample(sample_shape=10)\n\n# and to combine it with the observations:\nrv_observations = tfd.MixtureSameFamily(\n    mixture_distribution=rv_assignments,\n    components_distribution=tfd.Normal(\n        loc=centers,\n        scale=sds))\n\nobservations = rv_observations.sample(sample_shape=10)\n\n[    \n    assignments_,\n    observations_,\n    sds_,\n    centers_\n] = evaluate([\n    assignments,\n    observations,\n    sds,\n    centers\n])\n\nprint(\"simulated data: \", observations_[:4], \"...\")\nprint(\"Random assignments: \", assignments_[:4], \"...\")\nprint(\"Assigned center: \", centers_[:4], \"...\")\nprint(\"Assigned standard deviation: \", sds_[:4],\"...\")\ntfp.distributions.Uniform(\"rv_sds/\", batch_shape=[2], event_shape=[], dtype=float32)\nshape of sds sample: (2,)\nsimulated data:  [143.32745 203.31703 138.44893 157.09035] ...\nRandom assignments:  [1 0 0 0] ...\nAssigned center:  [140.52045 187.54768] ...\nAssigned standard deviation:  [34.886158 39.296032] ...\n类似地，在下面的联合log_prob函数中，我们创建了两个集群，每个集群都有我们在中心和标准偏差上的先验。 然后，我们根据我们的Categorical变量确定的权重按比例混合它们创造了高斯分布的混合体。最后，对于每个数据点，我们从该混合分布生成样本。\n请注意，此模型将群集分配变量边缘化，这样所有剩余的随机变量都是连续的，这使得它特别适合简单的MCMC– HamiltonianMonteCarlo\ndef joint_log_prob(data_, sample_prob_1, sample_centers, sample_sds):\n    \"\"\"\n    Joint log probability optimization function.\n        \n    Args:\n      data: tensor array representation of original data\n      sample_prob_1: Scalar representing probability (out of 1.0) of assignment \n        being 0\n      sample_sds: 2d vector containing standard deviations for both normal dists\n        in model\n      sample_centers: 2d vector containing centers for both normal dists in model\n    Returns: \n      Joint log probability optimization function.\n    \"\"\"  \n    ### Create a mixture of two scalar Gaussians:\n    rv_prob = tfd.Uniform(name='rv_prob', low=0., high=1.)\n    sample_prob_2 = 1. - sample_prob_1\n    rv_assignments = tfd.Categorical(probs=tf.stack([sample_prob_1, sample_prob_2]))\n    \n    rv_sds = tfd.Uniform(name=\"rv_sds\", low=[0., 0.], high=[100., 100.])\n    rv_centers = tfd.Normal(name=\"rv_centers\", loc=[120., 190.], scale=[10., 10.])\n    \n    rv_observations = tfd.MixtureSameFamily(\n        mixture_distribution=rv_assignments,\n        components_distribution=tfd.Normal(\n          loc=sample_centers,       # One for each component.\n          scale=sample_sds))        # And same here.\n    return (\n        rv_prob.log_prob(sample_prob_1)\n        + rv_prob.log_prob(sample_prob_2)\n        + tf.reduce_sum(rv_observations.log_prob(data_))      # Sum over samples.\n        + tf.reduce_sum(rv_centers.log_prob(sample_centers)) # Sum over components.\n        + tf.reduce_sum(rv_sds.log_prob(sample_sds))         # Sum over components.\n    )\n我们将使用我们的HMC采样方法，通过使用下面的25000个样本迭代来探索空间。\nnumber_of_steps=25000 #@param {type:\"slider\", min:0, max:50000, step:1000}\nburnin=1000 #@param {type:\"slider\", min:0, max:2000, step:100}\nnum_leapfrog_steps=3\n\n# Set the chain's start state.\ninitial_chain_state = [\n    tf.constant(0.5, name='init_probs'),\n    tf.constant([120., 190.], name='init_centers'),\n    tf.constant([10., 10.], name='init_sds')\n]\n\n# Since MCMC operates over unconstrained space, we need to transform the\n# samples so they live in real-space.\nunconstraining_bijectors = [\n    tfp.bijectors.Identity(),       # Maps R to R.\n    tfp.bijectors.Identity(),       # Maps R to R.\n    tfp.bijectors.Identity(),       # Maps R to R.\n]\n\n# Define a closure over our joint_log_prob.\nunnormalized_posterior_log_prob = lambda *args: joint_log_prob(data_, *args)\n\n\n# Initialize the step_size. (It will be automatically adapted.)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    step_size = tf.get_variable(\n        name='step_size',\n        initializer=tf.constant(0.5, dtype=tf.float32),\n        trainable=False,\n        use_resource=True\n    )\n\n# Defining the HMC\nhmc=tfp.mcmc.TransformedTransitionKernel(\n    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_posterior_log_prob,\n        num_leapfrog_steps=num_leapfrog_steps,\n        step_size=step_size,\n        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n        state_gradients_are_stopped=True),\n    bijector=unconstraining_bijectors)\n\n# Sample from the chain.\n[\n    posterior_prob,\n    posterior_centers,\n    posterior_sds\n], kernel_results = tfp.mcmc.sample_chain(\n    num_results=number_of_steps,\n    num_burnin_steps=burnin,\n    current_state=initial_chain_state,\n    kernel=hmc)\n\n# Initialize any created variables.\ninit_g = tf.global_variables_initializer()\ninit_l = tf.local_variables_initializer()\nW0727 20:52:11.363048 140336188528448 deprecation.py:323] From &lt;ipython-input-31-144a4acba7c5&gt;:39: make_simple_step_size_update_policy (from tensorflow_probability.python.mcmc.hmc) is deprecated and will be removed after 2019-05-22.\nInstructions for updating:\nUse tfp.mcmc.SimpleStepSizeAdaptation instead.\nW0727 20:52:11.368744 140336188528448 deprecation.py:506] From &lt;ipython-input-31-144a4acba7c5&gt;:40: calling HamiltonianMonteCarlo.__init__ (from tensorflow_probability.python.mcmc.hmc) with step_size_update_fn is deprecated and will be removed after 2019-05-22.\nInstructions for updating:\nThe `step_size_update_fn` argument is deprecated. Use `tfp.mcmc.SimpleStepSizeAdaptation` instead.\nevaluate(init_g)\nevaluate(init_l)\n[\n    posterior_prob_,\n    posterior_centers_,\n    posterior_sds_,\n    kernel_results_\n] = evaluate([\n    posterior_prob,\n    posterior_centers,\n    posterior_sds,\n    kernel_results\n])\n    \nnew_step_size_initializer_ = kernel_results_.inner_results.is_accepted.mean()\nprint(\"接受率: {}\".format(\n    new_step_size_initializer_))\nnew_step_size_initializer_\nprint(\"结束步: {}\".format(\n    kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))\n接受率: 0.7698\n结束步: 0.052764225751161575\n让我们检查未知参数的迹线。换句话说，到目前为止，未知参数（中心，精度和p）的路线已经采用。\nplt.figure(figsize(12.5, 9))\nplt.subplot(311)\nlw = 1\ncenter_trace = posterior_centers_\n\n# for pretty colors later in the book.\ncolors = [TFColor[3], TFColor[0]] if center_trace[-1, 0] &gt; center_trace[-1, 1] \\\n    else [TFColor[0], TFColor[3]]\n\nplt.plot(center_trace[:, 0], label=\"中心0的轨迹\", c=colors[0], lw=lw)\nplt.plot(center_trace[:, 1], label=\"中心1的轨迹\", c=colors[1], lw=lw)\nplt.title(\"未知参数的轨迹\")\nleg = plt.legend(loc=\"upper right\")\nleg.get_frame().set_alpha(0.7)\n\nplt.subplot(312)\nstd_trace = posterior_sds_\nplt.plot(std_trace[:, 0], label=\"聚类0的方差轨迹\",\n     c=colors[0], lw=lw)\nplt.plot(std_trace[:, 1], label=\"聚类1的方差估计\",\n     c=colors[1], lw=lw)\nplt.legend(loc=\"upper left\")\n\nplt.subplot(313)\np_trace = posterior_prob_\nplt.plot(p_trace, label=\"$p$: 分配给聚类0的概率\",\n     color=TFColor[2], lw=lw)\nplt.xlabel(\"Steps\")\nplt.ylim(0, 1)\nplt.legend();\n\n请注意以下特征： 1. 迹线会聚，而不是单个点，而是概率点的分布。这是MCMC算法中的* 收敛 。 2. 使用前几千个点的推断是一个坏主意，因为它们与我们感兴趣的最终分布无关。因此，在使用样本进行推断之前丢弃这些样本是个好主意。我们在收敛老化期*之前称这个时期。 3. 迹线显示为围绕空间的随机“行走”，即，路径表现出与先前位置的相关性。这既好又坏。我们将始终在当前位置和之前的位置之间建立相关性，但是太多意味着我们没有很好地探索空间。这将在本章后面的“诊断”部分详细介绍。\n为了实现进一步的融合，我们将执行更多的MCMC步骤。在上面MCMC的伪代码算法中，唯一重要的位置是当前位置（在当前位置附近调查新位置）。为了继续我们离开的地方，我们将未知参数的当前值传递给initial_chain_state（）变量。我们已经计算过的值不会被覆盖。这样可以确保我们的采样在停止的地方继续进行。\n我们将对MCMC进行五万次采样，并将以下进度可视化：\nnumber_of_steps=50000 #@param {type:\"slider\", min:0, max:50000, step:1000}\nburnin=10000 #@param {type:\"slider\", min:0, max:2000, step:100}\n\n# Set the chain's start state.\ninitial_chain_state = [\n    tf.constant(posterior_prob_[-1], name='init_probs_2'),\n    tf.constant(posterior_centers_[-1], name='init_centers_2'),\n    tf.constant(posterior_sds_[-1], name='init_sds_2')\n]\n\n\n# Initialize the step_size. (It will be automatically adapted.)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    step_size = tf.get_variable(\n        name='step_size_2',\n        #initializer=tf.constant(new_step_size_initializer_, dtype=tf.float32),\n        initializer=tf.constant(0.5, dtype=tf.float32),\n        trainable=False,\n        use_resource=True\n    )\n    step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8))\n\n# Defining the HMC\nhmc=tfp.mcmc.TransformedTransitionKernel(\n    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_posterior_log_prob,\n        num_leapfrog_steps=num_leapfrog_steps,\n        step_size=step_size,\n        step_size_update_fn=step_size_update_fn,\n        state_gradients_are_stopped=True),\n    bijector=unconstraining_bijectors)\n\n# Sample from the chain.\n[\n    posterior_prob_2,\n    posterior_centers_2,\n    posterior_sds_2\n], kernel_results = tfp.mcmc.sample_chain(\n    num_results=number_of_steps,\n    num_burnin_steps=burnin,\n    current_state=initial_chain_state,\n    kernel=hmc)\n\n\n# Initialize any created variables.\ninit_g = tf.global_variables_initializer()\ninit_l = tf.local_variables_initializer()\n\nevaluate(init_g)\nevaluate(init_l)\n[\n    posterior_prob_2_,\n    posterior_centers_2_,\n    posterior_sds_2_,\n    kernel_results_\n] = evaluate([\n    posterior_prob_2,\n    posterior_centers_2,\n    posterior_sds_2,\n    kernel_results\n])\n\nprint(\"acceptance rate: {}\".format(\n    kernel_results_.inner_results.is_accepted.mean()))\nnew_step_size_initializer_\nprint(\"final step size: {}\".format(\n    kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))\n\nacceptance rate: 0.60624 final step size: 0.05713607743382454\n\nplt.figure(figsize(12.5, 4))\ncenter_trace = posterior_centers_2_\nprev_center_trace = posterior_centers_\n\nx = np.arange(25000)\nplt.plot(x, prev_center_trace[:, 0], label=\"之前的中心0轨迹\",\n      lw=lw, alpha=0.4, c=colors[1])\nplt.plot(x, prev_center_trace[:, 1], label=\"之前的中心1轨迹\",\n      lw=lw, alpha=0.4, c=colors[0])\n\nx = np.arange(25000, 75000)\nplt.plot(x, center_trace[:, 0], label=\"中心0的新轨迹\", lw=lw, c=\"#5DA5DA\")\nplt.plot(x, center_trace[:, 1], label=\"中心1的新轨迹\", lw=lw, c=\"#F15854\")\n\nplt.title(\"未知中心参数的痕迹\")\nleg = plt.legend(loc=\"upper right\")\nleg.get_frame().set_alpha(0.8)\nplt.xlabel(\"Steps\");\n\n\n聚类调查\n我们没有忘记我们的主要挑战：确定聚类。我们确定了未知数的后验分布。我们绘制下面的中心和标准差变量的后验分布：\nplt.figure(figsize(12.5, 8))\nstd_trace = posterior_sds_2_\nprev_std_trace = posterior_sds_\n\n_i = [1, 2, 3, 4]\nfor i in range(2):\n    plt.subplot(2, 2, _i[2 * i])\n    plt.title(\"聚类%d的后验\" % i)\n    plt.hist(center_trace[:, i], color=colors[i], bins=30,\n             histtype=\"stepfilled\")\n\n    plt.subplot(2, 2, _i[2 * i + 1])\n    plt.title(\"聚类%d方差的后验\" % i)\n    plt.hist(std_trace[:, i], color=colors[i], bins=30,\n             histtype=\"stepfilled\")\n    # plt.autoscale(tight=True)\n\nplt.tight_layout()\n\nMCMC算法提出两个聚类中最可能的中心分别接近120和200。类似的推断可以应用于标准偏差。\n在本章的PyMC3版本中，我们描述了每个数据点的标签的后验分布。但是，在我们的TFP版本中，由于我们的模型边缘化了赋值变量，因此我们没有来自MCMC的该变量的迹线。\n作为替代，下面我们可以在分配上生成后验预测分布，然后从中生成一些样本。\n以下是对此的可视化。 y轴代表来自后验预测分布的样本。 x轴是原始数据点的排序值。红色方块是对簇0的赋值，蓝色方块是对簇1的赋值。\n# 将数据放入张量\ndata = tf.constant(data_,dtype=tf.float32)\ndata = data[:,tf.newaxis]\n\n# 他根据MCMC链生成一个聚类\nrv_clusters_1 = tfd.Normal(posterior_centers_2_[:, 0], posterior_sds_2_[:, 0])\nrv_clusters_2 = tfd.Normal(posterior_centers_2_[:, 1], posterior_sds_2_[:, 1])\n\n#计算每个群集的对数概率\ncluster_1_log_prob = rv_clusters_1.log_prob(data) + tf.math.log(posterior_prob_2_)\ncluster_2_log_prob = rv_clusters_2.log_prob(data) + tf.math.log(1. - posterior_prob_2_)\n\nx = tf.stack([cluster_1_log_prob, cluster_2_log_prob],axis=-1)\ny = tf.math.reduce_logsumexp(x,-1)\n\n# 贝叶斯规则计算分配概率：P（cluster = 1 | data）αP（data | cluster = 1）P（cluster = 1）\nlog_p_assign_1 = cluster_1_log_prob - tf.math.reduce_logsumexp(tf.stack([cluster_1_log_prob, cluster_2_log_prob], axis=-1), -1)\n\n# 整个MCMC链的平均值\nlog_p_assign_1 = tf.math.reduce_logsumexp(log_p_assign_1, -1) - tf.math.log(tf.cast(log_p_assign_1.shape[-1], tf.float32))\n \np_assign_1 = tf.exp(log_p_assign_1)\np_assign = tf.stack([p_assign_1,1-p_assign_1],axis=-1)\n\n# for charting \nprobs_assignments = p_assign_1 \nburned_assignment_trace_ = evaluate(tfd.Categorical(probs=p_assign).sample(sample_shape=200))\nplt.figure(figsize(12.5, 5))\nplt.cmap = mpl.colors.ListedColormap(colors)\nplt.imshow(burned_assignment_trace_[:, np.argsort(data_)],\n       cmap=plt.cmap, aspect=.4, alpha=.9)\nplt.xticks(np.arange(0, data_.shape[0], 40),\n       [\"%.2f\" % s for s in np.sort(data_)[::40]])\nplt.ylabel(\"后验样本\")\nplt.xlabel(\"第$i$个数据点的值\")\nplt.title(\"数据点的后验标签\");\n\n看看上面的绘图，似乎最不确定性在150到170之间。上面的绘图略微歪曲事物，因为x轴不是真正的比例（它显示第\\(i\\)排序数据点的值。 ）下面是一个更清晰的图表，我们估算了属于标签0和1的每个数据点的频率。\nplt.figure(figsize(12.5, 5))\n\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"BMH\", colors)\nassign_trace = evaluate(probs_assignments)[np.argsort(data_)]\nplt.scatter(data_[np.argsort(data_)], assign_trace, cmap=cmap,\n        c=(1 - assign_trace), s=50)\nplt.ylim(-0.05, 1.05)\nplt.xlim(35, 300)\nplt.title(\"属于聚类0的数据点的概率\")\nplt.ylabel(\"概率\")\nplt.xlabel(\"数据点的值\");\n\n即使我们使用正态分布对集群进行建模，我们也没有得到最佳适合数据的正态分布（无论我们的最佳定义是什么），而是正态分布的值的分布。我们如何才能为均值和方差选择一对值并确定八九不离十的高斯分布？\n一种快速而肮脏的方式（我们将在第5章中看到它具有很好的理论属性），就是使用后验分布的mean。下面我们使用我们观察到的数据覆盖正态密度函数，使用后验分布的平均值作为所选参数：\nx_ = np.linspace(20, 300, 500)\nposterior_center_means_ = evaluate(tf.reduce_mean(posterior_centers_2_, axis=0))\nposterior_std_means_ = evaluate(tf.reduce_mean(posterior_sds_2_, axis=0))\nposterior_prob_mean_ = evaluate(tf.reduce_mean(posterior_prob_2_, axis=0))\n\nplt.hist(data_, bins=20, histtype=\"step\", density=True, color=\"k\",\n     lw=2, label=\"数据直方图\")\ny_ = posterior_prob_mean_ * evaluate(tfd.Normal(loc=posterior_center_means_[0],\n                                scale=posterior_std_means_[0]).prob(x_))\nplt.plot(x_, y_, label=\"聚类 0 (使用后验平均参数)\", lw=3)\nplt.fill_between(x_, y_, color=colors[1], alpha=0.3)\n\ny_ = (1 - posterior_prob_mean_) * evaluate(tfd.Normal(loc=posterior_center_means_[1],\n                                      scale=posterior_std_means_[1]).prob(x_))\nplt.plot(x_, y_, label=\"聚类 1 (使用后验平均参数)\", lw=3)\nplt.fill_between(x_, y_, color=colors[0], alpha=0.3)\n\nplt.legend(loc=\"upper left\")\nplt.title(\"使用后验平均参数可视化聚类\");\n\n\n\n重要提示：不要混合后验样本\n在上面的示例中，可能的（尽管不太可能）场景是聚类0具有非常大的标准偏差，聚类1具有小的标准偏差。这仍然可以满足证据，尽管不如我们原来的推论那么多。或者，由于数据根本不支持这一假设，因此两个分布都不太可能具有较小的标准偏差。因此，两个标准偏差相互依赖：如果一个很小，另一个必须很大。事实上，所有未知数都以类似的方式相关。例如，如果标准偏差很大，则均值具有更宽的可能实现空间。相反，较小的标准偏差将平均值限制在较小的区域。\n在MCMC期间，我们返回的矢量代表来自未知后验的样本。不同矢量的元素不能一起使用，因为这会破坏上述逻辑：可能样本已经返回，聚类1具有小的标准偏差，因此该样本中的所有其他变量将合并并相应地进行调整。但是很容易避免这个问题，只需确保正确索引跟踪。\n另一个小例子来说明这一点。假设两个变量\\(x\\)和\\(y\\)通过\\(x + y = 10\\)相关联。我们将\\(x\\)建模为普通随机变量，均值为4，并探索500个样本。\nnumber_of_steps = 10000 #@param {type:\"slider\", min:0, max:20000, step:1000}\nburnin = 500 #@param {type:\"slider\", min:0, max:500, step:100}\n\n# Set the chain's start state.\ninitial_chain_state = [\n    tf.to_float(1.) * tf.ones([], name='init_x', dtype=tf.float32),\n]\n\n# Initialize the step_size. (It will be automatically adapted.)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    step_size = tf.get_variable(\n        name='step_size',\n        initializer=tf.constant(0.5, dtype=tf.float32),\n        trainable=False,\n        use_resource=True\n    )\n    step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8))\n\n# Defining the HMC\n# Since we're only using one distribution for our simplistic example, \n# the use of the bijectors and unnormalized log_prob function is \n# unneccesary\n#\n# While not a good example of what to do if you have dependent \n# priors, this IS a good example of how to set up just one variable \n# with a simple distribution\nhmc=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=tfd.Normal(name=\"rv_x\", loc=tf.to_float(4.), \n                                      scale=tf.to_float(1./np.sqrt(10.))).log_prob,\n        num_leapfrog_steps=2,\n        step_size=step_size,\n        step_size_update_fn=step_size_update_fn,\n        state_gradients_are_stopped=True)\n\n# Sampling from the chain.\n[\n    x_samples,\n], kernel_results = tfp.mcmc.sample_chain(\n    num_results = number_of_steps,\n    num_burnin_steps = burnin,\n    current_state=initial_chain_state,\n    kernel=hmc,\n    name='HMC_sampling'\n)\n\ny_samples = 10 - x_samples\n\n# Initialize any created variables for preconditions\ninit_g = tf.global_variables_initializer()\n\n#Running\nevaluate(init_g)\n[\n    x_samples_,\n    y_samples_,\n] = evaluate([\n    x_samples,\n    y_samples,\n])\n\nplt.figure(figsize=(12,6))\nplt.plot(np.arange(number_of_steps), x_samples_, color=TFColor[3], alpha=0.8)\nplt.plot(np.arange(number_of_steps), y_samples_, color=TFColor[0], alpha=0.8)\nplt.title('Displaying (extreme) case of dependence between unknowns', fontsize=14)\n\nText(0.5, 1.0, ‘Displaying (extreme) case of dependence between unknowns’)\n\n\n正如您所看到的，这两个变量并不相关，将\\(x\\)样本的\\(x\\)添加到\\(y\\)的\\(j\\)样本中是错误的，除非\\(i=j\\)。\n\n\n回到聚类：预测\n上面的聚类可以推广到\\(k\\) 个聚类。选择\\(k = 2\\)可以让我们更好地可视化MCMC，并检查一些非常有趣的图。\n预测怎么样？假设我们观察到一个新的数据点，比如\\(x = 175\\)，我们希望将它标记为一个集群。简单地将其分配给接近集群中心是愚蠢的，因为这忽略了集群的标准偏差，我们从上面的图中看到，这种考虑非常重要。更正式地说：我们对向集群1分配\\(x = 175\\)的概率（因为我们不能确定标签）感兴趣。将$ x \\(的赋值表示为\\)L_x\\(，等于0或1 ，我们感兴趣的是\\)P(L_x = 1 ; | ; x = 175)$。\n一种简单的计算方法是重新运行上面的MCMC并附加附加数据点。这种方法的缺点是推断每个新颖数据点的速度很慢。或者，我们可以尝试不太精确，但更快的方法。\n我们将使用贝叶斯定理。如果你还记得，贝叶斯定理看起来像：\n\\[ P( A | X ) = \\frac{ P( X  | A )P(A) }{P(X) }\\]\n在我们的例子中，$ A \\(代表\\) L_x = 1 \\(和\\) X \\(是我们的证据：我们观察到\\) x = 175 \\(。对于我们后验分布的特定样本参数集\\)（_0，_0，_1，_1，p）\\(，我们有兴趣询问“\\)x$在群集1 中的概率是否更大比它在集群0中的概率？“，其中概率取决于所选择的参数。\n\\[\n\\begin{align}\n& P(L_x = 1| x = 175 ) \\gt P(L_x = 0| x = 175 ) \\\\[5pt]\n& \\frac{ P( x=175  | L_x = 1  )P( L_x = 1 ) }{P(x = 175) } \\gt \\frac{ P( x=175  | L_x = 0  )P( L_x = 0 )}{P(x = 175) }\n\\end{align}\n\\]\n由于分母是相等的，它们可以被忽略（并且很好地消除，因为计算数量$ P（x = 175）$可能很困难）。\n\\[  P( x=175  | L_x = 1  )P( L_x = 1 ) \\gt  P( x=175  | L_x = 0  )P( L_x = 0 ) \\]\np_trace = posterior_prob_2_[25000:]\n\nx = 175\n\nv = (1 - p_trace) * evaluate(tfd.Normal(loc=center_trace[25000:, 1], \n                                        scale=std_trace[25000:, 1]).log_prob(x)) &gt; \\\n                                        p_trace * evaluate(tfd.Normal(loc=center_trace[25000:, 0], \\\n                                        scale=std_trace[25000:, 0]).log_prob(x))\n    \n\nprint(\"属于聚类1的概率:\", (v.mean()))\n\n属于聚类1的概率: 0.03192\n\n给我们一个概率而不是标签是一件非常有用的事情。而不是简单的：\n\nL = 1 if prob &gt; 0.5 else 0\n\n我们可以使用损失函数来优化我们的猜测，这是第五章的全部内容。"
  },
  {
    "objectID": "posts/tfp-ch3.html#diagnosing-convergence",
    "href": "posts/tfp-ch3.html#diagnosing-convergence",
    "title": "概率模型第三章 ： MCMC",
    "section": "Diagnosing Convergence",
    "text": "Diagnosing Convergence\n\n自相关\n自相关是衡量一系列数字与自身相关程度的指标。 1.0的测量是完全正自相关，0没有自相关，-1是完全负相关。如果你熟悉标准相关，那么自相关就是$ t_k \\(时刻和时刻\\) t_k $序列的相关性。\n\\[R(k) = \\text{Corr}( x_t, x_{t-k} ) \\]\n例如，考虑两个序列：\n\\[x_t \\sim \\text{Normal}(0,1), \\;\\; x_0 = 0\\] \\[y_t \\sim \\text{Normal}(y_{t-1}, 1 ), \\;\\; y_0 = 0\\]\n其中包含示例路径：\nx_t = evaluate(tfd.Normal(loc=0., scale=1.).sample(sample_shape=200))\nx_t[0] = 0\ny_t = evaluate(tf.zeros(200))\nfor i in range(1, 200):\n    y_t[i] = evaluate(tfd.Normal(loc=y_t[i - 1], scale=1.).sample())\n\nplt.figure(figsize(12.5, 4))\nplt.plot(y_t, label=\"$y_t$\", lw=3)\nplt.plot(x_t, label=\"$x_t$\", lw=3)\nplt.xlabel(\"time, $t$\")\nplt.legend();\n\n想到自相关的一种方法是“如果我知道系列在$ s \\(时的位置，它能帮助我知道我在时间\\) t \\(的位置吗？”在\\) x_t \\(系列中，答案是否定的。通过构造，\\) x_t \\(是随机变量。如果我告诉你\\) x_2 = 0.5 \\(，你能给我一个更好的猜测\\) x_3 $吗？没有。\n另一方面，$ y_t \\(是自相关的。通过构造，如果我知道\\) y_2 = 10 \\(，我可以非常自信\\) y_3 \\(与10相差不多。同样，我甚至可以对\\) y_4 \\(进行（不太自信的猜测）：它可能会不要接近0或20，但值不是太小。我可以就\\) y_5 \\(做出类似的争论，但同样，我不太自信。考虑到这个逻辑结论，我们必须承认，作为\\) k $，时间点之间的滞后会增加自相关减少。我们可以想象这个：\ndef autocorr(x):\n    # from http://tinyurl.com/afz57c4\n    result = np.correlate(x, x, mode='full')\n    result = result / np.max(result)\n    return result[result.size // 2:]\n\ncolors = [TFColor[3], TFColor[0], TFColor[6]]\n\nx = np.arange(1, 200)\nplt.bar(x, autocorr(y_t)[1:], width=1, label=\"$y_t$\",\n        edgecolor=colors[0], color=colors[0])\nplt.bar(x, autocorr(x_t)[1:], width=1, label=\"$x_t$\",\n        color=colors[1], edgecolor=colors[1])\n\nplt.legend(title=\"自相关\")\nplt.ylabel(\"$y_t$ 和 $y_{t-k}$ \\n测量相关性 \")\nplt.xlabel(\"k (lag)\")\nplt.title(\"$y_t$和$x_t$不同$k$滞后的相关性图表\");\n\n请注意，随着\\(k\\)的增加，\\(y_t\\)的自相关从非常高的点开始减少。与\\(x_t\\)的自相关相比，它看起来像噪声（实际上是它），因此我们可以得出结论，在这个系列中不存在自相关。\n\n\n这与MCMC收敛有何关系?\n根据MCMC算法的性质，我们将始终返回显示自相关的样本（这是因为从您当前位置开始的步骤，移动到您附近的位置）。\n没有很好地探索太空的链条将表现出非常高的自相关性。在视觉上，如果迹象似乎像河流一样蜿蜒而不能安定下来，那么链条将具有很高的自相关性。\n这并不意味着收敛的MCMC具有低自相关性。因此，收敛不需要低自相关，但这已足够。 TFP也有内置的自相关工具。\n\n\n细化\n如果后验样本之间存在高度自相关，则会出现另一个问题。许多后处理算法要求样本彼此独立。这可以通过仅每个$ n \\(样本返回给用户来解决或至少减少，从而消除一些自相关。下面我们使用不同的细化级别执行\\) y_t $的自相关图：\nmax_x = 200 // 3 + 1\nx = np.arange(1, max_x)\n\nplt.bar(x, autocorr(y_t)[1:max_x], edgecolor=colors[0],\n        label=\"no thinning\", color=colors[0], width=1)\nplt.bar(x, autocorr(y_t[::2])[1:max_x], edgecolor=colors[1],\n        label=\"keeping every 2nd sample\", color=colors[1], width=1)\nplt.bar(x, autocorr(y_t[::3])[1:max_x], width=1, edgecolor=colors[2],\n        label=\"keeping every 3rd sample\", color=colors[2])\n\nplt.autoscale(tight=True)\nplt.legend(title=\"$y_t$自相关图\", loc=\"upper right\")\nplt.ylabel(\"$y_t$和$y_{t-k}$\\n测量相关性 \")\nplt.xlabel(\"k (lag)\")\nplt.title(\"$y_t$和不同的$k$滞后的自相关 (没有细化 vs. 细化).\");\n\n随着更薄，自相关性下降得更快。但是需要权衡：更高的细化需要更多的MCMC迭代才能获得相同数量的返回样本。例如，未填充的10 000个样本为10万，稀疏度为10（尽管后者具有较少的自相关性）。\n什么是稀释量很大？无论进行多少细化，返回的样本将始终显示一些自相关。只要自相关趋于零，你就可以了。通常不需要超过10的减薄。"
  },
  {
    "objectID": "posts/tfp-ch3.html#mcmc的有用提示",
    "href": "posts/tfp-ch3.html#mcmc的有用提示",
    "title": "概率模型第三章 ： MCMC",
    "section": "MCMC的有用提示",
    "text": "MCMC的有用提示\n如果不是MCMC的计算困难，贝叶斯推断将是事实上的方法。事实上，MCMC是大多数用户拒绝实际贝叶斯推理的原因。下面我介绍一些很好的启发式方法来帮助收敛并加速MCMC引擎：\n\n智能启动值\n在后验分布附近启动MCMC算法会很棒，因此开始正确采样将花费很少的时间。通过在“随机”变量创建中指定testval参数，我们可以通过告诉我们认为后验分布将在何处来帮助算法。在许多情况下，我们可以对参数进行合理的猜测。例如，如果我们有来自Normal分布的数据，并且我们希望估计$ $参数，那么一个好的起始值将是数据的* mean *。\nmu = tfd.Uniform(name=\"mu\", low=0., high=100.).sample(seed=data.mean())\n对于模型中的大多数参数，有一个频繁的估计。这些估计值对于我们的MCMC算法来说是一个很好的起始值。当然，对于某些变量来说，这并不总是可行的，但包括尽可能多的适当初始值总是一个好主意。即使您的猜测是错误的，MCMC仍然会收敛到正确的分布，因此几乎没有损失。\n\nPriors 先验\n如果先验选择不当，MCMC算法可能不会收敛，或者至少难以收敛。考虑如果先前选择的甚至不包含真实参数可能发生的事情：先验为未知分配0概率，因此后验也将分配0概率。这可能导致病理结果。\n因此，最好仔细选择先验。通常情况下，缺乏掩盖或样本拥挤到边界的证据意味着所选择的先验有些问题（参见下面的统计计算的民间定理）。\n\n\n统计计算的民间定理\n\n如果您遇到计算问题，可能您的模型是错误的."
  },
  {
    "objectID": "posts/tfp-ch3.html#结论",
    "href": "posts/tfp-ch3.html#结论",
    "title": "概率模型第三章 ： MCMC",
    "section": "结论",
    "text": "结论\nTFP为执行贝叶斯推理提供了非常强大的后端，主要是因为它允许用户微调MCMC的内部工作。\n\nReferences\n[1] Tensorflow Probability API docs. https://www.tensorflow.org/probability/api_docs/python/tfp"
  },
  {
    "objectID": "posts/tfmulcompare.html",
    "href": "posts/tfmulcompare.html",
    "title": "对比tensordot、matmul、einsum速度",
    "section": "",
    "text": "准备自己实现capsule Net，今天看了下别人实现的版本，感觉里面的矩阵乘积应该是可以优化的。\n然后我写代码的时候，感觉一个可以优化的点是不同维度之间的Tensor的矩阵乘积，所以我做了一个小测试。\n\n\n说明\n因为capsule net中全连接需要权值乘上输入向量： \\[\n\\begin{aligned}\n    \\hat{u}_{j|i}&=W_{ij}u_i \\\\\n    W_{ij} &= [Len_{l},Len_{l+1}] \\\\\n    u_i &= [batch,N_l,Len_{l}]\n\\end{aligned}\n\\]\n他的实例是: \\[\n\\begin{aligned}\n    W_{ij} &= [8,16] \\\\\n    u_i &= [batch,1152,8]\n\\end{aligned}\n\\]\n因为两个Tensor的维度不一样,所以在他的代码中都是tile然后进行计算的.然后我找了几个矩阵计算的函数进行比较(使用 tensorflow 2.0).\nimport tensorflow.python as tf\nimport numpy as np\nimport os\nimport timeit\n\n\n# @tf.function\ndef test_tensordot(W: tf.Tensor, u: tf.Tensor) -&gt; tf.Tensor:\n    v = tf.tensordot(u, W, axes=[[2], [0]])\n    return v\n\n\n# @tf.function\ndef test_matmul(W: tf.Tensor, u: tf.Tensor) -&gt; tf.Tensor:\n    W_ = W[tf.newaxis, tf.newaxis, ...]\n    u_ = u[..., tf.newaxis]\n    W_ = tf.tile(W_, [u.shape[0], 1152, 1, 1])\n    v = tf.matmul(W_, u_, transpose_a=True)\n    return tf.squeeze(v)\n\n\n# @tf.function\ndef test_einsum(W: tf.Tensor, u: tf.Tensor) -&gt; tf.Tensor:\n    return tf.einsum('ij,aki-&gt;akj', W, u)\n\n\ndef test_compare():\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n    batch = 16\n    tf.set_random_seed(1)\n    W = tf.get_variable('W', shape=(8, 16), dtype=tf.float32, initializer=tf.initializers.random_normal())\n    u = tf.get_variable('u', shape=(batch, 1152, 8), dtype=tf.float32, initializer=tf.initializers.random_normal())\n\n    start = timeit.default_timer()\n    for i in range(100):\n        v1 = test_tensordot(W, u)\n    tim = timeit.default_timer()-start\n    print(\"tensordot\", tim)\n\n    start = timeit.default_timer()\n    for i in range(100):\n        v2 = test_matmul(W, u)\n    tim = timeit.default_timer()-start\n    print(\"matmul\", tim)\n\n    start = timeit.default_timer()\n    for i in range(100):\n        v3 = test_einsum(W, u)\n    tim = timeit.default_timer()-start\n    print(\"einsum\", tim)\n\n    print(np.allclose(v1, v2, atol=0.5e-6))\n    print(np.allclose(v1, v3, atol=0.5e-6))\n\n\ntest_compare()\n\n\n结果\n(tf2) ➜  tf2 /home/zqh/miniconda3/envs/tf2/bin/python /home/zqh/Documents/tf2/test/test_fuc.py\ntensordot 0.2818375900023966\nmatmul 0.09134677500696853\neinsum 0.051768514000286814\nTrue\nTrue\n实验发现einsum的效率更加高.\n\n\n疑问\n在tensorflow 2.0中明明可以使用@tf.function来优化运行速度.但是我在上面的程序中使用这个方式,反而速度更慢了…\n(tf2) ➜  tf2 /home/zqh/miniconda3/envs/tf2/bin/python /home/zqh/Documents/tf2/test/test_fuc.py \n# 不使用 @tf.function\ntensordot 0.21580070699565113\nmatmul 0.08182674000272527\neinsum 0.044429186993511394\nTrue\nTrue\n(tf2) ➜  tf2 /home/zqh/miniconda3/envs/tf2/bin/python /home/zqh/Documents/tf2/test/test_fuc.py\n# 使用 @tf.function\ntensordot 0.27514774599694647\nmatmul 0.15171915300015826\neinsum 0.0524767349998001\nTrue\nTrue"
  },
  {
    "objectID": "posts/tf2-global-step.html",
    "href": "posts/tf2-global-step.html",
    "title": "Tensorflow 2.0中使用global steps",
    "section": "",
    "text": "用了一段时间的tensorflow 2.0,总的来说默认eager模式操作数据十分的方便,并且可以适当的转为tf.function加快速度.但是和keras的结合还是不够灵活,比如可以单独用fit可以执行,但是想用更加灵活的方式训练有时候就会出现莫名其妙的问题,让人抓狂.\n\n今天我想用以前的方式使用global step,在教程里面找了只能设置step=optimizer.iterations,这也太蠢了8,如果我要在训练过程中进行测试,step也必须要增加的.然后我摸索到了如下使用方式:\nwriter = summary.create_file_writer(os.path.join('log', datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')))\nsteps = tf.train.create_global_step()\nwith writer.as_default():\n    testiter = iter(dataset_test)\n    for i in range(3):\n        for x, y in dataset:\n            loss, acc = train_one_step(model, optimizer, x, y)\n            summary.scalar('train_loss', loss, step=steps)\n            summary.scalar('train_acc', acc, step=steps)\n            if steps.numpy() % 20 == 0:\n                test_x, test_y = next(testiter)\n                loss, acc = test_one_step(model, test_x, test_y)\n                summary.scalar('test_loss', loss, step=steps)\n                summary.scalar('test_acc', acc, step=steps)\n            steps.assign_add(1)\n\n            print('\\rsteps:{}\\t\\tloss:{:.4f}\\t\\tacc:{:.4f}%'.format(steps.numpy(), loss, acc * 100), end='')"
  },
  {
    "objectID": "posts/tf-torch.html",
    "href": "posts/tf-torch.html",
    "title": "tensorflow与pytorch代码差异",
    "section": "",
    "text": "可能会长期更新,因为经常需要从pytorch偷代码翻译成tensorflow😑因此记录一下差异的地方.\n\n\n1. torch中nn.Conv2d的groups参数\ntorch中groups控制输入和输出之间的连接,in_channels和out_channels必须都可以被组整除. - groups=1 传统的卷积方式. - groups=2 等效于并排设置两个conv层，每个conv层看到一半的输入通道，并产生一半的输出通道，并且随后将它们都连接在一起. - groups=in_channels 每个输入通道都有自己的滤波器.\n等价写法:\nnn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, \n          stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)\n\nkl.DepthwiseConv2D(kernel_size=kernel_size,\n                  strides=stride, padding='same', use_bias=False)\nNOTE:\n这里pytorch生成的卷积核shape = [out_channel, 1, kh, kw] 这里tflite生成的卷积核shape = [1, kh, kw, out_channel]\n\n\n2. nn.AdaptiveAvgPool2d与kl.GlobalAveragePooling2D\n当nn.AdaptiveAvgPool2d(1)时和kl.GlobalAveragePooling2D()相同,但是注意torch的输出是保持4维的,而tensorflow不保持维度.\n等价写法:\nx=nn.AdaptiveAvgPool2d(1)(x)\n# -----------------------------\npool=kl.GlobalAveragePooling2D()\nx=k.backend.expand_dims(k.backend.expand_dims(pool(x),1),1)\n当然直接修改GlobalAveragePooling2D里,添加keepdims=true参数也可以.\n\n\ntf.contrib.layers.layer_norm与tf.keras.LayerNorm与nn.LayerNorm\n\ntf.contrib.layers.layer_norm\ntf以前遗留代码还是挺蛋疼的。在tf.contrib.layers.layer_norm中，对于输入为(4, 10, 10, 3)的张量，是对(h,w,c)进行归一化处理，但是他的仿射系数默认只对c有效：\nx = tf.reshape(tf.range(4 * 3 * 10 * 10, dtype=tf.float32), (4, 10, 10, 3))\nxout = tf_contrib.layers.layer_norm(x,\n                                    center=True, scale=True,\n                                    scope='layer_norm')\nmean.shape = (4, 1, 1, 1) \ngamma.shape = (3,)\n\n\ntf.keras.LayerNorm\ntf.keras.LayerNorm我就属实不懂了，讲道理他的归一化是对(h,w,c)进行归一化处理，仿射系数对c有效，但是输出归一化结果是400=4×10x10，这就很奇怪了，他默认的特征维度是-1，但是看起来却没有干LayerNorm应该做的事情，反而把batch维度也归一化了，但是在最终测试输出的时候发现结果是符合预期的。。属实不理解。\ninputs_np = tf.convert_to_tensor(\n    np.arange(4 * 3 * 10 * 10).reshape((4, 10, 10, 3)), dtype=tf.float32)\ninputs = k.Input((10, 10, 3), batch_size=None)\nlm = k.layers.LayerNormalization()\nlm.weights\nlm_out = lm(inputs)\nmd = k.Model(inputs, lm_out) \nscale.shape # (3,)\nmean.shape # (400,1)\n\nlm_out_np = md(inputs_np)\nlm_out_np = lm_out_np.numpy()\nnp.mean(lm_out_np[0, ...]) # -3.8146972e-08\nnp.var(lm_out_np[0, ...]) # 0.9985023\n\n\nnn.LayerNorm\nnn.LayerNorm是对(c,h,w)进行归一化处理，仿射系数对c,h,w有效，但有个非常蛋疼的问题就是，他没有办法复现老版本tf的行为，即只用c作为仿射系数，如果开启仿射会导致参数非常大。。。\ninputs = torch.tensor(np.arange(4 * 3 * 10 * 10).reshape((4, 3, 10, 10)), dtype=torch.float32)\nlm = nn.LayerNorm([3, 10, 10], elementwise_affine=True)\nln_out = lm(inputs)\nlm.weight.shape # torch.Size([3, 10, 10])\n我继续检查他的源码,在aten/src/ATen/native/layer_norm.h中，将输入维度分为M*N，按照我们上面的做法即M=4,N=3*10*10。 然后进入cuda代码aten/src/ATen/native/cuda/layer_norm_kernel.cu利用RowwiseMomentsCUDAKernel计算均值与方差：\ntemplate &lt;typename T&gt;\nvoid LayerNormKernelImplInternal(\n    const Tensor& X,\n    const Tensor& gamma,\n    const Tensor& beta,\n    int64_t M,\n    int64_t N,\n    T eps,\n    Tensor* Y,\n    Tensor* mean,\n    Tensor* rstd) {\n  DCHECK_EQ(X.numel(), M * N);\n  DCHECK(!gamma.defined() || gamma.numel() == N);\n  DCHECK(!beta.defined() || beta.numel() == N);\n  const T* X_data = X.data_ptr&lt;T&gt;();\n  const T* gamma_data = gamma.defined() ? gamma.data_ptr&lt;T&gt;() : nullptr;\n  const T* beta_data = beta.defined() ? beta.data_ptr&lt;T&gt;() : nullptr;\n  T* Y_data = Y-&gt;data_ptr&lt;T&gt;();\n  T* mean_data = mean-&gt;data_ptr&lt;T&gt;();\n  T* rstd_data = rstd-&gt;data_ptr&lt;T&gt;();\n  cudaStream_t cuda_stream = at::cuda::getCurrentCUDAStream();\n  RowwiseMomentsCUDAKernel&lt;T&gt;\n      &lt;&lt;&lt;M, cuda_utils::kCUDABlockReduceNumThreads, 0, cuda_stream&gt;&gt;&gt;(\n          N, eps, X_data, mean_data, rstd_data);\n  LayerNormForwardCUDAKernel&lt;T&gt;&lt;&lt;&lt;M, kCUDANumThreads, 0, cuda_stream&gt;&gt;&gt;(\n      N, X_data, mean_data, rstd_data, gamma_data, beta_data, Y_data);\n  AT_CUDA_CHECK(cudaGetLastError());\n}\n接下来我们检查一下group norm，首先给定group，他将模型输入分为N,C,HxW。在aten/src/ATen/native/cuda/group_norm_kernel.cu中，当group=1的时候，D=C/G=C，N×G=N,也就是group=1的是等同于layer norm，并且此时他的可变化参数为C，可以用来等效tf.contrib.layers.layer_norm。\ntemplate &lt;typename T&gt;\nvoid GroupNormKernelImplInternal(\n    const Tensor& X,\n    const Tensor& gamma,\n    const Tensor& beta,\n    int64_t N,\n    int64_t C,\n    int64_t HxW,\n    int64_t group,\n    T eps,\n    Tensor* Y,\n    Tensor* mean,\n    Tensor* rstd) {\n  using T_ACC = acc_type&lt;T, true&gt;;\n  TORCH_CHECK(X.numel() == N * C * HxW);\n  TORCH_CHECK(!gamma.defined() || gamma.numel() == C);\n  TORCH_CHECK(!beta.defined() || beta.numel() == C);\n  if (N == 0) {\n    return;\n  }\n  const int64_t G = group;\n  const int64_t D = C / G;\n  const T* X_data = X.data_ptr&lt;T&gt;();\n  const T* gamma_data = gamma.defined() ? gamma.data_ptr&lt;T&gt;() : nullptr;\n  const T* beta_data = beta.defined() ? beta.data_ptr&lt;T&gt;() : nullptr;\n  T* Y_data = Y-&gt;data_ptr&lt;T&gt;();\n  T* mean_data = mean-&gt;data_ptr&lt;T&gt;();\n  T* rstd_data = rstd-&gt;data_ptr&lt;T&gt;();\n  const auto kAccType = X.scalar_type() == kHalf ? kFloat : X.scalar_type();\n  Tensor a = at::empty({N, C}, X.options().dtype(kAccType));\n  Tensor b = at::empty({N, C}, X.options().dtype(kAccType));\n  T_ACC* a_data = a.data_ptr&lt;T_ACC&gt;();\n  T_ACC* b_data = b.data_ptr&lt;T_ACC&gt;();\n  cudaStream_t cuda_stream = at::cuda::getCurrentCUDAStream();\n  RowwiseMomentsCUDAKernel&lt;T&gt;\n      &lt;&lt;&lt;N * G, cuda_utils::kCUDABlockReduceNumThreads, 0, cuda_stream&gt;&gt;&gt;(\n          D * HxW, eps, X_data, mean_data, rstd_data);\n  int64_t B = (N * C + kCUDANumThreads - 1) / kCUDANumThreads;\n  ComputeFusedParamsCUDAKernel&lt;T&gt;&lt;&lt;&lt;B, kCUDANumThreads, 0, cuda_stream&gt;&gt;&gt;(\n      N, C, G, mean_data, rstd_data, gamma_data, beta_data, a_data, b_data);\n  if (HxW &lt; kCUDANumThreads) {\n    B = (N * C * HxW + kCUDANumThreads - 1) / kCUDANumThreads;\n    GroupNormForwardSimpleCUDAKernel&lt;T&gt;&lt;&lt;&lt;B, kCUDANumThreads, 0, cuda_stream&gt;&gt;&gt;(\n        N, C, HxW, X_data, a_data, b_data, Y_data);\n  } else {\n    GroupNormForwardCUDAKernel&lt;T&gt;&lt;&lt;&lt;N * C, kCUDANumThreads, 0, cuda_stream&gt;&gt;&gt;(\n        HxW, X_data, a_data, b_data, Y_data);\n  }\n  AT_CUDA_CHECK(cudaGetLastError());\n}"
  },
  {
    "objectID": "posts/tf-lookahead.html",
    "href": "posts/tf-lookahead.html",
    "title": "Lookahead优化器的tf.Keras实现",
    "section": "",
    "text": "论文《Lookahead Optimizer: k steps forward, 1 step back》的tf.Keras实现.\n参考自苏剑林的repo\n\n\ntf 1.14 的实现\n因为tf.keras的keras改动有点大,所以这里的实现和原本的不一样.\n# NOTE from https://github.com/bojone/keras_lookahead\nclass Lookahead(object):\n    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n    \"\"\"\n\n    def __init__(self, k=5, alpha=0.5):\n        self.k = k\n        self.alpha = alpha\n        self.count = 0\n\n    def inject(self, model: keras.models.Model):\n        \"\"\"Inject the Lookahead algorithm for the given model.\n        The following code is modified from keras's _make_train_function method.\n        See: https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497\n        \"\"\"\n        if not hasattr(model, 'train_function'):\n            raise RuntimeError('You must compile your model before using it.')\n\n        model._check_trainable_weights_consistency()\n        metrics_tensors = [\n            model._all_metrics_tensors[m] for m in model.metrics_names[1:]\n        ]\n        if model.train_function is None:\n            inputs = (model._feed_inputs +\n                      model._feed_targets +\n                      model._feed_sample_weights)\n            if not isinstance(K.symbolic_learning_phase(), int):\n                inputs += [K.symbolic_learning_phase()]\n            fast_params = model._collected_trainable_weights\n\n            with K.name_scope('training'):\n                with K.name_scope(model.optimizer.__class__.__name__):\n                    training_updates = model.optimizer.get_updates(\n                        params=fast_params,\n                        loss=model.total_loss)\n                    slow_params = [K.variable(p) for p in fast_params]\n\n                fast_updates = (model.updates +\n                                training_updates +\n                                model.get_updates_for(None) +\n                                model.get_updates_for(model.inputs))\n\n                slow_updates, copy_updates = [], []\n                for p, q in zip(fast_params, slow_params):\n                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n                    copy_updates.append(K.update(p, q))\n\n                # Gets loss and metrics. Updates weights at each call.\n                fast_train_function = K.function(\n                    inputs, [model.total_loss] + metrics_tensors,\n                    updates=fast_updates,\n                    name='fast_train_function',\n                    **model._function_kwargs)\n\n                def F(inputs):\n                    self.count += 1\n                    R = fast_train_function(inputs)\n                    if self.count % self.k == 0:\n                        K.batch_get_value(slow_updates)\n                        K.batch_get_value(copy_updates)\n                    return R\n\n                model.train_function = F\n\n\ntf 1.15 的实现\n因为新版本的tf.keras的keras改动又有点大,所以这里的实现和原本的又不一样.\nclass Lookahead(object):\n    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n    \"\"\"\n\n    def __init__(self, k=5, alpha=0.5):\n        self.k = k\n        self.alpha = alpha\n        self.count = 0\n\n    def inject(self, model: keras.models.Model):\n        \"\"\" from tensorflow.keras `_make_train_function` refer from\n         https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/keras/engine/training.py#L2091 and https://github.com/bojone/keras_lookahead/blob/master/lookahead.py\n        \"\"\"\n        has_recompiled = model._recompile_weights_loss_and_weighted_metrics()\n        model._check_trainable_weights_consistency()\n        if isinstance(model.optimizer, list):\n            raise ValueError('The `optimizer` in `compile` should be a single '\n                             'optimizer.')\n        # If we have re-compiled the loss/weighted metric sub-graphs then create\n        # train function even if one exists already. This is because\n        # `_feed_sample_weights` list has been updated on re-copmpile.\n        if getattr(self, 'train_function', None) is None or has_recompiled:\n            # Restore the compiled trainable state.\n            current_trainable_state = model._get_trainable_state()\n            model._set_trainable_state(model._compiled_trainable_state)\n\n            inputs = (model._feed_inputs +\n                      model._feed_targets +\n                      model._feed_sample_weights)\n            if not isinstance(K.symbolic_learning_phase(), int):\n                inputs += [K.symbolic_learning_phase()]\n\n            fast_params = model._collected_trainable_weights\n\n            with K.get_graph().as_default():\n                with K.name_scope('training'):\n                    # Training updates\n                    training_updates = model.optimizer.get_updates(\n                        params=fast_params, loss=model.total_loss)\n                    slow_params = [K.variable(p) for p in fast_params]\n\n                    fast_updates = (\n                        training_updates +\n                        # Unconditional updates\n                        model.get_updates_for(None) +\n                        # Conditional updates relevant to this model\n                        model.get_updates_for(model.inputs))\n\n                metrics = model._get_training_eval_metrics()\n                metrics_tensors = [\n                    m._call_result for m in metrics if hasattr(m, '_call_result')  # pylint: disable=protected-access\n                ]\n\n            with K.name_scope('training'):\n                slow_updates, copy_updates = [], []\n                for p, q in zip(fast_params, slow_params):\n                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n                    copy_updates.append(K.update(p, q))\n\n                # Gets loss and metrics. Updates weights at each call.\n                fast_train_function = K.function(\n                    inputs, [model.total_loss] + metrics_tensors,\n                    updates=fast_updates,\n                    name='train_function',\n                    **model._function_kwargs)\n\n                def F(inputs):\n                    self.count += 1\n                    R = fast_train_function(inputs)\n                    if self.count % self.k == 0:\n                        K.batch_get_value(slow_updates)\n                        K.batch_get_value(copy_updates)\n                    return R\n\n                setattr(model, 'train_function', F)\n\n            # Restore the current trainable state\n            model._set_trainable_state(current_trainable_state)\n\n\ntf2.0的实现\n# NOTE from https://github.com/bojone/keras_lookahead\nclass Lookahead(object):\n    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n    \"\"\"\n\n    def __init__(self, k=5, alpha=0.5):\n        self.k = k\n        self.alpha = alpha\n        self.count = 0\n\n    def inject(self, model: k.models.Model):\n        has_recompiled = model._recompile_weights_loss_and_weighted_metrics()\n        model._check_trainable_weights_consistency()\n        if isinstance(model.optimizer, list):\n            raise ValueError('The `optimizer` in `compile` should be a single '\n                             'optimizer.')\n        # If we have re-compiled the loss/weighted metric sub-graphs then create\n        # train function even if one exists already. This is because\n        # `_feed_sample_weights` list has been updated on re-copmpile.\n        if getattr(model, 'train_function', None) is None or has_recompiled:\n            current_trainable_state = model._get_trainable_state()\n            model._set_trainable_state(model._compiled_trainable_state)\n\n            inputs = (model._feed_inputs +\n                      model._feed_targets +\n                      model._feed_sample_weights)\n            if not isinstance(K.symbolic_learning_phase(), int):\n                inputs += [K.symbolic_learning_phase()]\n\n            with K.get_graph().as_default():\n                with K.name_scope('training'):\n                    # Training updates\n                    fast_params = model._collected_trainable_weights\n                    training_updates = model.optimizer.get_updates(\n                        params=fast_params, loss=model.total_loss)\n                    slow_params = [K.variable(p) for p in fast_params]\n\n                    fast_updates = (\n                        training_updates +\n                        model.get_updates_for(None) +\n                        model.get_updates_for(model.inputs)\n                    )\n                metrics = model._get_training_eval_metrics()\n                metrics_tensors = [\n                    m._call_result for m in metrics if hasattr(m, '_call_result')  # pylint: disable=protected-access\n                ]\n\n            with K.name_scope('training'):\n                slow_updates, copy_updates = [], []\n                for p, q in zip(fast_params, slow_params):\n                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n                    copy_updates.append(K.update(p, q))\n\n                # Gets loss and metrics. Updates weights at each call.\n                fast_train_function = K.function(\n                    inputs, [model.total_loss] + metrics_tensors,\n                    updates=fast_updates,\n                    name='fast_train_function',\n                    **model._function_kwargs)\n\n                def F(inputs):\n                    self.count += 1\n                    R = fast_train_function(inputs)\n                    if self.count % self.k == 0:\n                        K.batch_get_value(slow_updates)\n                        K.batch_get_value(copy_updates)\n                    return R\n\n                setattr(model, 'train_function', F)\n            # Restore the current trainable state\n            model._set_trainable_state(current_trainable_state)"
  },
  {
    "objectID": "posts/tf-keras-shape-error.html",
    "href": "posts/tf-keras-shape-error.html",
    "title": "tf.dataset无法推断shape导致错误",
    "section": "",
    "text": "使用tensorflow.keras的时候，tf.dataset在执行model.fit的时候报错：\nValueError: Cannot take the length of shape with unknown rank.\n这里大概率是因为tf.dataset中使用了tf.py_function导致无法自动推导出张 良的形状，所以需要自己手动设置形状。\n\n\n解决方案\n这里一定要使用tensorflow 1.x版本，2.0中我也没找到解决方案😓,使用tf.contrib.data.assert_element_shape 函数直接指定形状即可。\nimport tensorflow as tf\nfrom tensorflow.python import keras\n\nyolo_model = keras_yolo_mobilev2((240, 320, 3), 3, 20, 1., True)\n\nshapes = (yolo_model.input.shape, tuple(out.shape for out in yolo_model.output))\nh.train_dataset = h.train_dataset.apply(tf.contrib.data.assert_element_shape(shapes))\n\nyolo_model.fit(h.train_dataset, epochs=max_nrof_epochs, \n                steps_per_epoch=h.train_epoch_step,callbacks=[tbcall])"
  },
  {
    "objectID": "posts/tf-keras-mult-out.html",
    "href": "posts/tf-keras-mult-out.html",
    "title": "tf.keras多输出模型自定义loss",
    "section": "",
    "text": "自从看了苏剑林的博客之后,我对keras是越来越喜欢了,但是我更喜欢在tensorflow中使用keras,今天就来看看如何在tf.keras中自定义多输出模型的loss,并且搭配高效的tf.dataset.\nNOTE: tensorflow==2.0.0b0"
  },
  {
    "objectID": "posts/tf-keras-mult-out.html#使用自定义loss函数的方式",
    "href": "posts/tf-keras-mult-out.html#使用自定义loss函数的方式",
    "title": "tf.keras多输出模型自定义loss",
    "section": "1.使用自定义loss函数的方式",
    "text": "1.使用自定义loss函数的方式\nkeras中使用自定义loss函数后,他是自动将每一个输出与标签进行误差计算,这样的话要求你的loss函数必须适用到每一个输出,当然也可以根据输出尺寸什么的做一些动态调整,总的来说灵活度还差点.\n这样的好处是变量用的少,并且tf.dataset的操作比较方便,不需要写太多.\nimport tensorflow.python as tf\nfrom tensorflow.python import keras\nimport numpy as np\n\"\"\" 这个方式比较优雅,但是loss函数必须对所有输出适用 \"\"\"\nkeras.backend.clear_session()\nx = keras.Input(shape=(10))\nx_1 = keras.layers.Dense(10)(x)\nx_2 = keras.layers.Dense(10)(x)\nmodel = keras.Model(inputs=x, outputs=[x_1, x_2])\nmodel.summary()\n\ndef l(y_true: tf.Tensor, y_pred: tf.Tensor) -&gt; tf.Tensor:\n    return y_true - y_pred\n\ntrain_x = tf.data.Dataset.from_tensor_slices(np.random.rand(100, 10)).repeat().batch(32)\ntrain_y = tf.data.Dataset.from_tensor_slices((np.random.rand(100, 10), np.random.rand(100, 10))).repeat().batch(32)\n\ntrain_set = tf.data.Dataset.zip((train_x, train_y))\n\nmodel.compile('adam', l)\n\nmodel.fit(train_set, steps_per_epoch=30)"
  },
  {
    "objectID": "posts/tf-keras-mult-out.html#使用add_loss的方式",
    "href": "posts/tf-keras-mult-out.html#使用add_loss的方式",
    "title": "tf.keras多输出模型自定义loss",
    "section": "2.使用add_loss的方式",
    "text": "2.使用add_loss的方式\n这个方式更加灵活,但是必须要把输入输出都用到loss里面,这样一开始的灵活可能会造成后面的阻碍,并且对tf.dataset的处理也造成了一些困扰.\nimport tensorflow.python as tf\nfrom tensorflow.python import keras\nimport numpy as np\nkeras.backend.clear_session()\n\nx = keras.Input(shape=(10))\nlabel_1 = keras.Input(shape=(10))\nlabel_2 = keras.Input(shape=(10))\npred_1 = keras.layers.Dense(10)(x)\npred_2 = keras.layers.Dense(10)(x)\n\nmodel = keras.Model(inputs=[x, label_1, label_2], outputs=[pred_1, pred_2])\nmodel.summary()\n\ntrain_set = tf.data.Dataset.from_tensor_slices({'input_1': np.random.rand(100, 10), 'input_2': np.random.rand(100, 10), 'input_3': np.random.rand(100, 10)}).repeat().batch(32)\ntrain_set = train_set.map(lambda x: {'input_1': x['input_1'] + .1, 'input_2': x['input_2'] - .2, 'input_3': x['input_3'] - .5})\n\ndef losses(labels: list, preds: list):\n    l = 0\n    for i in range(len(labels)):\n        # 这里我可以给不同的label不同的loss操作\n        l += tf.reduce_sum(((labels[i] - preds[i])**2) * (i + 1))\n    return l\n\nmodel.add_loss(losses([label_1, label_2], [pred_1, pred_2]))\nmodel.compile('adam')\n\nmodel.fit(train_set, steps_per_epoch=30)"
  },
  {
    "objectID": "posts/tf-keras-callback.html",
    "href": "posts/tf-keras-callback.html",
    "title": "测试tf.keras中callback的运行状态",
    "section": "",
    "text": "要给yolo添加多尺度训练,因为tf.keras无法对dataset对象进行callback操作这也就算了,但是我没法得知dataset对象目前在生成训练数据还是测试数据,这个就很蛋疼,需要能在尽量不大改代码的同时添加多尺度训练方式,所以还得看tf.keras.callback.\n\n\n测试1\n最重要的就是能得到目前是训练还是测试状态,我写了个小程序去测试:\nclass T(k.callbacks.Callback):\n    def __init__(self):\n        super().__init__()\n\n    def on_train_begin(self, logs=None):\n        print('on_train_begin')\n\n    def on_train_end(self, logs=None):\n        print('on_train_end')\n\n    def on_test_begin(self, logs=None):\n        print('on_test_begin')\n\n    def on_test_batch_end(self, batch, logs=None):\n        print('on_test_batch_end')\n\n    def on_test_end(self, logs=None):\n        print('on_test_begin')\n\n    def on_predict_begin(self, logs=None):\n        print('predict')\n\n\ndef test_train_callback():\n    train_x = np.random.randn(1000, 10).astype(np.float32)\n    train_y = np.random.randn(1000, 1).astype(np.float32)\n\n    test_x = np.random.randn(1000, 10).astype(np.float32)\n    test_y = np.random.randn(1000, 1).astype(np.float32)\n    train_ds = (tf.data.Dataset.from_tensor_slices((train_x, train_y)).\n                shuffle(5000).\n                repeat().\n                batch(400, True).\n                map(lambda x, y: ((x), y)))\n    test_ds = (tf.data.Dataset.from_tensor_slices((test_x, test_y)).\n               shuffle(5000).\n               repeat().\n               batch(400, True).\n               map(lambda x, y: ((x), y)))\n\n    model = k.Sequential([kl.Dense(1)])\n    model.compile(k.optimizers.Adam(), 'mse')\n    model.fit(train_ds, epochs=3, steps_per_epoch=4, validation_data=test_ds, validation_steps=4, callbacks=[T()], verbose=0)\n\n\ntest_train_callback()\n获得:\non_train_begin\non_test_begin\non_test_batch_end\non_test_batch_end\non_test_batch_end\non_test_batch_end\non_test_begin\non_test_begin\non_test_batch_end\non_test_batch_end\non_test_batch_end\non_test_batch_end\non_test_begin\non_test_begin\non_test_batch_end\non_test_batch_end\non_test_batch_end\non_test_batch_end\non_test_begin\non_train_end\n这里可以分析得整一个周期都是train,但在验证过程中只有on_test_begin,没有on_test_end,只有on_test_batch_end.这就很难受了.除非我每个on_test_batch_end都调用一下禁止多尺度训练.\n不过使用dataset至少还是有个好处的,可以使用validation_steps统计on_test_batch_end次数~\n\n\n测试2\n我发现其实在进入测试阶段之后,on_train_batch_end是不会被调用的,那么我们其实直接可以直接设定就完事了,不需要再搞什么计数啥的,完美解决问题\nclass T(k.callbacks.Callback):\n    def __init__(self):\n        super().__init__()\n        self.cnt = 0\n        self.change_flag = True\n\n    def on_train_batch_end(self, batch, logs=None):\n        if self.cnt == 3:\n            print('change input scale')\n            self.change_flag = True\n            self.cnt = 0\n        else:\n            self.cnt += 1\n        print(f'Now change is {self.change_flag}')\n\n    def on_test_batch_end(self, batch, logs=None):\n        print(f'Now change is {self.change_flag}')\n\n    def on_test_begin(self, batch, logs=None):\n        print('reverse change input scale')\n        self.change_flag = False\n\n\ndef test_train_callback():\n    train_x = np.random.randn(1000, 10).astype(np.float32)\n    train_y = np.random.randn(1000, 1).astype(np.float32)\n\n    test_x = np.random.randn(1000, 10).astype(np.float32)\n    test_y = np.random.randn(1000, 1).astype(np.float32)\n    train_ds = (tf.data.Dataset.from_tensor_slices((train_x, train_y)).\n                shuffle(5000).\n                repeat().\n                batch(400, True).\n                map(lambda x, y: ((x), y)))\n    test_ds = (tf.data.Dataset.from_tensor_slices((test_x, test_y)).\n               shuffle(5000).\n               repeat().\n               batch(400, True).\n               map(lambda x, y: ((x), y)))\n\n    model = k.Sequential([kl.Dense(1)])\n    model.compile(k.optimizers.Adam(), 'mse')\n    model.fit(train_ds, epochs=3, steps_per_epoch=10, validation_data=test_ds, validation_steps=3, callbacks=[T()], verbose=0)\n\n\ntest_train_callback()\nNow change is True\nNow change is True\nNow change is True\nchange input scale\nNow change is True\nNow change is True\nNow change is True\nNow change is True\nchange input scale\nNow change is True\nNow change is True\nNow change is True\nreverse change input scale\nNow change is False\nNow change is False\nNow change is False\nNow change is False\nchange input scale\nNow change is True\nNow change is True\nNow change is True\nNow change is True\nchange input scale\nNow change is True\nNow change is True\nNow change is True\nNow change is True\nchange input scale\nNow change is True\nreverse change input scale\nNow change is False\nNow change is False\nNow change is False\nNow change is False\nNow change is False\nNow change is False\nchange input scale\nNow change is True\nNow change is True\nNow change is True\nNow change is True\nchange input scale\nNow change is True\nNow change is True\nNow change is True\nreverse change input scale\nNow change is False\nNow change is False\nNow change is False"
  },
  {
    "objectID": "posts/tf-ious.html",
    "href": "posts/tf-ious.html",
    "title": "tensorflow实现各种iou",
    "section": "",
    "text": "最近想搞半监督的东西，但是我发现一个人的精力着实不够，而且这个tensorflow也让我很难受，莫名其妙只要一用jit就core dump，不用jit训练又慢，显存又狂吃，再这样下去准备转mxnet了😤。然后今天把之前本来想做没做完的东西做做完，就是各种iou的tensorflow实现，顺便为自己做一个总结。\n\n\nTLDR\n先上代码就完事了：\nimport numpy as np\nimport tensorflow as tf\n\n\ndef center_to_corner(bbox: np.ndarray, to_all_scale=True, in_hw=None) -&gt; np.ndarray:\n    \"\"\"convert box coordinate from center to corner\n\n    Parameters\n    ----------\n    bbox : np.ndarray\n        bbox [c_x,c_y,w,h]\n    to_all_scale : bool, optional\n        weather to all image scale, by default True\n    in_hw : np.ndarray, optional\n        in hw, by default None\n\n    Returns\n    -------\n    np.ndarray\n        bbox [x1,y1,x2,y2]\n    \"\"\"\n    if to_all_scale:\n        x1 = (bbox[:, 0:1] - bbox[:, 2:3] / 2) * in_hw[1]\n        y1 = (bbox[:, 1:2] - bbox[:, 3:4] / 2) * in_hw[0]\n        x2 = (bbox[:, 0:1] + bbox[:, 2:3] / 2) * in_hw[1]\n        y2 = (bbox[:, 1:2] + bbox[:, 3:4] / 2) * in_hw[0]\n    else:\n        x1 = (bbox[:, 0:1] - bbox[:, 2:3] / 2)\n        y1 = (bbox[:, 1:2] - bbox[:, 3:4] / 2)\n        x2 = (bbox[:, 0:1] + bbox[:, 2:3] / 2)\n        y2 = (bbox[:, 1:2] + bbox[:, 3:4] / 2)\n\n    xyxy = np.hstack([x1, y1, x2, y2])\n    return xyxy\n\n\ndef tf_center_to_corner(bbox: tf.Tensor, to_all_scale=True, in_hw=None) -&gt; tf.Tensor:\n    \"\"\"convert box coordinate from center to corner\n\n    Parameters\n    ----------\n    bbox : tf.Tensor\n        bbox [c_x,c_y,w,h]\n    to_all_scale : bool, optional\n        weather to all image scale, by default True\n    in_hw : tf.Tensor, optional\n        in hw, by default None\n\n    Returns\n    -------\n    np.ndarray\n        bbox [x1,y1,x2,y2]\n    \"\"\"\n    if to_all_scale:\n        x1 = (bbox[..., 0:1] - bbox[..., 2:3] / 2) * in_hw[1]\n        y1 = (bbox[..., 1:2] - bbox[..., 3:4] / 2) * in_hw[0]\n        x2 = (bbox[..., 0:1] + bbox[..., 2:3] / 2) * in_hw[1]\n        y2 = (bbox[..., 1:2] + bbox[..., 3:4] / 2) * in_hw[0]\n    else:\n        x1 = (bbox[..., 0:1] - bbox[..., 2:3] / 2)\n        y1 = (bbox[..., 1:2] - bbox[..., 3:4] / 2)\n        x2 = (bbox[..., 0:1] + bbox[..., 2:3] / 2)\n        y2 = (bbox[..., 1:2] + bbox[..., 3:4] / 2)\n\n    xyxy = tf.concat([x1, y1, x2, y2], -1)\n    return xyxy\n\n\ndef corner_to_center(bbox: np.ndarray, from_all_scale=True, in_hw=None) -&gt; np.ndarray:\n    \"\"\"convert box coordinate from corner to center\n\n    Parameters\n    ----------\n    bbox : np.ndarray\n        bbox [x1,y1,x2,y2]\n    to_all_scale : bool, optional\n        weather to all image scale, by default True\n    in_hw : np.ndarray, optional\n        in hw, by default None\n\n    Returns\n    -------\n    np.ndarray\n        bbox [c_x,c_y,w,h]\n    \"\"\"\n    if from_all_scale:\n        x = ((bbox[..., 2:3] + bbox[..., 0:1]) / 2) / in_hw[1]\n        y = ((bbox[..., 3:4] + bbox[..., 1:2]) / 2) / in_hw[0]\n        w = (bbox[..., 2:3] - bbox[..., 0:1]) / in_hw[1]\n        h = (bbox[..., 3:4] - bbox[..., 1:2]) / in_hw[0]\n    else:\n        x = ((bbox[..., 2:3] + bbox[..., 0:1]) / 2)\n        y = ((bbox[..., 3:4] + bbox[..., 1:2]) / 2)\n        w = (bbox[..., 2:3] - bbox[..., 0:1])\n        h = (bbox[..., 3:4] - bbox[..., 1:2])\n\n    xywh = np.hstack([x, y, w, h])\n    return xywh\n\n\ndef tf_corner_to_center(bbox: tf.Tensor, from_all_scale=True, in_hw=None) -&gt; tf.Tensor:\n    \"\"\"convert box coordinate from corner to center\n\n    Parameters\n    ----------\n    bbox : tf.Tensor\n        bbox [x1,y1,x2,y2]\n    to_all_scale : bool, optional\n        weather to all image scale, by default True\n    in_hw : tf.Tensor, optional\n        in hw, by default None\n\n    Returns\n    -------\n    np.ndarray\n        bbox [c_x,c_y,w,h]\n    \"\"\"\n    if from_all_scale:\n        x = ((bbox[..., 2:3] + bbox[..., 0:1]) / 2) / in_hw[1]\n        y = ((bbox[..., 3:4] + bbox[..., 1:2]) / 2) / in_hw[0]\n        w = (bbox[..., 2:3] - bbox[..., 0:1]) / in_hw[1]\n        h = (bbox[..., 3:4] - bbox[..., 1:2]) / in_hw[0]\n    else:\n        x = ((bbox[..., 2:3] + bbox[..., 0:1]) / 2)\n        y = ((bbox[..., 3:4] + bbox[..., 1:2]) / 2)\n        w = (bbox[..., 2:3] - bbox[..., 0:1])\n        h = (bbox[..., 3:4] - bbox[..., 1:2])\n\n    xywh = np.concatenate([x, y, w, h], -1)\n    return xywh\n\n\ndef bbox_iou(a: np.ndarray, b: np.ndarray, offset: int = 0) -&gt; np.ndarray:\n    \"\"\"Calculate Intersection-Over-Union(IOU) of two bounding boxes.\n\n    Parameters\n    ----------\n    a : np.ndarray\n\n        (n,4) x1,y1,x2,y2\n\n    b : np.ndarray\n\n        (m,4) x1,y1,x2,y2\n\n\n    offset : int, optional\n        by default 0\n\n    Returns\n    -------\n    np.ndarray\n\n        iou (n,m)\n    \"\"\"\n    tl = np.maximum(a[:, None, :2], b[:, :2])\n    br = np.minimum(a[:, None, 2:4], b[:, 2:4])\n\n    area_i = np.prod(br - tl + offset, axis=2) * (tl &lt; br).all(axis=2)\n    area_a = np.prod(a[:, 2:4] - a[:, :2] + offset, axis=1)\n    area_b = np.prod(b[:, 2:4] - b[:, :2] + offset, axis=1)\n    return area_i / (area_a[:, None] + area_b - area_i)\n\n\ndef tf_bbox_iou(a: tf.Tensor, b: tf.Tensor, offset: int = 0) -&gt; tf.Tensor:\n    \"\"\"Calculate Intersection-Over-Union(IOU) of two bounding boxes.\n\n    Parameters\n    ----------\n    a : tf.Tensor\n\n        (n,4) x1,y1,x2,y2\n\n    b : tf.Tensor\n\n        (m,4) x1,y1,x2,y2\n\n\n    offset : int, optional\n        by default 0\n\n    Returns\n    -------\n    tf.Tensor\n\n        iou (n,m)\n    \"\"\"\n    a = a[..., None, :]\n    tl = tf.maximum(a[..., :2], b[..., :2])\n    br = tf.minimum(a[..., 2:4], b[..., 2:4])\n\n    area_i = tf.reduce_prod(tf.maximum(br - tl, 0) + offset, axis=-1)\n    area_a = tf.reduce_prod(a[..., 2:4] - a[..., :2] + offset, axis=-1)\n    area_b = tf.reduce_prod(b[..., 2:4] - b[..., :2] + offset, axis=-1)\n    return area_i / (area_a + area_b - area_i)\n\n\ndef tf_bbox_giou(a: tf.Tensor, b: tf.Tensor, offset: int = 0) -&gt; tf.Tensor:\n    \"\"\"Calculate GIOU of two bounding boxes.\n\n    Parameters\n    ----------\n    a : tf.Tensor\n\n        (n,4) x1,y1,x2,y2\n\n    b : tf.Tensor\n\n        (m,4) x1,y1,x2,y2\n\n\n    offset : int, optional\n        by default 0\n\n    Returns\n    -------\n    tf.Tensor\n\n        giou (n,m)\n    \"\"\"\n    a = a[..., None, :]\n    tl = tf.maximum(a[..., :2], b[..., :2])\n    br = tf.minimum(a[..., 2:4], b[..., 2:4])\n\n    area_i = tf.reduce_prod(tf.maximum(br - tl, 0) + offset, axis=-1)\n    area_a = tf.reduce_prod(a[..., 2:4] - a[..., :2] + offset, axis=-1)\n    area_b = tf.reduce_prod(b[..., 2:4] - b[..., :2] + offset, axis=-1)\n\n    outer_tl = tf.minimum(a[..., :2], b[..., :2])\n    outer_br = tf.maximum(a[..., 2:4], b[..., 2:4])\n    area_o = tf.reduce_prod(tf.maximum(outer_br - outer_tl, 0) + offset, axis=-1)\n    union = (area_a + area_b - area_i)\n    return (area_i / union) - ((area_o - union) / area_o)\n\n\ndef tf_bbox_diou(a: tf.Tensor, b: tf.Tensor, offset: int = 0) -&gt; tf.Tensor:\n    \"\"\"Calculate DIoU of two bounding boxes.\n\n    Parameters\n    ----------\n    a : tf.Tensor\n\n        (n,4) x1,y1,x2,y2\n\n    b : tf.Tensor\n\n        (m,4) x1,y1,x2,y2\n\n    offset : int, optional\n        by default 0\n\n    Returns\n    -------\n    tf.Tensor\n\n        diou (n,m)\n    \"\"\"\n    a = a[..., None, :]\n    tl = tf.maximum(a[..., :2], b[..., :2])\n    br = tf.minimum(a[..., 2:4], b[..., 2:4])\n\n    area_i = tf.reduce_prod(tf.maximum(br - tl, 0) + offset, axis=-1)\n    area_a = tf.reduce_prod(a[..., 2:4] - a[..., :2] + offset, axis=-1)\n    area_b = tf.reduce_prod(b[..., 2:4] - b[..., :2] + offset, axis=-1)\n    iou = area_i / (area_a + area_b - area_i)\n\n    outer_tl = tf.minimum(a[..., :2], b[..., :2])\n    outer_br = tf.maximum(a[..., 2:4], b[..., 2:4])\n    # two bbox center distance sum((b_cent-a_cent)^2)\n    inter_diag = tf.reduce_sum(tf.square((b[..., :2] + b[..., 2:]) / 2\n                                         - (a[..., :2] + a[..., 2:]) / 2 + offset), -1)\n    # two bbox diagonal distance\n    outer_diag = tf.reduce_sum(tf.square(outer_tl - outer_br + offset), -1)\n    return tf.clip_by_value(iou - inter_diag / outer_diag, -1., 1.)\n\n\ndef tf_bbox_ciou(a: tf.Tensor, b: tf.Tensor, offset: int = 0) -&gt; tf.Tensor:\n    \"\"\"Calculate CIoU of two bounding boxes.\n\n    Parameters\n    ----------\n    a : tf.Tensor\n\n        (n,4) x1,y1,x2,y2\n\n    b : tf.Tensor\n\n        (m,4) x1,y1,x2,y2\n\n    offset : int, optional\n        by default 0\n\n    Returns\n    -------\n    tf.Tensor\n\n        ciou (n,m)\n    \"\"\"\n    a = a[..., None, :]\n    tl = tf.maximum(a[..., :2], b[..., :2])\n    br = tf.minimum(a[..., 2:4], b[..., 2:4])\n\n    area_i = tf.reduce_prod(tf.maximum(br - tl, 0) + offset, axis=-1)\n    area_a = tf.reduce_prod(a[..., 2:4] - a[..., :2] + offset, axis=-1)\n    area_b = tf.reduce_prod(b[..., 2:4] - b[..., :2] + offset, axis=-1)\n    iou = area_i / (area_a + area_b - area_i)\n\n    outer_tl = tf.minimum(a[..., :2], b[..., :2])\n    outer_br = tf.maximum(a[..., 2:4], b[..., 2:4])\n    # two bbox center distance sum((b_cent-a_cent)^2)\n    inter_diag = tf.reduce_sum(tf.square((b[..., :2] + b[..., 2:]) / 2\n                                         - (a[..., :2] + a[..., 2:]) / 2 + offset), -1)\n    # two bbox diagonal distance\n    outer_diag = tf.reduce_sum(tf.square(outer_tl - outer_br + offset), -1)\n    # calc ciou alpha paramter\n\n    arctan = tf.stop_gradient(\n        (tf.math.atan(tf.math.divide_no_nan(b[..., 2] - b[..., 0],\n                                            b[..., 3] - b[..., 1]))\n         - tf.math.atan(tf.math.divide_no_nan(a[..., 2] - a[..., 0],\n                                              a[..., 3] - a[..., 1]))))\n\n    v = tf.stop_gradient(tf.math.square(2 / np.pi * arctan))\n    alpha = tf.stop_gradient(v / ((1 - iou) + v))\n    w_temp = tf.stop_gradient(2 * (a[..., 2] - a[..., 0]))\n\n    ar = (8 / tf.square(np.pi)) * arctan * ((a[..., 2] - a[..., 0] - w_temp) * (a[..., 3] - a[..., 1]))\n\n    return tf.clip_by_value(iou - (inter_diag / outer_diag + alpha * ar), -1., 1.)\n\n\ndef bbox_iof(a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate Intersection-Over-Foreground(IOF) of two bounding boxes.\n\n    Parameters\n    ----------\n    a : np.ndarray\n\n        (n,4) x1,y1,x2,y2\n\n    b : np.ndarray\n\n        (m,4) x1,y1,x2,y2\n\n\n    offset : int, optional\n        by default 0\n\n    Returns\n    -------\n    np.ndarray\n\n        iof (n,m)\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt &lt; rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    return area_i / np.maximum(area_a[:, np.newaxis], 1)\n\n\ndef nms_oneclass(bbox: np.ndarray, score: np.ndarray, thresh: float) -&gt; np.ndarray:\n    \"\"\"Pure Python NMS oneclass baseline.\n\n    Parameters\n    ----------\n    bbox : np.ndarray\n\n        bbox, n*(x1,y1,x2,y2)\n\n    score : np.ndarray\n\n        confidence score (n,)\n\n    thresh : float\n\n        nms thresh\n\n    Returns\n    -------\n    np.ndarray\n        keep index\n    \"\"\"\n    x1 = bbox[:, 0]\n    y1 = bbox[:, 1]\n    x2 = bbox[:, 2]\n    y2 = bbox[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = score.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n因为我是按原来yolo中交叉求iou的方式统一编写的，所以默认是交叉求iou值的，如果说要按对应元素求iou，那么给boxes2添加一个维度即可：\ndef test_bbox():\n    boxes1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]]) / 20.\n    boxes2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]]) / 20.\n    \"\"\" 按元素求iou \"\"\"\n    print(tf_bbox_iou(boxes1, boxes2[..., None, :]).numpy())\n    # [[0.12500001] [0.        ]]\n    print(tf_bbox_diou(boxes1, boxes2[..., None, :]).numpy())\n    # [[ 0.00304878] [-0.6243095 ]]\n    print(tf_bbox_ciou(boxes1, boxes2[..., None, :]).numpy())\n    # [[ 0.00283995] [-0.6250417 ]]\n    print(tf_bbox_giou(boxes1, boxes2[..., None, :]).numpy())\n    # [[-0.07499996] [-0.93333334]]\n\n    \"\"\" 交叉求iou \"\"\"\n    print(tf_bbox_iou(boxes1, boxes2).numpy())\n    # [[0.12500001 0.        ] [0.0625     0.        ]]\n    print(tf_bbox_diou(boxes1, boxes2).numpy())\n    # [[ 0.00304878 -0.72169816] [-0.07980768 -0.6243095 ]]\n    print(tf_bbox_ciou(boxes1, boxes2).numpy())\n    # [[ 0.00283995 -0.7217355 ] [-0.08119209 -0.6250417 ]]\n    print(tf_bbox_giou(boxes1, boxes2).numpy())\n    # [[-0.07499996 -0.9469697 ] [-0.3660715  -0.93333334]]\n\n\nIOU\n\niou比较简单，交集除以并集即可，只需要注意没有交集时设置交集为0即可：\narea_i = tf.reduce_prod(tf.maximum(br - tl, 0) + offset, axis=-1)\n\n\nGIOU\n传统我们拟合bbox靠mse，如下图： \n图a与图b中，mse的值都一样，但是他们的不重叠方式却完全不同，也就是说mse的优化不代表iou就可以变好。那么咱们就直接来优化iou好了，但是直接优化iou又有一系列问题：\n\n两个bbox不相交的时候iou值为0，这样没法优化。\niou不知道bbox的相交形式，比如下图，三个iou值都相同。\n\n\n所以giou就是来解决这两个问题的，假如现在有两个矩形：A、B，我们找到一个最小的封闭矩形C，让C可以把A、B包含在内，然后我们计算C中没有覆盖A和B的面积占C总面积的比值，然后用A与B的iou减去这个比值(以下A均代表预测矩形、B均代表标签矩形)：\n\n公式化： \\[\n\\begin{aligned}\n    giou&=iou-\\frac{|\\frac{C}{A\\cup B}|}{|C|},giou\\in[-1,1]\\\\\n    Loss_{giou}&= 1-giou\n\\end{aligned}\n\\]\n当两个矩形完全重叠时，\\(giou==iou==1\\)，当两个矩形重叠区域越少\\(giou\\rightarrow -1\\),这个性质就相当好。\n\n\nDIOU\n但是giou也有没考虑到的地方，那就是如果一个矩形被另一个矩形包含的时候，他的后一项就退化为了1，也就是说退化为了iou，因此提出diou来解决在矩形重合时的进一步优化问题(此处参考知乎作者，下面的公式我按自己实现的来写)。\n\n也就是在iou中添加中心点距离作为损失： \n\\[\n\\begin{aligned}\n    center\\ diag&= \\sum((A_{cent} -B_{cent})^2)\\\\\n    outer\\ diag&= \\sum((C_{tl} -C_{rb})^2)\\\\\n    diou&=iou-\\frac{center\\ diag}{outer\\ diag}，diou\\in[-1,1]\n\\end{aligned}\n\\]\ndiou的性质和giou类似，且加入了中心距离惩罚。\n\n\nCIOU\n在diou中还有矩形的长宽没有考虑到，在传统mse中，矩形回归有一个关于长宽的权重:\\(2-w^{g t}\\times h^{g t}\\)，为了更好的利用矩形的长宽，同时考虑预测框与标签框的长宽，提出了ciou (这里他提出的公式中\\(\\rho\\)代表欧式距离，下面我就按我的理解来写公式了)：\n\\[\n\\begin{aligned}\n   v&=[\\frac{2}{\\pi}(\\arctan\\frac{w^{g t}}{h^{g t}}-\\arctan\\frac{w}{h})]^2\\\\\n   \\alpha&=\\frac{v}{1-iou+v}\\\\\n   ciou&=iou-\\frac{\\rho^2(b,b_{gt})}{c^2} - \\alpha v\\\\\n      &=iou-  \\frac{center\\ diag}{outer\\ diag} - \\alpha v\n\\end{aligned}\n\\]\n然后作者还考虑的求导数时的情况： \\[\n\\begin{aligned}\n\\frac{\\partial v}{\\partial w}&=\\frac{8}{\\pi^{2}}\\left(\\arctan \\frac{w^{g t}}{h^{g t}}-\\arctan \\frac{w}{h}\\right) \\times \\frac{h}{w^{2}+h^{2}} \\\\\n\\frac{\\partial v}{\\partial h}&=-\\frac{8}{\\pi^{2}}\\left(\\arctan \\frac{w^{g t}}{h^{g t}}-\\arctan \\frac{w}{h}\\right) \\times \\frac{w}{w^{2}+h^{2}}\n\\end{aligned}\n\\]\n我参考的知乎文章说当\\(w,h\\in[0,1]\\)时，\\(\\frac{w}{w^{2}+h^{2}}\\)和\\(\\frac{h}{w^{2}+h^{2}}\\)的值都比较小，容易导致梯度爆炸，因此作者在代码实现上进行了一些修改。\n但是当我检查了作者做的修改后，发现好像并不是那样，我的代码已经按作者修改的来改了，实际作者实现的ciou公式是这样的：\n\\[\n\\begin{aligned}\n    \\text{atan} &= \\arctan\\frac{w^{g t}}{h^{g t}}-\\arctan\\frac{w}{h}\\\\\n    v&=(\\frac{2}{\\pi}\\ \\text{atan})^2\\\\  \n   \\alpha&=\\frac{v}{1-iou+v}\\\\\n   w_{temp}&=2w\\\\\n   ar&= \\frac{8}{\\pi^2}\\ \\text{atan}\\ (w-w_{temp})\\ h\\\\\n    ciou &= iou-  \\frac{center\\ diag}{outer\\ diag} - \\alpha\\ ar   \n\\end{aligned}\n\\]\n其中\\(\\text{atan},v,w_{temp}\\)均屏蔽梯度，我又做了些梯度测试，在测试代码中可以看到，对于\\(ar\\)求梯度只有预测框包含梯度，我估计作者的主要目的应该是消除标签框的梯度，因为最后一项只有\\(ar\\)中的预测\\(w,h\\)包含了梯度。\ndef test_grad():\n    offset = 0.\n    a = tf.Variable([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    b = tf.Variable([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])\n    with tf.GradientTape(True) as tape:\n        a = a[..., None, :]\n        b = b[..., None, :]\n        tl = tf.maximum(a[..., :2], b[..., :2])\n        br = tf.minimum(a[..., 2:4], b[..., 2:4])\n\n        area_i = tf.reduce_prod(tf.maximum(br - tl, 0) + offset, axis=-1)\n        area_a = tf.reduce_prod(a[..., 2:4] - a[..., :2] + offset, axis=-1)\n        area_b = tf.reduce_prod(b[..., 2:4] - b[..., :2] + offset, axis=-1)\n        iou = area_i / (area_a + area_b - area_i)\n\n        outer_tl = tf.minimum(a[..., :2], b[..., :2])\n        outer_br = tf.maximum(a[..., 2:4], b[..., 2:4])\n        # two bbox center distance sum((b_cent-a_cent)^2)\n        inter_diag = tf.reduce_sum(tf.square((b[..., :2] + b[..., 2:]) / 2\n                                             - (a[..., :2] + a[..., 2:]) / 2 + offset), -1)\n        # two bbox diagonal distance\n        outer_diag = tf.reduce_sum(tf.square(outer_tl - outer_br + offset), -1)\n        # calc ciou alpha paramter\n\n        arctan = (tf.math.atan(tf.math.divide_no_nan(b[..., 2] - b[..., 0],\n                                                     b[..., 3] - b[..., 1]))\n                  - tf.math.atan(tf.math.divide_no_nan(a[..., 2] - a[..., 0],\n                                                       a[..., 3] - a[..., 1])))\n\n        v = tf.math.square(2 / np.pi) * tf.square(arctan)\n        alpha = v / ((1 - iou) + v)\n        w_temp = 2 * (a[..., 2] - a[..., 0])\n\n        ar = (8 / tf.square(np.pi)) * arctan * ((a[..., 2] - a[..., 0] - w_temp) * (a[..., 3] - a[..., 1]))\n\n        ciou = iou - (inter_diag / outer_diag) - (alpha * ar)\n\n    tape.gradient(v, [a, b])\n    \"\"\" [[[-0.04231081,  0.06346621,  0.04231081, -0.06346621]],\n        [[-0.01833142,  0.09165711,  0.01833142, -0.09165711]]]\n\n        [[[ 0.04400324, -0.03300243, -0.04400324,  0.03300243]],\n        [[ 0.23830847, -0.23830847, -0.23830847,  0.23830847]]]\"\"\"\n    tape.gradient(ciou, [a, b])\n    \"\"\"\n        [[[ 0.06351729, -0.08257562, -0.15244228,  0.27827212]],\n        [[ 0.08103281,  0.01117781, -0.07266921,  0.01513398]]],\n\n        [[[-0.0851735 , -0.06970734,  0.17409849, -0.12598914]],\n        [[-0.7246207 ,  0.6417478 ,  0.7162571 , -0.6680595 ]]]\n    \"\"\"\n\n    offset = 0.\n    a = tf.Variable([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    b = tf.Variable([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])\n    with tf.GradientTape(True) as tape:\n        a = a[..., None, :]\n        b = b[..., None, :]\n        tl = tf.maximum(a[..., :2], b[..., :2])\n        br = tf.minimum(a[..., 2:4], b[..., 2:4])\n\n        area_i = tf.reduce_prod(tf.maximum(br - tl, 0) + offset, axis=-1)\n        area_a = tf.reduce_prod(a[..., 2:4] - a[..., :2] + offset, axis=-1)\n        area_b = tf.reduce_prod(b[..., 2:4] - b[..., :2] + offset, axis=-1)\n        iou = area_i / (area_a + area_b - area_i)\n\n        outer_tl = tf.minimum(a[..., :2], b[..., :2])\n        outer_br = tf.maximum(a[..., 2:4], b[..., 2:4])\n        # two bbox center distance sum((b_cent-a_cent)^2)\n        inter_diag = tf.reduce_sum(tf.square((b[..., :2] + b[..., 2:]) / 2\n                                             - (a[..., :2] + a[..., 2:]) / 2 + offset), -1)\n        # two bbox diagonal distance\n        outer_diag = tf.reduce_sum(tf.square(outer_tl - outer_br + offset), -1)\n        # calc ciou alpha paramter\n\n        arctan = tf.stop_gradient(\n            (tf.math.atan(tf.math.divide_no_nan(b[..., 2] - b[..., 0],\n                                                b[..., 3] - b[..., 1]))\n             - tf.math.atan(tf.math.divide_no_nan(a[..., 2] - a[..., 0],\n                                                  a[..., 3] - a[..., 1]))))\n\n        v = tf.stop_gradient(tf.math.square(2 / np.pi) * tf.square(arctan))\n        alpha = tf.stop_gradient(v / ((1 - iou) + v))\n        w_temp = tf.stop_gradient(2 * (a[..., 2] - a[..., 0]))\n        ar = (8 / tf.square(np.pi)) * arctan * ((a[..., 2] - a[..., 0] - w_temp) * (a[..., 3] - a[..., 1]))\n\n        ciou = iou - (inter_diag / outer_diag) - (alpha * ar)\n\n    tape.gradient(ar, [a, b])\n    \"\"\"\n        [[[ 0.5500405 , -0.8250607 , -0.5500405 ,  0.8250607 ]],\n        [[ 0.47661695, -2.3830848 , -0.47661695,  2.3830848 ]]],\n\n        None\n     \"\"\"\n    tape.gradient(v, [a, b])\n    \"\"\" [None, None] \"\"\"\n    tape.gradient(ciou, [a, b])\n    \n    \"\"\"\n        [[[-0.10692195,  0.08424009,  0.01162432,  0.12420168]],\n        [[-0.08888854,  0.27500343,  0.09725215, -0.24869165]]]\n\n        [[[ 0.03184488, -0.16596799,  0.06345274, -0.04247379]],\n        [[-0.03867403, -0.0441989 ,  0.03031043,  0.01788712]]]\n    \"\"\"\n到这里应该差不多了，注意ciou的值因为加上了wh，因此框的尺度会影响结果，记得把框的尺寸归一化到0-1之间。\n累了累了。。休息了"
  },
  {
    "objectID": "posts/tf-facerec.html",
    "href": "posts/tf-facerec.html",
    "title": "tensorflow人脸识别",
    "section": "",
    "text": "在此记录一下参考insightface用tensorflow实现人脸识别的过程。\n\n\n难点1: tf.keras不支持动态切换输出\n  训练的时候需要使用softmax进行训练，但是验证的时候需要输出embedding层的结果进行距离比较判断是否为同一个人。使用tf.keras的方式无法做到方便的动态切换，因此我写了上面一篇博客“tf2.0 自定义Model高级用法”。但是发现这样还是没法让训练和测试时输出维度不同，后来在群里问了苏神，可以直接分成3个模型infer_model, val_model, train_model，并删除train_model.fit()时期的验证，自己重新定义一个callback对val_model进行验证，最后导出时使用infer_model。\nclass FacerecValidation(k.callbacks.Callback):\n    def __init__(self, validation_model: k.Model, validation_data: tf.data.Dataset, validation_steps: int,\n                 distance_fn: str, threshold: float):\n        self.val_model = validation_model\n        self.val_iter: Iterable[Tuple[Tuple[tf.Tensor, tf.Tensor], tf.Tensor]] = iter(validation_data)\n        self.val_step = validation_steps\n        self.distance_fn: l2distance = distance_register[distance_fn]\n        self.threshold = threshold\n\n    def on_epoch_end(self, epoch: int, logs=None):\n        logs = logs or {}\n        acc: List[tf.Tensor] = []\n        for i in range(validation_steps):\n            x, actual_issame = next(self.val_iter)  # actual_issame:tf.Bool\n            y_pred: Tuple[tf.Tensor] = self.val_model.predict(x)\n            dist = self.distance_fn(*y_pred)  # [batch]\n            pred_issame = tf.less(dist, self.threshold)\n\n            tp = tf.reduce_sum(tf.logical_and(pred_issame, actual_issame))\n            fp = tf.reduce_sum(tf.logical_and(pred_issame, tf.logical_not(actual_issame)))\n            tn = tf.reduce_sum(tf.logical_and(tf.logical_not(pred_issame), tf.logical_not(actual_issame)))\n            fn = tf.reduce_sum(tf.logical_and(tf.logical_not(pred_issame), actual_issame))\n\n            tpr = tf.math.divide_no_nan(tf.cast(tp, tf.float32), tf.cast(tp + fn, tf.float32))\n            fpr = tf.math.divide_no_nan(tf.cast(fp, tf.float32), tf.cast(fp + tn, tf.float32))\n            acc.append[tf.cast(tp + tn, tf.float32) / tf.cast(dist.size, tf.float32)]\n        acc = tf.reduce_mean(acc)\n        logs['val_acc'] = acc.numpy()\n        \n        return super().on_epoch_end(epoch, logs=logs)\n\n\n难点2: tf.data难以输出triplet对数据\n  本来想用传统的索引+原始图片的方式输入数据，但是索引矩阵长就58万了，我的电脑构建好数据管道之后直接爆炸。。。然后我就思考怎么用tfrecord的方式构建triplet对，对于训练softmax来说比较简单，既可以把所有图片都做成一个tfrecord，还可以把每个id对应的图片做成一个tfrecord，里面只要包含图像和标签数据即可。\n  但是所有图片做成一个tfrecord肯定是不行的，因为咱们没办法在tfrecord里面根据索引进行查找图像。这里我真的很想换到mxnet，他的iamgerecorditer是高度压缩且带索引的格式，tf为何就是不能把tfrecord里面的索引暴露出来，哪怕一个索引对应一块数据也可以啊。。但现在只能退而求其次，把每个id对应一个tfrecord，然后使用tf.data里面的函数进行构建。\n  一番尝试之后，构建代码如下，首先h.train_list包含了所有tfrecord的路径，然后利用interleave将所有的路径转换为dataset对象，这里需要设置block_length=2，这样就保证每个采样周期为2，接着再使用batch(4)，这样我们可以从两个不同类别中分别采样两个样本，此时得到了4张图像，取前三张即可满足要求。\nh = FcaeRecHelper('data/ms1m_img_ann.npy', [112, 112], 128, use_softmax=False)\nlen(h.train_list)\nimg_shape = list(h.in_hw) + [3]\n\nis_augment = True\nis_normlize = False\n\ndef parser(stream: bytes):\n    examples: dict = tf.io.parse_single_example(\n        stream,\n        {'img': tf.io.FixedLenFeature([], tf.string),\n         'label': tf.io.FixedLenFeature([], tf.int64)})\n    return tf.image.decode_jpeg(examples['img'], 3), examples['label']\n\ndef pair_parser(raw_imgs, labels):\n    # imgs do same augment ~\n    if is_augment:\n        raw_imgs, _ = h.augment_img(raw_imgs, None)\n    # normlize image\n    if is_normlize:\n        imgs: tf.Tensor = h.normlize_img(raw_imgs)\n    else:\n        imgs = tf.cast(raw_imgs, tf.float32)\n\n    imgs.set_shape([4] + img_shape)\n    labels.set_shape([4, ])\n    # Note y_true shape will be [batch,3]\n    return (imgs[0], imgs[1], imgs[2]), (labels[:3])\n\nbatch_size = 1\nds = (tf.data.Dataset.from_tensor_slices(h.train_list)\n      .interleave(lambda x: tf.data.TFRecordDataset(x)\n                  .shuffle(100)\n                  .repeat(), cycle_length=-1,\n                  block_length=2,\n                  num_parallel_calls=-1)\n      .map(parser, -1)\n      .batch(4, True)\n      .map(pair_parser, -1)\n      .batch(batch_size, True))\n\niters = iter(ds)\nfor i in range(20):\n    imgs, labels = next(iters)\n    fig, axs = plt.subplots(1, 3)\n    axs[0].imshow(imgs[0].numpy().astype('uint8')[0])\n    axs[1].imshow(imgs[1].numpy().astype('uint8')[0])\n    axs[2].imshow(imgs[2].numpy().astype('uint8')[0])\n    plt.show()"
  },
  {
    "objectID": "posts/tf-custom-model.html",
    "href": "posts/tf-custom-model.html",
    "title": "tf2.0 自定义Model高级用法",
    "section": "",
    "text": "最近看insightface深受启发，他的人脸识别在训练时可以训练softmax的，还可以训练triplet loss的，并且在验证时是对图像pair进行验证的。这几天弄论文顺便先给tf里面的模型写个骨架出来。这里的主要难点在于如何用tf.keras自带Model类实现训练和测试时不同的行为，今天尝试了一下，做个总结。"
  },
  {
    "objectID": "posts/tf-custom-model.html#单输出前向推导不同行为",
    "href": "posts/tf-custom-model.html#单输出前向推导不同行为",
    "title": "tf2.0 自定义Model高级用法",
    "section": "单输出前向推导不同行为",
    "text": "单输出前向推导不同行为\n这个比较简单，参考dropout层的实现方式即可。不过如果想控制训练和验证输出维度不同，这个好像暂时没有解决方案，除非使用动态图的写法。\nimport tensorflow as tf\nfrom tensorflow.python.keras.utils.tf_utils import smart_cond\nfrom typing import List\nimport numpy as np\nk = tf.keras\nkl = tf.keras.layers\nK = tf.keras.backend\n\n\nclass Model_1(k.Model):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def call(self, inputs, training=None, mask=None):\n        \"\"\" 使用tf.cond可以控制循环，参考dropout\"\"\"\n        print(training)\n        if training is None:\n            training = K.learning_phase()\n        return smart_cond(training, lambda: tf.ones([1]), lambda: tf.zeros([1]))\n\n\ndef test_model_train_validation():\n    \"\"\" 测试模型train与validation的不同行为，目前是成功的\n    NOTE 训练和验证的输出必须是一样的 \"\"\"\n    train_x = tf.ones([1000, 1])\n    train_y = tf.ones([1000, 1])\n    md = Model_1()\n    md.compile(loss=lambda y_ture, y_pred: tf.reduce_sum(y_pred))\n    md.fit(train_x, train_y, batch_size=32)\n\n    # md.predict(train_x)\n    md.evaluate(train_x, train_y, batch_size=32)\n\n多输出前向推导不同行为\n如果想要一个多输出的模型，并且执行方式不同，那么这个过程还是比较蛋疼的。因为继承Model类的方式定义模型，我好像暂时没找到合适的指定输入输出形状的函数，这样就会导致程序不知道形状导致报错。\n\ncall中定义多输出模型，两个输出可以维度不同，但是这样loss就没法复合。\n首先随意输入一些数据，这是为了让他自动推断输出个数与形状。\n手动执行compile，并提供target_tensors否则模型还是不知道输出形状。\n训练即可\n\n\nclass Model_2(k.Model):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.a = tf.constant(1., tf.float32)  # 1\n        self.b = tf.constant(2., tf.float32)  # 2\n\n    def call(self, inputs: List[tf.Tensor], training=None, mask=None):\n        \"\"\" 测试使用多个输入时，控制不同行为\"\"\"\n        a = self.a * inputs[0]\n        b = smart_cond(training, lambda: self.b * inputs[1], lambda: tf.ones_like(a, tf.float32))\n        return a, b\n\n    def compute_output_shape(self, input_shape):\n        return super().compute_output_shape(input_shape)\n\n\ndef test_model_train_validation():\n    \"\"\" 测试模型train与validation的不同行为下实现多输出，目前是成功的\n    NOTE 训练和验证的输出维度必须一样 \"\"\"\n    train_x = [np.ones([1000, 1], 'float32'), np.ones([1000, 1], 'float32')]\n    train_y = [np.ones([1000, 1], 'float32'), np.ones([1000, 1], 'float32')]\n    md = Model_2()\n    # NOTE 首先输入一次数据来build\n    md.predict((train_x[0][0:1], train_x[1][0:1]))\n    print(md.built)\n    print(md.inputs)\n    print(md.outputs)\n\n    # NOTE 然后在compile的时候提供target_tensors\n    md.compile(loss=lambda y_ture, y_pred: tf.reduce_sum(y_pred),\n               target_tensors=[K.placeholder([None, 1], 1, tf.float32), K.placeholder([None, 1], 1, tf.float32)])\n    print(md._is_compiled)\n\n    md.fit(train_x, train_y, batch_size=32)\n    md.evaluate(train_x, train_y, batch_size=32)\n\n\n单输出前向推导、loss均不同行为\n模型和之前没什么区别，在损失中需要使用K.learning_phase获得当前的推理状态来执行不同过程～\nclass Model_3(k.Model):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dense = kl.Dense(2)\n\n    def call(self, inputs: List[tf.Tensor], training=None, mask=None):\n        a = self.dense(inputs[0])\n        b = smart_cond(training, lambda: self.dense(inputs[1]), lambda: tf.ones_like(a, tf.float32))\n        return tf.stack([a, b], 1)\n\n    def compute_output_shape(self, input_shape):\n        return super().compute_output_shape(input_shape)\n\n\nclass faceloss(k.losses.Loss):\n    def __init__(self, reduction='auto', name=None):\n        super().__init__(reduction=reduction, name=name)\n\n    def call(self, y_true, y_pred):\n        # y_true=[1]\n        training = K.learning_phase()\n        print(training)\n\n        def false_fn():\n            return tf.cast(tf.math.reduce_sum(tf.square(y_pred[:, 0] - y_pred[:, 1])) &lt; 0.7, tf.float32)\n        loss = smart_cond(training, lambda: K.sparse_categorical_crossentropy(tf.cast(y_true, tf.float32), y_pred[:, 0], True),\n                          lambda: false_fn())\n        return loss\n\n\ndef test_model_train_validation():\n    \"\"\" 测试模型train与validation的不同行为，目前是成功的\n    NOTE 训练和验证的输出必须是一样的 \"\"\"\n    train_x = [np.ones([1000, 1], 'float32'), np.ones([1000, 1], 'float32')]\n    train_y = [np.ones([1000], 'int32')]\n\n    md = Model_3()\n\n    # NOTE 然后在compile的时候提供target_tensors\n    md.compile(loss=faceloss())\n\n    md.fit(train_x, train_y, batch_size=32)\n    md.evaluate(train_x, train_y, batch_size=32)"
  },
  {
    "objectID": "posts/tf-custom-model.html#展望",
    "href": "posts/tf-custom-model.html#展望",
    "title": "tf2.0 自定义Model高级用法",
    "section": "展望",
    "text": "展望\n接下来应该对于要写的代码心里有数了，现在想想，最好的方式是先定义一个Sequential对象，然后call的时候反复执行这个模型即可，导出时只需导出这个Sequential对象。"
  },
  {
    "objectID": "posts/tf-boolmask-index.html",
    "href": "posts/tf-boolmask-index.html",
    "title": "tf2.0得到子boolmask在boolmask中的索引",
    "section": "",
    "text": "在yolo中计算了单层的anchor与全局的gt间的iou score，但是我需要在其中过滤出单层的anchor对应单层的gt的iou score。目前有单一层的gt的loc_mask，以及全局的gt的glob_mask，其中loc_mask中有效区域是glob_mask的子集，因此需要找到loc_mask在glob_mask的对应索引。"
  },
  {
    "objectID": "posts/tf-boolmask-index.html#问题解决",
    "href": "posts/tf-boolmask-index.html#问题解决",
    "title": "tf2.0得到子boolmask在boolmask中的索引",
    "section": "问题解决",
    "text": "问题解决\n我想了半天，终于发现一个简单的方式，先利用where和boolmask找到有效位置，再使用gather_nd就完事了：\niou = tf.random.normal((4, 4, 9))\nloc_mask = tf.constant([[False, False, False, False, ],\n                        [False, True, False, False, ],\n                        [False, False, False, False, ],\n                        [False, False, True, False, ]])\nglob_mask = tf.constant([[False, False, True, False, ],\n                         [False, True, False, True, ],\n                         [True, False, False, False, ],\n                         [False, True, True, False, ]])\nloc_iou = tf.boolean_mask(iou, loc_mask)  # (2, 9)\nglob_iou = tf.boolean_mask(iou, glob_mask)  # (6, 9)\n\nidx = tf.where(tf.boolean_mask(loc_mask, glob_mask)) # (2, 1) [[1],[5]]\nmask_iou = tf.gather_nd(tf.boolean_mask(iou, glob_mask), idx, 0) # (2, 9)"
  },
  {
    "objectID": "posts/tensorir.html",
    "href": "posts/tensorir.html",
    "title": "TVM TensorIR",
    "section": "",
    "text": "关于TVM的Tensor level IR."
  },
  {
    "objectID": "posts/tensorir.html#block-reads-writes",
    "href": "posts/tensorir.html#block-reads-writes",
    "title": "TVM TensorIR",
    "section": "6.1 block reads && writes",
    "text": "6.1 block reads && writes\nblock是tvm调度的基本单元,他的调度器通常是获得一个block,然后对这个块进行融合/分割/并行等等操作,同时还可以分析多个块 在parser的block的流程,他的func.body是只会有一个赋值的操作C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vj, vk](忽略了前面的iter var定义,应该是这些定义到时候都会被固化到代码中,所以也不会出现在计算流程中的原因),然后在func.exit_scope时,他会进入tvm的callback函数中 python/tvm/script/tir/scope_handler.py line 255,构造出带有bind以及reads/writes的tir. (实际上底层还分有BlockRealize和Block两部分)\n      with T.block(\"update\"):\n        vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n        C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vj, vk]\n        func.enter_scope(node, self.context, arg_list, node.rhs.func_name.span)\n        func.body = self.parse_body(node)\n        res = func.exit_scope(node, self.context, arg_list, node.rhs.func_name.span)\n得到的结果,实际上是把remap的定义融合到了block这个ir中.\nfor (k, 0, 128) {\n  block update(iter_var(vi, range(min=0, ext=128)), iter_var(vj, range(min=0, ext=128)), iter_var(vk, range(min=0, ext=128))) {\n    bind(vi, i)\n    bind(vj, j)\n    bind(vk, k)\n    reads([C[vi, vj], A[vi, vk], B[vj, vk]])\n    writes([C[vi, vj]])\n    C[vi, vj] = (C[vi, vj] + (A[vi, vk]*B[vj, vk]))\n  }\n}"
  },
  {
    "objectID": "posts/tensorir.html#block-iter_var",
    "href": "posts/tensorir.html#block-iter_var",
    "title": "TVM TensorIR",
    "section": "6.2 block iter_var",
    "text": "6.2 block iter_var\niter_var我个人把他看作一个symbol var,他的好处就是我们可以任意绑定一个时机的value,等到schedule做完后再消除他得到真正的索引操作. 这里要说明一下iter_var对于一个Buffer的索引操作将会得到是BufferLoad的ir,他的表现形式就是多维索引B[vi,vj]. 在后续这个BufferLoad会被lower到Load,表现形式就是B.Handle[i * w + j]. 即我们取symbol var绑定的value并计算出对于一个指针真正的索引.\n🌰 原始TIR:\nfor (i: int32, 0, 128) {\n  for (j: int32, 0, 128) {\n    block([128, 128], \"B\") as [vi, vj] {\n      bind(vi, i)\n      bind(vj, j)\n      tir.reads([A[vi, vj]])\n      tir.writes([B[vi, vj]])\n      B[vi, vj] = (A[vi, vj]*2f32)\n  }\n}\n经过split之后, 可以发现我们只需要修改iter var的绑定即可实现split, 不然得递归把所有的i改成((i_0*64) + i_1),写transform就巨麻烦了.\nfor (i_0: int32, 0, 2) {\n  for (i_1: int32, 0, 64) {\n    for (j: int32, 0, 128) {\n      block([128, 128], \"B\") as [vi, vj] {\n        bind(vi, ((i_0*64) + i_1))\n        bind(vj, j)\n        tir.reads([A[vi, vj]])\n        tir.writes([B[vi, vj]])\n        B[vi, vj] = (A[vi, vj]*2f32)\n    }\n  }\n}"
  },
  {
    "objectID": "posts/tensorir.html#bufferload-lower",
    "href": "posts/tensorir.html#bufferload-lower",
    "title": "TVM TensorIR",
    "section": "6.3 BufferLoad lower",
    "text": "6.3 BufferLoad lower\n\n利用ConvertBlocksToOpaque的transform把iter_var.var都替换成对应的value, 这里我其实没明白,为什么不把itervar也设计成expr, 理论上应该没啥问题吧."
  },
  {
    "objectID": "posts/tensorir.html#ssa赋值",
    "href": "posts/tensorir.html#ssa赋值",
    "title": "TVM TensorIR",
    "section": "7.1 ssa赋值",
    "text": "7.1 ssa赋值\n我自己写了一下c代码生成才发现不能无脑对综合了stmt以及expr的ir进行ssa赋值.怪不得tvm的c代码生成默认不开ssa赋值.\n🌰 把下面的代码转换为c代码\nvoid RefFunc(int[] A, int n)\n{\n    for (i in (0, n))\n    {\n        A[i] = A[i] + 1;\n        for (j in (0, 10))\n        {\n            A[i] = A[i] + j;\n        }\n    }\n}\n如果使用ssa赋值,同时我这里的visit expression的时候是用结构化比较的,所以内外两个循环中相同的load A[i]都变成了_1这个tmep var了. 然后第二次load的时候就会出现没有更新值的问题.\n#include &lt;stdint.h&gt;\nvoid func_0(int32_t* A, int32_t n) {\n  for (int32_t i = 0; i &lt; n; i++) {\n    int32_t _3 = (i * 1);\n    int32_t _2 = (0 + _3);\n    int32_t _1 = A[_2];\n    int32_t _0 = (_1 + 1);\n     A[_2] = _0;\n    for (int32_t j = 0; j &lt; 10; j++) {\n      int32_t _4 = (_1 + j); // 这里就会出现load没有更新值的问题\n       A[_2] = _4;\n    }\n  }\n}\n所以我目前也是按照tvm的方法,把这些计算流程都转换成线性的计算. 这样就保证所有的表达式都会被emit,不过也带来了一个计算冗余的问题,这个后续我们可以继续优化.\n#include &lt;stdint.h&gt;\nvoid func_0(int32_t* A, int32_t n) {\n  for (int32_t i = 0; i &lt; n; i++) {\n     A[(0 + (i * 1))] = (A[(0 + (i * 1))] + 1);\n    for (int32_t j = 0; j &lt; 10; j++) {\n       A[(0 + (i * 1))] = (A[(0 + (i * 1))] + j);\n    }\n  }\n}"
  },
  {
    "objectID": "posts/tensorir.html#如果在tvm中",
    "href": "posts/tensorir.html#如果在tvm中",
    "title": "TVM TensorIR",
    "section": "如果在TVM中:",
    "text": "如果在TVM中:\n如果是手写tiling的话,最麻烦的一点就是每次都需要手动算tile大小,然后开辟出n个for循环进行写操作.\n@T.prim_func\ndef simple_split(a: T.handle) -&gt; None:\n  A = T.match_buffer(a, [16])\n  for i in T.serial(0, 16):\n    with T.block(\"block\"):\n      vi = T.axis.remap(\"S\", [i])\n      A[vi] = i + 100\n\n\ndef test_simple_split():\n  sch = tir.Schedule(simple_split)\n  b = sch.get_block(\"block\")\n  lps = sch.get_loops(b)\n  sch.split(lps[0], [7,10])\n  print(sch.mod.script())\n\n# from tvm.script import tir as T\n@tvm.script.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.handle) -&gt; None:\n        A = T.match_buffer(a, [16], dtype=\"float32\")\n        # body\n        # with T.block(\"root\")\n        for i_0, i_1 in T.grid(7, 10):\n            with T.block(\"block\"):\n                vi = T.axis.spatial(16, i_0 * 10 + i_1)\n                T.where(i_0 * 10 + i_1 &lt; 16)\n                T.reads([])\n                T.writes([A[vi]])\n                A[vi] = i_0 * 10 + i_1 + 100\n不过tvm的tir中是简化了for循环,也就是无法自定义stride,因为他面向的对象都是cpu/gpu这些的设备. 但是如果对于一些大颗粒算子的dsa来说,最好还是带有stride的for循环比较合理,否则对于一段程序我们需要这样写:\n@T.prim_func\ndef simple_split(a: T.handle) -&gt; None:\n  A = T.match_buffer(a, [16])\n  chunk_n = 3\n  chunk_c = 5\n  for n in T.serial(0, compute_segment(16, chunk_n)):\n    for c in T.serial(0, compute_segment(32, chunk_c)):\n      with T.block(\"block\"):\n        vi, vj = T.axis.remap(\"SS\", [n,c])\n        A[vi * chunk_n + vj * chunk_c] = 100\n如果每次都自己控制chunk,那么如果有6d的tensor,也就是6层循环, 那么变量绝对多到难以控制的程度.\n如果可以这样写肯定就舒服多了, 然后关键是就是chunk固定但是length还得每次求, 不过应该是合理一些了:\n@T.prim_func\ndef simple_split(a: T.handle) -&gt; None:\n  A = T.match_buffer(a, [16])\n  chunk_n = 3\n  chunk_c = 5\n  for n in T.serial(0, 16, chunk_n):\n    for c in T.serial(0, 32, chunk_c):\n      with T.block(\"block\"):\n        vi, vj = T.axis.remap(\"SS\", [n,c])\n        with T.let(length_n, min(chunk_n, 16 - vi)):\n          with T.let(length_c, min(chunk_c, 32 - vj)):\n            A[vi + vj] = 100\n但是还是有一点非常麻烦,那就是求tir中定义一个变量就需要声明他的作用域,那么对于真的多层的循环复杂逻辑肯定还是很麻烦的."
  },
  {
    "objectID": "posts/tensorir.html#如果在csharp中",
    "href": "posts/tensorir.html#如果在csharp中",
    "title": "TVM TensorIR",
    "section": "如果在CSharp中:",
    "text": "如果在CSharp中:\n我的想法是在csharp中基于Linq实现两套写法, 那些shape之类的可能还是没法用expr进行lazy的运算,因为一旦那样就很难用linq语法, 写起来就复杂很多.\n\n1. 适配老架构的segment的方式\n之前因为是cpp的语法,所以要实现一套基于Enumerable的dsl还是比较麻烦,所以for循环之类的刻板代码比较多, 目前我也先支持这种写法. 通过linq拆分出segment之后构造segment 4d然后进行计算. csharp的linq可以再嵌套linq所以不用担心复杂的逻辑无法处理, 最后返回出expr即可.\nT.PrimFunc(\"TileLoadStore\").Body(\n  (from item in glb.items\n    let mmu = item.Value\n    select I.MmuConf((MMU_CONF_WIDTH)mmu.width, mmu.id, mmu.start_bank, mmu.start_depth, mmu.depth)).ToSequential(),\n  (from glb_input_batch in SegmentByChunk(0, glb.last_out_shape[0], input_shape[0])\n    from glb_input_channel in SegmentByChunk(0, glb.last_out_shape[1], input_shape[1])\n      from glb_input_row in SegmentByChunk(0, glb.last_out_shape[2], input_shape[2])\n        from glb_input_column in SegmentByChunk(0, glb.last_out_shape[3], input_shape[3])\n          let ofmap = new tensor4d_segment( glb_input_batch.OutputByStride(strides[0]),\n                                            glb_input_channel.OutputByStride(strides[1]),\n                                            glb_input_row.OutputByStride(strides[2]),\n                                            glb_input_column.OutputByStride(strides[3]))\n          let ifmap = new tensor4d_segment(glb_input_batch, glb_input_channel, glb_input_row, glb_input_column)\n          let c_pp_split_size = (uint)Math.Ceiling(1.0 * glb_input_channel.Length / glb.n_ping_pong_split)\n          let in_chan_split = SegmentByChunk((int)glb_input_channel.Start, (int)c_pp_split_size, (int)glb_input_channel.End)\n          from inst in in_chan_split.Select(c_pp_split =&gt;\n          {\n              // load ifmap\n              // 再次对c进行切分. 然后更新ifmap中c的segment.\n              tensor4d_segment ifmap_pp = new(ifmap[0], c_pp_split, ifmap[2], ifmap[3]);\n              // 然后再把ifmap_pp的start全部减去一个base,因为这个segment起始地址是切分后的.\n              tensor4d_segment ifmap_pp_glb = glb_tensor_index_shift(ifmap_pp, ifmap);\n\n              bool clear_qarg_ccr = false;\n              if (input_type.IsQuantType())\n              {\n                  // action_updater.update_load_load_qarg(i_pp, ifmap_pp, ifmap_pp_glb, load_type);\n                  clear_qarg_ccr = true;\n              }\n\n              CcrSet ifmap_pp_ccrset = new(0, 0, 0);\n              tensor4d_segment ifmap_pp_ld_glb = glb_tensor_index_shift(ifmap_pp, ifmap_pp);\n              // action_updater.update_load_if(ifmap_pp_ccrset, ifmap_pp, ifmap_pp_glb, ifmap_pp_ld_glb, load_type, dt_bfloat16, false, i_pp, clear_qarg_ccr);\n\n              segment oc_pp_split = c_pp_split.OutputByStride(strides[1]);\n              tensor4d_segment ofmap_pp = new(ofmap[0], oc_pp_split, ofmap[2], ofmap[3]);\n              tensor4d_segment ofmap_pp_glb = glb_tensor_index_shift(ofmap_pp, ofmap);\n\n              if (output_type.IsQuantType())\n              {\n                  // action_updater.update_load_store_qarg(i_pp, ofmap_pp, ofmap_pp_glb, store_type);\n              }\n\n              tensor4d_segment ofmap_pp_st = new(ofmap_pp.Segments);\n              for (int i = 0; i &lt; 4; i++) { ofmap_pp_st[i] = ofmap_pp_st[i] with { Start = ofmap_pp_st[i].Start * (uint)strides[i] }; }\n              tensor4d_segment ofmap_pp_st_glb = glb_tensor_index_shift(ofmap_pp_st, ifmap_pp);\n              // action_updater.update_store_t(item_name::ifmap, ofmap_pp, ofmap_pp_glb, ofmap_pp_st_glb, store_type, of_buf_num, i_pp, i_pp);\n              return new Var(\"1\", AnyType.Default);\n          })\n    select inst).ToSequential()\n);\n\n\n2. 输入glb_tensor,可以通过索引的方式进行tiling, 而后构造指令.\n这个glb_tensor应该是一个可以多层级的数据结构,比如当前的sub_tensor可以求关于上一层tensor的地址偏移,然后也可以求关于父节点的内存偏移. 然后基于之前segment的逻辑,就可以把写出一个优雅的tensor处理逻辑.\nfrom in_seg in compute_segment(N,chunk_n)\n  from ic_seg in compute_segment(C,chunk_c)\n    from ih_seg in compute_segment(H,chunk_h)\n      from iw_seg in compute_segment(W,chunk_w)\n        let sub_input = input[in_seg, ic_seg, ih_seg, iw_seg];\n        from cpp_seg in compute_segment(ic_seg,pp_chunk)\n          let ping_input = sub_input[..,cpp_seg,..,..]\n          // ! can't direct add Expr in here.\n          select I.Load(ping_input.addr,ping_input.stride,....)"
  },
  {
    "objectID": "posts/stm32uselib.html",
    "href": "posts/stm32uselib.html",
    "title": "stm32使用静态库",
    "section": "",
    "text": "我这两天写程序的时候，使用了大量的宏定义去开启和关闭代码块。但是我发现每次我切换一个宏定义的时候就会将所以的stm32工程中的所有文件进行重新编译，耗费太长时间，效率不如之前用注释代码块的方式。所以我仔细看了看。发现每次编译stm32hal库的时间是最长的，那么我就可以将所有的hal库文件生成一个静态库，让编译的时候连接即可。\n\n\n生成静态库\n我使用的是偷懒的方法。首先我的工程是stm32cubemx生成的，其中的makefile，直接可以进行make。编译之后，自动在工程的build目录下生成了一系列的.o .d文件。那么我的任务就是将这些.o文件合成一个静态库。\n首先查看那些是hal库的.o文件\n➜  build ls stm32l4xx_hal*.o\nstm32l4xx_hal_adc_ex.o  stm32l4xx_hal_dma_ex.o    stm32l4xx_hal_flash.o          stm32l4xx_hal_i2c_ex.o  stm32l4xx_hal.o         stm32l4xx_hal_rcc_ex.o  stm32l4xx_hal_tim.o\nstm32l4xx_hal_adc.o     stm32l4xx_hal_dma.o       stm32l4xx_hal_flash_ramfunc.o  stm32l4xx_hal_i2c.o     stm32l4xx_hal_pwr_ex.o  stm32l4xx_hal_rcc.o     stm32l4xx_hal_uart_ex.o\nstm32l4xx_hal_cortex.o  stm32l4xx_hal_flash_ex.o  stm32l4xx_hal_gpio.o           stm32l4xx_hal_msp.o     stm32l4xx_hal_pwr.o     stm32l4xx_hal_tim_ex.o  stm32l4xx_hal_uart.o\n要注意这里面stm32l4xx_hal_msp.o不能加入静态库！，因为这个文件是经常被修改的！所以我们先要删除这个文件\n➜  build rm stm32l4xx_hal_msp.o\n接下来生成静态库（多添加两个文件是因为我觉得这两个文件也不太会改变）：\n➜  build arm-none-eabi-ar cr  libhal.a  stm32l4xx_hal*.o system_stm32l4xx.o startup_stm32l431xx.o\n➜  build mv libhal.a ../\n移动到libhal.a到与makefile同级目录后修改makefile\n\n注释源文件\n\n将c源文件和asm源文件中的库文件注释掉。\n\n######################################\n# source\n######################################\n# C sources\nC_SOURCES =  \\\nSrc/main.c \\\nSrc/gpio.c \\\nSrc/adc.c \\\nSrc/tim.c \\\nSrc/usart.c \\\nSrc/delay.c \\\nHardware/BH1750/BH1750.c \\\nHardware/DHT11/DHT11_BUS.c \\\nHardware/GPS/gps.c \\\nHardware/OLED/oled.c \\\nSrc/stm32l4xx_it.c \\\nSrc/stm32l4xx_hal_msp.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_adc.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_adc_ex.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_tim.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_tim_ex.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_uart.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_uart_ex.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_i2c.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_i2c_ex.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_rcc.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_rcc_ex.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash_ex.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash_ramfunc.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_gpio.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_dma.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_dma_ex.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_pwr.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_pwr_ex.c \\\n# Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c \\\n# Src/system_stm32l4xx.c  \n\n# ASM sources\nASM_SOURCES =  \\\n# startup_stm32l431xx.s\n\n添加连接库\n\n在LIBS后面添加上静态库的名字，然后编译即可\n#######################################\n# LDFLAGS\n#######################################\n# link script\nLDSCRIPT = STM32L431RBTx_FLASH.ld\n\n# libraries\nLIBS = -lc -lm -lnosys libhal.a\nLIBDIR = \nLDFLAGS = $(MCU) -specs=nano.specs -T$(LDSCRIPT) $(LIBDIR) $(LIBS) -Wl,-Map=$(BUILD_DIR)/$(TARGET).map,--cref -Wl,--gc-sections\n\n\n效果\n编译快了许多～～～"
  },
  {
    "objectID": "posts/stft-1.html",
    "href": "posts/stft-1.html",
    "title": "python实现stft",
    "section": "",
    "text": "以后还是要自己实现k210中的代码,所以准备还是在python中先实现一下.下面记录两种stft谱图的实现方式"
  },
  {
    "objectID": "posts/stft-1.html#效果",
    "href": "posts/stft-1.html#效果",
    "title": "python实现stft",
    "section": "效果",
    "text": "效果"
  },
  {
    "objectID": "posts/stft-1.html#效果-1",
    "href": "posts/stft-1.html#效果-1",
    "title": "python实现stft",
    "section": "效果",
    "text": "效果"
  },
  {
    "objectID": "posts/stft-1.html#效果-2",
    "href": "posts/stft-1.html#效果-2",
    "title": "python实现stft",
    "section": "效果",
    "text": "效果"
  },
  {
    "objectID": "posts/statis-learn-cp8.html",
    "href": "posts/statis-learn-cp8.html",
    "title": "统计学习方法:EM算法",
    "section": "",
    "text": "关于EM算法.\n\n去年我已经写了一个详细的博客，所以这里就不再赘述。"
  },
  {
    "objectID": "posts/statis-learn-cp6.html",
    "href": "posts/statis-learn-cp6.html",
    "title": "统计学习方法:支持向量机",
    "section": "",
    "text": "这次实现中，关于如何选择违反KKT条件最严重的点在书中没有提到，我首先按照书上的方法实现了一下，但是发现没找到\\(\\epsilon\\)和违反KKT条件的量化方法，而且只按书上来，实现的SVM效果并不理想。看来我还是没有完全弄透…先写了个初级版的，以后需要再深入了解时可以重温。"
  },
  {
    "objectID": "posts/statis-learn-cp6.html#原理",
    "href": "posts/statis-learn-cp6.html#原理",
    "title": "统计学习方法:支持向量机",
    "section": "原理",
    "text": "原理\n\n首先我们还是使用感知机中的分类例子,在感知机中分类决策面有无数个,为了找到最优的决策面(人主观地认为能使数据间gap最大的决策面是最好的),提出了最大间隔的线性分类模型.\n我们定义分类决策面为\\(w^Tx+b=0\\),任意一点到决策面的距离为\\(r=\\frac{|w^Tx+b|}{||w||}\\),对于带标签的数据定义其函数间隔为\\(r^*=y_i(w^Tx+b)\\),几何间隔为\\(r=\\frac{r^*}{||w||}\\),对于最大间隔的线性分类模型我们的目标就是最大化所有数据点到决策面的几何间隔: \\[\n\\begin{aligned}\n  \\max\\ &\\frac{y_i(w^Tx+b)}{||w||}=\\frac{r^*}{||w||}\\\\\n  \\text{s.t.}\\ \\ \\ &y_i(w^Tx_i+b)\\geq r^*,\\ i=1,2,...,N\n\\end{aligned}\n\\]\n为了求解上述函数的极值,需要做两步:\n\n1. 转换为凸函数\n\n1.1 令\\(r^*=1\\).\n因为间隔只是一个尺度,不影响对于\\(w\\)的求解.\n\\[\n\\begin{aligned}\n  \\max\\ &\\frac{1}{||w||}\\\\\n  \\text{s.t.}\\ \\ \\ &y_i(w^Tx_i+b)\\geq 1,\\ i=1,2,...,N\n\\end{aligned}\n\\]\n\n\n1.2 转换为凸函数求最小值(应该是凸优化问题比较便于求解)\n\\[\n\\begin{aligned}\n  \\min\\ &\\frac{1}{2}||w||^2\\\\\n  \\text{s.t.}\\ \\ \\ &y_i(w^Tx_i+b)\\geq 1,\\ i=1,2,...,N\n\\end{aligned}\n\\]\n\\(\\frac{1}{2}\\)是为了便于求导后计算所加的常数项.\n\n\n\n2. 求解\n\n2.1 拉格朗日乘数法\n先应用拉格朗日乘数法,转换约束条件(如果不理解请参考高等数学第七版下册p118):\n\\[\n\\begin{aligned}\n  \\min_{w,b}\\ &\\frac{1}{2}||w||^2\\\\\n  \\text{s.t.}\\ \\ \\ & -y_i(w^Tx_i+b)+1\\leq 0,\\ i=1,2,...,N\n\\end{aligned}\n\\]\n将约束条件逐一带入得到:\n\\[\n\\begin{aligned}\n  L(w,b,\\alpha)=\\frac{1}{2}||w||^2+ \\sum_{i=1}^N\\alpha_i \\left[-y_i(w^T x_i+b)+1\\right]\n\\end{aligned}\n\\]\n\n\n2.2 拉格朗日乘对偶形式\n根据统计学习方法附录C中关于拉格朗日原始问题的对偶问题中的证明,将上述原始问题转换为对偶形式后得到: \\[\n\\max_{\\alpha}\\ \\min_{w,b}\\ L(w,b,\\alpha)\n\\]\n接下来求解过程就变成了先求\\(\\min_{w,b}\\ L(w,b,\\alpha)\\)对\\(w,b\\)的极小:\n\\[\n\\begin{aligned}\n  \\text{求导并使其为0}\\ \\ \\ \\  \\frac{\\partial }{\\partial w}L(w, b, \\alpha)&=w-\\sum\\alpha_iy_ix_i=0\\\\\n  \\frac{\\partial }{\\partial b}L(w, b, \\alpha)&=\\sum\\alpha_iy_i=0\\\\\n  \\\\\n  \\text{得到}\\ \\ \\ \\ w&=\\sum_{i=1}^N \\alpha_i y_i x_i\\\\\n  \\alpha_i& y_i =0\\\\\n  \\\\\n  \\text{带入}\\ \\ \\ \\   \\min_{w,b}\\  L(w, b, \\alpha)&=\\frac{1}{2}||w||^2+\\sum^N_{i=1}\\alpha_i(-y_i(w^Tx_i+b)+1)\\\\\n  &=\\frac{1}{2}w^Tw-\\sum^N_{i=1}\\alpha_iy_iw^Tx_i-b\\sum^N_{i=1}\\alpha_iy_i+\\sum^N_{i=1}\\alpha_i\\\\\n  &=\\frac{1}{2}w^T\\sum^N_{i=1}\\alpha_iy_ix_i-\\sum^N_{i=1}\\alpha_iy_iw^Tx_i+\\sum^N_{i=1}\\alpha_i\\\\\n  &=\\sum^N_{i=1}\\alpha_i-\\frac{1}{2}\\sum^N_{i=1}\\alpha_iy_iw^Tx_i\\\\\n  &=\\sum^N_{i=1}\\alpha_i-\\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\alpha_jy_iy_j(x_ix_j)\n\\end{aligned}\n\\]\n再求上式对于\\(\\alpha\\)的极大:\n\\[\n\\begin{aligned}\n  \\max_\\alpha\\ \\ \\ \\ &\\sum^N_{i=1}\\alpha_i-\\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\alpha_jy_iy_j(x_ix_j)\\\\\n  \\text{再转换为极小问题} &\\Downarrow\\\\\n  \\min_\\alpha\\ \\ \\ \\ &\\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\alpha_jy_iy_j(x_ix_j)-\\sum^N_{i=1}\\alpha_i\\\\\n  \\text{s.t.}\\ \\ \\ \\ &\\begin{cases}\\sum^N_{i=1}a_iy_i=0\\\\a_i\\geq 0,\\ \\ i=1,2,...,N \\end{cases}\n\\end{aligned}\n\\]\n最后求解时先求解最优的\\(\\alpha\\),求得后带入之前公式求解\\(w,b\\).\n\n\n2.3 SMO算法\n最小最优化算法(SMO)是用于求解SVM对偶问题解的。\n方法是不断固定其他变量，对两个变量构造二次规划、并通过求出其解析解来优化原始的对偶问题。步骤如下：\n\n检查所有变量\\(\\alpha_1,...,\\alpha_N\\)及对应的样本点\\(\\left( x_{1},y_{1} \\right),\\ldots,(x_{N},y_{N})\\)满足KKT条件的情况。\n如果均满足KKT条件那么完成训练。\n如果有未满足KKT条件的变量，对他们进行优化：\n\n选择违反KKT条件最严重的样本点，对应的\\(\\alpha_i\\)作为第一个变量。\n第二个变量\\(\\alpha_j\\)为对应\\(|E_i-E_j|\\)最大的变量，\\(E_i\\)为对于输入样本点\\(x_i\\)的预测误差。\n\n固定其他变量后，仅对这两个变量进行优化。\n\n\n\n2.4 KKT条件\n\\(a_i\\)与对应样本的\\(x_i,y_i\\)的KKT条件为： \\[\n\\begin{aligned}\n\\alpha_{i} = 0 &\\Leftrightarrow y_{i}g\\left( x_{i} \\right) \\geq 1 \\\\\n0 &lt; \\alpha_{i} &lt; C &\\Leftrightarrow y_{i}g\\left( x_{i} \\right) = 1 \\\\\n\\alpha_{i} = C &\\Leftrightarrow y_{i}g\\left( x_{i} \\right) \\leq 1\n\\end{aligned}\n\\]\n不满足KKT条件的量化：\n\n计算所有样本点的损失\\(c=|y_ig(x_i)-1|\\)\n将损失\\(c\\)带入上述三个条件中将如果满足，对应的损失置为0\n将三个处理后的损失相加，其中的最大值对应的索引就是第一个变量。"
  },
  {
    "objectID": "posts/statis-learn-cp4.html",
    "href": "posts/statis-learn-cp4.html",
    "title": "统计学习方法:决策树",
    "section": "",
    "text": "由于时间关系我没有实现决策树的剪枝与CART算法."
  },
  {
    "objectID": "posts/statis-learn-cp4.html#熵",
    "href": "posts/statis-learn-cp4.html#熵",
    "title": "统计学习方法:决策树",
    "section": "熵",
    "text": "熵\n熵度量了不确定性,具体参考书中给出例子进行理解. \\[\n\\begin{aligned}\n  H(x) = -\\sum_{i=1}^{n}p_i\\log{p_i}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/statis-learn-cp4.html#条件熵",
    "href": "posts/statis-learn-cp4.html#条件熵",
    "title": "统计学习方法:决策树",
    "section": "条件熵",
    "text": "条件熵\n既然熵度量了不确定性,引用条件概率后,可以得到条件熵,即给定条件下的不确定性.\n注意: 公式第二行对应实际计算的情况,因为分类问题中\\(Y\\)的类别必然大于1.\n\\[\n\\begin{aligned}\n  H(Y|X)&=\\sum_{i=1}^{n}{P(X=x_i)}H(Y|X=x_i)\\\\\n  &=\\sum_{i=1}^{n}\\left[{P(X=x_i)}\\sum_{j=1}^{k}\\left[P(Y=y_j|X=x_i)\\log{P(Y=y_j|X=x_i)}\\right]\\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/statis-learn-cp4.html#信息增益",
    "href": "posts/statis-learn-cp4.html#信息增益",
    "title": "统计学习方法:决策树",
    "section": "信息增益",
    "text": "信息增益\n在极大似然估计中的得到的熵与条件熵称为经验熵与经验条件熵.将经验熵减去经验条件熵即可表示得知到\\(X\\)后对分类\\(Y\\)的不确定性减少的程度,也就是信息增益.一般地\\(H(Y)-H(Y|X)\\)被称为互信息,决策树中的信息增益等价于最大化特征与标签的互信息,只不过这里的概率都是统计出来的,结合到深度学习中还是有很大用武之地的,我个人觉得比如可以用互信息做度量代替attention机制选择特征的激活概率,当然互信息的估计没有attention那么方便.\n\\[\n\\begin{aligned}\n  g(D, A)=H(D)-H(D|A)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/statis-learn-cp2.html",
    "href": "posts/statis-learn-cp2.html",
    "title": "统计学习方法:KNN",
    "section": "",
    "text": "K近邻法比较简单,我就讲下流程.\n\n\n确定距离度量\n\n在\\(L_p\\)距离中选择任意一个即可\n\\[\n\\begin{aligned}\n  L_{p}\\left(x_{i}, x_{j}\\right)=\\left(\\sum_{l=1}^{n}\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right|^{p}\\right)^{\\frac{1}{p}}\n\\end{aligned}\n\\]\n\n计算待分类样本点与已知样本点的距离\n根据\\(k\\)值确定待分类样本点类别"
  },
  {
    "objectID": "posts/statis-learn-cp17.html",
    "href": "posts/statis-learn-cp17.html",
    "title": "统计学习方法:潜在狄利克雷分配模型",
    "section": "",
    "text": "潜在狄利克雷分配模型其实就是一个确定结构的概率图模型，他主要做两件事情:\n\n学习到话题的单词分布\\(p(w|z_k)\\),意为给定对应话题得到对应的单词.\n学习到文本的话题分布\\(p(z|\\mathbf{w}_m)\\),意为给定对应文本得到对应的话题.\n\n如下图所示:\n\n\\(\\varphi_k\\sim \\text{Dir}(\\beta)\\)控制了多项分布\\(p(w|z_k)\\)根据主题随机生成当前文本下的单词分布\\(\\mathbf{w}\\).\n\\(\\theta_{m}\\sim \\text{Dir}(\\alpha)\\)控制了多项分布\\(p(z|\\mathbf{w}_m)\\)根据当前文本得到所对应主题的概率分布.\n\n\n\n\nLDA的生成过程\n流程见书本图20.3:\n\n从\\(\\theta_m\\sim Dir(\\alpha)\\)中随机采样,确定文本的话题分布\\(p(z|\\mathbf{w}_m)\\),这个分布是一个多项分布.\n在文本的话题分布\\(p(z|\\mathbf{w}_m)\\)中采样,一篇文章一共\\(N\\)个单词,因此采样每个单词对应的\\(N\\)个主题,即\\(z_{mn}\\sim p(z|\\mathbf{w}_m)\\).\n从\\(\\varphi_k\\sim Dir{\\beta}\\)中随机采样,确定话题的单词分布\\(p(w|z_k)\\),这个分布也是一个多项分布.\n一共有\\(M\\)篇文章,每个文章有\\(N\\)个单词,因此对之前采样得到单词对应的主题\\(z_{mn}\\)可以在话题的单词分布\\(p(w|z_k)\\)中采样,代入\\(z_{mn}\\)后,得到生成的单词\\(w_{mn}\\).\n\n实际上如果我们生成的整个数据集的单词\\(w_{mn}\\)恰巧符合于真实的单词分布,那么LDA模型的参数\\(\\alpha,\\beta\\)就确定了.\n\n\nLDA的Gibbs抽样算法\n具体推导流程见书本.gibbs抽样的缺点就是效率太低，一个step就需要遍历所有的文本内容，并且需要非常多的step才能进入收敛状态。因此我的代码实现减小了文章个数和特征个数。\n\n\nLDA的变分EM算法\nTODO，此方法效率较高，sklearn中也是使用变分EM进行计算的。太过复杂所以我就没有准备复现。"
  },
  {
    "objectID": "posts/statis-learn-cp15.html",
    "href": "posts/statis-learn-cp15.html",
    "title": "统计学习方法:概率潜在语义模型",
    "section": "",
    "text": "最近因为各种杂事,导致我很难专心做一些事情.我觉得需要反省一下自己.\n概率潜在语义分析(probabilistic latent semantic analysis, PLSA),也称概率潜在语义索引(probabilistic latent semantic indexing, PLSI),是一种利用概率生成模型对文本集合进行话题分析的无监督学习方法.\n\n此方法理解起来并不难,其实和潜在语义模型类似,只不过他的单词-话题矩阵和话题-文本矩阵在PLSA中是以非负的概率的形式的进行表示.同时我们使用EM算法进行迭代时可忽略话题特征值矩阵.\n我将迭代过程绘制在单纯形中,大家可以发现这个还是很有趣的."
  },
  {
    "objectID": "posts/statis-learn-cp10.html",
    "href": "posts/statis-learn-cp10.html",
    "title": "统计学习方法:条件随机场",
    "section": "",
    "text": "条件随机场的理论实在令人头疼,上来就介绍概率无向图,我是基本没看懂😹.而且又要准备别的东西,很难静心看下去,最后还是看苏神的博客大致了解了,因此这次的代码我也主要是修改了苏神的代码.\nNOTE 我写可视化的时候也发现了crf的状态转移矩阵的变化比较缓慢,然后搜索了一份果然苏神也发现这个问题,并且给出了增大学习率的解决方案,因此我也在代码中实现了crf层的学习率修改方案,最终效果的确相当明显了.\n\n\n原理\n建议大家参考苏神的博客\n实际上李航书里面的讲的也是线性链条件随机场,书里面一开始讲概率无向图,然后介绍最大团等等,这样很难对线性条件随机场有一个直接的认识.实际上对于标注问题,条件随机场就是输入一个序列,通过状态转移矩阵对输出序列进行控制,实际上是学习到输出序列间的关系. 具体的公式推导请大家参考李航书以及苏神的博客,我就不献丑了."
  },
  {
    "objectID": "posts/stack.html",
    "href": "posts/stack.html",
    "title": "栈",
    "section": "",
    "text": "栈的粗略实现~~~\n\n\n代码\n```c /  @Author: Zheng Qihang * @Date: 2018-07-05 20:14:48 * @Last Modified by: Zheng Qihang * @Last Modified time: 2018-11-08 16:38:26 */ #include &lt;stdint.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;time.h&gt;\ntypedef int ElementType; // data type #define Node ptrNode // Node defination #define Stack ptrNode // list defination typedef struct _Node // node defination { ElementType data; struct _Node next; }  ptrNode; // Node is a pointer to _Node\nint IsEmpty(Stack S) { return S-&gt;next == NULL; }\nStack CreateStack(void) { Stack S = (Stack)malloc(sizeof(struct _Node)); S-&gt;next = NULL; return S; }\nElementType Pop(Stack S) { int temp; Node tmepNode = S-&gt;next; if (tmepNode != NULL) { temp = tmepNode-&gt;data; S-&gt;next = tmepNode-&gt;next; free(tmepNode); return temp; } else { return -1; } }\nvoid MakeEmpty(Stack S) { while (S-&gt;next != NULL) { Pop(S); } } void DisposeStack(Stack S) { MakeEmpty(S); free(S); }\nvoid Push(ElementType X, Stack S) { Node tempNode = S-&gt;next; Node newNode = (Node)malloc(sizeof(struct _Node)); newNode-&gt;data = X; // printf(“S-&gt;next %p”,S-&gt;next); if (tempNode != NULL) { newNode-&gt;next = tempNode; // printf(“%p = %p”,newNode-&gt;next,tempNode); } else { newNode-&gt;next = NULL; } S-&gt;next = newNode; // printf(“S-&gt;next %p”,S-&gt;next); }\nElementType Top(Stack S) { if (S-&gt;next != NULL) { return S-&gt;next-&gt;data; } else { return -1; } }\nvoid PrintStack(Stack S) { Node tep = S-&gt;next; while (tep != NULL) { printf(“addr=0x%3lX data=%d nextaddr=0x%3lX”, (unsigned long int)tep & 0xFFF, tep-&gt;data, (unsigned long int)tep-&gt;next & 0xFFF); tep = tep-&gt;next; } }\nint main(int argc, char const *argv[]) { int tempint; Stack MyStack = NULL; printf(“\n基本操作：\n(a).make new Stack;\n(b).push data;\n(c).pop data;\n(d).find Stack’s top;\n(e).empty the Stack;\n(f).dispose the Stack;\n(p).print the stack:\n(q).quit;\n”); while (1) { switch (getchar()) { case ‘a’: printf(“create a new Stack”); MyStack = CreateStack(); if (MyStack != NULL) { printf(“create stack success!”); } break; case ‘b’: printf(“enter and Push data in Stack:”); scanf(“%d”, &tempint); Push(tempint, MyStack); break; case ‘c’: printf(“pop data is:%d”, Pop(MyStack)); break; case ‘d’: printf(“stack top is:%d”, Top(MyStack)); break; case ‘e’: MakeEmpty(MyStack); break; case ‘f’: DisposeStack(MyStack); break; case ‘p’: PrintStack(MyStack); break; case ‘q’: exit(0); break; default: break; } } return 0; } ``\n`"
  },
  {
    "objectID": "posts/ssl-uda.html",
    "href": "posts/ssl-uda.html",
    "title": "半监督学习：Unsupervised Data Augmentation",
    "section": "",
    "text": "第八个算法UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING。此算法与VAT的想法类似，都是通过加强扰动的质量来获得更好的一致性正则化。"
  },
  {
    "objectID": "posts/ssl-uda.html#unsupervised-data-augmentation-uda",
    "href": "posts/ssl-uda.html#unsupervised-data-augmentation-uda",
    "title": "半监督学习：Unsupervised Data Augmentation",
    "section": "Unsupervised Data Augmentation (UDA)",
    "text": "Unsupervised Data Augmentation (UDA)\n半监督一致性训练流程如下：\n\n对于图像数据，UDA使用的是RandAugment方法，来自于Randaugment:  Practical data aug-mentation with no separate search。"
  },
  {
    "objectID": "posts/ssl-uda.html#低数据区域训练退火",
    "href": "posts/ssl-uda.html#低数据区域训练退火",
    "title": "半监督学习：Unsupervised Data Augmentation",
    "section": "低数据区域训练退火",
    "text": "低数据区域训练退火\n在半监督学习中，经常遇到一种情况，即未标记数据量和已标记数据量存在巨大差距。然后模型通常会快速拟合有限的已标记数据，而不足以拟合未标记数据。为了解决这个问题，提出了一种训练信号退火TSA的方法，随着训练会逐渐释放带标签的样本的信号，就是在模型对于样本的置信度低于阈值\\(\\eta_{t}\\)的时候情况下，才使用带标签的样本，如果模型对于正确类别的预测概率高于阈值，那么要从损失函数中删除这个样本，这就是为了防止出现过度训练。\n对于\\(\\eta_{t}\\)的变化曲线，设计了三种模式，对于标签训练数据较小时容易过拟合可以使用exp曲线，对于标签训练数据较多时可以使用log曲线。"
  },
  {
    "objectID": "posts/ssl-remixmatch.html",
    "href": "posts/ssl-remixmatch.html",
    "title": "半监督学习：ReMixMatch",
    "section": "",
    "text": "第九个算法ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring，这也是谷歌MixMatch的同一作者提出的，是对MixMatch的改进。"
  },
  {
    "objectID": "posts/ssl-remixmatch.html#distribution-alignment",
    "href": "posts/ssl-remixmatch.html#distribution-alignment",
    "title": "半监督学习：ReMixMatch",
    "section": "Distribution Alignment",
    "text": "Distribution Alignment\n分布对齐的目标是使无标签数据的预测汇总与提供的标签数据分布相匹配。这个概念是25年前的Unsupervised classifiers, mutualinformation and’phantom targets所引入的，但是在ReMixMatch之前还没有人在半监督学习中用过这个方法。\n\n\n\n分布对齐.根据经验性的ground-truth类别分布除以未标记数据的平均预测的比例调整猜测标签的分布\n\n\n半监督算法的主要目标是利用未标记数据提升模型性能，bridle等人首先提出一种这种直觉形式化的方法，最大化无标签数据的输入与输出间的互信息。将此操作公式化为如下，以下公式推导时，假设了\\(p(x),p(y)\\)相互独立则有\\(p(y) = \\int p(x)p(y|x)\\ dx\\)。：\n\\[\n\\begin{align}\n\\mathcal{I}(y ; x) &=\\iint p(y, x) \\log \\frac{p(y, x)}{p(y) p(x)} \\mathrm{d} y \\mathrm{d} x\\\\\n&=\\int p(x) \\mathrm{d} x \\int p(y | x) \\log \\frac{p(y | x)}{p(y)} \\mathrm{d} y\\\\\n&=\\int p(x) \\mathrm{d} x \\int p(y | x) \\log \\frac{p(y | x)}{\\int p(x) p(y | x) \\mathrm{d} x} \\mathrm{d} y\\\\\n&=\\mathbb{E}_{x}\\left[\\int p(y | x) \\log \\frac{p(y | x)}{\\mathbb{E}_{x}[p(y | x)]} \\mathrm{d} y\\right]\\\\\n\\text{离散化:}\\\\\n&=\\mathbb{E}_{x}\\left[\\sum_{i=1}^{L} p\\left(y_{i} | x\\right) \\log \\frac{p\\left(y_{i} | x\\right)}{\\mathbb{E}_{x}\\left[p\\left(y_{i} | x\\right)\\right]}\\right]\\\\\n&=\\mathbb{E}_{x}\\left[\\sum_{i=1}^{L} p\\left(y_{i} | x\\right) \\log p\\left(y_{i} | x\\right)\\right]-\\mathbb{E}_{x}\\left[\\sum_{i=1}^{L} p\\left(y_{i} | x\\right) \\log \\mathbb{E}_{x}\\left[p\\left(y_{i} | x\\right)\\right]\\right]\\\\\n&=\\mathbb{E}_{x}\\left[\\sum_{i=1}^{L} p\\left(y_{i} | x\\right) \\log p\\left(y_{i} | x\\right)\\right]-\\sum_{i=1}^{L} \\mathbb{E}_{x}\\left[p\\left(y_{i} | x\\right)\\right] \\log \\mathbb{E}_{x}\\left[p\\left(y_{i} | x\\right)\\right]\n\\end{align}\\tag{1}\n\\] \\[\n\\begin{align}\n&=\\mathcal{H}\\left(\\mathbb{E}_{x}\\left[p_{\\text {model }}(y | x ; \\theta)\\right]\\right)-\\mathbb{E}_{x}\\left[\\mathcal{H}\\left(p_{\\text {model }}(y | x ; \\theta)\\right)\\right]\n\\end{align}\\tag{2}\n\\]\n其中\\(\\mathcal{H}(\\cdot)\\)为熵。其中公式2是熟悉的熵最小化目标，它简单的鼓励模型输出具有更低的熵(对当前类别标签置信度更高)。但其中公式1并未广泛使用，其目标是鼓励模型在整个训练集平均预测每个类别的频率相同，bridle等人称之为公平性。\n在MixMatch中已经使用了sharpening函数使猜测标签的熵最小化。现在要通过互信息的概念引入公平性这一原则，注意目标\\(\\mathcal{H}\\left(\\mathbb{E}_{x}\\left[p_{\\text {model }}(y | x ; \\theta)\\right]\\right)\\)本来已经暗示了它应该以相同的频率预测每个标签，但如果数据集中的\\(p(y)\\)的分布并不是均匀的，那这个目标就不一定有效了。虽然可以按batch最小化这个目标，但是为了不引入更多的超参数，因此为了解决以上问题，引入了另外一种公平性形式Distribution Alignment。其过程如下：\n训练过程中维持模型对未标记数据的预测结果的平均值\\(\\tilde{p}(y)\\),给定模型在未标记数据\\(u\\)上的预测为\\(q=P_{\\text{model}}(y|u,\\theta)\\)，我们将利用\\(\\frac{p(y)}{\\tilde{p}(y)}\\)作为比例缩放\\(q\\)，然后在重新放大到有效的概率分布区间： \\(\\tilde{q}=\\text{Normalize}(q\\times\\frac{p(y)}{\\tilde{p}(y)})\\)，其中Normalize为\\(\\text{Normalize}(x)_i=\\frac{x_i}{\\sum_j x_j}\\)。 然后我们使用\\(\\tilde{q}\\)作为\\(u\\)的猜测标签，然后可以再用sharpening或其他的处理方式。实际操作中，将计算过去128个batch中无标签数据预测值的滑动平均作为\\(\\tilde{p}(y)\\)，如果我们直接知道\\(\\tilde{p}(y)\\)的某些先验分布，那么应该还可以更好。"
  },
  {
    "objectID": "posts/ssl-remixmatch.html#改进一致性正则化",
    "href": "posts/ssl-remixmatch.html#改进一致性正则化",
    "title": "半监督学习：ReMixMatch",
    "section": "改进一致性正则化",
    "text": "改进一致性正则化\n论文中说，使用了最新提出AutoAugment数据增强算法来代替原本MixMatch中的数据弱增强看看能不能提高性能，但是发现训练并不能收敛。因此提出了一个解决方法Augmentation Anchoring，它的基本想法是将模型对弱增强的未标记图像的预测结果作为同一图像的强增强的猜测标签。\n同时因为AutoAugment是使用强化学习策略来搜索的，需要对有监督模型做多次尝试。在半监督学习中难以做到，为了解决这个问题，提出了一个名为CTAugment的方法，使用控制理论的思想在线适应，而无需任何形式的基于强化学习的训练。\n\nAugmentation Anchoring\n我们假设带有AutoAugment的MixMatch不稳定的原因是MixMatch对\\(K\\)个的预测取了平均值。由于增强效果可能会导致不同的预测，因此其平均值可能不是有意义的目标，取而代之的是，给定一个未标记的输入\\(u\\)，我们首先通过对其应用弱增强来生成一个Anchor。然后使用CTAugment生成\\(K\\)个\\(u\\)的增强，然后将(经过distribution alignment和sharpening后的)猜测标签作为\\(K\\)个增强后的目标。\n\n在实验中发现，使用Augmentation Anchoring之后，可以直接使用交叉熵代替原本的mse损失，更易于实现，同时\\(K=2\\)即可取得不错的效果，当然\\(K&gt;8\\)效果更好。\n\n\nControl Theory Augment\n像AutoAugment一样，CTAugment均匀的随机采样要实施的变换，但是会在训练过程中动态推断每次变换的幅度大小。由于CTAugment具有不敏感的超参数，因此可以直接包含在半监督模型中。直观的，对于每个建议的参数，CTAugment都知道它将产生被分类正确标签的图像的概率，然后使用这些概率，仅对网络可忍受范围内的误差进行采样。这个过程在FastAutoAugment中被称为density-matching。\n首先，CTAugment将每个变化的每个参数范围划分为数个分组，在开始训练时将每个分组的权重设置为1，然后令权重向量\\(m\\)向某些分组变化，这些权重决定了那些幅度级别是需要实施变化的。在每个训练step中，对于每个图像随机地均匀采样两个变换，用于图像增强。使用改变过的权重参数\\(\\bar{m}\\)，其中${m}_i=m_i     m_i&gt;0.8    {m}i =0 \\(，否则使用\\){m}\\(作为权重进行随机分类采样。为了更新权重，首先随机地对每个转换参数均匀的采样一个\\)m_i\\(，将结果转换应用于带标签样本\\)x\\(以获得增强版本\\){x}\\(，然后测量模型的预测与标签的匹配程度为\\)-|p{}(y|{x};)-p|\\(，每个采样权重的权重更新为\\)m_i=m_i+(1-)\\(，其中\\)$是固定的指数衰减超参数。\n\n\n综合\n综合算法流程如下：\n\n主要是生成两个集合\\(\\mathcal{X}'\\)和\\(\\mathcal{U}'\\)，由增强后的带标记的有标签无标签数据mixup生成。\\(\\mathcal{X}'\\)和\\(\\mathcal{U}'\\)的标签与猜测标签根据模型预测输入到标准的交叉熵损失中。还有\\(\\mathcal{U}_1\\)是由无标签数据经过单个强增强组成的，并且他的猜测标签没有应用mixup，\\(\\mathcal{U}_1\\)是用在两个额外的损失项中，它能提供很大的改善。\nPre-mixup unlabeled loss： 将\\(\\mathcal{U}_1\\)的猜测标签和预测输入一个单独的交叉熵损失项。\nRotation loss ：最近的结果表明，将自我监督学习的思想应用于半监督学习可以产生出色的性能( Self-supervised semi-supervised learning)。将这个想法通过旋转每个图像\\(\\text{Rotate}(u,r) \\in \\mathcal{U}_1\\)来整合，\\(r \\sim {0,90,180,270}\\)，然后要求模型将旋转量预测为四类分类问题。\n\\[\n\\begin{align}\n\\begin{aligned}\n\\sum_{x, p \\in \\mathcal{X}^{\\prime}} \\mathrm{H}\\left(p, p_{\\text {model }}(y | x ; \\theta)\\right)+\\lambda_{\\mathcal{U}} \\sum_{u, q \\in \\mathcal{U}^{\\prime}} \\mathrm{H}\\left(q, p_{\\text {model }}(y | u ; \\theta)\\right) \\\\\n+\\lambda_{\\hat{u}_{1}} \\sum_{u, q \\in \\hat{\\mathcal{U}}_{1}} \\mathrm{H}\\left(q, p_{\\text {model }}(y | u ; \\theta)\\right)+\\lambda_{r} \\sum_{u \\in \\hat{\\mathcal{U}}_{1}} \\mathrm{H}\\left(r, p_{\\text {model }}(r | \\text { Rotate }(u, r) ; \\theta)\\right)\n\\end{aligned}\n\\end{align}\n\\]\n根据消融测试结果：\n\n如果没有弱增强和强增强间的augment anchoring错误率就立马上升非常多。其次是将guess label的损失从交叉熵变成l2 loss，不过这里我挺奇怪的，之前其他的算法都是说l2 loss的约束性更大，效果会更好。"
  },
  {
    "objectID": "posts/ssl-pi-model.html",
    "href": "posts/ssl-pi-model.html",
    "title": "半监督学习：Π model",
    "section": "",
    "text": "第二个算法Temporal Ensembling for Semi-Supervised Learning,它提出了一个Π model以及Temporal ensembling的方法。"
  },
  {
    "objectID": "posts/ssl-pi-model.html#π-model",
    "href": "posts/ssl-pi-model.html#π-model",
    "title": "半监督学习：Π model",
    "section": "Π model",
    "text": "Π model\n读作pi model但实际上代表着模型有着双输入，其示意图如下所示，对于无标签数据\\(x_i\\)经过两次不同的随机变换后再使用相同模型(模型的dropout也是不同的)的到两个输出\\(z_i,\\tilde{z}_i\\)，由于样本是相同的，因此两次输出的概率分布间应该尽可能相同，计算两个输出概率的l2 loss并乘上warmup系数，因为在训练刚开始我们希望带标签样本的分类损失权重更大些。\n\nΠ model感觉非常简单，实际上是如pseudo label一样，考虑到了熵的正则化，但他的做法比pseudo label的更加高明一些，模型为何一定要知道无标签的样本的实际标签呢？直接利用两个同类样本间概率分布的相似度损失，提升模型的一致性；同时利用少量的带标签数据指导模型分类，over～"
  },
  {
    "objectID": "posts/ssl-pi-model.html#temporal-ensembling",
    "href": "posts/ssl-pi-model.html#temporal-ensembling",
    "title": "半监督学习：Π model",
    "section": "Temporal ensembling",
    "text": "Temporal ensembling\n\nTemporal ensembling时序组合模型，是针对Π model的优化，我们分析了Π model所做的两件事情，1)利用扰动样本学习一致性;2)利用有标签样本学习分类。在Π model中，\\(z_i,\\tilde{z}_i\\)都是来自同一迭代时间内产生的两次结果，但实际上并没有必要，因为首先这样一个step就要推理两次模型，而且只在一个batch生成的概率分布偶然性较大，所以使用时序组合模型，\\(\\tilde{z}_i\\)来自上个迭代周期产生的结果，\\(z_i\\)来自当前迭代时间内产生的结果，也就是比较了两次不同时间内产生的概率分布。在时序组合模型中，一个step只执行一次，那么相比于Π model，它就有了两倍的加速。同时这个\\(\\tilde{z}_i\\)是历史\\(z_i\\)的加权和。这样做的好处是能够保留历史信息，消除扰动和稳定当前值。\n这个做法就很像上一篇pseudo label最后，有的人发现一个epoch去打伪标签效果好于每个batch都打伪标签一样。"
  },
  {
    "objectID": "posts/ssl-mixmatch.html",
    "href": "posts/ssl-mixmatch.html",
    "title": "半监督学习：MixMatch",
    "section": "",
    "text": "第七个算法MixMatch: A Holistic Approach toSemi-Supervised Learning。此算法将之前的各个半监督学习算法进行融合，统一了主流方法，得到了最优的效果。此算法好，就是训练的过程慢一些。"
  },
  {
    "objectID": "posts/ssl-mixmatch.html#mixmatch",
    "href": "posts/ssl-mixmatch.html#mixmatch",
    "title": "半监督学习：MixMatch",
    "section": "MixMatch",
    "text": "MixMatch\n首先给出一系列符号。给定一个batch的标记数据\\(\\mathcal{X}\\)以及one-hot标签，一个batch的无标签数据\\(\\mathcal{U}\\)，通过数据增强得到\\(\\mathcal{X}',\\mathcal{U}'\\)，然后对他们分别计算损失，最终整合所有损失：\n\\[\n\\begin{align}\n\\mathcal{X}^{\\prime}, \\mathcal{U}^{\\prime}=\\operatorname{MixMatch}(\\mathcal{X}, \\mathcal{U}, T, K, \\alpha)\n\\end{align}\\tag{2}\n\\]\n\\[\n\\begin{align}\n\\mathcal{L}_{\\mathcal{X}}=\\frac{1}{\\left|\\mathcal{X}^{\\prime}\\right|} \\sum_{x, p \\in \\mathcal{X}^{\\prime}} \\mathrm{H}\\left(p, \\mathrm{p}_{\\text {model }}(y | x ; \\theta)\\right)\n\\end{align}\\tag{3}\n\\]\n\\[\n\\begin{align}\n\\mathcal{L}_{\\mathcal{U}}=\\frac{1}{L\\left|\\mathcal{U}^{\\prime}\\right|} \\sum_{u, q \\in \\mathcal{U}^{\\prime}}\\left\\|q-\\mathrm{p}_{\\text {model }}(y | u ; \\theta)\\right\\|_{2}^{2}\n\\end{align}\\tag{4}\n\\]\n\\[\n\\begin{align}\n\\mathcal{L}=\\mathcal{L}_{\\mathcal{X}}+\\lambda_{\\mathcal{U}} \\mathcal{L}_{\\mathcal{U}}\n\\end{align}\\tag{5}\n\\]\n其中\\(H(p,q)\\)是分布\\(p\\)和\\(q\\)间的交叉熵，\\(T,K,\\alpha,\\lambda_{\\mathcal{U}}\\)是超参数。完整的MixMatch如算法1所示。\n\n现在来描述各个部分：\n\n数据增强\n对于一个batch\\(\\mathcal{X}\\)中的每一个\\(x_b\\)，通过变化得到\\(\\hat{x}_{b}=\\text { Augment }\\left(x_{b}\\right)\\)(算法1第3行)。对于每个无标签数据\\(u_b\\)，我们生成\\(K\\)个增强\\(\\hat{u}_{b, k}=\\text { Augment }\\left(u_{b}\\right), k \\in(1, \\ldots, K)\\)(算法1第5行)。再使用每个\\(u_b\\)送入模型得到对应的猜测标签\\(q_b\\)。\n标签猜测\n有了猜测标签，我们将它用在无监督损失中，平均对\\(u_b\\)做\\(K\\)个增强的模型预测输出分布：\n\\[\n\\begin{align}\n\\bar{q}_{b}=\\frac{1}{K} \\sum_{k=1}^{K} \\operatorname{Prodel}\\left(y | \\hat{u}_{b, k} ; \\theta\\right)\n\\end{align}\\tag{6}\n\\]\nsharpening： 为了达到对熵最小化的目的，我们需要对给定数据增强预测的平均值\\(\\bar{q}_{b}\\)进行sharpening，通过sharpening函数减小标签的分布熵。在代码中，是调整分类分布的温度系数：\n\\[\n\\begin{align}\n\\text { Sharpen }(p, T)_{i}:=p_{i}^{\\frac{1}{T}} / \\sum_{j=1}^{L} p_{j}^{\\frac{1}{T}}\n\\end{align}\\tag{7}\n\\]\n其中\\(p\\)是一些输入分类分布(在此算法中为\\(\\bar{q}_{b}\\))，\\(T\\)是超参数。当\\(T\\rightarrow0\\)，\\(\\text{Sharpen}(p,T)\\)的输出会趋近于Dirac(one-hot)分布，降低温度系数会鼓励模型产生较低熵的预测。\nmixup\n要应用mixup，我们首先需要将所有的带标签的增强数据和所有无标签样本以及对应的猜测标签收集起来(算法1第10-11行): \\[\n\\begin{align}\n\\hat{\\mathcal{X}}=\\left(\\left(\\hat{x}_{b}, p_{b}\\right) ; b \\in(1, \\ldots, B)\\right)\n\\end{align}\\tag{12}\n\\] \\[\n\\begin{align}\n\\hat{\\mathcal{U}}=\\left(\\left(\\hat{u}_{b, k}, q_{b}\\right) ; b \\in(1, \\ldots, B), k \\in(1, \\ldots, K)\\right)\n\\end{align}\\tag{13}\n\\]\n然后我们联合以上分布并进行混洗得到新的数据集\\(\\mathcal{W}\\)作为mixup的输入，对每第\\(i\\)个样本对\\(\\hat{\\mathcal{X}}\\)，我们计算\\(\\operatorname{MixUp}\\left(\\hat{\\mathcal{X}}_{i}, \\mathcal{W}_{i}\\right)\\)并将结果添加到\\(\\mathcal{X}'\\)(算法1第13行)，对于\\(i\\in(1,\\ldots,|\\bar{\\mathcal{U}}|)\\)我们计算\\(\\mathcal{U}_{i}^{\\prime}=\\operatorname{MixUp}\\left(\\hat{\\mathcal{U}}_{i}, \\mathcal{W}_{i+|\\hat{\\mathcal{X}}|}\\right)\\) for \\(i \\in(1, \\ldots,|\\hat{\\mathcal{U}}|)\\)。在这个过程中，带标签数据可能会和无标签数据产生混合。\n损失函数\n损失即标签数据的交叉熵结合无标签数据的差异性损失。\n超参数\n因为MixMatch结合了很多算法，所以超参数也特别的多，一般固定\\(T=0.5，K=2\\)，然后\\(\\alpha=0.75,\\lambda_{\\mathcal{U}}=100\\)\n\n消融测试结果：\n\n可以发现关键提升点在于锐化以及无标签数据间的mixup"
  },
  {
    "objectID": "posts/ssl-infomax-error.html",
    "href": "posts/ssl-infomax-error.html",
    "title": "infomax中一些错误总结",
    "section": "",
    "text": "最近想用infomax算法和对比学习结合起来，然后应用在半监督学习中。在实验过程中遇到了一个非常奇怪的问题。\n\n\n问题描述\n我一开始以为infomax中的判别器也可以随便来，然后就定义了判别器如下：\n# get global discriminate result\nglobaldiscriminator = compose(\n    # kl.InputLayer(input_shape=[2 * z_dim]),\n    kl.Dense(z_dim, activation='relu'),\n    kl.BatchNormalization(),\n    kl.Dense(z_dim, activation='relu'),\n    kl.BatchNormalization(),\n    kl.Dense(z_dim, activation='relu'),\n    kl.BatchNormalization(),\n    kl.Dense(1, activation='sigmoid'),\n)\n然后在cifar10数据集中进行实验，发现判别器损失一直没有降低，反而先验损失会降低到一个相当小的值，但编码器输出的隐变量明显不符合先验分布。\n下面分别是全局互信息损失和局部互信息损失，可以看到这么多step过去后，几乎没有下降：\n \n对于先验分布的损失可以看到下降到一个非常低的值了。\n\n隐变量输出分布统计结果：\nz mean 0.005774456\nz std 0.120639555\nz mean 0.00078799046\nz std 0.08064849\nz mean 0.0028064575\nz std 0.09524193\nz mean 7.149822e-05\nz std 0.08297808\n相似度采样结果，虽然是测试集上的结果，差一点说的过去，但是这还不如不训练：\n\n\n\n问题解决\n经过各种修改，发现问题就在判别器中，只要将判别器中的bn删除：\n# get global discriminate result\nglobaldiscriminator = compose(\n    # kl.InputLayer(input_shape=[2 * z_dim]),\n    kl.Dense(z_dim, activation='relu'),\n    kl.Dense(z_dim, activation='relu'),\n    kl.Dense(z_dim, activation='relu'),\n    kl.Dense(1, activation='sigmoid'),\n)\n然后在cifar10数据集中进行实验，发现判别器损失有效降低，先验损失会维持在一个范围，这样才有对抗的感觉了。\n下面分别是全局互信息损失和局部互信息损失，降到了相当低：\n \n对于先验分布的损失有下降，当没有那么强烈。\n\n隐变量输出分布统计结果，这次就接近于独立正态分布了：\nz mean 0.0043534352\nz std 1.0862136\nz mean 0.009636099\nz std 1.1103721\nz mean -0.011318588\nz std 1.0901983\nz mean -0.0093264235\nz std 1.112159\nz mean -0.010979499\nz std 1.1168234\n相似度采样结果，这次就好了很多：\n\n\n\n思考\n个人认为是这个问题是因为判别器的输入是两个隐变量直接concat在一起的，如果进行bn就导致分布相同丧失判别能力了。下一步可以考虑借鉴simclr中的方法，infomax中的负采样估计只是简单的shuffle一下，而simclr中利用矩阵乘法在一个batch中直接组合出batch*batch-batch个样本对，特别是他的论文中batch size最大是8192，相当暴力。\n在实验的过程中我还发现了一个比较有趣的地方，就是如果直接初始化模型然后对编码器的输出做相似度采样，结果其实不差。让我感觉我训练这个模型这么久好像就改变了一下编码器输出的分布。。。"
  },
  {
    "objectID": "posts/ssl-fixmatch.html",
    "href": "posts/ssl-fixmatch.html",
    "title": "半监督学习：FixMatch",
    "section": "",
    "text": "第十个算法FixMatch: Simplifying Semi-Supervised Learning withConsistency and Confidence，这依旧是谷歌研究组的作者提出的，是对MixMatch的改进。"
  },
  {
    "objectID": "posts/ssl-fixmatch.html#fixmatch中的增强",
    "href": "posts/ssl-fixmatch.html#fixmatch中的增强",
    "title": "半监督学习：FixMatch",
    "section": "FixMatch中的增强",
    "text": "FixMatch中的增强\n其中弱增强为50%随机水平翻转，12.5%随机上下翻转，并添加一部分水平平移。对于强增强，使用了RandAugment和CTAugment。\n对于RandAugment他使用一个全局的幅度来控制增强，幅度是通过验证集来优化的。不过论文中发现每个step直接从预定义范围中随机采样幅度即可获得良好的效果，方法类似与UDA。\n对于CTAugment，在ReMixMatch讲过了。"
  },
  {
    "objectID": "posts/ssl-fixmatch.html#其他重要因素",
    "href": "posts/ssl-fixmatch.html#其他重要因素",
    "title": "半监督学习：FixMatch",
    "section": "其他重要因素",
    "text": "其他重要因素\n发现对于优化器，使用Adam反而效果不好。对于学习率衰减，使用余弦衰减比较好。"
  },
  {
    "objectID": "posts/ssl-fixmatch.html#相关工作总结",
    "href": "posts/ssl-fixmatch.html#相关工作总结",
    "title": "半监督学习：FixMatch",
    "section": "相关工作总结",
    "text": "相关工作总结\n\n\n\n\n\n\n\n\n\n\n算法\n人工标签增强\n预测增强\n人工标签后处理\n备注\n\n\n\n\nΠ-Model\n弱\n弱\n无\n\n\n\nTemporal Ensembling\n弱\n弱\n无\n使用较早训练的模型\n\n\nMean Teacher\n弱\n弱\n无\n使用参数指数移动平滑\n\n\nVirtual Adversarial Training\n无\n对抗\n无\n\n\n\nUDA\n无\n强\n锐化\n忽略低置信度的人工标签\n\n\nMixMatch\n弱\n弱\n锐化\n平均多个人工标签\n\n\nReMixMatch\n弱\n强\n锐化\n汇总多个预测的损失\n\n\nFixMatch\n弱\n强\n伪标签\n\n\n\n\n实际上经过MixMatch，ReMixMatch中大量的消融测试，作者应该已经找到了其中最重要的几个因素，所以这篇论文的理论部分比较少，因为之前的论文都已经介绍过了。实际上我认为主要加强点还是在于如何更好的使用一致性正则化，弱增强与弱增强间的所能学习到的一致性还不够，需要在弱增强与强增强间学习。同时对于人工标签的处理方式相当于选择如何进行熵最小化，也是次重要的。"
  },
  {
    "objectID": "posts/ssl-fixmatch.html#实验结果",
    "href": "posts/ssl-fixmatch.html#实验结果",
    "title": "半监督学习：FixMatch",
    "section": "实验结果",
    "text": "实验结果"
  },
  {
    "objectID": "posts/ssl-fixmatch.html#消融测试",
    "href": "posts/ssl-fixmatch.html#消融测试",
    "title": "半监督学习：FixMatch",
    "section": "消融测试",
    "text": "消融测试\n这里的消融测试主要就是在优化器上的了。"
  },
  {
    "objectID": "posts/source-generator.html",
    "href": "posts/source-generator.html",
    "title": "C# Source Generator使用",
    "section": "",
    "text": "C#中提供一个代码生成的功能,基于模板的生成和这个是没法比的.因为我们是直接调用编译器对当前项目进行分析,然后生成代码."
  },
  {
    "objectID": "posts/source-generator.html#项目配置",
    "href": "posts/source-generator.html#项目配置",
    "title": "C# Source Generator使用",
    "section": "1. 项目配置",
    "text": "1. 项目配置\n首先对于生成器,我们需要新建一个项目,并且他TargetFramework必须设置为netstandard2.0."
  },
  {
    "objectID": "posts/source-generator.html#分析指定的项目",
    "href": "posts/source-generator.html#分析指定的项目",
    "title": "C# Source Generator使用",
    "section": "2. 分析指定的项目",
    "text": "2. 分析指定的项目\n我们可以把新建的生成器看作为一个分析器,某一个项目引用他,那么当前的分析器就分析当前项目(并且引用时需要设置属性 OutputItemType=\"Analyzer\"). 比如: 比如下面这个例子,我们的Nncase.IR引用了生产器,此时的代码就是基于Nncase.IR分析所产生的.\n|-- Nncase.Pattern.Generator ----\n|                               |\n|-- Nncase.IR     &lt;--------------"
  },
  {
    "objectID": "posts/source-generator.html#编写指南",
    "href": "posts/source-generator.html#编写指南",
    "title": "C# Source Generator使用",
    "section": "3. 编写指南",
    "text": "3. 编写指南\n\n3.1 尽量使用ISyntaxContextReceiver.\n我们可以直接通过ctx.SemanticModel.Compilation.GetTypeByMetadataName(\"Nncase.IR.Expr\")获取一些重要的基类对象的symbol.\n\n\n3.2 尽量使用SemanticModel\n比如我们从receiver获得到语法节点后,转换为symbol可以更加方便的获取类型的属性/继承/接口/类型等等信息.\n// 0. check inherit from base class;\nif (classSymbol.BaseType is not { IsGenericType: true, Name: \"RewriteRule\" })\n    Diagnostics.Add(Diagnostic.Create(RecriverUtil.ClassNotFromBaseClassError, Location.None, classSymbol.ToDisplayString(), \"RewriteRule\"));\n\n// 1. check is Partial\nif (!classDeclaration.Modifiers.Any(tok =&gt; tok.IsKind(SyntaxKind.PartialKeyword)))\n    Diagnostics.Add(Diagnostic.Create(RecriverUtil.ClassNotPartialError, Location.None, classSymbol.ToDisplayString()));\n\n\n3.3 如何构建正确的语法树\n因为他的api太多太杂, 每个语法节点都有自己类型,然后方法多的根本不知道用哪个. 所以需要用visual studio,先用syntax visualizer看一下代码的语法树再分析.\n或者使用https://roslynquoter.azurewebsites.net."
  },
  {
    "objectID": "posts/source-generator.html#debug代码生成过程",
    "href": "posts/source-generator.html#debug代码生成过程",
    "title": "C# Source Generator使用",
    "section": "4. Debug代码生成过程",
    "text": "4. Debug代码生成过程\n务必请使用vs 2022, 我录制了一个启动调试的视频."
  },
  {
    "objectID": "posts/source-generator.html#直接分析整个项目生成代码",
    "href": "posts/source-generator.html#直接分析整个项目生成代码",
    "title": "C# Source Generator使用",
    "section": "5. 直接分析整个项目生成代码",
    "text": "5. 直接分析整个项目生成代码\n后来发现内置的源代码生成有个问题就是他只能对当前项目进行分析,生成的代码也只能在这个项目中,但是我这里的需求是分析的项目a和b为c生成代码,这样的话必须要abc同时依赖代码分析器了. 会造成项目结构混乱的问题. 后面我是重新写了一个第三方项目,直接分析整个sln, 然后生成文件的形式. 代码放在了 https://github.com/zhen8838/PatternSourceGenerator.0\n不过我现在发现其实可以先得到symbol信息"
  },
  {
    "objectID": "posts/som.html",
    "href": "posts/som.html",
    "title": "som算法",
    "section": "",
    "text": "相比于bp神经网络算法，som相对来说比较容易理解。自组织神经网络，是一种用于聚类的神经网络算法，从名字便可以看出，这是一种无监督式的算法，意味着，它不需要任何训练样本，便可以直接对输入样本根据其特征分类，将具有相似特征的划分为一类。"
  },
  {
    "objectID": "posts/som.html#向量归一化",
    "href": "posts/som.html#向量归一化",
    "title": "som算法",
    "section": "1.向量归一化",
    "text": "1.向量归一化\n对自组织网络中的当前输入模式向量\\(X_i(i=1,2,...n)\\)、随机生成输出层神经元\\(w_j(j=1,2,...m)\\)，全部进行归一化处理,得到\\(\\hat{X_i}\\)和\\(\\hat{W_j}\\):\n\\[ \\begin{aligned}\n    \\hat{X_i}=\\frac{X_i}{||X_i||}\\ \\ ,\\ \\ \\hat{W_j}=\\frac{W_j}{||W_j||}\n\\end{aligned} \\]"
  },
  {
    "objectID": "posts/som.html#寻找获胜神经元",
    "href": "posts/som.html#寻找获胜神经元",
    "title": "som算法",
    "section": "2.寻找获胜神经元",
    "text": "2.寻找获胜神经元\n将\\(\\hat{X_i}\\)和\\(\\hat{W_j}\\)进行相似性对比，设获胜神经元矩阵为\\(Win\\)：\n\n基于距离的相似性对比 例如在二维情况下，利用欧式距离做相似性对比。 \\[ \\begin{aligned}\n    Win_j=min\\{||\\hat{X}_{i(i=1,2,...n)}-\\hat{W_j}||\\}\\ \\ \\ \\ (j=1,2,...m)\n\\end{aligned} \\]\n基于方向的相似性对比 利用矩阵的空间性质，夹角越小，余弦越大做相似性对比。 \\[ \\begin{aligned}\n    Win_j=max\\{\\hat{X_i}*\\hat{W_j}\\}\n\\end{aligned} \\]"
  },
  {
    "objectID": "posts/som.html#网络输出与权调整",
    "href": "posts/som.html#网络输出与权调整",
    "title": "som算法",
    "section": "3.网络输出与权调整",
    "text": "3.网络输出与权调整\n按照Winner take all的学习法则，获胜的神经元可以调整其权值: \\[ \\begin{aligned}\n    Win_j(t+1)=\\hat{Win_j}(t)+\\eta(t)(\\hat{X_i}-\\hat{Win_j})\\ \\ \\ \\ 0&lt;\\eta(t)\\leq1\n\\end{aligned} \\]"
  },
  {
    "objectID": "posts/som.html#循环迭代",
    "href": "posts/som.html#循环迭代",
    "title": "som算法",
    "section": "4.循环迭代",
    "text": "4.循环迭代\n先将上一步中的学习率按照梯度下降的缩减 \\[  \n\\begin{aligned}\n\\eta(t)=\\eta(t)e^{-N}\n\\end{aligned}\n\\]\n接着进行循环进行第一步。当梯度小于某个临界点，或者\\(N\\)大于某个临界值时结束。"
  },
  {
    "objectID": "posts/som.html#流程",
    "href": "posts/som.html#流程",
    "title": "som算法",
    "section": "流程",
    "text": "流程"
  },
  {
    "objectID": "posts/som.html#代码",
    "href": "posts/som.html#代码",
    "title": "som算法",
    "section": "代码",
    "text": "代码\nimport numpy as np\nimport pylab as pl\n\n\nclass SOM(object):\n    def __init__(self, X, output, iteration, batch_size):\n        \"\"\"\n        :param X: 形状是N*D， 输入样本有N个,每个D维\n        :param output: (n,m)一个元组，为输出层的形状是一个n*m的二维矩阵\n        :param iteration:迭代次数\n        :param batch_size:每次迭代时的样本数量\n        初始化一个权值矩阵，形状为D*(n*m)，即有n*m权值向量，每个D维\n        \"\"\"\n        self.X = X  # 30 行 2 列 =&gt;  30个数据 2个参数\n        self.output = output  # 输出 5x5 的矩阵\n        self.iteration = iteration  # 迭代次数\n        self.batch_size = batch_size  # 迭代时的样本数量 30\n        self.W = np.random.rand(\n            X.shape[1], output[0] * output[1])  # 权值矩阵 2行 25 列，\n        print(\"W mat shape is\", self.W.shape)\n\n    def GetN(self, t):\n        \"\"\"\n        :param t:时间t, 这里用迭代次数来表示时间\n        :return: 返回一个整数，表示拓扑距离，时间越大，拓扑邻域越小\n        \"\"\"\n        a = min(self.output)  # 选取输出矩阵中最小的值\n        return int(a-float(a)*t/self.iteration)  # a减去迭代次数的百分比\n\n    def Geteta(self, t, n):\n        \"\"\"\n        :param t: 时间t, 这里用迭代次数来表示时间\n        :param n: 拓扑距离\n        :return: 返回学习率，\n        \"\"\"\n        return np.power(np.e, -n)/(t+2)\n\n    def updata_W(self, X, t, winner):\n        \"\"\" \n        用于更新权值矩阵\n        \"\"\"\n        N = self.GetN(t)  # 设置邻域半径\n        for x, i in enumerate(winner):  # 获取winner的各个值\n            to_update = self.getneighbor(i[0], N)  # i(0)就是当前的winner元素\n            for j in range(N+1):\n                e = self.Geteta(t, j)\n                for w in to_update[j]:\n                    self.W[:, w] = np.add(\n                        self.W[:, w], e*(X[x, :] - self.W[:, w]))\n\n    def getneighbor(self, index, N):\n        \"\"\"\n        :param index:获胜神经元的下标\n        :param N: 邻域半径\n        :return ans: 返回一个集合列表，分别是不同邻域半径内需要更新的神经元坐标\n        \"\"\"\n        a, b = self.output\n        length = a*b  # 获得输出矩阵的长度\n\n        def distence(index1, index2):\n            i1_a, i1_b = index1 // a, index1 % b\n            i2_a, i2_b = index2 // a, index2 % b\n            return np.abs(i1_a - i2_a), np.abs(i1_b - i2_b)\n        # 创建N+1个集合\n        ans = [set() for i in range(N+1)]\n        for i in range(length):\n            # 求每一个元素与index的距离\n            dist_a, dist_b = distence(i, index)\n            if dist_a &lt;= N and dist_b &lt;= N:  # 若小于邻域半径\n                ans[max(dist_a, dist_b)].add(i)  # ans添加数据\n        return ans\n\n    def train(self):\n        \"\"\"\n        train_Y:训练样本与形状为batch_size*(n*m)\n        winner:一个一维向量，batch_size个获胜神经元的下标\n        :return:返回值是调整后的W\n        \"\"\"\n        count = 0  # 迭代次数计数器\n        while self.iteration &gt; count:\n            # 开始\n            # 从X的总数中随机选择 batch_size 个数做训练\n            train_X = self.X[np.random.choice(\n                self.X.shape[0], self.batch_size)]\n            # 归一化 权值矩阵\n            normal_W(self.W)\n            # 归一化 训练矩阵\n            normal_X(train_X)\n            # 训练矩阵[30,2]与权值矩阵[2,25]相乘\n            train_Y = train_X.dot(self.W)  # train_Y 为 [30,25]\n            # 这里的相似度判别使用的是余弦法，方向越接近，值越接近1\n            winner = np.argmax(train_Y, axis=1).tolist()  # 找到每行中最大元素下标\n            # 更新权值矩阵\n            self.updata_W(train_X, count, winner)\n            count += 1\n        return self.W\n\n    def train_result(self):\n        normal_X(self.X)  # 归一化数据矩阵\n        train_Y = self.X.dot(self.W)  # 输出矩阵为 数据矩阵与权值矩阵叉乘所得\n        # train_Y 为 [30,25]\n        winner = np.argmax(train_Y, axis=1).tolist()  # 在30行中找到每行最大的元素\n        print(winner)\n        return winner\n\n\ndef normal_X(X):\n    \"\"\"\n    :param X:二维矩阵，N*D，N个D维的数据\n    :return: 将X归一化的结果\n    \"\"\"\n    N, D = X.shape\n    for i in range(N):\n        temp = np.sum(np.multiply(X[i], X[i]))\n        X[i] /= np.sqrt(temp)\n    return X\n\n\ndef normal_W(W):\n    \"\"\"\n    :param W:二维矩阵，D*(n*m)，D个n*m维的数据\n    :return: 将W归一化的结果\n    \"\"\"\n    for i in range(W.shape[1]):\n        temp = np.sum(np.multiply(W[:, i], W[:, i]))\n        W[:, i] /= np.sqrt(temp)\n    return W\n\n# 画图\n\n\ndef draw(C):\n    colValue = ['r', 'y', 'g', 'b', 'c', 'k', 'm']\n    for i in range(len(C)):\n        coo_X = []  # x坐标列表\n        coo_Y = []  # y坐标列表\n        for j in range(len(C[i])):\n            coo_X.append(C[i][j][0])\n            coo_Y.append(C[i][j][1])\n        pl.scatter(coo_X, coo_Y, marker='x',\n                   color=colValue[i % len(colValue)], label=i)\n\n    pl.legend(loc='upper right')\n    pl.show()\n\n\n# 数据集：每三个是一组分别是西瓜的编号，密度，含糖量\ndata = \"\"\"\n1,0.697,0.46,2,0.774,0.376,3,0.634,0.264,4,0.608,0.318,5,0.556,0.215,\n6,0.403,0.237,7,0.481,0.149,8,0.437,0.211,9,0.666,0.091,10,0.243,0.267,\n11,0.245,0.057,12,0.343,0.099,13,0.639,0.161,14,0.657,0.198,15,0.36,0.37,\n16,0.593,0.042,17,0.719,0.103,18,0.359,0.188,19,0.339,0.241,20,0.282,0.257,\n21,0.748,0.232,22,0.714,0.346,23,0.483,0.312,24,0.478,0.437,25,0.525,0.369,\n26,0.751,0.489,27,0.532,0.472,28,0.473,0.376,29,0.725,0.445,30,0.446,0.459\"\"\"\n\na = data.split(',')\ndataset = np.mat([[float(a[i]), float(a[i+1])] for i in range(1, len(a)-1, 3)])\ndataset_old = dataset.copy()\n\nsom = SOM(dataset, (5, 5), 1, 30)\nsom.train()\nres = som.train_result()  # 返回winner节点的index\nclassify = {}  # 分类\nfor i, win in enumerate(res):\n    # winner 的index作为类别\n    if not classify.get(win[0]):\n        # 不存在字典中则添加 win[0]作为key\n        classify.setdefault(win[0], [i])\n    else:\n        # 存在则继续append\n        classify[win[0]].append(i)\nC = []  # 未归一化的数据分类结果\nD = []  # 归一化的数据分类结果 \nfor i in classify.values():\n    C.append(dataset_old[i].tolist())\n    D.append(dataset[i].tolist())\ndraw(C) \ndraw(D) # 归一化到一个半径为1的圆上"
  },
  {
    "objectID": "posts/searchtree.html",
    "href": "posts/searchtree.html",
    "title": "二叉搜索树",
    "section": "",
    "text": "这几天搬寝室烦的一批，都没时间写代码，很烦。明天出去玩了，今天赶紧把这个写完。\n\n\n程序\n/*\n * @Author: Zheng Qihang\n * @Date: 2018-07-14 21:29:18\n * @Last Modified by: Zheng Qihang\n * @Last Modified time: 2018-11-08 16:38:15\n */\n/*\n * @Author: Zheng Qihang\n * @Date: 2018-07-10 09:38:39\n * @Last Modified by: Zheng Qihang\n * @Last Modified time: 2018-07-13 14:38:44\n */\n#include &lt;limits.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;time.h&gt;\n\n/*\n        data\n       /    \\\n      V      V\n    Left    Right\n*/\ntypedef int ElementType;\ntypedef struct TreeNode {\n    ElementType Data;\n    struct TreeNode *Left;\n    struct TreeNode *Right;\n} * BinTree_t;\n\n/************************* Queue define! **************************/\n#define ElementQueueType BinTree_t\ntypedef struct QueueNode {\n    ElementQueueType data;\n    struct QueueNode *next;\n} * QNode_t;\n\ntypedef struct QueueHeader {\n    QNode_t front;\n    QNode_t rear;\n} * Queue_t;\n\n/**\n * description  Create the Queue!\n * @param[in]   void\n * @retval      no\n **/\nQueue_t CreateQueue(void) {\n    Queue_t Q = (Queue_t)malloc(sizeof(struct QueueHeader));\n    Q-&gt;front = NULL;\n    Q-&gt;rear = NULL;\n    return Q;\n}\n\n/**\n * description  Add Queue Node !\n * @param[in]   Queue header and a item\n * @retval      void\n **/\nvoid AddQ(Queue_t Q, ElementQueueType item) {\n    if (!Q) {\n        return;\n    }\n    /* 取消此处为了便于绘图\n    if (!item) {\n        return;\n    }\n    */\n    QNode_t temp = (QNode_t)malloc(sizeof(struct QueueNode));\n\n    if (!item) { //添加此处为了便于绘图\n        temp-&gt;data = NULL;\n    } else {\n        temp-&gt;data = item;\n    }\n    temp-&gt;next = NULL;\n    // when the Queue is null\n    if (!Q-&gt;front && !Q-&gt;rear) {\n        Q-&gt;front = temp;\n        Q-&gt;rear = temp;\n    } else {\n        Q-&gt;rear-&gt;next = temp;\n        Q-&gt;rear = temp;\n    }\n}\n\n/**\n * description  check the queue is empty\n * @param[in]   queue\n * @retval      int\n **/\nint IsEmptyQ(Queue_t Q) { return !Q-&gt;front; }\n\n/**\n * description  Delete the Queue Node\n * @param[in]   Queue header\n * @retval      data\n **/\nElementQueueType DeleteQ(Queue_t Q) {\n    if (!Q) {\n        return NULL;\n    }\n    if (IsEmptyQ(Q)) {\n        return NULL;\n    }\n    QNode_t temp = Q-&gt;front;\n    ElementQueueType tempdata;\n    if (Q-&gt;front == Q-&gt;rear) {\n        Q-&gt;front = NULL;\n        Q-&gt;rear = NULL;\n    } else {\n        Q-&gt;front = Q-&gt;front-&gt;next;\n    }\n    tempdata = temp-&gt;data;\n    free(temp);\n    return tempdata;\n}\n\n/**\n * description  Delete the queue\n * @param[in]   Queue_t\n * @retval      void\n **/\nvoid DisposeQueue(Queue_t Q) {\n    while (!IsEmptyQ(Q)) {\n        DeleteQ(Q);\n    }\n    free(Q);\n}\n\nvoid PrintQueue(Queue_t Q) {\n\n    if (IsEmptyQ(Q)) {\n        return;\n    }\n    printf(\"打印队列数据元素：\\n\");\n    QNode_t temp = Q-&gt;front;\n\n    for (; temp; temp = temp-&gt;next) {\n        printf(\"0x%3lX   \", (unsigned long int)temp-&gt;data & 0xFFF);\n    }\n    printf(\"\\n\");\n}\n\n/************************* Queue define! **************************/\n\n/************************* Stack define! **************************/\n\n#define ElementStackType BinTree_t\ntypedef struct StackNode // node defination\n{\n    ElementStackType data;\n    struct StackNode *next;\n} * Stack_t; // Node is a pointer to _Node\n\nint IsEmpty(Stack_t S) { return !S-&gt;next; }\n\nStack_t CreateStack(void) {\n    Stack_t S = (Stack_t)malloc(sizeof(struct StackNode));\n    S-&gt;next = NULL;\n    return S;\n}\n\nElementStackType Pop(Stack_t S) {\n    ElementStackType temp;\n    Stack_t tmepNode = S-&gt;next;\n    if (tmepNode != NULL) {\n        temp = tmepNode-&gt;data;\n        S-&gt;next = tmepNode-&gt;next;\n        free(tmepNode);\n        return temp;\n    } else {\n        return NULL;\n    }\n}\n\nvoid MakeEmpty(Stack_t S) {\n    while (S-&gt;next != NULL) {\n        Pop(S);\n    }\n}\nvoid DisposeStack(Stack_t S) {\n    MakeEmpty(S);\n    free(S);\n}\n\nvoid Push(Stack_t S, ElementStackType X) {\n    Stack_t tempNode = S-&gt;next;\n    Stack_t newNode = (Stack_t)malloc(sizeof(struct StackNode));\n    newNode-&gt;data = X;\n    // printf(\"S-&gt;next %p\\n\",S-&gt;next);\n    if (tempNode != NULL) {\n        newNode-&gt;next = tempNode;\n        // printf(\"%p = %p\\n\",newNode-&gt;next,tempNode);\n    } else {\n        newNode-&gt;next = NULL;\n    }\n    S-&gt;next = newNode;\n    // printf(\"S-&gt;next %p\\n\",S-&gt;next);\n}\n\nElementStackType Top(Stack_t S) {\n    if (S-&gt;next != NULL) {\n        return S-&gt;next-&gt;data;\n    } else {\n        return NULL;\n    }\n}\n\nvoid PrintStack(Stack_t S) {\n    Stack_t tep = S-&gt;next;\n    while (tep != NULL) {\n        printf(\"addr=0x%3lX  dataadd=0x%3lX  nextaddr=0x%3lX\\n\",\n               (unsigned long int)tep & 0xFFF,\n               (unsigned long int)tep-&gt;data & 0xFFF,\n               (unsigned long int)tep-&gt;next & 0xFFF);\n        tep = tep-&gt;next;\n    }\n}\n\n/************************* Stack define! **************************/\n\n/**\n * description  create the binary tree node\n * @param[in]   data\n * @retval      bintree_t\n **/\nBinTree_t CreateTreeNode(int dat) {\n    BinTree_t new = (BinTree_t)malloc(sizeof(struct TreeNode));\n    new-&gt;Data = dat;\n    new-&gt;Left = NULL;\n    new-&gt;Right = NULL;\n    return new;\n}\n\nvoid ConnectNode(BinTree_t root, BinTree_t left, BinTree_t right) {\n\n    if (!root) {\n        printf(\"error in null\\n\");\n        return;\n    }\n    root-&gt;Left = left;\n    root-&gt;Right = right;\n}\n\n/**\n * description  层序遍历二叉树\n * @param[in]   BinTree_t\n * @retval      void\n **/\nvoid LevelOrderTraversal(BinTree_t BT) {\n    Queue_t myqueue = CreateQueue();\n    BinTree_t temp = NULL;\n    AddQ(myqueue, BT); //将头节点入队\n    while (!IsEmptyQ(myqueue)) {\n        temp = DeleteQ(myqueue); //将上一层节点出队\n        if (temp != NULL) {\n            printf(\"%d   \", temp-&gt;Data);\n            AddQ(myqueue, temp-&gt;Left); //再将上层节点左右子节点入队\n            AddQ(myqueue, temp-&gt;Right);\n        }\n    }\n    DisposeQueue(myqueue);\n}\n\n/**\n * description  输出二叉树的高度\n * @param[in]   BinTree_t\n * @retval      int\n **/\nint FindTreeHeight(BinTree_t BT) {\n    int rightlen, leftlen, maxlen;\n    if (BT) {\n        rightlen = FindTreeHeight(BT-&gt;Right);\n        leftlen = FindTreeHeight(BT-&gt;Left);\n        maxlen = leftlen &gt; rightlen ? leftlen : rightlen;\n        return maxlen + 1;\n    } else {\n        return 0;\n    }\n}\n\n/**\n * description  绘制二叉树图像\n * @param[in]   BinTree_t\n * @retval      void\n **/\nvoid DrawTree(BinTree_t BT) {\n    int height = FindTreeHeight(BT);\n    int cnt = 0;\n\n    /* 开始层序遍历二叉树并且绘制图形 */\n    Queue_t myqueue = CreateQueue();\n    BinTree_t temp = NULL;\n    int width = 0;\n    AddQ(myqueue, BT); //将头节点入队\n    /*现在修改了入队函数,空指针也可以入队\n    所以在出队的时候就需要进行判断  */\n    for (int i = 0; i &lt; height; i++) {\n        cnt = (int)pow(2, i);           //当前行的个数21\n        width = pow(2, height - i + 1); //设置宽度为2^(height-i+1)\n        // printf(\"width:%d\\n\",width);\n        while (cnt--) {\n            temp = DeleteQ(myqueue); //将上一层节点出队\n            if (temp != NULL) {\n                printf(\"%*d%*c\", width, temp-&gt;Data, width, ' '); //输出数据\n                AddQ(myqueue, temp-&gt;Left); //再将上层节点左右子节点入队\n                AddQ(myqueue, temp-&gt;Right);\n            } else {\n                printf(\"%p   \", temp);\n            }\n        }\n        printf(\"\\n\");\n\n        if (i != height - 1) {\n            /* 先记录下一行元素个数 */\n            int nextwidth = (int)pow(2, height - i);\n            /* 如果不是最后一行那么按位置打印'+' */\n            for (int J = 0; J &lt; (int)pow(2, i); J++) {\n                printf(\"%*c%*c\", width, '+', width, ' ');\n            }\n            printf(\"\\n\");\n            /* 并且以他下一层元素的个数打印'-' */\n            for (int k = 0; k &lt; (int)pow(2, i + 1); k++) {\n                /* 每隔一位去打印width个'-' */\n                if ((k % 2) == 0) {\n                    printf(\"%*c\", nextwidth, '-');\n                    for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                        printf(\"-\");\n                    }\n                } else {\n                    for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                        printf(\"-\");\n                    }\n                    printf(\"%*c\", nextwidth, ' ');\n                }\n            }\n            printf(\"\\n\");\n            /* 再继续向下按位置打印'|' */\n            // for (int J = 0; J &lt; (int)pow(2, i + 1); J++) {\n            //     printf(\"%*c%*c\", nextwidth, '+', nextwidth, ' ');\n            // }\n            // printf(\"\\n\");\n        }\n        printf(\"\\r\");\n    }\n    DisposeQueue(myqueue);\n}\n\n/**\n * description  在二叉搜索树中寻找元素\n * @param[in]   BinTree_t DATA\n * @retval      BinTree_t\n **/\nBinTree_t FindElement(BinTree_t BST, ElementType Dat) {\n\n    BinTree_t temp = BST;\n    if (BST == NULL) {\n        return NULL;\n    }\n    while (temp) {\n        if (Dat &gt; temp-&gt;Data) {\n            temp = temp-&gt;Right;\n        } else if (Dat &lt; temp-&gt;Data) {\n            temp = temp-&gt;Left;\n        } else {\n            return temp;\n        }\n    }\n    return NULL;\n}\n\n/**\n * description  寻找最大的元素\n * @param[in]   BinTree_t\n * @retval      BinTree_t\n **/\nBinTree_t FindMax(BinTree_t BST) {\n\n    if (BST-&gt;Right == NULL) {\n        return BST;\n    }\n    return FindMax(BST-&gt;Right);\n}\n\n/**\n * description  寻找最小的元素\n * @param[in]   BinTree_t\n * @retval      BinTeww\n **/\nBinTree_t FindMin(BinTree_t BST) {\n    BinTree_t tmp = BST;\n    while (tmp-&gt;Left) {\n        tmp = tmp-&gt;Left;\n    }\n    return tmp;\n}\n\n/**\n * @brief  用于删除二叉树中的某个元素\n * @param[in]   BST Dat\n * @param[out]  void\n * @return      void\n **/\nBinTree_t DeleteElementBST(BinTree_t BST, ElementType Dat) {\n    BinTree_t tmpNode;\n    if (BST == NULL) {\n        return NULL;\n    } else if (Dat &lt; BST-&gt;Data) {\n        BST-&gt;Left = DeleteElementBST(BST-&gt;Left, Dat);\n    } else if (Dat &gt; BST-&gt;Data) {\n        BST-&gt;Right = DeleteElementBST(BST-&gt;Right, Dat);\n    } else if (BST-&gt;Left && BST-&gt;Right) {\n        tmpNode = FindMin(BST-&gt;Right);\n        BST-&gt;Data = tmpNode-&gt;Data;\n        BST-&gt;Right = DeleteElementBST(BST-&gt;Right, BST-&gt;Data);\n    } else {\n        tmpNode = BST;\n\n        if (BST-&gt;Left == NULL) {\n            BST = BST-&gt;Right;\n        } else if (BST-&gt;Right == NULL) {\n            BST = BST-&gt;Left;\n        }\n        free(tmpNode);\n    }\n    return BST;\n}\n\n// BinTree_t DeleteElementBST(BinTree_t BST, ElementType Dat) {\n\n//     BinTree_t tempNode = BST;\n//     BinTree_t lastNode = BST;\n//     if (BST == NULL) {\n//         return NULL;\n//     }\n//     while (tempNode) {\n//         if (Dat &gt; tempNode-&gt;Data) {\n//             lastNode = tempNode; //记录值\n//             tempNode = tempNode-&gt;Right;\n//         } else if (Dat &lt; tempNode-&gt;Data) {\n//             lastNode = tempNode; //记录值\n//             tempNode = tempNode-&gt;Left;\n//         } else if (Dat == tempNode-&gt;Data) { //寻找到了该节点\n//             /* 有两个子节点 替换右子树中最小值并且删除那个节点 */\n//             if (tempNode-&gt;Right && tempNode-&gt;Left) {\n//                 BinTree_t minNode = FindMin(tempNode-&gt;Right);\n//                 tempNode-&gt;Data = minNode-&gt;Data;\n//                 tempNode-&gt;Right =\n//                     DeleteElementBST(tempNode-&gt;Right, minNode-&gt;Data);\n//             } else { /* 有一个或没有子节点 */\n\n//                 /* 如果左边为空 则指向右子节点 */\n//                 if (tempNode-&gt;Left == NULL) {\n//                     lastNode-&gt;Right = tempNode-&gt;Right;\n//                 } else if (tempNode-&gt;Right == NULL) {\n//                     lastNode-&gt;Left = tempNode-&gt;Left;\n//                 }\n\n//                 if (lastNode == tempNode) {\n//                     /* 防止只有一个节点并递归时发生错误 */\n//                     free(tempNode);\n//                     lastNode = tempNode = NULL;\n//                 } else {\n//                     free(tempNode);\n//                 }\n\n//                 printf(\"lastNode=%p\\n\", lastNode);\n//                 printf(\"tempNode=%p\\n\", tempNode);\n//                 return lastNode;\n//             }\n//         }\n//     }\n//     return NULL;\n// }\n\n/**\n * description  在二叉搜索树中插入一个节点\n * @param[in]   数据\n * @retval      void\n **/\nvoid InsertNode(ElementType X, BinTree_t BST) {\n    if (BST == NULL) {\n        return;\n    }\n    BinTree_t newNode = (BinTree_t)malloc(sizeof(struct TreeNode));\n    BinTree_t temp = BST;\n    BinTree_t last = BST;\n    newNode-&gt;Data = X;\n    newNode-&gt;Left = newNode-&gt;Right = NULL;\n    while (temp) {\n        last = temp; //记录上一次的位置\n        if (X &gt; temp-&gt;Data) {\n            temp = temp-&gt;Right;\n        } else if (X &lt; temp-&gt;Data) {\n            temp = temp-&gt;Left;\n        } else if (X == temp-&gt;Data) {\n            printf(\"已经存在相同元素\");\n            return;\n        }\n    }\n\n    if (X &gt; last-&gt;Data) { //判断插入左右节点\n        last-&gt;Right = newNode;\n    } else {\n        last-&gt;Left = newNode;\n    }\n}\n\n/**\n * @brief  打印使用说明\n * @param[in]   void\n * @param[out]  void\n * @return      void\n **/\nvoid UseageHelp(void) {\n    printf(\"\\n \\\n    搜索树的操作\\n \\\n    (1).创建新的搜索树;\\n \\\n    (2).输出树高度;\\n \\\n    (3).层序遍历;\\n \\\n    (4).绘制二叉树;\\n \\\n    (5).寻找某元素:\\n \\\n    (6).递归寻找最大值:\\n \\\n    (7).循环寻找最小值:\\n \\\n    (8).插入一个元素:\\n \\\n    (9).删除一个元素:\\n \\\n    (h).帮助;\\n \\\n    (q).quit;\\n \\\n    \");\n}\n\n/**\n * description  创建一个我想要的树\n * @param[in]   void\n * @retval      BinTree_t\n **/\nBinTree_t CreateBinTree(void) {\n    printf(\"Create Binary Tree like this:        \\n\\\n                     90                          \\n\\\n                     |                           \\n\\\n         ------------------------                \\n\\\n         |                      |                \\n\\\n         50                     150              \\n\\\n         |                      |                \\n\\\n   -------------          -------------          \\n\\\n   |           |          |           |          \\n\\\n   20          75         95          175        \\n\\\n   |           |          |           |          \\n\\\n-------     -------    -------     -------       \\n\\\n|     |     |     |    |     |     |     |       \\n\\\n5     nil   nil   80   92    111   nil   nil     \\n\\\n    \");\n    BinTree_t A = CreateTreeNode(90);\n    BinTree_t B = CreateTreeNode(50);\n    BinTree_t C = CreateTreeNode(150);\n    BinTree_t D = CreateTreeNode(20);\n    BinTree_t E = CreateTreeNode(75);\n    BinTree_t F = CreateTreeNode(95);\n    BinTree_t G = CreateTreeNode(175);\n    BinTree_t H = CreateTreeNode(5);\n    BinTree_t I = CreateTreeNode(25);\n    BinTree_t J = CreateTreeNode(66);\n    BinTree_t K = CreateTreeNode(80);\n    BinTree_t L = CreateTreeNode(92);\n    BinTree_t M = CreateTreeNode(111);\n    ConnectNode(A, B, C);\n    ConnectNode(B, D, E);\n    ConnectNode(D, H, NULL);\n    ConnectNode(E, NULL, K);\n    ConnectNode(C, F, G);\n    ConnectNode(F, L, M);\n    ConnectNode(G, NULL, NULL);\n    return A;\n}\n\nBinTree_t BinaryTree = NULL;\nint main(int argc, char const *argv[]) {\n    BinTree_t tmp = NULL;\n    int empdat = 0;\n    UseageHelp();\n    while (1) {\n        switch (getchar()) {\n        case '1':\n            BinaryTree = CreateBinTree();\n            printf(\"创建成功!\\n\");\n            break;\n        case '2':\n            printf(\"输出树高度:%d\", FindTreeHeight(BinaryTree));\n            printf(\"\\n\");\n            break;\n        case '3':\n            printf(\"层序遍历:\");\n            LevelOrderTraversal(BinaryTree);\n            printf(\"\\n\");\n            break;\n        case '4':\n            printf(\"绘制二叉树:\\n\");\n            DrawTree(BinaryTree);\n            printf(\"\\n\");\n            break;\n        case '5':\n            printf(\"寻找某元素:\\n\");\n            printf(\"请输入元素值:\");\n            scanf(\"%d\", &empdat);\n            tmp = FindElement(BinaryTree, empdat);\n            if (!tmp) {\n                printf(\"not find the node\");\n            } else {\n                printf(\"find the node\");\n            }\n\n            printf(\"\\n\");\n            break;\n        case '6':\n            printf(\"递归寻找最大值:\\n\");\n            tmp = FindMax(BinaryTree);\n            printf(\"%d\", tmp-&gt;Data);\n            printf(\"\\n\");\n            break;\n        case '7':\n            printf(\"递归寻找小最值:\\n\");\n            tmp = FindMin(BinaryTree);\n            printf(\"%d\", tmp-&gt;Data);\n            printf(\"\\n\");\n            break;\n        case '8':\n            printf(\"插入一个元素:\\n\");\n            printf(\"请输入需要插入的值:\");\n            scanf(\"%d\", &empdat);\n            InsertNode(empdat, BinaryTree);\n            printf(\"\\n\");\n            break;\n        case '9':\n            printf(\"删除一个元素:\\n\");\n            printf(\"请输入需要删除的值:\");\n            scanf(\"%d\", &empdat);\n            DeleteElementBST(BinaryTree, empdat);\n            printf(\"\\n\");\n            break;\n\n        case 'h':\n            UseageHelp();\n            break;\n        case 'q':\n            exit(0);\n            break;\n        default:\n            break;\n        }\n    }\n    return 0;\n}\n\n\n运行结果\n我使用这个文本作为输入\n☁  serachtree  cat test\n1\n2\n3\n4\n5\n90\n6\n7\n8\n30\n4\n9\n90\n4\nq\n☁  serachtree  gcc searchtree.c -lm &&  cat test | ./a.out\n\n     搜索树的操作\n     (1).创建新的搜索树;\n     (2).输出树高度;\n     (3).层序遍历;\n     (4).绘制二叉树;\n     (5).寻找某元素:\n     (6).递归寻找最大值:\n     (7).循环寻找最小值:\n     (8).插入一个元素:\n     (9).删除一个元素:\n     (h).帮助;\n     (q).quit;\n     Create Binary Tree like this:\n                     90\n                     |\n         ------------------------\n         |                      |\n         50                     150\n         |                      |\n   -------------          -------------\n   |           |          |           |\n   20          75         95          175\n   |           |          |           |\n-------     -------    -------     -------\n|     |     |     |    |     |     |     |\n5     nil   nil   80   92    111   nil   nil\n    创建成功!\n输出树高度:4\n层序遍历:90   50   150   20   75   95   175   5   80   92   111\n绘制二叉树:\n                              90\n                               +\n               ---------------------------------\n              50                             150\n               +                               +\n       -----------------               -----------------\n      20              75              95             175\n       +               +               +               +\n   ---------       ---------       ---------       ---------\n   5    (nil)   (nil)     80      92     111    (nil)   (nil)\n\n寻找某元素:\n请输入元素值:find the node\n递归寻找最大值:\n175\n递归寻找小最值:\n5\n插入一个元素:\n请输入需要插入的值:\n绘制二叉树:\n                              90\n                               +\n               ---------------------------------\n              50                             150\n               +                               +\n       -----------------               -----------------\n      20              75              95             175\n       +               +               +               +\n   ---------       ---------       ---------       ---------\n   5      30    (nil)     80      92     111    (nil)   (nil)\n\n删除一个元素:\n请输入需要删除的值:\n绘制二叉树:\n                              92\n                               +\n               ---------------------------------\n              50                             150\n               +                               +\n       -----------------               -----------------\n      20              75              95             175\n       +               +               +               +\n   ---------       ---------       ---------       ---------\n   5      30    (nil)     80    (nil)    111    (nil)   (nil)"
  },
  {
    "objectID": "posts/roofline.html",
    "href": "posts/roofline.html",
    "title": "roofline Model",
    "section": "",
    "text": "学习一下roofline Model相关内容.\nNERSC中给出了很好的说明. 这里就记录一些我想知道的问题."
  },
  {
    "objectID": "posts/roofline.html#纵坐标performance",
    "href": "posts/roofline.html#纵坐标performance",
    "title": "roofline Model",
    "section": "1. 纵坐标Performance",
    "text": "1. 纵坐标Performance\nPerformance是硬件的理论算力, 每秒浮点运算(FLOPS, flops or flop/s)\n\\[\n\\begin{align}\n  FLOPS = \\frac{FLOPs}{second} * cores  \n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/roofline.html#横坐标arithmetic-intensity",
    "href": "posts/roofline.html#横坐标arithmetic-intensity",
    "title": "roofline Model",
    "section": "2. 横坐标Arithmetic Intensity",
    "text": "2. 横坐标Arithmetic Intensity\nArithmetic Intensity是算力除带宽\\(\\frac{FLOPS}{byte}\\)."
  },
  {
    "objectID": "posts/roofline.html#带宽线",
    "href": "posts/roofline.html#带宽线",
    "title": "roofline Model",
    "section": "3. 带宽线",
    "text": "3. 带宽线\n左侧的斜线为带宽线, 考虑到横坐标为算力除带宽, 那么也就是在纵坐标算力固定的情况下, 带宽越大, 横坐标越小, 也就是斜率越高."
  },
  {
    "objectID": "posts/roofline.html#绘制理论算力点",
    "href": "posts/roofline.html#绘制理论算力点",
    "title": "roofline Model",
    "section": "4. 绘制理论算力点",
    "text": "4. 绘制理论算力点\n\n确定任务的计算量和访存量,从而计算出任务的算术强度\n在roofline model上用一个点表示任务的算术强度和性能\n比较任务的性能和roofline model的曲线,判断任务是受限于计算还是受限于带宽\n如果任务的点在曲线下方,说明任务没有达到计算平台的最大性能,可以通过优化提高性能.如果任务的点在曲线上方,说明任务已经达到计算平台的最大性能,无法进一步提高性能."
  },
  {
    "objectID": "posts/roofline.html#绘制实际算力点",
    "href": "posts/roofline.html#绘制实际算力点",
    "title": "roofline Model",
    "section": "5. 绘制实际算力点",
    "text": "5. 绘制实际算力点\n如果你得到了任务的实际执行时间,你可以用它来计算任务的性能,然后在roofline model中绘制一个点表示任务的算术强度和性能. 具体步骤如下:\n\n假设你知道任务的计算量(浮点运算次数或整数运算次数)和访存量(内存交换字节数),以及任务的实际执行时间(秒)\n用计算量除以访存量,得到任务的算术强度\n用计算量除以执行时间,得到任务的实际性能\n参考4中的方法进行绘制"
  },
  {
    "objectID": "posts/roofline.html#存在多个级别的存储",
    "href": "posts/roofline.html#存在多个级别的存储",
    "title": "roofline Model",
    "section": "6. 存在多个级别的存储",
    "text": "6. 存在多个级别的存储\n对于每个任务统计他在各个级别上的访存量, 然后统计计算/内存移动并行时的最大时间作为理想时间.\n\\(T_{ideal} = max(M_{L1} / B_{L1},\\ M_{L2} / B_{L2},\\ FLOPs / FLOPs)\\) 其中B为带宽,M为访存量.\n这个时候就可以看到当前任务是被哪个级别的带宽限制住了,或者被哪个级别的算力限制住了."
  },
  {
    "objectID": "posts/roofline.html#macs-与-flops",
    "href": "posts/roofline.html#macs-与-flops",
    "title": "roofline Model",
    "section": "7. MACs 与 FLOPs",
    "text": "7. MACs 与 FLOPs\nMACs(Multiply–accumulate operations), FLOPs(floating operations). 如果目前的硬件是包含FMA(a &lt;- a + (b x c))指令的话, 那么在计算乘加的时候, MACs会比FLOPs小一倍."
  },
  {
    "objectID": "posts/raspberry-nb.html",
    "href": "posts/raspberry-nb.html",
    "title": "树莓派NB-IOT使用",
    "section": "",
    "text": "我的同学给了我一个nb-iot的小开发板 ,让我在树莓派上移植一个nb-iot的程序。\n\n\n1.配置交叉编译\n\n首先给树莓派烧系统。 看看编译器版本是什么：\npi@raspberrypi:~$ gcc -v\ngcc version 6.3.0 20170516 (Raspbian 6.3.0-18+rpi1+deb9u1)\n下载交叉编译器。\n百度一搜，去官方下载gcc-linaro-5.5.0-2017.10-x86_64_arm-linux-gnueabihf.tar.xz\n解压安装\n注意： 这里我mv是为了对这个文件夹改名字\n➜  Downloads sudo tar -xvf gcc-linaro-5.5.0-2017.10-x86_64_arm-linux-gnueabihf.tar.xz -C /opt\n➜  Downloads cd /opt/ \n➜  /opt sudo mv gcc-linaro-5.5.0-2017.10-x86_64_arm-linux-gnueabihf gccRaspPI\n➜  /opt cd gccRaspPI/bin \n设置环境变量\n注意： 我修改zshrc是因为我使用的是zsh，一般情况下使用的是bash\n➜  bin realpath .\n/opt/gccRaspPI/bin\n➜  bin sudo vi ~/.zshrc\n最后一行添加export  PATH=\"$PATH:/opt/gccRaspPI/bin\"\n➜  bin source ~/.zshrc           \n➜  bin arm-linux-gnueabihf-gcc -v\ngcc version 5.5.0 (Linaro GCC 5.5-2017.10) \n设置成功。\n\n\n\n2.编译程序\n先写个程序：\n#include &lt;stdio.h&gt;\nint main(int argc, char const *argv[]) {\n    printf(\"hello world\\n\");\n    return 0;\n}\n编译：\n➜  nb-proj arm-linux-gnueabihf-gcc main.c -o test\n\n\n传输程序\n利用sftp发送文件后：\npi@raspberrypi:~ $ ./test \nhello world\n执行成功。"
  },
  {
    "objectID": "posts/quant-1.html",
    "href": "posts/quant-1.html",
    "title": "神经网络量化-基本原理",
    "section": "",
    "text": "准备系统的学习一下神经网络量化，参考网络上的一些教程同时再次整理消化，这次首先对基本原理进行了解。\n参考资料： jermmyxu的神经网络量化系列教程"
  },
  {
    "objectID": "posts/quant-1.html#网络量化的基本原理",
    "href": "posts/quant-1.html#网络量化的基本原理",
    "title": "神经网络量化-基本原理",
    "section": "网络量化的基本原理",
    "text": "网络量化的基本原理\n\n背景知识\n我们通常会将一张 uint8 类型、数值范围在 \\(0\\sim255\\) 的图片归一成 float32 类型、数值范围在 \\(0.0\\sim1.0\\) 的张量，这个过程就是反量化。\n最简单的量化公式是min_max_map，假设使用这里我们用\\(r\\)表示浮点实数，\\(q\\)表示量化后的定点整数。浮点和整型之间的换算公式为： \\[q = round(\\frac{r}{S}+Z)\\] \\[r = S(q-Z)\\]\n其中，\\(S\\) 是 scale，表示实数和整数之间的比例关系，\\(Z\\) 是 zero point，表示实数中的 0 经过量化后对应的整数，它们的计算方法为： \\[S = \\frac{r_{max}-r_{min}}{q_{max}-q_{min}}\\] \\[Z = round(q_{max} - \\frac{r_{max}}{S})\\]\n首先我先找一个mobilenet，得到符合真实场景的参数均值和方差。\nmbv2 = mobilenet_v2(pretrained=True)\nconv2d: nn.Conv2d = mbv2.features[13].conv[0][0]\nR= conv2d.weight[:,:,0,0].detach().numpy()\nR.mean(), R.std()\n(0.00031194088, 0.055000253)\nr = np.random.normal(loc=0.00031194088,scale=0.055000253,size=10)\ndef calcScaleZeroPoint(min_val, max_val, num_bits=8):\n    qmin = 0.\n    qmax = 2. ** num_bits - 1.\n    scale = float((max_val - min_val) / (qmax - qmin))\n    zero_point = qmax - max_val / scale\n    if zero_point &lt; qmin:\n        zero_point = qmin\n    elif zero_point &gt; qmax:\n        zero_point = qmax\n    zero_point = int(zero_point)\n    return scale, zero_point\ndef quantFloat(r,scale, zero_point, num_bits=8):\n    return np.round(np.clip(r/scale + zero_point,0,2**num_bits-1)).astype('int')\nS,Z=calcScaleZeroPoint(r.min(),r.max())\nq=quantFloat(r,S,Z)\nprint('r:',r,'\\nS:',S,'\\nZ:',Z,'\\nq:',q)\nr: [ 0.025   0.0205 -0.0064 -0.0634  0.0538  0.0205  0.0099  0.0321 -0.0302\n -0.0709] \nS: 0.0004889321179676555 \nZ: 145 \nq: [196 187 132  15 255 187 165 211  83   0]\n可以发现反量化的时候将出现一定的误差：\ndef dequantUint8(q, scale, zero_point):\n    return scale * (q - zero_point)\nre_r=dequantUint8(q,S,Z)\nprint(r-re_r)\n[ 0.     -0.     -0.      0.0002 -0.     -0.0001  0.0002 -0.0002  0.0001\n -0.    ]\n\n\n矩阵运算的量化\n假设\\(r_1,r_2\\)是两个矩阵,他们的维度分别为\\(N\\times M,M\\times K\\),\\(r_3\\)作为两个矩阵的乘积结果,维度是\\(N \\times K\\),乘积计算过程如下:\n\\[\nr_3^{n,k}=\\sum_{m=1}^M r_1^{n,m}r_2^{m,k}\n\\]\n设\\(S_1,Z_1\\)对应\\(r_1\\)的量化因子,同理得到\\(S_2,Z_2\\)和\\(S_3,Z_3\\),这时候将上述矩阵相乘公式的量化计算过程如下: \\[\n\\begin{aligned}\nS_3(q_3^{n,k}-Z_3)&=\\sum_{m=1}^{M}S_1(q_{1}^{n,m}-Z_1)S_2(q_2^{m,k}-Z_2)\\\\\nS_3q_3^{n,k}&=S_1S_2 \\sum_{m=1}^{M}(q_{1}^{n,m}-Z_1)(q_2^{m,k}-Z_2)+S_3 Z_3\\\\\nq_3^{n,k}&=\\frac{S_1S_2}{S_3} \\sum_{m=1}^{M}(q_{1}^{n,m}-Z_1)(q_2^{m,k}-Z_2)+Z_3\n\\end{aligned}\n\\] 此时上述公式中只有\\(\\frac{S_1S_2}{S_3}\\)部分是浮点运算(并且这个浮点数被大量实验证明了位于0-1之间),假设未经过重新量化的矩阵计算结果为\\(Q\\),固定浮点数\\(\\frac{S_1S_2}{S_3}\\)定义为\\(F\\),那么对于一个浮点数可以利用一个技巧进行近似,然后可以将这个浮点计算转换为定点计算.即将这个浮点数用一个定点整数\\(F_0\\)*定点小数\\(2^{-n}\\)的方法来近似: \\[\n\\begin{aligned}\nq_3^{n,k}&=F Q+Z_3\\\\\n&=2^{-n}F_0 Q+Z_3\n\\end{aligned}\n\\]\ndef get_r_q_mat(h,w,loc=0.001,scale=0.004):\n    r = np.random.normal(loc=loc,scale=scale,size=(h,w))\n    s,z=calcScaleZeroPoint(r.min(),r.max())\n    q = quantFloat(r,s,z)\n    return r,q,s,z\nr_1,q_1,S_1,Z_1= get_r_q_mat(3,5)\nr_2,q_2,S_2,Z_2= get_r_q_mat(5,4)\nr_3 = r_1 @ r_2\nS_3,Z_3=calcScaleZeroPoint(r_3.min(),r_3.max())\nq_3 = quantFloat(r_3,S_3,Z_3)\nQ = (q_1-Z_1) @ (q_2-Z_2)\nF= (S_1*S_2)/S_3\nprint(F)\n0.008822082202930992\n此时我们计算\\(F\\)的定点数\\(F_0\\)和\\(2^{-n}\\),其实就是计算哪一个\\(F_0\\)和\\(n\\)近似\\(F\\)的误差最小: \\[\n\\begin{aligned}\n    \\text{argmin}_{n}&abs(FQ-(2^{-n} \\times F_0)Q,\\ \\  F_0 \\in [q_{min},q_{max}]\\\\\n    F_0 &= round(\\frac{F}{2^{-n}}) = round(F * (1 &lt;&lt; n)) \\\\\n    将F_0展开,并将2^{-n}转换为位运算&: \\\\\n    \\text{argmin}_{n}&abs(FQ-(round(F * (1 &lt;&lt; n))Q &gt;&gt; n))\n\\end{aligned}\n\\] 接下来给出一个确定\\(F_0\\)与\\(n\\)的代码:\ndef get_f_0(F,Q,is_print=True):\n    mind=1e9\n    minn=-1\n    for n in range(0,16):\n        F_0=int(round(F * (1&lt;&lt;n)))\n        diff = abs(F*Q - (int(F_0*Q)&gt;&gt;n))\n        if diff&lt;mind:\n            mind,minn=diff,n\n        if is_print:\n            print(f\"n={n},F_0={F_0},diff={diff}\")\n    return int(round(F * (1&lt;&lt;minn))),minn\nF_0,n = get_f_0(F,int(Q[0][0]))\nn=0,F_0=0,diff=175.27712920783293\nn=1,F_0=0,diff=175.27712920783293\nn=2,F_0=0,diff=175.27712920783293\nn=3,F_0=0,diff=175.27712920783293\nn=4,F_0=0,diff=175.27712920783293\nn=5,F_0=0,diff=175.27712920783293\nn=6,F_0=1,diff=135.72287079216707\nn=7,F_0=1,diff=19.277129207832928\nn=8,F_0=2,diff=19.277129207832928\nn=9,F_0=5,diff=19.722870792167072\nn=10,F_0=9,diff=0.27712920783292816\nn=11,F_0=18,diff=0.27712920783292816\nn=12,F_0=36,diff=0.27712920783292816\nn=13,F_0=72,diff=0.27712920783292816\nn=14,F_0=145,diff=0.7228707921670718\nn=15,F_0=289,diff=0.7228707921670718\n观察上述输出,可以发现在\\(n=10\\)的时候就可以得到较好的量化因子了,这样所有的运算就可以用定点的方式来计算了.\n此时我们写出一个完整的例子，分别使用\\(q_3\\)的两种解量化方式来检查量化运算的精度损失有多大：\n$$\n\\[\\begin{aligned}\n\\text{deq\\_r}_{3}^1 &= dequant(q_3) \\\\\n\n\\text{deq\\_r}_{3}^2 &= dequant(F Q + Z_3) \\\\\n\n\\text{deq\\_r}_{3}^3 &= dequant(2^{-n} F_0 Q + Z_3)\n\n\\end{aligned}\\]\n$$\ndeq_r_3_1=dequantUint8(q_3,S_3,Z_3)\ndeq_r_3_2=dequantUint8(F*Q+Z_3,S_3,Z_3)-r_3\ndeq_r_3_3=dequantUint8(np.right_shift(F_0*Q,n)+Z_3,S_3,Z_3)\nprint((deq_r_3_1-r_3).max())\nprint((deq_r_3_2-r_3).max())\nprint((deq_r_3_3-r_3).max())\n1.6667435495922793e-07\n4.1159863929325536e-05\n3.4088182772356416e-07\n可以发现精度损失并不大，表明了当前的方法有用。"
  },
  {
    "objectID": "posts/pytorch-feature-extract.html",
    "href": "posts/pytorch-feature-extract.html",
    "title": "pytorch从任意层截断并提取数据",
    "section": "",
    "text": "我想尝试利用预训练模型的各个层的特征进行重构并检查效果，但是对于任意的已经训练好的模型，我无法修改其forward流程，这个时候我们想到了利用hook函数。使用hook之后，我们可能需要提取中间层的输出，但模型还是运行所有，造成了不必要的时间浪费，因此需要想一个办法在hook的同时对模型进行截断。\n\n\n解决方案\n幸好python与pytorch均具备强大的动态特性，我们可以利用异常处理达到想要的效果，如下demo代码所示： 1. 首先将原始预训练模型用新模型包裹，将forward流程封装为_forward_impl 2. 接下来获取子类对象的句柄，大家可以用model.named_children()，这里我自己魔改了一下，跳过了一些层。 3. 为对应层添加hook，并且抛出异常。 4. 覆盖模型forward函数，处理异常。\n可惜，魔改代码一时爽，适配起来就想哭。。想要一次性写出灵活性强的代码是真的难，现在还得回去把之前的所有预训练模型特征提取的代码都修改一下..\ndef dev_get_pretrained_model_name():\n  from networks.pretrainnet import Res18FaceLandmarkPreTrained, named_basic_children\n  import types\n  md = Res18FaceLandmarkPreTrained('models/facelandmark_full.pth')\n  md.setup('cpu')\n  named_basic = named_basic_children(md)\n\n  x = torch.rand(4, 3, 256, 256)\n  y = md(x)\n  print(y.shape)  # torch.Size([4, 5, 2])\n\n  # add hook\n  features = []\n\n  def hook(module: nn.Module, input: torch.Tensor):\n    features.append(input[0])\n    raise StopIteration\n  named_basic[5][1].register_forward_pre_hook(hook)\n  print(named_basic[5])\n  \"\"\" \n  ('BatchNorm2d-5', BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) \n  \"\"\"\n\n  def new_forward(self, x):\n    try:\n      y = self._forward_impl(x)\n    except StopIteration as e:\n      return features[0]\n    return y\n  md.forward = types.MethodType(new_forward, md)\n\n  y = md(x)\n  print(y.shape)  # torch.Size([4, 64, 64, 64])"
  },
  {
    "objectID": "posts/python-unpack.html",
    "href": "posts/python-unpack.html",
    "title": "python返回值进行unpack",
    "section": "",
    "text": "最近在写yolov3,因为yolov3的多输出性质,所以我打算写适配多输出的工具函数,在numpy中可以在一个array中包含多个不同维度的array,但在tensorflow中一个tensor只能保存相同维度的矩阵,这就十分蛋疼了.下面记录一下我是如何解决的.\n\n\n问题描述\n在做parser的时候,让其返回值第一个为img,然后是一个动态的label数组,接下来使用tensorflow的包装函数进行包装,最后执行:\ndef t():\n    img = np.zeros((240, 320, 3))\n    labels = [np.array((7, 10, 3, 25)), np.array((14, 20, 3, 25))]\n    return img, labels\n\nimg, *label = py_function(t, inp=[], Tout=[tf.float32] * 3)\n\nwith tf.Session() as sess:\n    _img = sess.run([img])\n但是这样执行会出现一个问题,我虽然给定的输出是3个,但是实际执行的时候函数返回值是2个:\npyfunc_10 returns 2 values, but expects to see 3 values.\n\n\n问题解决\n所以我需要进行返回时的解包,最终程序如下:\ndef t():\n    img = np.zeros((240, 320, 3))\n    labels = [np.array((7, 10, 3, 25)), np.array((14, 20, 3, 25))]\n    return (img, *labels)\n\nimg, *label = py_function(t, inp=[], Tout=[tf.float32] * 3)\n\nwith tf.Session() as sess:\n    _img, *_label = sess.run([img, *label])\n只要注意到一点,返回*list是可以的,但是必须要加括号保证语法!"
  },
  {
    "objectID": "posts/py-format.html",
    "href": "posts/py-format.html",
    "title": "Python格式化配置",
    "section": "",
    "text": "之前一直用autopep8作为格式化公式，后来发现不能设置缩进两个空格就换成了yapf。但yapf实在是一言难尽，我不太喜欢这种整个文档都帮你格式化的，autopep8这样可以只考虑每一行的内部的格式化就足够了，可以给程序员更多的调整空间。\n\n今天本来想说哪怕autopep8不支持缩进两个空格也要给他加上去，但是找了一番发现他虽然帮助文档里面没有写，但是实际上指定了缩进参数也是没问题的。所以给出一个vscode的配置：\n\"python.formatting.autopep8Args\": [\n    \"--indent-size=2\",\n    \"-j=2\",\n    \"--max-line-length=100\",\n],\n\"python.formatting.provider\": \"autopep8\","
  },
  {
    "objectID": "posts/proxy-anchor.html",
    "href": "posts/proxy-anchor.html",
    "title": "Proxy Anchor Loss for Deep Metric Learning论文解读",
    "section": "",
    "text": "这是CVPR2020的一篇度量学习的论文.来自韩国.我觉得还蛮有意思的,因此学习一番.\n他的图画的是真好看,虽然做的事情其实就是把amsoftmax换个皮…"
  },
  {
    "objectID": "posts/proxy-anchor.html#额外实验1",
    "href": "posts/proxy-anchor.html#额外实验1",
    "title": "Proxy Anchor Loss for Deep Metric Learning论文解读",
    "section": "额外实验1",
    "text": "额外实验1\n我做的cifar10分类实现，实际上batchsize改成500之后\\(|P^+|\\)有99.99的概率为10，\\(|P|\\)为10。我训练结果只和circle loss差0.8个点。然后我认为删除这个系数没有关系，但结果立马打脸了，差7个点了。其实这个问题还是在于\\(\\alpha\\)系数上面，对于proxy anchor loss和amsoftmax loss来说没有像circle loss中的自适应pace，优化到后期可以说学习率太大也可以说是梯度太大，模型参数波动性会较大。这个实验佐证了自适应pace的重要性。"
  },
  {
    "objectID": "posts/proxy-anchor.html#额外实验2",
    "href": "posts/proxy-anchor.html#额外实验2",
    "title": "Proxy Anchor Loss for Deep Metric Learning论文解读",
    "section": "额外实验2",
    "text": "额外实验2\n后面我又做了一下关于\\(margin\\)的实验，实际上我们考虑softmax分类的过程，要使他分类难度增强应该是降低正确类别的数值，因此应该是\\(s_p=cos(\\theta_{y_i})-m\\)，这样才会强制正确类别的向量夹角更小。那么对于不正确的的分类类别，应该是加大他的数值，\\(s_n=cos(\\theta_{j\\neq y_i})+m\\),这样会强制降低向量更加接近垂直。\n这里留个小坑，对于circle loss里面的\\(margin\\)设置我还是得重新仔细看看。\nNOTE 2020-6-23日更新，因为circle loss设置了自定义pace，因此他计算决策面的时候将\\(\\alpha\\)考虑进去了，所以他得到的\\(margin\\)和我之前设想的不一样。然后我又做了一下他的间距分布图，其实这个损失还是可以继续改进的，大家可以看到当\\(\\cos(\\theta_p)=0，\\cos(\\theta_n)\\in(\\pi,\\frac{\\pi}{2})\\)时，对于负pair的损失是较小的。他所说的circle区域实际上是在\\(\\cos(\\theta_p)=0，\\cos(\\theta_n)=\\frac{\\pi}{2}\\)有一个更小的凹槽，这里是我们的ideal区域。\n接下来如果有老哥可以把\\(\\cos(\\theta_p)=0，\\cos(\\theta_n)\\in(\\pi,\\frac{\\pi}{2})\\)这块区域的损失重新设计一下，应该可以得到更好的收敛效果。\n等等。。这样判断太武断了，应该还需要分析一下梯度的变化。我这里就不继续深入了。"
  },
  {
    "objectID": "posts/precision-analysis.html",
    "href": "posts/precision-analysis.html",
    "title": "yolo中precision降低的原因分析",
    "section": "",
    "text": "最近在训练yolo模型一类检测模型,我在训练过程中发现precision会降低,思考之后对其做出一些分析.\n\n\n起因\n我在训练yolo的时候,首先是进行分类检测,即只训练class_loss,obj_loss,noobj_loss,这个时候训练precision会达到60%.\n接下来进行第二次迭代,这次同时训练class_loss,obj_loss,noobj_loss,xy_loss,wh_loss,这个时候precision会降低到40%.\n如下图所示: \n\n\n原因分析\n经过思考,我找到问题所在:\n\"\"\" calc the noobj mask ~ \"\"\"\nif train_classifier == 'True':\n    noobj_mask = tf.logical_not(obj_mask)\nelse:\n    noobj_mask = calc_noobj_mask(true_xy, true_wh, pred_xy, pred_wh, obj_mask, iou_thresh=iou_thresh, helper=helper)\n这段代码就是当我开始训练xy_loss,wh_loss时,那么noobj_mask就需要通过预测出来的box与ground truth box计算iou来筛选.\n当我可以拟合box之后,那么也就是说如果其他的cell预测出来的box与ground truth box的iou大于iou_threshold之后,那么这个cell我就不会将他设置为noobj cell,同时也不会对这个cell进行惩罚措施. \n再看precision和recall的计算方式: \\[\n\\begin{aligned}\nprecision&=\\frac{true\\ positive}{true\\ positive+false\\ positive} \\\\\nrecall&=\\frac{true\\ positive}{true\\ positive+false\\ negative}\n\\end{aligned}\n\\]\n因为我没有对预测box与true box的iou大于iou_threshold进行惩罚,所以我们的\\(false\\ positive\\)会增加,\\(false\\ negative\\)会减少.最终导致precision减少,recall增加."
  },
  {
    "objectID": "posts/polyherdal-playground.html",
    "href": "posts/polyherdal-playground.html",
    "title": "Polyhedral Tutorials",
    "section": "",
    "text": "关于Polyhedral Tutorials的一个中文翻译归档,其中所有章节原文位于我的仓库中."
  },
  {
    "objectID": "posts/polyherdal-playground.html#basic-set",
    "href": "posts/polyherdal-playground.html#basic-set",
    "title": "Polyhedral Tutorials",
    "section": "Basic Set",
    "text": "Basic Set\nBasic Set 是 Presburger Set的最简单形式,仅允许描述单个凸（但可能是稀疏）集的 Presburger 公式.\n集合的space被tuple的维度定义.一个集合包含pair被称为2维space.\n\nExamples\n以下两个基本集合 Triangle 和 Square :\n\nTriangle: \\((\\{A[i,j] \\mid 0 &lt; i &lt; j &lt; 10\\})\\)\nSquare: \\((\\{ A[i,j] \\mid 5 &lt; i &lt; 10 \\land 0 &lt; j &lt; 5 \\})\\)\n\nTriangle = isl.BasicSet(\"{A[i,j] : 0 &lt; i &lt; j &lt; 10}\")\nSquare = isl.BasicSet(\"{A[i,j] : 5 &lt; i &lt; 10 and 0 &lt; j &lt; 5}\")\nplot_set_points(Triangle,color='blue')\nplot_set_points(Square,color='orange')"
  },
  {
    "objectID": "posts/polyherdal-playground.html#sparse-basic-sets",
    "href": "posts/polyherdal-playground.html#sparse-basic-sets",
    "title": "Polyhedral Tutorials",
    "section": "Sparse Basic Sets",
    "text": "Sparse Basic Sets\n这个例子中展示了集合中存在取模约束,这样集合元素会变成稀疏的.\n\nExample\nbasic set Sparse 是排除某些对角线的正方形.\nSparse = isl.BasicSet(\"{A[i,j] : 0 &lt; i,j &lt; 10 and (i + j) % 3 != 0}\")\nplot_set_points(Sparse,color='blue')"
  },
  {
    "objectID": "posts/polyherdal-playground.html#exercises",
    "href": "posts/polyherdal-playground.html#exercises",
    "title": "Polyhedral Tutorials",
    "section": "Exercises",
    "text": "Exercises\n\nPlot a set UpperTriangle with a base of width 7.\n\n       x\n     x x x\n   x x x x x\n x x x x x x x\ns = isl.BasicSet(\"{A[x,y] : 0 &lt;= x &lt; 7 and 0 &lt;= y &lt; 4 and y &lt;= x &lt; 7-y }\")\nplot_set_points(s,color='blue')\n\n\nPlot a set LowerTriangle with a base of width 7\n\nx x x x x x x\n  x x x x x\n    x x x\n      x\ns = isl.BasicSet(\"{A[x,y] : 0 &lt;= x &lt; 7 and -4 &lt; y &lt;= 0 and  (-y) &lt;= x &lt; 7+y }\")\nplot_set_points(s,color='blue')\n\n\nPlot a set Diamond with a width and height of 7.\n\n       x\n     x x x\n   x x x x x\n x x x x x x x\n   x x x x x\n     x x x\n       x\ns = isl.BasicSet(\"{A[x,y] : 0 &lt;= x &lt; 7 and -4 &lt; y &lt; 4 and (-y) &lt;= x &lt; 7+y and y &lt;= x &lt; 7-y }\")\nplot_set_points(s,color='blue')\n\n\nPlot a set Parallelogram with a height of 4 and a width of 7 with a slope of 1/2.\n\n      x x x x x x x\n    x x x x x x x  \n  x x x x x x x\nx x x x x x x\ns = isl.BasicSet(\"{A[x,y] : 0 &lt;= y &lt; 4 and y &lt;= x &lt; y + 7 }\")\nplot_set_points(s,color='blue')"
  },
  {
    "objectID": "posts/polyherdal-playground.html#basic-map",
    "href": "posts/polyherdal-playground.html#basic-map",
    "title": "Polyhedral Tutorials",
    "section": "Basic Map",
    "text": "Basic Map\nbasic map关联了basic set.\n\nExample\n比如下面关联set A和position X.\nTranslate = \\((\\{A[i,j] \\rightarrow X[i+10,j+1]\\})\\)\nTo visualize this set, we use it to translate the earlier defined set Triangle. Without constraining Translate to Triangle the map is infinite and cannot be rendered.\nfrom islplot.plotter import *\nTranslate = isl.BasicMap(\"{A[i,j] -&gt; X[i+10,j+1]}\")\nb  = Triangle.apply(Translate)\nTranslate = Translate.intersect_domain(Triangle)\n\nplot_map(Translate)\nplot_set_points(Triangle,color='blue')\nplot_set_points(b,color='orange')"
  },
  {
    "objectID": "posts/polyherdal-playground.html#statement-instances",
    "href": "posts/polyherdal-playground.html#statement-instances",
    "title": "Polyhedral Tutorials",
    "section": "Statement instances",
    "text": "Statement instances\n考虑以下计算多项式乘积的代码片段 每个多项式都由其系数数组表示\nconst int N = 100;\ndouble X[N], Y[N], Z[2*N];\n\nfor (int i = 0; i &lt;= 2*N; ++i)\nS:  Z[i] = 0.;\nfor (int i = 0; i &lt;= N; ++i)\n    for (int j = 0; j &lt;= N; ++j)\nT:      Z[i + j] += A[i] * B[j];\n为其中一些statement添加label作为identifier.\nStatement S 初始化数组Z的元素,statement T计算它们. Statement S 被包含在循环中,他将会按以下顺序被执行 \\(((2\\mathtt{N} + 1))\\) 次:\n\nZ[0] = 0.; /* i = 0 */\nZ[1] = 0.; /* i = 1 */\n…\nZ[2*N] = 0. /* i = 2*N */;\n\n把循环中每个单独执行的statement称为 statement instances 这样每个instance可以通过语句label和封闭循环迭代器的值来标识, 例如：\n\n\\((\\mathtt{S}(0))\\)\n\\((\\mathtt{S}(1))\\)\n…\n\\((\\mathtt{S}(2 \\mathtt{N}))\\)\n\n如果一个statement包含在多个循环中, 其instance由所有迭代器的值按循环的顺序来标识, 比如statement T 会包含以下这些例子:\n\n\\((\\mathtt{T}(0,0))\\) for Z[0] += A[0] * B[0] /* i = 0, j = 0 */,\n\\((\\mathtt{T}(0,1))\\) for Z[1] += A[0] * B[1] /* i = 0, j = 1 */,\n…\n\\((\\mathtt{T}(\\mathtt{N},\\mathtt{N}))\\) for Z[2*N] += A[N] * B[N] /* i = N, j = N */."
  },
  {
    "objectID": "posts/polyherdal-playground.html#iteration-domain",
    "href": "posts/polyherdal-playground.html#iteration-domain",
    "title": "Polyhedral Tutorials",
    "section": "Iteration domain",
    "text": "Iteration domain\n一个statement的所有instance的集合被称为 (iteration) domain.\n迭代域可以使用 set-builder 表示法来表示:\n比如, \\(( \\mathcal{D}_\\mathtt{S} = \\{ \\mathtt{S}[i] : 0 \\leq i \\leq \\mathtt{N} \\}. )\\) 表达式 \\(( 0 \\leq i \\leq \\mathtt{N} )\\) 被循环起始(i=0)到结束(i&lt;=N)所约束.\n注意, 这里 \\(( \\mathtt{N} )\\) 被看作是符号常量. 在多面体模型中,这些符号常量通常被称为 (structure) parameters:\n\\(( \\mathcal{D}\\_\\mathtt{S} = [N] \\rightarrow \\{ \\mathtt{S}[i] : 0 \\leq i \\leq N \\} )\\), 本质上是将其转换为从参数值到domain set的具体instance的映射. 这样的参数集可以在 isl 中如下定义:\nimport islpy as isl\nD_S = isl.Set(\"[N] -&gt; {S[i] : 0 &lt;= i &lt;= N}\")\nprint(D_S)\n[N] -&gt; { S[i] : 0 &lt;= i &lt;= N }\n同样, 我们可以为statement T定义iteration domain, \\(( \\mathcal{D}_\\mathcal{T} = [N] \\rightarrow \\{ \\mathtt{T}(i,j) : 0 \\leq i,j \\leq N \\} )\\).\n这个domain被定义为一组二维向量, 向量的每个分量都以嵌套顺序对应于一个封闭循环.\n\nQuestion\n使用 isl 表示定义变量 D_T 使其包含 T 的迭代域, 然后打印它.\nD_T = isl.Set(\"[N] -&gt; { T[i,j] : 0 &lt;= i, j &lt;= N }\")\nprint(D_T)\n[N] -&gt; { T[i, j] : 0 &lt;= i &lt;= N and 0 &lt;= j &lt;= N }\n如上所示, isl 输出使用连词(logical and)来组合不同迭代器周围的不等式, 这可以很方便循环边界不同的情况.\n\n\nQuestion\n使用 and operator 分离 i 和 j的bounds来重新定义D_T .\nD_T = isl.Set(\" [N] -&gt; { T[i, j] : 0 &lt;= i &lt;= N and 0 &lt;= j &lt;= N }\")\nprint(D_T)\n[N] -&gt; { T[i, j] : 0 &lt;= i &lt;= N and 0 &lt;= j &lt;= N }\n注意, print的输出不一定再现输入的文本形式 相反, 它表示简化后的同一集合, 比如消除了多余的不等式, 并使用更简单的方程来表示出现在第一位的分量. 比如下面这个例子:\nprint(isl.Set(\"{[i,j]: i+j &gt;= 0 and i &gt;= 0 and j &gt; 0 and j &gt;= 1}\"))\n{ [i, j] : i &gt;= 0 and j &gt; 0 and j &gt;= -i }"
  },
  {
    "objectID": "posts/polyherdal-playground.html#handling-non-unit-strides",
    "href": "posts/polyherdal-playground.html#handling-non-unit-strides",
    "title": "Polyhedral Tutorials",
    "section": "Handling Non-Unit Strides",
    "text": "Handling Non-Unit Strides\n考虑以下代码片段, 它取数组中的每个奇数元素的负数.\nconst int N;\ndouble A[2*N];\n\nfor (int i = 1; i &lt; 2*N; i += 2)\nR:  A[i] = -A[i];\n如果数组A存储的是复数的实部和虚部, 那么上面的代码操作计算复共轭.\nR 的迭代域现在应该限制为 i 的奇数值, 这可以使用模运算符来实现:\n$( _: [N] { [i] : 0 i &lt; N i = 1 } $)\nD_R = isl.Set(\"[N] -&gt; {R[i]: (i mod 2) = 1 and 0 &lt;= i &lt; N}\")\nprint(D_R)\n[N] -&gt; { R[i] : (1 + i) mod 2 = 0 and 0 &lt;= i &lt; N }\nisl 将模运算转换为带底舍入的除法, 这种转换是模运算的两个属性的组合\n\\(( a \\mod b = c \\Leftrightarrow (a + c) \\mod b = 0 )\\),\n\\(( a \\mod b \\equiv a - b \\lfloor a/b \\rfloor )\\).\n\nQuestion\n在下面的代码中定义代表Q的迭代域的集合, 然后打印出来\nconst int N;\ndouble A[2*N];\n\nfor (int i = 1; i &lt; 2*N; i += 2)\nQ:  A[i] = -A[i];\nD_Q = isl.Set(\"[N] -&gt; {Q[i]: (i mod 2) = 1 and 0 &lt;= i &lt; N}\")\nprint(D_Q)\n[N] -&gt; { Q[i] : (1 + i) mod 2 = 0 and 0 &lt;= i &lt; N }"
  },
  {
    "objectID": "posts/polyherdal-playground.html#handling-conditions",
    "href": "posts/polyherdal-playground.html#handling-conditions",
    "title": "Polyhedral Tutorials",
    "section": "Handling Conditions",
    "text": "Handling Conditions\n循环内的条件构造也限制了它们所包含的语句的迭代域, 复共轭计算也可以使用单位步长循环内的分支语句来重写.\nconst int N;\ndouble A[2*N];\n\nfor (int i = 1; i &lt; 2*N; ++i)\n    if (i % 2 == 1)\nP:    A[i] = -A[i];\n迭代域的定义还应该包括语句周围的分支所施加的约束.\n\nQuestion\n定义代表P的迭代域的集合并打印它.\nD_P = isl.Set(\"[N] -&gt; {P[i]: (i mod 2) = 1 and 0 &lt;= i &lt; N}\")\nprint(D_P)\n[N] -&gt; { P[i] : (1 + i) mod 2 = 0 and 0 &lt;= i &lt; N }\n即使 P 和 R 的语句实例集是相同的, 这些域也会被认为是不同的因为是不同的statement name.\n\n\nQuestion\n如何修改D_P, 让他等价于D_R?\nD_P = isl.Set(\"[N] -&gt; {R[i]: i mod 2 = 1 and 0 &lt;= i &lt; N}\")\nprint(D_P.is_equal(D_R))\nTrue"
  },
  {
    "objectID": "posts/polyherdal-playground.html#iteration-domains-as-presburger-sets",
    "href": "posts/polyherdal-playground.html#iteration-domains-as-presburger-sets",
    "title": "Polyhedral Tutorials",
    "section": "Iteration Domains as Presburger Sets",
    "text": "Iteration Domains as Presburger Sets\n由于isl在Presburger Sets上运行, 因此它可以编码任何可以使用Presburger公式表示的迭代域, 这通常涉及由具有所谓的static control flow的循环和分支包围的语句.\n也就是说, loop bounds和分支条件是外部边界和参数的Presburger公式, 其中参数的值未知, 但在整个执行过程中必须保持不变.\n作为推论, 控制流不能依赖于被计算的value, 因此, 适合多面体建模的程序部件被称为static control parts或SCoPs.\n\nQuestion\n定义包含在两个循环和一个具有析取约束的分支中的statement的迭代域.\nHint: 如果有必要, 请使用运算符or和括号来确保优先级\nfor (int i = 0; i &lt; 10; ++i)\n  for (int j = 0; j &lt; 10; ++j)\n    if (i &lt; j - 1 || i &gt; j + 1)\n      Z[i][j] = 0.;\nD_Z = isl.Set(\" { Z[i,j] : 0 &lt;= i,j &lt; 10 and (i &lt; j - 1 or i &gt; j + 1) } \")\nprint(D_Z)\n{ Z[i, j] : (i &gt;= 0 and 2 + i &lt;= j &lt;= 9) or (i &lt;= 9 and 0 &lt;= j &lt;= -2 + i) }\n\n\n\n也可以使用Presburger公式表达某些常见的数学运算:\n\ni &gt;= max(a,b) \\(( \\Leftrightarrow i \\geq a \\wedge i \\geq b )\\) (lower bound only)\ni &lt;= min(a,b) \\(( \\Leftrightarrow i \\leq a \\wedge i \\leq b )\\) (upper bound only)\na = ceil(b/c) \\(( \\Leftrightarrow a = \\lfloor (b - 1)/c \\rfloor + 1 )\\)"
  },
  {
    "objectID": "posts/polyherdal-playground.html#putting-domains-together",
    "href": "posts/polyherdal-playground.html#putting-domains-together",
    "title": "Polyhedral Tutorials",
    "section": "Putting Domains Together",
    "text": "Putting Domains Together\n总之, statement的迭代域是一组受仿射表达式约束的多维向量, 仿射表达式出现在statement周围的循环边界和分支条件下.\n由于statement名称不同, 多个语句的迭代域存在于不同的Spaces中, 即使它们被相同的循环包围, 通过将它们放入Union set中可以统一操作, 比如组合domain:\nfor (int i = 0; i &lt; 10; ++i)\n  for (int j = 0; j &lt; 10; ++j) {\n    if (i &lt; j - 1)\nS1:   Z[i][j] = 0.;\n    if (i &gt; j + 1)\nS2:   Z[i][j] = 0.;\n  }\n被定义为: \\(( \\mathcal{D} =\n    \\{\\mathtt{S1}[i,j]: 0 \\leq i,j &lt; 10 \\wedge i &lt; j - 1 \\} \\cup\n    \\{\\mathtt{S2}[i,j]: 0 \\leq i,j &lt; 10 \\wedge i &gt; j + 1 \\},\\))\nisl表示如下:\nD = isl.UnionSet(\"{S1[i,j]: 0 &lt;= i,j &lt;= 10 and i &lt; j - 1; S2[i,j]: 0 &lt;= i,j &lt;= 10 and i &gt; j + 1}\")\nprint(D)\n{ S2[i, j] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and j &lt;= -2 + i; S1[i, j] : 0 &lt;= i &lt;= 10 and j &gt;= 2 + i and 0 &lt;= j &lt;= 10 }"
  },
  {
    "objectID": "posts/polyherdal-playground.html#plotting-iteration-domains",
    "href": "posts/polyherdal-playground.html#plotting-iteration-domains",
    "title": "Polyhedral Tutorials",
    "section": "Plotting Iteration Domains",
    "text": "Plotting Iteration Domains\n可以绘制1D/2D的非参数迭代域集合:\nfrom islplot.plotter import *\nplot_set_points(D,color='blue')\n\n可视化可以有效的检查domain的size或者独立的domain交集.\n这里例子中,domain完全不相交,意味他们可以被分离的循环所穿过.\n如果domain是参数化的,我们首先需要fix所有的参数到constant, 通过以下几种方法:\n\n创建一个参数集, 其中域值是固定的;\n将domain与这个新set相交;\n映射所有参数.\n\n下面是对第一个例子的T statement 的操作方法:\nfixer = isl.Set(\"[N] -&gt; {T[i,j]: N = 5}\")\nD_T = D_T.intersect(fixer)\nD_T = D_T.project_out(isl.dim_type.param, 0, 1)\nplot_set_points(D_T)\n\nisl.dim_type.param表示你想映射的参数, 后面两个数据分别是第一个参数的位置和将要映射的连续参数的数量.\n为了plot出来, 因此把所有维度都映射出来.注意, 如果忘记fix参数大小, 该集将变得unbounds, 无法plot.\n与参数类似, 在plot之前可以投影domain dimensions获得两个维度. 使用’isl.dim_type.set’来获取这些\n\nQuestion\n下面执行LU分解的代码:\nfor (i = 0; i &lt; N; i++) {\n  for (j = 0; j &lt; i; j++) {\n    for (k = 0; k &lt; j; k++) {\nSa:    A[i][j] -= A[i][k] * A[k][j];\n    }\nSb: A[i][j] /= A[j][j];\n  }\n  for (j = i; j &lt; N; j++) {\n    for (k = 0; k &lt; i; k++) {\nSc:   A[i][j] -= A[i][k] * A[k][j];\n    }\n  }\n}\n\n为所有的iteration domain定义union set.\n检查iteration domain Sa 和 Sc 是否有overlap\n绘制domain在(i,j)和(j,k)\n\nHint: 不能直接从union set中映射出domain dimension,因为他们可能存在不同的space中, 但是可以从set中做union得到他.\nD_Sa = isl.Set(\"[N] -&gt; { Sa[i,j,k] :0 &lt;= k &lt; j &lt; i &lt; N }\")\nD_Sb = isl.Set(\"[N] -&gt; { Sb[i,j] :0 &lt;= j &lt; i &lt; N }\")\nD_Sc = isl.Set(\"[N] -&gt; { Sc[i,j,k] :0 &lt;= k &lt; i &lt;= j &lt; N }\")\nD = isl.UnionSet(D_Sa).union(D_Sb).union(D_Sc)\nprint(D)\n[N] -&gt; { Sc[i, j, k] : i &lt;= j &lt; N and 0 &lt;= k &lt; i; Sa[i, j, k] : i &lt; N and j &lt; i and 0 &lt;= k &lt; j; Sb[i, j] : i &lt; N and 0 &lt;= j &lt; i }\n# NOTE 这里需要去掉标识名再做交集.\nD_Sa_ = isl.Set(\"[N] -&gt; { [i,j,k] :0 &lt;= k &lt; j &lt; i &lt; N }\")\nD_Sc_ = isl.Set(\"[N] -&gt; { [i,j,k] :0 &lt;= k &lt; i &lt;= j &lt; N }\")\nD_Sa_.intersect(D_Sc_).is_empty()\nTrue\nD_Sa = D_Sa.intersect(isl.Set(\"[N] -&gt; { Sa[i,j,k]: N = 8}\"))\nD_Sb = D_Sb.intersect(isl.Set(\"[N] -&gt; { Sb[i,j]: N = 8}\"))\nD_Sc = D_Sc.intersect(isl.Set(\"[N] -&gt; { Sc[i,j,k]: N = 8}\"))\nD_Sa = D_Sa.project_out(isl.dim_type.param, 0, 1)\nD_Sb = D_Sb.project_out(isl.dim_type.param, 0, 1)\nD_Sc = D_Sc.project_out(isl.dim_type.param, 0, 1)\nD_Sa_ = D_Sa.project_out(isl.dim_type.set, 2, 1)\nD_Sc_ = D_Sc.project_out(isl.dim_type.set, 2, 1)\nD_ij = isl.UnionSet(D_Sa_).union(D_Sb).union(D_Sc_)\nprint(D_ij)\nplot_set_points(D_ij)\n{ [i, j] : (i &lt;= 7 and 0 &lt; j &lt; i) or (i &gt; 0 and i &lt;= j &lt;= 7); Sb[i, j] : i &lt;= 7 and 0 &lt;= j &lt; i }\n\nD_Sa_ = D_Sa.project_out(isl.dim_type.set, 0, 1)\nD_Sc_ = D_Sc.project_out(isl.dim_type.set, 0, 1)\nD_jk = isl.UnionSet(D_Sa_).union(D_Sc_)\nprint(D_jk)\nplot_set_points(D_jk)\n{ [j, k] : j &lt;= 7 and 0 &lt;= k &lt; j }"
  },
  {
    "objectID": "posts/polyherdal-playground.html#definition",
    "href": "posts/polyherdal-playground.html#definition",
    "title": "Polyhedral Tutorials",
    "section": "Definition",
    "text": "Definition\nstatement的iteration domain给出了instances的执行信息,但是并没有指定执行顺序.\n实际上, 我们可以通过为每个statement instance分配逻辑执行顺序来指定分段 quasi-linear 的顺序.\n简而言之, 此schedule可以表示为statement instance和逻辑顺序之间的Presburger映射."
  },
  {
    "objectID": "posts/polyherdal-playground.html#identity-schedule",
    "href": "posts/polyherdal-playground.html#identity-schedule",
    "title": "Polyhedral Tutorials",
    "section": "Identity Schedule",
    "text": "Identity Schedule\n默认情况下, statement instances按照循环迭代顺序执行. 这可以使用identity schedule relation来表示.\n比如一个简单的循环初始化:\nfor (i = 0; i &lt; N; ++i)\nS:  A[i] = 0.0;\niteration domain:\n\\(( \\mathcal{D}\\_\\mathtt{S} = [N] \\rightarrow \\{ \\mathtt{S}(i) : 0 \\leq i &lt; N \\} )\\)\n对应的identity schedule:\n\\(( \\mathcal{T}\\_\\mathtt{S} = [N] \\rightarrow \\{ \\mathtt{S}(i) \\rightarrow (t_0) : t_0 = i \\} )\\).\nIn isl notation:\nimport islpy as isl\nD_S = isl.Set(\"[N] -&gt; { S[i]: 0 &lt;= i &lt; N }\") # 迭代域\nT_S = isl.Map(\"[N] -&gt; {S[i] -&gt; [t0]: t0 = i}\") # schedule\nprint(T_S)\n[N] -&gt; { S[i] -&gt; [t0 = i] }"
  },
  {
    "objectID": "posts/polyherdal-playground.html#multidimensional-schedules",
    "href": "posts/polyherdal-playground.html#multidimensional-schedules",
    "title": "Polyhedral Tutorials",
    "section": "Multidimensional Schedules",
    "text": "Multidimensional Schedules\n如果一个statement instance由多个元素的向量标识, 则表示这个statement包含在多个嵌套循环中, 它通常映射到multidimensional逻辑顺序.\n下面的例子中, statement instances以逻辑顺序的lexicographical order进行执行.\n比如\\(((0,42))\\)在\\(((100,0))\\)之前, 写作\\(((0,42) \\prec (100,0))\\).\nlexicographical order通常扩展到比较不同大小的向量.\n短的的向量, 是和较长向量的前缀比较, 例如\\(((0,42) \\prec (0,42,0))\\).\n比如, 多维下的初始化:\nfor (i = 0; i &lt; N; ++i)\n  for (j = 0; j &lt; N; ++j)\nS:  A[i][j] = 0.0;\niteration domain:\n\\(( \\mathcal{D}_\\mathtt{S} = [N] \\rightarrow \\{ \\mathtt{S}(i): 0 \\leq i,j &lt; N \\} )\\)\nidentity schedule:\n\\(( \\mathcal{T}_\\mathtt{S} = [N] \\rightarrow \\{ \\mathtt{S}(i,j) \\rightarrow (t_0, t_1) : t_0 = i \\wedge t_1 = j \\} )\\).\nIn isl notation:\nD_S = isl.Set(\"[N] -&gt; {S[i,j]: 0 &lt;= i,j &lt; N}\")\nT_S = isl.Map(\"[N] -&gt; {S[i,j] -&gt; [t0,t1]: t0 = i and t1 = j}\")\nprint(T_S)\n[N] -&gt; { S[i, j] -&gt; [t0 = i, t1 = j] }\n即使理论上schedule可以用单维度来表示:\n\\(( \\mathcal{T}_\\mathtt{S} = [N] \\rightarrow \\{ \\mathtt{S}(i,j) \\rightarrow (t_0) : t_0 = Ni + j \\} )\\)\n但由于存在变量的乘法, 这种表达式是不能表示为Presburger映射的.\n不过当使用实际常量而不是常量参数时, 是可以构建这样的schedule的.\n\nQuestion\n写出三维数组循环初始化的identity schedule\nfor (i = 0; i &lt; N; ++i)\n  for (j = 0; j &lt; N; ++j)\n    for (k = 0; k &lt; N; ++k)\nT:    A[i][j] = 0.0;\nD_T = isl.Set(\"[N] -&gt; { A[i,j,k] : 0 &lt;= i,j,k &lt; N }\")\nT_T = isl.Map(\"[N] -&gt; { A[i,j,k] -&gt; [t0 = i,t1 = j,t2 = k] }\")\nT_T\nMap(\"[N] -&gt; { A[i, j, k] -&gt; [t0 = i, t1 = j, t2 = k] }\")\n\n\nQuestion\n尝试为同一域定义一个具有乘法的一维计划（会出现错误）.\ntry:\n  T_T_err = isl.Map(\"[N] -&gt; {S[i,j,k] -&gt; [t0]: t0 = N*N*i + N*j + k}\")\nexcept:\n  print(\"got error\")\ngot error"
  },
  {
    "objectID": "posts/polyherdal-playground.html#representing-lexical-order",
    "href": "posts/polyherdal-playground.html#representing-lexical-order",
    "title": "Polyhedral Tutorials",
    "section": "Representing Lexical Order",
    "text": "Representing Lexical Order\n考虑一个循环中包含两个statement:\nfor (i = 0; i &lt; 10; ++i) {\nP:  A[i] = 0.0;\nQ:  B[i] = 1.0;\n}\n对两个statement使用简单的identity schedule将导致他们具有相同的执行顺序.\n然而, 从代码中可以清楚地看出, Q的是在P之后执行的, statement的lexical order可以使用auxiliary维度在schedule中编码.\n他分配一个常量, 以便在Q之前强制执行P的statement, 即P的常数小于Q的常数.\n由于顺序存在于循环中内部, 因此辅助维度放置在循环维度之后.\n\\(( \\mathcal{T} =\n  \\{ P(i) \\rightarrow (t\\_0, t\\_1) : t\\_0 = i \\wedge t\\_1 = 0 \\} \\cup\n  \\{ Q(i) \\rightarrow (t\\_0, t\\_1) : t\\_0 = i \\wedge t\\_1 = 1 \\} )\\)\n这个map会将顺序\\(((i,0))\\)分配到P, 然后\\(((i,1))\\)分配到Q.\n从而清晰的表示\\(( \\forall i, (i,0) \\prec (i,1) )\\)\nisl中, 不同的 statements的schedules 可以被结合为 union map.\nD = isl.UnionSet(\"{P[i]: 0 &lt;= i &lt; 10; Q[i]: 0 &lt;= i &lt; 10}\") # 首先列出两个statement的set\nS = isl.UnionMap(\"{P[i] -&gt; [t0,t1]: t0 = i and t1 = 0; Q[i] -&gt; [t0,t1]: t0 = i and t1 = 1}\") # 为他们分别分配额外的执行顺序, P = 0, Q = 1\nprint(S)\n{ Q[i] -&gt; [t0 = i, t1 = 1]; P[i] -&gt; [t0 = i, t1 = 0] }\n\n\n考虑两个循环组成的SCoP\nfor (i = 0; i &lt; 10; ++i) {\nU:  A[i] = 0.0;\n}\nfor (i = 0; i &lt; 10; ++i) {\nV:  B[i] = 1.0;\n}\n这个例子中,所有的U的实例都在所有的V的实例之前执行,\n因此辅助维度会在循环维度之前加入:\n\\(( \\mathcal{T} =\n  \\{ \\mathtt{U}(i) \\rightarrow (t_0, t_1) : t_0 = 0 \\wedge t_1 = i \\} \\cup\n  \\{ \\mathtt{V}(i) \\rightarrow (t_0, t_1) : t_0 = 1 \\wedge t_1 = i \\}.\n)\\)\nOr, in isl notation:\nD2 = isl.UnionSet(\"{U[i]: 0 &lt;= i &lt; 10; V[i]: 0 &lt;= i &lt; 10}\") # 两个statement分别的set\nS2 = isl.UnionMap(\"{U[i] -&gt; [t0,t1]: t0 = 0 and t1 = i; V[i] -&gt; [t0,t1]: t0 = 1 and t1 = i}\") # 这里把t1作为loop 维度, t0作为辅助维度, 也就是表示两个statement执行循环顺序相同,但是执行整个循环的顺序不同.\nprint(S2)\n{ U[i] -&gt; [t0 = 0, t1 = i]; V[i] -&gt; [t0 = 1, t1 = i] }\n绘制两个scheduled domain可以发现不同:\nfrom islplot.plotter import *\nplot_set_points(D.apply(S))\nprint(D.apply(S)) # t0表示是外部循环的顺序, t1 表示的内部statement的执行顺序, 此时外部循环0~9, 内部执行顺序0~1\n{ [t0, t1 = 1] : 0 &lt;= t0 &lt;= 9; [t0, t1 = 0] : 0 &lt;= t0 &lt;= 9 }\n\nplot_set_points(D2.apply(S2))\nprint(D2.apply(S2)) # t0表示是循环外部的顺序, t1表示循环内statement的执行顺序, 此时循环外部有先后顺序, 循环内顺序 0~9\n{ [t0 = 1, t1] : 0 &lt;= t1 &lt;= 9; [t0 = 0, t1] : 0 &lt;= t1 &lt;= 9 }\n\n注意, 因为上面将logical dates和所有statement的实例绘制在同一个space中,所以比较难辨认.\n\n\nQuestion\n通常,如果auxiliary dimension被statement共享, 那么定义在循环的最内部,\n如果没有被任何loop共享, 那么放到最前面.\n为下面的schedule定义auxiliary dimension:\nfor (i = 0; i &lt; 10; ++i) {\n  for (j = 0; j &lt; 5; ++j)\nS1: A[i][j] = 0.;\n  for (j = 0; j &lt; 5; ++j)\nS2: B[i][j] = 0.;\n}\nD = isl.UnionSet(\"{ S1[i,j] : 0 &lt;= i &lt; 10 and 0 &lt;= j &lt; 5 ; S2[i,j] : 0 &lt;= i &lt; 10 and 0 &lt;= j &lt; 5 }\")\nS = isl.UnionMap(\"{ S1[i,j] -&gt; [t0 = i,t1 = 0,t2 = j]; S2[i,j] -&gt; [t0 = i,t1 = 1,t2 = j]; }\")\nprint(S) # t0 作为共享循环i, t1 控制循环j外部的顺序, t2 分别为每个循环j的顺序.\n{ S1[i, j] -&gt; [t0 = i, t1 = 0, t2 = j]; S2[i, j] -&gt; [t0 = i, t1 = 1, t2 = j] }"
  },
  {
    "objectID": "posts/polyherdal-playground.html#access-relations",
    "href": "posts/polyherdal-playground.html#access-relations",
    "title": "Polyhedral Tutorials",
    "section": "Access Relations",
    "text": "Access Relations\n每个statement instance可能会访问一个或多个的variables/ scalars/ arrays.\n如果数组的下标索引是loop iterators 和 structure parameters的仿射形式, 那么可以定义Presburger relation来将statement instances和他们访问的数组元素关联起来. 为了简单起见,在polyhedral model中将scalar作为0维数组来表示.\n考虑一个矩阵乘的算子:\ndouble X[100], Y[100], Z[200];\ndouble zero = 0.;\n\nfor (int i = 0; i &lt;= 200; ++i)\nS:  Z[i] = zero;\nfor (int i = 0; i &lt;= 100; ++i)\n    for (int j = 0; j &lt;= 100; ++j)\nT:      Z[i + j] += A[i] * B[j];\n如果只使用句法术语, 可以称为statement S 访问 array Z.\n但是,对于每一个独立的instance\\(( \\mathtt{S}(i) )\\) 只访问其中的一个元素Z[i].\n这可以被编码为 \\(( \\{ \\mathtt{S}(i) \\rightarrow \\mathtt{Z}(a): a = i \\} )\\).\n进一步, 我们知道array Z的size,因此可以定义额外的约束来避免index out of range.\n\\(( \\{ \\mathtt{Z}(a): 0 \\leq a \\leq 200 \\} )\\).\n同时S的iteration domain为:\n\\(( \\{ \\mathtt{S}(i): 0 \\leq i \\leq 200 \\} )\\).\n在access relation中加入了以上约束后, 我们可以将上述set和statement domain进行intersect.\nimport islpy as isl\nA_S_Z = isl.Map(\"{S[i] -&gt; Z[a]: a = i}\") # s访问数组z的映射\nD_S = isl.Set(\"{S[i]: 0 &lt;= i &lt;= 200}\") # S的迭代域\nC_Z = isl.Set(\"{Z[a]: 0 &lt;= a &lt;= 200}\") # 数组Z的domain\nA_S_Z = A_S_Z.intersect_domain(D_S).intersect_range(C_Z) # 对schedule添加instance domain和数组的range.\nprint(A_S_Z)\n{ S[i] -&gt; Z[a = i] : 0 &lt;= i &lt;= 200 }\n上面最终加入约束后的例子中并没有显式对\\((a)\\)的范围约束,因为存在了隐式的约束\\((a = i)\\),所以isl会简化最终的表示.\nscalar不存在下标索引, 因此他们表示为0维的向量.\n但是这个range依旧存在一个name, 比如用zero来表示.\n\\(( \\{ \\mathtt{S}(i) \\rightarrow \\mathtt{zero}(): 0 \\leq i \\leq 200 \\} )\\).\n此时额外的\\((i)\\)的约束将从iteration domain S中获取.\nA_S_zero = isl.Map(\"{S[i] -&gt; zero[]:}\") # s访问zero的映射\nA_S_zero = A_S_zero.intersect_domain(D_S) # 因为zero没有range,所以这里只需要添加domain信息即可.\nprint(A_S_zero)\n{ S[i] -&gt; zero[] : 0 &lt;= i &lt;= 200 }\n最终, 可以将不同数组的访问关系组合成一个描述语句所有访问的联合映射.\nA_S = isl.UnionMap(A_S_Z).union(A_S_zero)\nprint(A_S) # 这里表示的初始化循环中, S访问了浮点值0和Z[i]的relation\n{ S[i] -&gt; Z[a = i] : 0 &lt;= i &lt;= 200; S[i] -&gt; zero[] : 0 &lt;= i &lt;= 200 }\n因为我们想要区分reads 和 writes的顺序, 因此我们可以分离不同的access relations的unions.\n有时候他们会同时触发, 在这里例子中就是一个statement同时读写相同的variable.\n\nQuestion\n定义map A_T_Z并联合statement T和array Z:\nA_T_Z = isl.Map(\" { T[i,j] -&gt; Z[a = i + j]} \") # T访问Z的relation.\nprint(A_T_Z)\n{ T[i, j] -&gt; Z[a = i + j] }\n\n\nQuestion\n分别定义A_T_reads and A_T_writes的union map:\nA_T_reads = isl.UnionMap(\" { T[i,j] -&gt; Z[a = i + j] ; T[i,j] -&gt; A[a = i] ; T[i,j] -&gt; B[a = j]} \")\nprint(A_T_reads)\n{ T[i, j] -&gt; A[a = i]; T[i, j] -&gt; Z[a = i + j]; T[i, j] -&gt; B[a = j] }\nA_T_writes = isl.Map(\"{ T[i,j] -&gt; Z[a = i + j] }\")\nprint(A_T_writes)\n{ T[i, j] -&gt; Z[a = i + j] }"
  },
  {
    "objectID": "posts/polyherdal-playground.html#detecting-out-of-bounds-accesses",
    "href": "posts/polyherdal-playground.html#detecting-out-of-bounds-accesses",
    "title": "Polyhedral Tutorials",
    "section": "Detecting Out-of-Bounds Accesses",
    "text": "Detecting Out-of-Bounds Accesses\n通过将iteration domain和array size 约束set进行结合,就可以检测出越界访问,\n考虑如下代码:\ndouble A[99];\nfor (int i = 0; i &lt;= 99; i++)\nX:  A[i+1] = 0.;\n其中的access relation表示为\\(( a = i + 1 )\\)\n首先将 S的iteration domain 和他的数组A的range约束进行intersect.\n返回的access relation中包含了所有的access instances, 也就是所有的statement instance和array 元素的pair.\n从domain约束过的relation中减去这个relation得到的结果就是invalid accesses,\n表示了所有越界访问的statement instance.\nA_X_A = isl.Map(\"{X[i]-&gt;A[a]: a = i+1}\") # X的access relation\nC_A = isl.Set(\"{A[i]: 0 &lt;= i &lt;= 99}\") # 添加数组range信息\nD_X = isl.Set(\"{X[i]: 0 &lt;= i &lt;= 99}\") # 添加statement domain信息\ncorrect = A_X_A.intersect_domain(D_X).intersect_range(C_A) # 同时约束domain和range的则是可以执行的statement.\nincorrect = A_X_A.intersect_domain(D_X).subtract(correct) # 只约束domain的情况下,他的statement set将会大于等于 correct\nprint(correct)\nprint(incorrect)\n{ X[i] -&gt; A[a = 1 + i] : 0 &lt;= i &lt;= 98 }\n{ X[i = 99] -&gt; A[a = 100] }\n在上面的例子中,statement instance \\(X(99)\\) 就对数组A执行了越界的访问, 这个可以通过将循环上界修改为i &lt; 99来完成修复.\n\nQuestion\n验证修复后的结果是正确的, 即检查subtract后的集合为空.\nA_X_A = isl.Map(\"{X[i]-&gt;A[a]: a = i+1}\") # 首先定义访问关系\nC_A = isl.Set(\"{A[i]: 0 &lt;= i &lt;= 99}\") # 添加数组range信息\nD_X = isl.Set(\"{X[i]: 0 &lt;= i &lt; 99}\") # 添加statement domain信息\ncorrect = A_X_A.intersect_domain(D_X).intersect_range(C_A) # 同时约束domain和range的则是可以执行的statement.\nincorrect = A_X_A.intersect_domain(D_X).subtract(correct) # 只约束domain的情况下,他的statement set将会大于等于 correct\nassert incorrect.is_empty() == True\n公式\\((1 = 0)\\)为false.\n在Presburger sets表示中, 这被用于表示一个空的集合,但不丢失name和维度信息."
  },
  {
    "objectID": "posts/polyherdal-playground.html#potentially-dependent-instances",
    "href": "posts/polyherdal-playground.html#potentially-dependent-instances",
    "title": "Polyhedral Tutorials",
    "section": "Potentially Dependent Instances",
    "text": "Potentially Dependent Instances\n现在来定义inverse access relation, 即映射数组元素到每个访问了这个元素的statement instance.\n首先获取原始的access relation:\n\\(( \\mathcal{A}\\_{\\mathtt{S} \\rightarrow \\mathtt{Z}} = \\{ \\mathtt{S}(i) \\rightarrow \\mathtt{Z}(a):\n    a = i \\wedge 0 \\leq a,i \\leq 200 \\} )\\),\ninverse access relation被相同的约束定义的,但是交换了其中的domain和range.\n\\(( \\mathcal{A}\\_{\\mathtt{S} \\rightarrow \\mathtt{Z}}^{-1} = \\{ \\mathtt{Z}(a) \\rightarrow \\mathtt{S}(i):\n    a = i \\wedge 0 \\leq a,i \\leq 200 \\} )\\).\nisl can compute inverse relations using:\nA_S_Z_inv = A_S_Z.reverse()\nprint(A_S_Z)\nprint(A_S_Z_inv)\n{ S[i] -&gt; Z[a = i] : 0 &lt;= i &lt;= 200 }\n{ Z[a] -&gt; S[i = a] : 0 &lt;= a &lt;= 200 }\n如果两个statement instances访问了相同的array element, 他们可能会互相干扰.\n比如,第一个instance写入了值,然后后面第二个instance去读取他.\n在没有volatile限定符的情况下,两个statement读取相同的元素是不会被干扰的.\n将此定义转换为relations, 我们需要在访问相同数组元素的statement instances之间定义一个映射.\n通过access relation, 我们知道statement instance访问了哪些元素.\n使用inverse access relation,我们能知道哪些其他的 statement instances访问了数组元素.\n结合这两个access relation和数组下标索引, 可以为我们提供潜在的statement instances之间的依赖关系(potentially dependent).\n这可以通过access relation之间的组合来完成:\n\\(( \\mathcal{X} \\circ \\mathcal{Y} = \\{ \\pmb{x} \\rightarrow \\pmb{y} \\mid\n    \\exists \\pmb{z} : (\\pmb{x},\\pmb{z}) \\in \\mathcal{X} \\wedge (\\pmb{z},\\pmb{y}) \\in \\mathcal{Y} \\} )\\).\nisl中可以使用apply range操作来进行access relation之间的组合.\n比如计算statement S访问相同元素Z的关系:\ndep_S_Z = A_S_Z.apply_range(A_S_Z.reverse()) # A_S_Z 表示S访问数组Z的关系, A_S_Z_inv表示数组Z被S访问的关系.\nprint(dep_S_Z) # 得到了potentially dependent\n{ S[i] -&gt; S[i' = i] : 0 &lt;= i &lt;= 200 }\n得到了statement instances集合S的map结果.\n在这个例子中,本质上是一个恒等的关系, 因为\\((i^\\prime = i)\\)是相等的.\n这表示不同的S的instance实际访问的是不同的数组元素.\n\nQuestion\n定义关于instance S访问标量zero的映射dep_S_zero:\ndep_S_zero = A_S_zero.apply_range(A_S_zero.reverse())\nprint(dep_S_zero)\n{ S[i] -&gt; S[i'] : 0 &lt;= i &lt;= 200 and 0 &lt;= i' &lt;= 200 }\n上面就可以发现S的每个实例都是相关关联的, 因为他们访问的都是同一个标量值.\n不过目前他们只读取这个值, 并没有修改他, 因此是不会与其他的statement instance产生干扰."
  },
  {
    "objectID": "posts/polyherdal-playground.html#reads-and-writes",
    "href": "posts/polyherdal-playground.html#reads-and-writes",
    "title": "Polyhedral Tutorials",
    "section": "Reads and Writes",
    "text": "Reads and Writes\n通常,只有至少一次访问为write的时候才会被看作是潜在依赖.\n因此,还是需要将read/write分离为不同的relation.\n现在用一个更小的数据范围的例子来说明问题:\ndouble X[10], Y[10], Z[20];\n\nfor (int i = 0; i &lt;= 20; ++i)\nS:  Z[i] = 0.;\nfor (int i = 0; i &lt;= 10; ++i)\n    for (int j = 0; j &lt;= 10; ++j)\nT:      Z[i + j] += A[i] * B[j];\nNOTE 在isl中,复杂度取决于依赖的数量,而不是集合中的数据量.\n由于union中的map可以存在于不同的空间中, 因此可以通过组合来自不同语句的单个access (union) map来定义所有读取和写入的映射.\n\nQuestion\n下面给出了每个单独的access relations, 定义整个SCoP的reads and writes union map.\nA_S_Z = isl.Map(\"{S[i]-&gt;Z[a]: a = i and 0 &lt;= a,i &lt;= 20}\") # S 访问数组Z的 access relation\nA_T_Z = isl.Map(\"{T[i,j]-&gt;Z[a]: a = i + j and 0 &lt;= i,j &lt;= 10 and 0 &lt;= a &lt;= 20}\") # T 访问数组Z的access relation\nA_T_A = isl.Map(\"{T[i,j]-&gt;A[a]: a = i and 0 &lt;= a &lt;= 20 and 0 &lt;= i,j &lt;= 10}\") # T 访问数组A的access relation\nA_T_B = isl.Map(\"{T[i,j]-&gt;B[a]: a = j and 0 &lt;= a &lt;= 20 and 0 &lt;= i,j &lt;= 10}\") # T 访问数组B的access relation\nwrites = isl.UnionMap(A_S_Z).union(A_T_Z) # S写入Z[i], T写入Z[i+j]\nreads = isl.UnionMap(A_T_Z).union(A_T_A).union(A_T_B) # T读Z[i+j], A[i], B[j]\nprint(reads)\nprint(writes)\n{ T[i, j] -&gt; A[a = i] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10; T[i, j] -&gt; Z[a = i + j] : 0 &lt;= i &lt;= 10 and j &gt;= 0 and -i &lt;= j &lt;= 20 - i and j &lt;= 10; T[i, j] -&gt; B[a = j] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 }\n{ T[i, j] -&gt; Z[a = i + j] : 0 &lt;= i &lt;= 10 and j &gt;= 0 and -i &lt;= j &lt;= 20 - i and j &lt;= 10; S[i] -&gt; Z[a = i] : 0 &lt;= i &lt;= 20 }"
  },
  {
    "objectID": "posts/polyherdal-playground.html#selecting-one-write",
    "href": "posts/polyherdal-playground.html#selecting-one-write",
    "title": "Polyhedral Tutorials",
    "section": "Selecting one Write",
    "text": "Selecting one Write\n为了避免将read access与read access组合的情况, 我们必须确保reads不会同时出现在组合的两边:\n\\(( (\\mathtt{reads} \\circ \\mathtt{writes}^{-1}) \\cup\n    (\\mathtt{writes} \\circ \\mathtt{reads}^{-1}) \\cup\n    (\\mathtt{writes} \\circ \\mathtt{writes}^{-1}) )\\)\n注意 writes在右边出现了两次, 因此可以将表达式简化为:\n\\(( ((\\mathtt{reads} \\cup \\mathtt{writes}) \\circ \\mathtt{writes}^{-1}) \\cup (\\mathtt{writes} \\circ \\mathtt{reads}^{-1}) )\\).\n现在来计算union的第一部分:\nreads_writes = reads.union(writes)\nleft_part = reads_writes.apply_range(writes.reverse())\nprint(left_part)\n{ S[i] -&gt; S[i' = i] : 0 &lt;= i &lt;= 20; T[i, j] -&gt; S[i' = i + j] : 0 &lt;= i &lt;= 10 and j &gt;= 0 and -i &lt;= j &lt;= 20 - i and j &lt;= 10; T[i, j] -&gt; T[i', j' = i + j - i'] : 0 &lt;= i &lt;= 10 and j &gt;= 0 and -i &lt;= j &lt;= 20 - i and j &lt;= 10 and i' &gt;= -10 + i + j and 0 &lt;= i' &lt;= 10 and i' &lt;= i + j; S[i] -&gt; T[i', j = i - i'] : 0 &lt;= i &lt;= 20 and i' &gt;= -10 + i and 0 &lt;= i' &lt;= 10 and i' &lt;= i }\n\nQuestion\n计算整个union的第二部分:\nright_part = writes.apply_range(reads.reverse()) # (writes ∘ reads⁻¹)\nunion = left_part.union(right_part) # 再进行union\nprint(union)\n{ S[i] -&gt; T[i', j = i - i'] : i' &gt;= -10 + i and 0 &lt;= i' &lt;= 10 and i' &lt;= i; T[i, j] -&gt; S[i' = i + j] : 0 &lt;= i &lt;= 10 and j &gt;= 0 and -i &lt;= j &lt;= 20 - i and j &lt;= 10; S[i] -&gt; S[i' = i] : 0 &lt;= i &lt;= 20; T[i, j] -&gt; T[i', j' = i + j - i'] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and i' &gt;= -10 + i + j and 0 &lt;= i' &lt;= 10 and i' &lt;= i + j }\n\n\nQuestion:\n现在单独计算连接读取相同元素的statement instances的two_reads的relation:\ntwo_reads = reads.apply_range(reads.reverse())\nprint(two_reads)\n{ T[i, j] -&gt; T[i', j' = i + j - i'] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and i' &gt;= -10 + i + j and 0 &lt;= i' &lt;= 10 and i' &lt;= i + j; T[i, j] -&gt; T[i' = i, j'] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and 0 &lt;= j' &lt;= 10; T[i, j] -&gt; T[i', j' = j] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and 0 &lt;= i' &lt;= 10 }\n\n\n\n比较以上这些relation, 即使它们以不同的顺序打印, union和left_part也完全相同.\nprint(union.is_equal(left_part))\nTrue\n这里是因为在这个例子中left_part实际上就是right_part的一个子集,\n因为T读写的都是相同的元素, 因此pair \\(( \\mathtt{S}(i) \\rightarrow \\mathtt{T}(i',j=i-i') )\\)\nprint(right_part.is_subset(left_part))\nTrue\n因此,我们不能只计算潜在依赖statement pair的整体集合,然后减去那些有两次reads的集合. 如果其中出现两次相同的write的关系,就会被抵消.\n\n\nQuestion\n使用read和write的statement instance pair之间的relation, 从中减去 two_reads 并检查它确实只是 union 的一个子集.\nall_pairs = reads_writes.apply_range(reads_writes.reverse())\nall_pairs = all_pairs.subtract(two_reads)\nprint(all_pairs.is_equal(union))\nprint(all_pairs.is_subset(union))\nFalse\nTrue"
  },
  {
    "objectID": "posts/polyherdal-playground.html#visualizing-potentially-dependent-instances",
    "href": "posts/polyherdal-playground.html#visualizing-potentially-dependent-instances",
    "title": "Polyhedral Tutorials",
    "section": "Visualizing Potentially Dependent Instances",
    "text": "Visualizing Potentially Dependent Instances\n现在把限制只放在T的实例上\nT_only = isl.Map(\"{ T[i,j] -&gt; T[i',j']: }\") # 过滤出访问点.\nnew_union = union.intersect(T_only)\nfrom islplot.plotter import *\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 7), dpi=80)\nplot_map(new_union)\nprint(new_union)\n{ T[i, j] -&gt; T[i', j' = i + j - i'] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and i' &gt;= -10 + i + j and 0 &lt;= i' &lt;= 10 and i' &lt;= i + j }\n\n如上图所示, 每个statement instance都与 他自己 以及同一对角线上的一个或多个instance相关.\n\nQuestion\n定义一个map left_42 只包含 \\((\\mathtt{T}(4,2) )\\) 在左边的pair, 和一个map right_42, 它只包含它在右边的pair\n提示：map domain和range是集合, 可以对其进行操作\nleft_42 = union.intersect_domain(isl.Set(\"{ T[i = 4,j = 2] }\"))\nprint(left_42)\nright_42 = union.intersect_range(isl.Set(\"{ T[i = 4,j = 2] }\"))\nprint(right_42)\n{ T[i = 4, j = 2] -&gt; S[i' = 6]; T[i = 4, j = 2] -&gt; T[i', j' = 6 - i'] : 0 &lt;= i' &lt;= 6 }\n{ T[i, j = 6 - i] -&gt; T[i' = 4, j' = 2] : 0 &lt;= i &lt;= 6; S[i = 6] -&gt; T[i' = 4, j = 2] }\n从图上比较这些relation:\nplt.figure(figsize=(10, 7))\nplot_map(left_42)\n\nplt.figure(figsize=(10, 7), dpi=80)\nplot_map(right_42)\n\n从上图可以看出, 两个relation是相同的.\n这又是因为对 Z[i+j] 的唯一访问是read和write.\nstatement instance被连接到了访问相同数据的所有其他statement instance.\n但是, 它并不一定会创建依赖. 例如, 依赖于自身并不完全有意义. 对于两个依赖的statement instance, 其中一个应该在另一个之前执行. 也就是说, 第一个statement要么生成第二个statement所需的一些数据, 要么使用一些将被第二个statement覆盖的数据. 依赖计算需要知道statement执行的顺序.\n\n\nQuestion\n给定以下代码, 计算和可视化访问相同数组元素的statement pair, 并且至少其中一个访问是write.\nHint: 使用disjunction对同一数组的不同引用中的不同下标进行编码.\nfor (i = 0; i &lt; 6; ++i)\n  for (j = 0; j &lt; 5; ++j)\nS1: X[i][j] = i * j;\nfor (i = 8; i &lt; 13; ++i)\n  for (j = 11; j &lt; 15; ++j)\nS2: Y[i][j] = X[i - 8][j - 10] - X[i - 7][j - 11];\n# step 1. 定义每个statement instance的domain\ndomain = isl.UnionSet(\" { S1[i,j] : 0 &lt;= i &lt; 6 and 0 &lt;= j &lt; 5 ; S2[i,j] : 8 &lt;= i &lt; 13 and 11 &lt;= j &lt; 15 } \")\n# S1写入X ; S2写入Y\nwrites = isl.UnionMap(\" { S1[i,j] -&gt; X[a = i, b = j]; S2[i,j] -&gt; Y[a = i, b = j] }\")\n# S2读取X\nreads = isl.UnionMap(\" { S2[i,j] -&gt; X[a,b] : (a = i-8 and b = j-10) or ( a= i-7 and b = j - 11) }\")\n\n# 添加domain约束\nwrites = writes.intersect_domain(domain)\nreads = reads.intersect_domain(domain)\n\n# 依赖计算\ndeps = writes.apply_range(reads.reverse())\nprint(deps)\n{ S1[i, j] -&gt; S2[i' = 7 + i, j' = 11 + j] : 0 &lt; i &lt;= 5 and 0 &lt;= j &lt;= 3; S1[i, j] -&gt; S2[i' = 8 + i, j' = 10 + j] : 0 &lt;= i &lt;= 4 and 0 &lt; j &lt;= 4 }\nplt.figure(figsize=(10, 7), dpi=80)\nplot_map(deps)\n\n\n\nQuestion\n选择 S1 和 S2 的 sample 实例, 并绘制访问同一数组元素的其他语句的实例.\npoint_22 = isl.Set(\"{S1[i,j]: i = 2 and j = 2}\") # S1 写入X[2,2]\npoint_1012 = isl.Set(\"{S2[i,j]: i = 10 and j = 12}\") # S2 读取 X[2][2] 和 X[3][1];\nsinks = deps.intersect_domain(point_22)\nsources = deps.intersect_range(point_1012)\nplt.figure(figsize=(10, 7), dpi=80)\nplot_map([sinks,sources])"
  },
  {
    "objectID": "posts/polyherdal-playground.html#plugging-in-schedule-information",
    "href": "posts/polyherdal-playground.html#plugging-in-schedule-information",
    "title": "Polyhedral Tutorials",
    "section": "Plugging in Schedule Information",
    "text": "Plugging in Schedule Information\n因为statement instances是按照它们各自词典序执行的, 因此我们需要一种在relation中表达它的方法.\n最简单的例子就是强制所有的statements的schedule都在相同的space中, 比如我们添加一个辅助维度\\((t\\_2)\\)到schedule S的statement T中\nisl 允许我们定义lexicographic less-than的映射:\nschedule_space = isl.Set(\"{[t0,t1,t2]:}\").get_space()\nprecedes = isl.Map.lex_lt(schedule_space)\nprint(precedes)\n{ [t0, t1, t2] -&gt; [t0', t1', t2'] : t0' &gt; t0; [t0, t1, t2] -&gt; [t0' = t0, t1', t2'] : t1' &gt; t1; [t0, t1, t2] -&gt; [t0' = t0, t1' = t1, t2'] : t2' &gt; t2 }\n上面的map按照词典序的定义执行, 先比较第一对元素,如果一个在另一个之前, 则顺序成立. 否则假设它们相等并比较第二对, 继续直到最后一对.\nprecedes relation 即前一个词典上在后一个之前成对元组的集合.\n\nQuestion\n将 schedule 应用到 domain 以便将其映射到调度空间, 将结果保存为 scheduled_domain.\nscheduled_domain = domain.apply(schedule)\nprint(scheduled_domain)\n{ [t0 = 1, t1, t2] : 0 &lt;= t1 &lt;= 10 and 0 &lt;= t2 &lt;= 10; [t0 = 0, t1, t2 = 0] : 0 &lt;= t1 &lt;= 20 }\n现在我们知道如何将relation从domain空间移动到schedule空间: 将schedule relation apply到domain空间中的所有内容上.\n\n\nQuestion\n检查 \\(( \\mathtt{S}(2) )\\) 在 \\(( \\mathtt{T}(0,0) )\\) 之前执行.\nHint 1: 在两个instances之间定义映射.\nHint 2: 在schedule space中,可以检查relation是否是precedes的子集.\nrel = isl.UnionMap(\"{S[i] -&gt; T[a,b]: i = 2 and a = 0 and b = 0}\") # 构造出对应循环点的instance.\nrel = rel.apply_domain(schedule).apply_range(schedule) # 为instance添加约束.\nprint(rel)\nprint(rel.is_subset(precedes))\n{ [t0 = 0, t1 = 2, t2 = 0] -&gt; [t0' = 1, t1' = 0, t2' = 0] }\nTrue"
  },
  {
    "objectID": "posts/polyherdal-playground.html#memory-based-dependence-analysis",
    "href": "posts/polyherdal-playground.html#memory-based-dependence-analysis",
    "title": "Polyhedral Tutorials",
    "section": "Memory-based Dependence Analysis",
    "text": "Memory-based Dependence Analysis\n\nFlow Dependences\n当数组元素首先由一个statement instance 写入然后由另一个statement instance 读取时, 就会出现数据流依赖性, 必须首先执行writer instance.\n因此, 我们首先将左侧的 writes union map与右侧的反向 reads union map组合在一起, \\(( \\mathtt{writes} \\circ \\mathtt{reads}^{-1} )\\).\n然后我们将其转换为schedule space, 并将结果与schedule space中的词典序relation相交, 从而 只保留依赖 source(第一个时间点)在其 sink(第二个时间点)之前执行的对.\ndep_flow = writes.apply_range(reads.reverse()) # 获取潜在依赖\ndep_flow = dep_flow.apply_domain(schedule).apply_range(schedule) # 添加约束.\ndep_flow = dep_flow.intersect(precedes) # 和词典序做交集.\nprint(dep_flow) # 这里得到了执行顺序的交集\n{ [t0 = 1, t1, t2] -&gt; [t0' = 1, t1', t2' = t1 + t2 - t1'] : 0 &lt;= t1 &lt;= 10 and 0 &lt;= t2 &lt;= 10 and t1' &gt;= -10 + t1 + t2 and t1' &gt; t1 and 0 &lt;= t1' &lt;= 10 and t1' &lt;= t1 + t2; [t0 = 0, t1, t2 = 0] -&gt; [t0' = 1, t1', t2' = t1 - t1'] : t1' &gt;= -10 + t1 and 0 &lt;= t1' &lt;= 10 and t1' &lt;= t1 }\n由此产生的relation是在执行时间点之间,而不是在点之间. 要将其转换回domain space,重新调用一下relation可以很容易地反转. 即反向schedule relation将执行时间点映射到statement instances.\ndep_flow = dep_flow.apply_domain(schedule.reverse()).apply_range(schedule.reverse()) # 再将其反向映射到数据点上, \nprint(dep_flow)\n{ T[i, j] -&gt; T[i', j' = i + j - i'] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and i' &gt;= -10 + i + j and i' &gt; i and 0 &lt;= i' &lt;= 10 and i' &lt;= i + j; S[i] -&gt; T[i', j = i - i'] : i' &gt;= -10 + i and 0 &lt;= i' &lt;= 10 and i' &lt;= i }\n现在得到了 T 的不同instance之间的依赖关系, 我们可以分析它并与 potentially 依赖语句关系进行比较.\ndep_flow_T = dep_flow.intersect(isl.UnionMap(\"{T[i,j]-&gt;T[i',j']:}\"))\npotential = writes.apply_range(reads.reverse())\npotential = potential.intersect(isl.UnionMap(\"{T[i,j]-&gt;T[i',j']:}\"))\nfrom islplot.plotter import *\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,7))\nplot_map(dep_flow_T)\n\nplt.figure(figsize=(10,7))\nplot_map(potential)\n\n实际上每个statement instance是不能在其自身之前被执行, 因此观察到statement instances的self-dependences消失了.\n\n\nQuestion\n绘制instance $ (4,2) $ instance 的依赖 samples, 即它所依赖的statement instances和依赖它的statement instances.\npoint = isl.Set(\"{T[i,j]: i = 4 and j = 2}\") # 添加instance point约束\nsources = dep_flow_T.intersect_range(point) \nsinks = dep_flow_T.intersect_domain(point)\nprint(sources)\nprint(sinks)\n{ T[i, j = 6 - i] -&gt; T[i' = 4, j' = 2] : 0 &lt;= i &lt;= 3 }\n{ T[i = 4, j = 2] -&gt; T[i', j' = 6 - i'] : 5 &lt;= i' &lt;= 6 }\n# sources : 4 + 2 = 6, 他依赖于 [0,6], [1,5], [2,4], [3,3] 这些instance.\n# sinks: [5,1],[6,0]依赖于[4,2]\nplt.figure(figsize=(10,7))\nplot_map([sources, sinks], marker_size=10)  \n\n现在可以看到依赖于给定statement instances的实例都是访问相同数组元素并在它之前执行的实例, 类似地, 在给定statement instances之后执行的语句实例依赖于它.\n\n\nAnti and Output Dependences\nAnti-dependences是flow dependencs的reverse,即先读后写, 考虑Anti-dependences意味着在读取之前不覆盖他的值, Output dependences,或写后再写,需要保留写的顺序.\n这些通常都被称为 false dependences,因为 sink 实例实际上并不 依赖 source 语句实例产生的 data, 而是需要在后面执行以避免干扰其他实例.\nfalse dependences的 sink 通常是写访问,而source可以是读或写, 因此我们将左侧的读写union与右侧的reverse写组合:\n\\(( (\\mathtt{reads} \\cup \\mathtt{writes}) \\circ \\mathtt{writes}^{-1} )\\).\ndep_false = writes.union(reads).apply_range(writes.reverse())\ndep_false = dep_false.apply_domain(schedule).apply_range(schedule)\ndep_false = dep_false.intersect(precedes)\ndep_false = dep_false.apply_domain(schedule.reverse()).apply_range(schedule.reverse())\nprint(dep_false)\n{ T[i, j] -&gt; T[i', j' = i + j - i'] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and i' &gt;= -10 + i + j and i' &gt; i and 0 &lt;= i' &lt;= 10 and i' &lt;= i + j; S[i] -&gt; T[i', j = i - i'] : i' &gt;= -10 + i and 0 &lt;= i' &lt;= 10 and i' &lt;= i }\n在这个例子中,false dependences与flow dependences完全对应,因为S只写入,而T只读取和写入相同的元素. 实际上一般情况并非如此.\nprint(dep_false.is_equal(dep_flow))\nTrue\n\n\nQuestion: Input Dependences\n即使input 或连续读取通常不被强制执行, 但这个得到他们的依赖关系在程序优化中也很有用. 因此与前两种情况类似地计算input dependences:\ndep_input = reads.apply_range(reads.reverse()) # 获得reads after reads的map\ndep_input = dep_input.apply_domain(schedule).apply_range(schedule) # 添加约束.\ndep_input = dep_input.intersect(precedes) # 和词典序做交集.\ndep_input = dep_input.apply_domain(schedule.reverse()).apply_range(schedule.reverse()) # 再将其反向映射到数据点上, \nprint(dep_input)\n{ T[i, j] -&gt; T[i', j' = i + j - i'] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and i' &gt;= -10 + i + j and i' &gt; i and 0 &lt;= i' &lt;= 10 and i' &lt;= i + j; T[i, j] -&gt; T[i' = i, j'] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and j' &gt; j and 0 &lt;= j' &lt;= 10; T[i, j] -&gt; T[i', j' = j] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10 and i' &gt; i and 0 &lt;= i' &lt;= 10 }\nInput dependence看起来和flow dependence/false dependence不同, 其中只出现了T的实例, 这是因为S从来没有读过任何值.\n\n\nQuestion\n绘制input dependence的samples:\npoint = isl.Set(\"{T[i,j]: i = 4 and j = 2}\")\n# sources 其中竖线表示B[0,1,2]时多次读取A[4]的依赖, 横线表示B[2]时多次读取A[0,1,2,3]的依赖, 斜线表示Z[4,2]对Z[(0+6),(1+5),(2+4),(3+3)]的依赖\nsources = dep_input.intersect_range(point) \nsinks = dep_input.intersect_domain(point)\nplt.figure(figsize=(10,7))\nplot_map([sources, sinks], marker_size=5)\n\n复杂的依赖pattern会导致同一个statement instance从多个不同的array读取数据."
  },
  {
    "objectID": "posts/polyherdal-playground.html#value-based-analysis",
    "href": "posts/polyherdal-playground.html#value-based-analysis",
    "title": "Polyhedral Tutorials",
    "section": "Value-based Analysis",
    "text": "Value-based Analysis\n可以观察到每个statement instance依赖了所有访问相同数组元素的实例,但是其实并不是必要的. 比如T(4,2)和T(5,1)的flow dependence:\npoint1 = isl.Set(\"{T[i,j]: i = 4 and j = 2}\")\npoint2 = isl.Set(\"{T[i,j]: i = 5 and j = 1}\")\nsources1 = dep_flow_T.intersect_range(point1)\nsources2 = dep_flow_T.intersect_range(point2)\nplt.figure(figsize=(10,7))\nplot_map([sources1, sources2],marker_size=10)\n\n其中T(5,1) 依赖T(4,2)所依赖的所有实例以及T(4,2)本身.\nprint(sources2.domain().subtract(point1).is_equal(sources1.domain())) # 检查是否是子集.\nTrue\n如果将T(4,2)调度到T(3,3)之后, 那么T(5,1)也需要被调度到T(4,2)之后, 然后T(5,1)也需要调度到T(3,3)之后. 所以这两个实例之间的依赖关系transitively covered(传递地覆盖)其他实例之间的依赖.\n我们可以安全地移除传递覆盖的flow dependence. 通过计算词典序最大的source instance对于任意sink instance的依赖,然后删除其他的source instance来完成.\n但以上的计算是non- trivial的,并且可能涉及到线性优化问题的求解, isl提供了value-based分析的功能. 但是,它会使用更加精确的树结构来调度, 对于这个例子, 我们将直接提供这个schedule:\nschedule = isl.Schedule('{ domain: \"{ T[i, j] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 10; S[i] : 0 &lt;= i &lt;= 20 }\", child: { sequence: [{ filter: \"{ S[i] }\", child: { schedule: \"[{S[i] -&gt; [(i)]}, {S[i] -&gt; [(0)]}]\" }}, { filter: \"{ T[i, j] }\", child: { schedule: \"[{T[i, j] -&gt; [(i)]}, {T[i, j] -&gt; [(j)]}]\" }} ] } }')\n现在可以来进行value-based的依赖分析:\nuai = isl.UnionAccessInfo.from_sink(reads)\nuai = uai.set_must_source(writes)\nuai = uai.set_schedule(schedule)\nflow = uai.compute_flow()\ndep_flow_precise = flow.get_may_dependence()\nprint(dep_flow_precise)\n{ S[i] -&gt; T[i' = -10 + i, j = 10] : 10 &lt;= i &lt;= 20; S[i] -&gt; T[i' = 0, j = i] : 0 &lt;= i &lt;= 9; T[i, j] -&gt; T[i' = 1 + i, j' = -1 + j] : 0 &lt;= i &lt;= 9 and 0 &lt; j &lt;= 10 }\n现在依赖关系已经不再包含transitively covered的例子了.\n\nQuestion\n将依赖关系限制在T上, 然后绘制T(4,2) 和 T(5,1)的samples, 并将他们memory-based的分析结果进行比较.\ndep_flow_precise_T = dep_flow_precise.intersect(isl.UnionMap(\"{T[i,j]-&gt;T[i',j']:}\"))\nsources1 = dep_flow_precise_T.intersect_range(point1)\nsources2 = dep_flow_precise_T.intersect_range(point2)\nplt.figure(figsize=(10,7))\nplot_map([sources1, sources2], marker_size=10)\n\n\n\n\n可以发现,每个T的实例限制只依赖一个其他T的实例.\nMemory-based dependences可以通过计算依赖关系的transitive closure来从value-based dependences恢复出来:\n\n\nQuestion\n使用value-based的过程计算input dependences, 并且可视化:\nuai = isl.UnionAccessInfo.from_sink(reads)\nuai = uai.set_must_source(reads)\nuai = uai.set_schedule(schedule)\nflow = uai.compute_flow()\ndep_input_precise = flow.get_may_dependence()\nprint(dep_input_precise)\n{ T[i, j] -&gt; T[i' = i, j' = 1 + j] : 0 &lt;= i &lt;= 10 and 0 &lt;= j &lt;= 9; T[i, j] -&gt; T[i' = 1 + i, j' = j] : 0 &lt;= i &lt;= 9 and 0 &lt;= j &lt;= 10; T[i, j] -&gt; T[i' = 1 + i, j' = -1 + j] : 0 &lt;= i &lt;= 9 and 0 &lt; j &lt;= 10 }\ndep_flow_precise_T = dep_flow_precise.intersect(isl.UnionMap(\"{T[i,j]-&gt;T[i',j']:}\"))\nsources1 = dep_flow_precise_T.intersect_range(point1)\nsources2 = dep_flow_precise_T.intersect_range(point2)\nplt.figure(figsize=(10,7))\nplot_map([sources1, sources2], marker_size=10)"
  },
  {
    "objectID": "posts/polyherdal-playground.html#loop-reversal",
    "href": "posts/polyherdal-playground.html#loop-reversal",
    "title": "Polyhedral Tutorials",
    "section": "Loop Reversal",
    "text": "Loop Reversal\n循环反转可以改变循环元素被访问的方向, 反转之后, 之前迭代的第一个元素将会被最后执行, 最后一个元素将会被第一个执行.\nBenefits:\n\n可以被用来缩短依赖\n\ndomain = isl.UnionSet(\"[n] -&gt; {S[i] : 0 &lt;= i &lt; n}\") # 原始迭代域\noriginal = isl.UnionMap(\"{S[i] -&gt; [i]}\") # 原始schedule\ntransformation = isl.UnionMap(\"{[i] -&gt; [-i]}\")\n\ntransformed = original.apply_range(transformation) # 实施transform.\nprint_before_after(domain, original, transformed) # i \\in [0,n-1] =&gt; [-n+1,0]\nBefore Transform:\nfor (int c0 = 0; c0 &lt; n; c0 += 1)\n  S(c0);\n\nAfter Transform:\nfor (int c0 = -n + 1; c0 &lt;= 0; c0 += 1)\n  S(-c0);"
  },
  {
    "objectID": "posts/polyherdal-playground.html#generate-an-ast",
    "href": "posts/polyherdal-playground.html#generate-an-ast",
    "title": "Polyhedral Tutorials",
    "section": "Generate an AST",
    "text": "Generate an AST\n\nDefine a simple polyhedral program description\nimport islpy as isl\nfrom islplot.plotter import *\nimport matplotlib.pyplot as plt\ncontext = isl.Set(\"{ : }\")\ndomain = isl.Set(\"{ S[t,i] : 1 &lt;=t&lt;=5 and 1&lt;=i&lt;=10 }\")\nschedule = isl.Map(\"{ S[t,i] -&gt; T[t+1,i+t+10] }\")\n\nschedule_domain = schedule.intersect_domain(domain)\nplt.figure(figsize=[10, 7])\nplot_set_points(domain)\nplot_map(schedule_domain)"
  },
  {
    "objectID": "posts/polyherdal-playground.html#translate-polyhedral-program-description-to-an-ast",
    "href": "posts/polyherdal-playground.html#translate-polyhedral-program-description-to-an-ast",
    "title": "Polyhedral Tutorials",
    "section": "Translate polyhedral program description to an AST",
    "text": "Translate polyhedral program description to an AST\n最终生成的AST是实际的AST, 可以通过树操作对其进行遍历和检查.\n\nclass CSource():\n  def __init__(self, ast):\n    self.source = ast\n\n  def _repr_html_(self):\n    return \"&lt;pre class='code'&gt;&lt;code class=\\\"cpp hljs\\\"&gt;\" + self.source.to_C_str() + \"&lt;/code&gt;&lt;/pre&gt;\"\n\n\nclass CSourceComparer():\n  def __init__(self, before: CSource, after: CSource):\n    self.before = before\n    self.after = after\n\n  def _repr_html_(self):\n    s = \"&lt;b&gt;Before Transform:&lt;/b&gt;\\n\"\n    s += self.before._repr_html_()\n    s += \"&lt;b&gt;After Transform:&lt;/b&gt;\\n\"\n    s += self.after._repr_html_()\n    return s\n\ndef print_code(ast):\n  return CSource(ast)\nbuild = isl.AstBuild.from_context(context) \nast = build.node_from_schedule_map(schedule.intersect_domain(domain))\nprint_code(ast) # 这里生成的domain实际上被isl进行了优化.\nfor (int c0 = 2; c0 &lt;= 6; c0 += 1)\n  for (int c1 = c0 + 10; c1 &lt;= c0 + 19; c1 += 1)\n    S(c0 - 1, -c0 + c1 - 9);"
  },
  {
    "objectID": "posts/polyherdal-playground.html#ast-generation-for-constraint-sets",
    "href": "posts/polyherdal-playground.html#ast-generation-for-constraint-sets",
    "title": "Polyhedral Tutorials",
    "section": "AST Generation for Constraint Sets",
    "text": "AST Generation for Constraint Sets\n通常只有当满足特定条件时才可以使用一些transformation优化程序, 要始终实施这种优化方式的话, 那么需要将代码分为不同版本, 当满足特定条件时执行优化后的代码, 否则执行非优化的代码.\n一个简单的例子:\nvoid foo(long n, A[][100]) {\n  for (i = 0; i &lt; n; i++)\n    for (j = 0; j &lt; n; j++)\n      A[i][j] += i+j;\n}\n这个代码代码当j小于100时会有更简单的依赖, 也就是要求n&lt;100. 那么就可以使用如下的方法来保证优化后不出错误.\nvoid foo(long n, A[][100]) {\n  if (n &lt; 100)\n    // optimized code\n  else\n    // otherwise\n}\nisl的AST generator允许生成任意来自isl的约束bool条件集合."
  },
  {
    "objectID": "posts/polyherdal-playground.html#a-simple-constraint-set",
    "href": "posts/polyherdal-playground.html#a-simple-constraint-set",
    "title": "Polyhedral Tutorials",
    "section": "A simple constraint set",
    "text": "A simple constraint set\ncondition = isl.Set(\"[n] -&gt; {: n &lt; 100}\")\nexpr = build.expr_from_set(condition)\nprint(expr.to_C_str())\nn &lt;= 99"
  },
  {
    "objectID": "posts/polyherdal-playground.html#recovery-of-modulo-expressions",
    "href": "posts/polyherdal-playground.html#recovery-of-modulo-expressions",
    "title": "Polyhedral Tutorials",
    "section": "Recovery of modulo expressions",
    "text": "Recovery of modulo expressions\ncondition = isl.Set(\"[n, m] -&gt; {: n % 100 = 2}\")\nexpr = build.expr_from_set(condition)\nprint(expr.to_C_str())\n(n - 2) % 100 == 0"
  },
  {
    "objectID": "posts/polyherdal-playground.html#verification-of-complex-conditions",
    "href": "posts/polyherdal-playground.html#verification-of-complex-conditions",
    "title": "Polyhedral Tutorials",
    "section": "Verification of complex conditions",
    "text": "Verification of complex conditions\ncondition = isl.Set(\"[n, m] -&gt; {: (n != 0 implies m = 5) or n + m = 42}\")\nexpr = build.expr_from_set(condition)\nprint(expr.to_C_str())\nm == 5 || n == 0 || n + m == 42"
  },
  {
    "objectID": "posts/pip-err.html",
    "href": "posts/pip-err.html",
    "title": "安装conda之后pip执行全局的问题",
    "section": "",
    "text": "今天我要装3个Tensorflow但是我发现每次装一个就卸载前面那个,我很奇怪,然后就发现了这个大问题!\n\n\n问题描述\n我是发现我which出来的pip和执行的pip不是同一个pip:\n(tf1.12) ➜  TensorFlow2.0Tutorials-master which pip  \n/home/zqh/miniconda3/envs/tf1.12/bin/pip\n(tf1.12) ➜  TensorFlow2.0Tutorials-master pip --version              \npip 19.0.3 from /home/zqh/.local/lib/python3.6/site-packages/pip (python 3.6)\n让我很难受啊.\n\n\n解决方案\n直接卸载全局的pip! 然后都用虚拟环境.记得卸载前先关闭虚拟环境.\n➜  gitio python3 -m pip uninstall pip\nUninstalling pip-19.0.3:\n  Would remove:\n    /home/zqh/.local/bin/pip\n    /home/zqh/.local/bin/pip3\n    /home/zqh/.local/bin/pip3.6\n    /home/zqh/.local/lib/python3.6/site-packages/pip-19.0.3.dist-info/*\n    /home/zqh/.local/lib/python3.6/site-packages/pip/*\nProceed (y/n)? y\n➜  gitio sudo python3 -m pip uninstall pip"
  },
  {
    "objectID": "posts/pfld.html",
    "href": "posts/pfld.html",
    "title": "PFLD总结",
    "section": "",
    "text": "PFLD算法是我今年8月复现的,当时没有写总结,现在补上.\n\n\n网络设计\n  PFLD的模型相对简单,但是想法的确挺不错的.首先使用一个mobilenet或者别的网络做backbone,和传统的目标检测模型相同,抽取三个不同层次的feature map并汇总作为landmark输出,并且从网络中再抽取一个feature map用于预测euler angles.\n\n\n标签制作\n这里的制作label时会计算当前整个batch的样本数量,并根据人脸的属性类别去计算属性权重attribute_weight,然后就是归一化的landmark与euler angles.\n\n\n损失计算\n标签中包含landmark、euler angles、attribute weight，预测结果包含pred_landmark、pred euler angles。对于landmark的回归直接mse，对于euler angles的回归为\\(1-\\cos(\\text{abs}(true\\ eular-pred\\ eular))\\)\n\n\n推理\n推理就直接把所有landmark乘上输入图像大小就好了。"
  },
  {
    "objectID": "posts/param-free-nsf.html",
    "href": "posts/param-free-nsf.html",
    "title": "Parameter-Free Style Projection for Arbitrary Style Transfer",
    "section": "",
    "text": "这是一篇来自百度的风格迁移论文，提出了一种无参数的风格特征投影方法对原始图像进行风格迁移。下面来读读吧："
  },
  {
    "objectID": "posts/param-free-nsf.html#style-projection-algorithm",
    "href": "posts/param-free-nsf.html#style-projection-algorithm",
    "title": "Parameter-Free Style Projection for Arbitrary Style Transfer",
    "section": "Style Projection Algorithm",
    "text": "Style Projection Algorithm\n其实这个方法就是将style feature对content feature做一个投影，所以起这个名字。首先我们提取到源图像的content feature为\\(C\\cdot H\\cdot W\\)，参考图像的style feature也为\\(C\\cdot H\\cdot W\\)，然后分别Flatten为\\(C \\cdot V,V=H*W\\)，得到了content vector，style vector。然后根据数值大小对刚刚两个向量进行升序排列，得到了content sorting index和style sorting index。接下来最重要的Style Projection操作，用content sorting index把style vector重新索引一下得到\\(z\\)就完事了。\n\n作者还提了一下Style Projection和传统的Gram的区别，说Gram是用于测量纹理的表征，并且通过实验发现Style feature的重组并不影响Gram，也就是说Style Projection更好，并且因为是style feature按content feature重组所以内空不会被丢失(当然我觉得内容不丢失主要还是他的模型直接将content跳跃连接到了decoder)。下面还做了一下几种方法的区别，意思就是不加入原图信息，投影后的style feature也能体现原图的结构："
  },
  {
    "objectID": "posts/param-free-nsf.html#the-learning-of-style-transfer-model",
    "href": "posts/param-free-nsf.html#the-learning-of-style-transfer-model",
    "title": "Parameter-Free Style Projection for Arbitrary Style Transfer",
    "section": "The Learning of Style Transfer Model",
    "text": "The Learning of Style Transfer Model\n这个方法的Decoder还是得训练的，流程如下面模型结构图所示，损失主要分为三部分：\n\n\nstyle loss\n\n将参考风格图像\\(s\\)和风格化图像\\(\\hat{c}\\)，输入值预训练的Encoder中，提取每一层的输出并统计均值和方差，均值和方差的一致性即为style loss\n\\[\n\\begin{aligned}\n\\mathcal{L}_{s}=& \\sum_{i=1}^{N}\\left\\|\\mu\\left(E_{i}(s)\\right)-\\mu\\left(E_{i}(\\hat{c})\\right)\\right\\|_{2} \\\\\n&+\\sum_{i=1}^{N}\\left\\|\\sigma\\left(E_{i}(s)\\right)-\\sigma\\left(E_{i}(\\hat{c})\\right)\\right\\|_{2}\n\\end{aligned}\n\\]\n其中\\(\\mu,\\sigma\\)分别是均值和方差。\n\ncontent perceptual loss\n\n为了保存更加完备的原图细节，利用风格化图像的和原图的像素插值作为内容损失。\n\\[\n\\begin{aligned}\n\\mathcal{L}_{p}=\\|E(c)-E(\\hat{c})\\|_{2}\n\\end{aligned}\n\\]\n\nKL loss\n\n作者认为之前NSF的不自然生成结果可能是缺乏语义信息，所以来点分布匹配。不过我觉得这种损失就是加了一般是有用的，但是很难解释这个到底是不是语义信息。\n\\[\n\\begin{aligned}\n\\mathcal{L}_{K L}=\\mathcal{K} \\mathcal{L}[E(c) \\| E(\\hat{c})]\n\\end{aligned}\n\\]\n总损失即为求和：\n\\[\n\\begin{aligned}\n\\mathcal{L}=\\mathcal{L}_{p}+\\lambda \\mathcal{L}_{s}+\\kappa \\mathcal{L}_{K L}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/openpose.html",
    "href": "posts/openpose.html",
    "title": "OpenPose人体姿态估计",
    "section": "",
    "text": "我参考tf-pose-estimation重新构建一个人体姿态估计模型,下面主要记录一下坑点.\n\n\n标签制作\n\ntensorflow重写heatmap失败.\n\n这个姿态估计其实和CenterNet是差不多的实现思路,构建标签的时候都是通过对图像的对应位置放置高斯分布的方式制作出heatmap,我一开始看懂了之后就准备直接用tensorflow的方式重写,结果如下:\n  def get_heatmap(self, im_h, im_w, joint_list, th=4.6052, sigma=8.):\n\n    heatmap: tf.Variable = tf.Variable(tf.zeros((self.hparams.parts, im_h, im_w)), trainable=False)\n\n    for joints in joint_list:\n      for i, center in enumerate(joints):\n        if center[0] &lt; 0 or center[1] &lt; 0:\n          continue\n\n        delta = tf.sqrt(th * 2)\n        # p0 -&gt; x,y    p1 -&gt; x,y\n        im_wh = tf.cast((im_w, im_h), tf.float32)\n        p0 = tf.cast(tf.maximum(0., center - delta * sigma), tf.int32)\n        p1 = tf.cast(tf.minimum(im_wh, center + delta * sigma), tf.int32)\n\n        x = tf.range(p0[0], p1[0])[None, :, None]\n        y = tf.range(p0[1], p1[1])[:, None, None]\n\n        p = tf.concat([x + tf.zeros_like(y), tf.zeros_like(x) + y], axis=-1)\n        exp = tf.reduce_sum(tf.square(tf.cast(p, tf.float32) - center), -1) / (2. * sigma * sigma)\n        # use indices update point area\n        indices = tf.concat([tf.ones(p.shape[:-1] + [1], tf.int32) * i,\n                             p[..., ::-1]], -1)\n        # NOTE p is [x,y] , but `gather_nd` and `scatter_nd` require [y,x]\n        old_center_area = tf.gather_nd(heatmap, indices)\n        center_area = tf.minimum(tf.maximum(old_center_area, tf.exp(-exp)), 1.0)\n        center_area = tf.where(exp &gt; th, old_center_area, center_area)\n\n        heatmap.scatter_nd_update(indices, center_area)\n    # use indices update heatmap background NOTE scatter_nd can't use -1\n    heatmap.scatter_update(tf.IndexedSlices(\n        tf.clip_by_value(1. - tf.reduce_max(heatmap, axis=0), 0., 1.),\n        self.hparams.parts - 1))\n\n    heatmap_tensor = tf.transpose(heatmap, (1, 2, 0))\n\n    if self.target_hw:\n      heatmap_tensor = tf.image.resize(heatmap_tensor, self.target_hw)\n\n    return heatmap_tensor\n到这里其实是可以正确运行的,但是问题在于joint_list(关键点注释)他是个数是不确定的,所以还得把这个函数改成动态迭代的(用tf.while_loop),调这个函数我都调到吐了..所以我暂时先不搞全tf化.\n\n向量化制作heatmap\n\n之前tf化输入管道太麻烦我就直接把他全部的输入管道复制过来,但是性能问题让我吐血…而且有个很奇特问题,我本来是用tensor_slices制作tf.dataset的,虽然速度慢但还是十几秒还是有一个batch.然后我改成tfrecord的方式,发现可能是前端读的数据太快,后面标签制作太慢反而导致一分钟才能读取一个batch…\n然后我又得重写numpy版的制作heatmap.\ndef get_heatmap_v(self, target_hw, joint_list_mask):\n  heatmap = np.zeros((self.coco_parts, self.height, self.width), dtype=np.float32)\n  height, width = self.height, self.width\n  for (joints, masks) in zip(self.joint_list, joint_list_mask):\n    for idx, (point, mask) in enumerate(zip(joints, masks)):\n      if mask:\n        th = 4.6052\n        delta = np.sqrt(th * 2)\n        p0 = np.maximum(0., point - delta * self.sigma).astype('int32')\n        p1 = np.minimum([width, height], point + delta * self.sigma).astype('int32')\n\n        x = np.arange(p0[0], p1[0])\n        y = np.arange(p0[1], p1[1])\n\n        xv, yv = np.meshgrid(x, y, sparse=False, indexing='xy')\n\n        exp = (((xv - point[0]) ** 2 + (yv - point[1]) ** 2) /\n                (2.0 * self.sigma * self.sigma))\n\n        yidx = yv[exp &lt; th]\n        xidx = xv[exp &lt; th]\n        exp_valid = exp[exp &lt; th]\n\n        heatmap[idx, yidx, xidx] = np.minimum(\n            np.maximum(heatmap[idx, yidx, xidx],\n                        np.exp(-exp_valid)), 1.0)\n\n  heatmap = heatmap.transpose((1, 2, 0))\n\n  # background\n  heatmap[:, :, -1] = np.clip(1 - np.amax(heatmap, axis=2), 0.0, 1.0)\n\n  if target_hw:\n    heatmap = cv2.resize(heatmap, target_hw[::-1], interpolation=cv2.INTER_LINEAR)\n\n  return heatmap\n\ndef get_heatmap(self, target_hw):\n  heatmap = np.zeros((self.coco_parts, self.height, self.width), dtype=np.float32)\n\n  for joints in self.joint_list:\n    for idx, point in enumerate(joints):\n      if point[0] &lt; 0 or point[1] &lt; 0:\n        continue\n      ImageMeta.put_heatmap(heatmap, idx, point, self.sigma)\n\n  heatmap = heatmap.transpose((1, 2, 0))\n\n  # background\n  heatmap[:, :, -1] = np.clip(1 - np.amax(heatmap, axis=2), 0.0, 1.0)\n\n  if target_hw:\n    heatmap = cv2.resize(heatmap, target_hw[::-1], interpolation=cv2.INTER_LINEAR)\n\n  return heatmap\n\n@staticmethod\ndef put_heatmap(heatmap, plane_idx, center, sigma):\n  center_x, center_y = center  # point\n  _, height, width = heatmap.shape[:3]  # 热图大小\n\n  th = 4.6052\n  delta = math.sqrt(th * 2)\n\n  x0 = int(max(0, center_x - delta * sigma))\n  y0 = int(max(0, center_y - delta * sigma))\n\n  x1 = int(min(width, center_x + delta * sigma))\n  y1 = int(min(height, center_y + delta * sigma))\n\n  cnt = 0\n  uncnt = 0\n  for y in range(y0, y1):\n    for x in range(x0, x1):\n      d = (x - center_x) ** 2 + (y - center_y) ** 2\n      exp = d / 2.0 / sigma / sigma\n      # NOTE 如果这个点不是靠近图像边沿,exp就是th的两倍\n      if exp &gt; th:\n        continue\n      heatmap[plane_idx][y][x] = max(heatmap[plane_idx][y][x], math.exp(-exp))\n      heatmap[plane_idx][y][x] = min(heatmap[plane_idx][y][x], 1.0)\n然后看一下耗时对比,直接提高32倍~\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n   666                                           @func_line_time([])\n   667                                           def dev_vector_get_heatmap():\n   668                                             \"\"\" 构造一个向量化的heatmap函数,并测试速度 \"\"\"\n   669         1     653201.0 653201.0      1.3    h, ds = get_raw_dataset()\n   670                                             h: OpenPoseHelper\n   671         1       8555.0   8555.0      0.0    iters = iter(ds)\n   672       101        142.0      1.4      0.0    for i in range(100):\n   673       100     319597.0   3196.0      0.6      img, joint_list = next(iters)\n   674       100      12173.0    121.7      0.0      img, joint_list = img.numpy(), joint_list.numpy()\n   675                                           \n   676       100       3520.0     35.2      0.0      meta = ImageMeta(img, joint_list, h.in_hw, h.hparams.parts, h.hparams.vecs, h.hparams.sigma)\n   677                                           \n   678       100   48977247.0 489772.5     95.1      heatmap = meta.get_heatmap(h.target_hw)\n   679       100       4721.0     47.2      0.0      joint_list_mask = np.logical_not(np.all(joint_list == -1000, -1, keepdims=True))\n   680       100    1489782.0  14897.8      2.9      heatmap_v = meta.get_heatmap_v(h.target_hw, joint_list_mask)\n   681                                           \n   682       100      25532.0    255.3      0.0      print(np.allclose(heatmap, heatmap_v))\n我发现我有时候就是太喜欢过早优化代码..这真的是万恶之源,很容易吃力不讨好.一开始直接用numpy重写就能顺利提高速度还不用费那么多时期."
  },
  {
    "objectID": "posts/numpyslice.html",
    "href": "posts/numpyslice.html",
    "title": "numpy切片中的坑",
    "section": "",
    "text": "今天我需要在大矩阵中提取小矩阵，百度了一波之后，我以为他与matlab中一样，可以用i+:，但是我用了之后才发现他只支持了一半，把我坑到了、\n\n\n例子\n一个简单的程序，从[200,200]的矩阵中取出[20,20]的小矩阵.\nx = np.arange(400*100).reshape(200, 200)\nprint(x.shape)\n\nfor j in range(10):\n    for i in range(10):\n        print(x[j*20+0:20, i*20+0:20].shape)\n观察输出,发现只有第一次取的矩阵是正确的:\n(200, 200)\n(20, 20)\n(20, 0)\n(20, 0)\n(20, 0)\n(20, 0)\n(20, 0)\n(20, 0)\n(20, 0)\n(20, 0)\n(20, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 20)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n\n\n错误解析\n看这里\nx[j*20+0:20, i*20+0:20]\n其中\nj*20+0:20\n这里其实是python不支持matlab中先那样,从0:20中创建序列然后再对序列每一个元素做加法.\n而在python中是把他自动解析成两部分的:(j*20+0):(20),所以循环下去就会出现问题\n\n\n修改如下\n按照python中的规则即可~~\n(就是写起来好不爽\nx = np.arange(400*100).reshape(200, 200)\nprint(x.shape)\n\nfor j in range(10):\n    for i in range(10):\n        print(x[j*20+0:j*20+20, i*20+0:i*20+20].shape)"
  },
  {
    "objectID": "posts/npu-backend.html",
    "href": "posts/npu-backend.html",
    "title": "带宽受限下的DSA后端优化",
    "section": "",
    "text": "目前对于许多端侧NPU来说，是由一个可编程操作但容量较小的SRAM进行数据调度，需要尽可能的减少数据搬运, 从而避免DSA中的计算单元处于空闲状态[^1]。\n因此我们要解决的问题是: 1. 如何充分利用Local Memory并在其中计算尽可能多的kernel? 2. 如何调度Local Memory中的内存/指令从而充分利用计算单元?\n本文主要分享关于Fused Layer内部的Buffer Schedule与Instruction Schedule的一些经验体会."
  },
  {
    "objectID": "posts/npu-backend.html#无流水时情况",
    "href": "posts/npu-backend.html#无流水时情况",
    "title": "带宽受限下的DSA后端优化",
    "section": "2.1 无流水时情况",
    "text": "2.1 无流水时情况\n最简单的执行策略是将每个Tile中的Tensor Operation串行执行, 假设三个卷积的情况如下:\n\n此时我们可以在计算上一个结果时加载下一个操作所需要的数据,但是通常对于神经网络来说,越后面的层Weights越大,在带宽与算力无法平衡的时候就会等待Load从而产生IDLE. 因此可以选择将Weights等参数长驻在Local Memory中,通过空间换时间(Trade-off项加一).\n\n\n这里我选择将Weights等参数常驻后, 为6层卷积的Fusion进行无Bank Conflict的Buffer Schedule, 结果如下:\n\n对于带宽受限的DSA来说, 虽然优化内部Buffer的布局可以更好的避免Bank Conflict从而提升计算效率,但是也会因为数据不连续导致Load/Store效率降低, Trade-off项加一."
  },
  {
    "objectID": "posts/npu-backend.html#soft-pipeline",
    "href": "posts/npu-backend.html#soft-pipeline",
    "title": "带宽受限下的DSA后端优化",
    "section": "2.2 Soft PipeLine",
    "text": "2.2 Soft PipeLine\n为了充分利用器件, 每个Tile之间的IDLE也需要进行消除. 通常的做法是开辟并行器件数个Buffer来进行计算, 最理想的状态是每个器件的工作时间等长:\n\n虽然Load/Store是可以并行工作的, 但是他们会抢占带宽资源, 此时还无法准确估计时间, 因此在带宽受限的场景下可以默认将他们视为同一个器件. 由于带宽受限的问题, 在三器件并行双Buffer的情况下很容易出现每一对Ping Pong之间出现冲突与空闲:\n\n因此需要通过量化估计的硬件执行时间来选择Fuse足够多的层或切分足够的大小来保证Compute Time &gt;= (Load Time + Store Time), 从而让计算器件连续工作.\n\n当硬件中还有其他计算设备存在的情况下, 情况会更加多样, 假设再增加一个计算器件时(这里假设计算设备时间为3:7,同时总时间大于Load + Store):\n\n如果只有两个Buffer的情况下是会导致计算器件产生空闲, 他们空闲时间的比例与计算时间比例相同. 那么为了充分利用两个计算器件, 就需要再开辟新的Buffer, 此时只会因为计算时间不同导致其中一个计算设备出现空闲. 总之, 在有多个计算设备的情况下, 要量化增加Buffer数量带来的并行时间收益与随之增加的ReCompute进行Trade-off.\n\n下面就是三块Buffer的实际分配情况, 可以发现为了减少Bank Conflict所造成的内存浪费是比想象中大的."
  },
  {
    "objectID": "posts/npu-backend.html#instruction-schedule",
    "href": "posts/npu-backend.html#instruction-schedule",
    "title": "带宽受限下的DSA后端优化",
    "section": "2.3 Instruction Schedule",
    "text": "2.3 Instruction Schedule\n当多层Fuse之后, 生成的指令也会随之增多, 因此会遇到指令阻塞的情况, 比如当Compute的指令过多导致一下个循环中Load指令下发不及时的问题:\n\n需要通过模拟指令队列来调整指令顺序, 实际上就是需要找到合适的Prefetch时机, 从而做到真正的流水."
  },
  {
    "objectID": "posts/nextpermutation.html",
    "href": "posts/nextpermutation.html",
    "title": "字典序",
    "section": "",
    "text": "直接上程序：\n\n#include &lt;algorithm&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\nusing namespace std;\n\nvoid swap(string::iterator A, string::iterator B) {\n    char tmp;\n    tmp= *A;\n    *A= *B;\n    *B= tmp;\n}\n\nint main(int argc, char const *argv[]) {\n    string ss;\n    string::iterator Pj, Pk;\n    cin &gt;&gt; ss;\n    while (1) {\n        /* 从右端起，找出第一个比右边小的位置j（j从左端开始计算） */\n        for (Pj= ss.end() - 2; Pj != ss.begin(); Pj--) {\n            if (*Pj &lt; *(Pj + 1)) {\n                break;\n            }\n        }\n        if (*Pj &gt;= *(Pj + 1)) { /* 没有该位置则结束 */\n            return 0;\n        } else {\n            /* 在Pj的右边找出所有比Pj大的数中最小的数字Pk */\n            for (Pk= ss.end() - 1; Pk &gt; Pj; Pk--) {\n                if (*Pk &gt; *Pj) {\n                    break;\n                }\n            }\n            /* 对换Pj，Pk */\n            swap(Pj, Pk);\n            /* 再将Pj之后的元素重新排列 */\n            reverse(Pj + 1, ss.end());\n            cout &lt;&lt; ss &lt;&lt; endl;\n        }\n    }\n    return 0;\n}"
  },
  {
    "objectID": "posts/nand2tetris-week4.html",
    "href": "posts/nand2tetris-week4.html",
    "title": "Nand2Tetris week4",
    "section": "",
    "text": "上周我不知道自己在做什么，好像很忙，但是啥东西都没做出来，太难受了。这周得改改自己的生活节奏了。这周的课程是Machine Language，应该是要讲如何设计ISA了。"
  },
  {
    "objectID": "posts/nand2tetris-week4.html#他是特定的软硬件接口",
    "href": "posts/nand2tetris-week4.html#他是特定的软硬件接口",
    "title": "Nand2Tetris week4",
    "section": "他是特定的软硬件接口",
    "text": "他是特定的软硬件接口\n\n他定义了硬件支持怎样的操作（数学操作、逻辑操作、控制流）\n定义操作执行的对象"
  },
  {
    "objectID": "posts/nand2tetris-week4.html#内存层次化",
    "href": "posts/nand2tetris-week4.html#内存层次化",
    "title": "Nand2Tetris week4",
    "section": "内存层次化",
    "text": "内存层次化\n\n访问内存地址开销是昂贵的\n\n需要支持长地址，那么位数需要大\n从内存中取数需要时间\n\n\n这也就是为什么要设计多个cache，越接近cpu的内存块越小，速度越快。\n其中最靠近cpu内存块就是registers，其主要存储一些cpu操作时的参数，比如做加法时，所操作的数据就存放在寄存器中："
  },
  {
    "objectID": "posts/nand2tetris-week2.html",
    "href": "posts/nand2tetris-week2.html",
    "title": "Nand2Tetris week2",
    "section": "",
    "text": "第二周，主要实现基于boolean的数学运算以及ALU\n\n\nBinary Addition\n着就和十进制加法类似，输入a与b，输出加和以及进位，迭代即可。为了实现的简洁，把加法器分为半加器（两个数相加输出和与进位）、全加器（三个数相加输出和与进位）、加法器，\n\n\nNegative Numbers\n如果我们要用一个位来表示正负，那么想当然用最左边那一位就好了。\n000  0\n001  1\n010  2\n011  3\n100  4\n101  5\n110  6\n111  7\n那么直观的来做一下：\n000  0\n001  1\n010  2\n011  3\n100  -0\n101  -1\n110  -2\n111  -3\n我们发现来这里有个不自然的点，就是000=0,100=-0，所以我们转变思路,用补码的方式实现负数，这样就避免了重复。 正数部分与负数部分分别表示为： \\[\n\\begin{aligned}\n  \\text{pos} &\\in [0\\ldots 2^{n-1}-1]\\\\\n  \\text{neg} &\\in [-1\\ldots -2^{n-1}]\n\\end{aligned}\n\\]\n|- 000  0 \n| |- 001  1\n| | |- 010  2     \n| | |  |- 011  3\n| | |  |- 100  -4  (4 - 8)\n| | |- 101  -3  (5 - 8) \n| |- 110  -2  (6 - 8)\n|- 111  -1  (7 - 8)\n观察以上数列，不难看出负数的计算公式为\\(x-2^n\\)\n接下来有趣的来了，对于负数的加法我们甚至不需要做什么特别的，按照整数相加的方式可以得到正确的结果:\n   -1       (7-8)                111\n+  -2     + (6-8)              + 110\n------  = -------- = ------  =  ------   \n   -3       13-16     5-8        1101 = 101 = -3\n并且对于两数相减，也可以直接用加上一个负数进行等价替换，非常方便。\n最后还有就是取负数，可以在上面的表中发现，每个正数的反码等于正数+1的负数，通过简单的数学变换即可证明，这里的\\(2^n-1\\)正好就是全1，全1减去x就是x去反，因此-x就是x取反后+1。 \\[\n\\begin{aligned}\n  2^n-x = 1 + ((2^n-1) - x)  \n\\end{aligned}\n\\]\n总之，我们不用设计更多的运算电路去完成减法等操作了。\n\n\nALU\nALU其实比较简单。就是这里不能用规约的写法，对于判断全0可能比较麻烦。所以需要自己编写一些辅助函数。\n还有就是不能取内部数据的子集，比较蛋疼。\n\n\nproject 2\n\n// File name: projects/02/ALU.hdl\n\n/**\n * The ALU (Arithmetic Logic Unit).\n * Computes one of the following functions:\n * x+y, x-y, y-x, 0, 1, -1, x, y, -x, -y, !x, !y,\n * x+1, y+1, x-1, y-1, x&y, x|y on two 16-bit inputs, \n * according to 6 input bits denoted zx,nx,zy,ny,f,no.\n * In addition, the ALU computes two 1-bit outputs:\n * if the ALU output == 0, zr is set to 1; otherwise zr is set to 0;\n * if the ALU output &lt; 0, ng is set to 1; otherwise ng is set to 0.\n */\n\n// Implementation: the ALU logic manipulates the x and y inputs\n// and operates on the resulting values, as follows:\n// if (zx == 1) set x = 0        // 16-bit constant\n// if (nx == 1) set x = !x       // bitwise not\n// if (zy == 1) set y = 0        // 16-bit constant\n// if (ny == 1) set y = !y       // bitwise not\n// if (f == 1)  set out = x + y  // integer 2's complement addition\n// if (f == 0)  set out = x & y  // bitwise and\n// if (no == 1) set out = !out   // bitwise not\n// if (out == 0) set zr = 1\n// if (out &lt; 0) set ng = 1\n\nCHIP ALU {\n    IN  \n        x[16], y[16],  // 16-bit inputs        \n        zx, // zero the x input?\n        nx, // negate the x input?\n        zy, // zero the y input?\n        ny, // negate the y input?\n        f,  // compute out = x + y (if 1) or x & y (if 0)\n        no; // negate the out output?\n\n    OUT \n        out[16], // 16-bit output\n        zr, // 1 if (out == 0), 0 otherwise\n        ng; // 1 if (out &lt; 0),  0 otherwise\n\n    PARTS:\n    // Put you code here:\n    Mux16(a=x, b=false, sel=zx, out=xout);\n    Not16(in=xout, out=notxout);\n    Mux16(a=xout, b=notxout, sel=nx, out=nxout);\n\n    Mux16(a=y, b=false, sel=zy, out=yout);\n    Not16(in=yout, out=notyout);\n    Mux16(a=yout, b=notyout, sel=ny, out=nyout);\n\n    Add16(a=nxout, b=nyout, out=addout);\n    And16(a=nxout, b=nyout, out=andout);\n\n    Mux16(a=andout, b=addout, sel=f, out=fout);\n\n    Not16(in=fout, out=notfout);\n    Mux16(a=fout, b=notfout, sel=no, out=finalout);\n    Or16(a=finalout, b=false, out=out);\n\n    // Adder16(a=finalout, b=true, out=o, carry=sign);\n    // Not(in=sign, out=zr);\n    Or16Way(in=finalout, out=sign);\n    Not(in=sign, out=zr);\n\n    Or16(a=finalout, b=false, out[15]=ng);\n}\n// File name: projects/02/Adder16.hdl\n\n/**\n * Adds two 16-bit values.\n * The most significant carry bit is ignored.\n */\n\nCHIP Add16 {\n    IN a[16], b[16];\n    OUT out[16];\n\n    PARTS:\n    // Put you code here:\n    HalfAdder(a=a[0], b=b[0], sum=out[0], carry=ca);\n    FullAdder(a=a[1], b=b[1], c=ca, sum=out[1], carry=cb);\n    FullAdder(a=a[2], b=b[2], c=cb, sum=out[2], carry=cc);\n    FullAdder(a=a[3], b=b[3], c=cc, sum=out[3], carry=cd);\n    FullAdder(a=a[4], b=b[4], c=cd, sum=out[4], carry=ce);\n    FullAdder(a=a[5], b=b[5], c=ce, sum=out[5], carry=cf);\n    FullAdder(a=a[6], b=b[6], c=cf, sum=out[6], carry=cg);\n    FullAdder(a=a[7], b=b[7], c=cg, sum=out[7], carry=ch);\n    FullAdder(a=a[8], b=b[8], c=ch, sum=out[8], carry=ci);\n    FullAdder(a=a[9], b=b[9], c=ci, sum=out[9], carry=cj);\n    FullAdder(a=a[10], b=b[10], c=cj, sum=out[10], carry=ck);\n    FullAdder(a=a[11], b=b[11], c=ck, sum=out[11], carry=cl);\n    FullAdder(a=a[12], b=b[12], c=cl, sum=out[12], carry=cm);\n    FullAdder(a=a[13], b=b[13], c=cm, sum=out[13], carry=cn);\n    FullAdder(a=a[14], b=b[14], c=cn, sum=out[14], carry=co);\n    FullAdder(a=a[15], b=b[15], c=co, sum=out[15], carry=cp);\n}\n// File name: projects/02/Adder16.hdl\n\n/**\n * Adds two 16-bit values.\n * The most significant carry bit is ignored.\n */\n\nCHIP Adder16 {\n    IN a[16], b[16];\n    OUT out[16], carry;\n\n    PARTS:\n    // Put you code here:\n    HalfAdder(a=a[0], b=b[0], sum=out[0], carry=ca);\n    FullAdder(a=a[1], b=b[1], c=ca, sum=out[1], carry=cb);\n    FullAdder(a=a[2], b=b[2], c=cb, sum=out[2], carry=cc);\n    FullAdder(a=a[3], b=b[3], c=cc, sum=out[3], carry=cd);\n    FullAdder(a=a[4], b=b[4], c=cd, sum=out[4], carry=ce);\n    FullAdder(a=a[5], b=b[5], c=ce, sum=out[5], carry=cf);\n    FullAdder(a=a[6], b=b[6], c=cf, sum=out[6], carry=cg);\n    FullAdder(a=a[7], b=b[7], c=cg, sum=out[7], carry=ch);\n    FullAdder(a=a[8], b=b[8], c=ch, sum=out[8], carry=ci);\n    FullAdder(a=a[9], b=b[9], c=ci, sum=out[9], carry=cj);\n    FullAdder(a=a[10], b=b[10], c=cj, sum=out[10], carry=ck);\n    FullAdder(a=a[11], b=b[11], c=ck, sum=out[11], carry=cl);\n    FullAdder(a=a[12], b=b[12], c=cl, sum=out[12], carry=cm);\n    FullAdder(a=a[13], b=b[13], c=cm, sum=out[13], carry=cn);\n    FullAdder(a=a[14], b=b[14], c=cn, sum=out[14], carry=co);\n    FullAdder(a=a[15], b=b[15], c=co, sum=out[15], carry=carry);\n}\n// File name: projects/02/FullAdder.hdl\n\n/**\n * Computes the sum of three bits.\n */\n\nCHIP FullAdder {\n    IN a, b, c;  // 1-bit inputs\n    OUT sum,     // Right bit of a + b + c\n        carry;   // Left bit of a + b + c\n\n    PARTS:\n    // Put you code here:\n    HalfAdder(a=a, b=b, sum=sumab, carry=carryab);\n    Xor(a=sumab, b=c, out=sum);\n    And(a=a, b=b, out=aband);\n    And(a=b, b=c, out=bcand);\n    And(a=a, b=c, out=acand);\n    Or(a=aband, b=bcand, out=abcor);\n    Or(a=abcor, b=acand, out=carry);\n}\n\n/**\n * Computes the sum of two bits.\n */\n\nCHIP HalfAdder {\n    IN a, b;    // 1-bit inputs\n    OUT sum,    // Right bit of a + b \n        carry;  // Left bit of a + b\n\n    PARTS:\n    // Put you code here:\n    Xor(a=a, b=b, out=sum);\n    And(a=a, b=b, out=carry);\n}\n\n// File name: projects/02/Inc16.hdl\n\n/**\n * 16-bit incrementer:\n * out = in + 1 (arithmetic addition)\n */\n\nCHIP Inc16 {\n    IN in[16];\n    OUT out[16];\n\n    PARTS:\n   // Put you code here:\n    HalfAdder(a=in[0], b=true, sum=out[0], carry=ca);\n    FullAdder(a=in[1], b=false, c=ca, sum=out[1], carry=cb);\n    FullAdder(a=in[2], b=false, c=cb, sum=out[2], carry=cc);\n    FullAdder(a=in[3], b=false, c=cc, sum=out[3], carry=cd);\n    FullAdder(a=in[4], b=false, c=cd, sum=out[4], carry=ce);\n    FullAdder(a=in[5], b=false, c=ce, sum=out[5], carry=cf);\n    FullAdder(a=in[6], b=false, c=cf, sum=out[6], carry=cg);\n    FullAdder(a=in[7], b=false, c=cg, sum=out[7], carry=ch);\n    FullAdder(a=in[8], b=false, c=ch, sum=out[8], carry=ci);\n    FullAdder(a=in[9], b=false, c=ci, sum=out[9], carry=cj);\n    FullAdder(a=in[10], b=false, c=cj, sum=out[10], carry=ck);\n    FullAdder(a=in[11], b=false, c=ck, sum=out[11], carry=cl);\n    FullAdder(a=in[12], b=false, c=cl, sum=out[12], carry=cm);\n    FullAdder(a=in[13], b=false, c=cm, sum=out[13], carry=cn);\n    FullAdder(a=in[14], b=false, c=cn, sum=out[14], carry=co);\n    FullAdder(a=in[15], b=false, c=co, sum=out[15], carry=cp);\n}\n// File name: projects/01/Or8Way.hdl\n\n/**\n * 8-way Or: \n * out = (in[0] or in[1] or ... or in[7])\n */\n\nCHIP Or16Way {\n    IN in[16];\n    OUT out;\n\n    PARTS:\n    // Put your code here:\n    Or8Way(in=in[0..7], out=a);\n    Or8Way(in=in[8..15], out=b);\n    Or(a=a, b=b, out=out);\n}"
  },
  {
    "objectID": "posts/mxnet2tflite.html",
    "href": "posts/mxnet2tflite.html",
    "title": "mxnet模型转tflite",
    "section": "",
    "text": "今天尝试把insightface的模型转换到tflite格式，在此做个记录。\n\n\n安装依赖\n转换工具使用的是微软的mmdnn，感觉好像有段时间没更新了，还是需要手动修改一些地方才可以正常运行。\npip install mxnet-cu101mkl scikit-learn mmdnn tensorflow==1.15.2\n\n修改np.load默认参数\n因为现在的numpy默认load的时候allow_pickle=False，所以需要进入mmdnn中全局搜索np.load，并修改其参数，我这次是mxnet转tensorflow，就只要改conversion/common/DataStructure/emitter.py和conversion/tensorflow/tensorflow_emitter.py文件就好了。\n\n\n\n下载模型\n从insightface的modell zoo中下载一个模型，我下载的是mobilenet模型\n\n\n修改模型\n我下载的这个模型可能是因为之前训练的模型构建问题，有一个名为pre_fc1的层的权重参数却不叫pre_fc1_weight，会导致转换出错。\n\n需要修改一下模型参数名称。我网上看了下没发现有好的修改param参数名称的方法，直接替换json和param文件中的fc1_weight为pre_fc1_weight会导致加载失败，因此我就将mmdnn/conversion/mxnet/mxnet_parser.py中410行改成如下：\nif source_node.name=='pre_fc1':\n    weight = self.weight_data.get('fc1' + \"_weight\").asnumpy().transpose((1, 0))\nelse:\n    weight = self.weight_data.get(source_node.name + \"_weight\").asnumpy().transpose((1, 0))\n同时，因为insightface的模型输入时的归一化操作被固定在了模型中，因此需要修改网络，我找了半天也没找到办法。。于是就直接修改json文件，使用下列程序删除前面两个节点：\n s= json.load(open('/home/zqh/workspace/insightface/test/model-symbol.json.bak'))\n \n for nodes in s['nodes']:\n     for inputs in nodes['inputs']:\n         inputs[0]=inputs[0]-2\n \n for i in range( len(s['arg_nodes'])):\n     s['arg_nodes'][i]=s['arg_nodes'][i]-2\n \n for i in range( len(s['node_row_ptr'])):\n     s['node_row_ptr'][i]=s['node_row_ptr'][i]-2\n\n \n for heads in s['heads']:\n     heads[0]=heads[0]-2\n \n ss=json.dumps(s)\n with open('/home/zqh/workspace/insightface/test/model-symbol.json','w') as f:\n     print(ss, file=f)\nNOTE： 上面的代码只是修改节点索引，后面需要手动把模型json文件不需要的节点删除。\n\n\n转换模型\n运行命令：\nmmconvert -sf mxnet -in ../models/model-symbol.json -iw ../models/model-0000.params  -df tensorflow -om mbv1face --inputShape 3,112,112 --dump_tag SERVING\n即可得到tensorflow.savemodel格式的模型，再利用toco：\ntoco --saved_model_dir mbv1face/ ----output_format tflite --output_file mbv1face.tflite"
  },
  {
    "objectID": "posts/monte-carlo.html",
    "href": "posts/monte-carlo.html",
    "title": "蒙特卡洛法",
    "section": "",
    "text": "在实际过程中,有的算法不能保证每次都能的到正确的解.蒙特卡洛算法则是在一般情况下可以保证对问题的所有实例都以高概率给出正确解,但是通常无法判定一个具体解是否正确."
  },
  {
    "objectID": "posts/monte-carlo.html#主元素问题",
    "href": "posts/monte-carlo.html#主元素问题",
    "title": "蒙特卡洛法",
    "section": "主元素问题",
    "text": "主元素问题\n设\\(T[1:n]\\)是一个含有\\(n\\)个元素的数组,当\\(|\\ \\{i\\ |\\ T[i]=x\\}\\ |&gt;\\frac{n}{2}\\)时,称元素\\(x\\)是数组\\(T\\)的主元素.\n编写一个程序如下:\nRandomNumber rnd;\ntemplate&lt;class Type&gt; bool Majority(Type *T, int n) {\n    // 判断主元素的蒙卡罗特方法\n    int i= rnd.Random(n) + 1;\n    Type x= T[i];\n    int k= 0;\n    // 统计主元个数\n    for (int j= 1; j &lt;= n; ++j) {\n        if (T[j] == x) { k++; }\n    }\n    return (k &gt; n / 2); //返回是否为主元\n}\n上述算法是一个偏真的\\(\\frac{1}{2}\\)算法,因为当此数组里面没有主元时,必然返回False,否则将以大于$ $的概率返回True."
  },
  {
    "objectID": "posts/monte-carlo.html#执行两次",
    "href": "posts/monte-carlo.html#执行两次",
    "title": "蒙特卡洛法",
    "section": "执行两次",
    "text": "执行两次\n执行两次上述程序:\ntemplate&lt;class Type&gt; bool Majority2(Type *T, int n) {\n    // 调用两次Majority算法\n    if (Majority(T, n)) {\n        return true;\n    } else {\n        return Majority(T, n);\n    }\n}\n如果\\(T\\)不含主元素 - 算法Majority2肯定返回False.\n如果\\(T\\)含有主元素:\n\n第一次算法Majority返回True的概率大于$ $,此时算法Majority2必然返回True\n第一次算法Majority返回False的概率为\\(1-p\\),第二次算法Majority返回True的概率为p\n\n因此: \\[\n  p+(1-p)p=1-(1-p)^2&gt;\\frac{3}{4}  \n\\]"
  },
  {
    "objectID": "posts/monte-carlo.html#执行logfrac1varepsilon次",
    "href": "posts/monte-carlo.html#执行logfrac1varepsilon次",
    "title": "蒙特卡洛法",
    "section": "执行\\(\\log(\\frac{1}{\\varepsilon})次\\)",
    "text": "执行\\(\\log(\\frac{1}{\\varepsilon})次\\)\n此时的算法错误概率小于\\(\\varepsilon\\)\ntemplate&lt;class Type&gt; bool MajorityMC(Type *T, int n, double e) {\n    // 重复log(1/ε)次调用算法\n    int k= ceil(log(1 / e) / log(2));\n    for (int i= 1; i &lt;= k; ++i) {\n        if (Majority(T, n)) { return true; }\n    }\n    return false;\n}\n算法的时间复杂度为:\\(O(n\\log(\\frac{1}{\\varepsilon}))\\)"
  },
  {
    "objectID": "posts/mobilenet-test.html",
    "href": "posts/mobilenet-test.html",
    "title": "mobilenet测试",
    "section": "",
    "text": "我前段时间一直在利用23层卷积层实现yolo模型,但是前两天的训练结果显示我还是太naive.本来还准备成功了发布到git,现在就按照群里大佬的指点,用pre-train的mobilenet模型,后面再加上yolo.今天我就先测试了一下mobilenet模型效果.因为k210里面只能放5.9mb的模型,所以我准备了mobilenet_v1_0.5_224模型."
  },
  {
    "objectID": "posts/mobilenet-test.html#new_test_model.py",
    "href": "posts/mobilenet-test.html#new_test_model.py",
    "title": "mobilenet测试",
    "section": "new_test_model.py",
    "text": "new_test_model.py\nimport tensorflow as tf\nimport shutil\nfrom utils import *\n\n\ndef load_pb_graph(pb_path: str)-&gt;tf.Graph:\n    # 新建空白图\n    g = tf.Graph()\n    # 空白图列为默认图\n    with g.as_default():\n        # 二进制读取模型文件\n        with tf.gfile.GFile(pb_path, 'rb') as f:\n            # 新建GraphDef文件，用于临时载入模型中的图\n            graph_def = tf.GraphDef()\n            # GraphDef加载模型中的图\n            graph_def.ParseFromString(f.read())\n            # 在空白图中加载GraphDef中的图\n            tf.import_graph_def(graph_def, name='')\n    return g\n\n\ndef load_label():\n    label = ['其他']\n    with open('label.csv', 'r', encoding='utf-8') as r:\n        lines = r.readlines()\n        for l in lines:\n            l = l.strip()\n            arr = l.split(',')\n            label.append(arr[1])\n    return label\n\n\ndir_path = 'test_images'\nif os.path.exists(\"pb_graph\"):\n    shutil.rmtree(\"pb_graph\")\nos.mkdir(\"pb_graph\")\n\nif __name__ == \"__main__\":\n    # get the defalut graph\n    g = load_pb_graph('mobilenet_v1_0.5_224/mobilenet_v1_0.5_224_frozen.pb')\n    # wirter into file to view the\n    writer = tf.summary.FileWriter('pb_graph', g)\n    # get the model input tensor, shape should be [?,224,224,3]\n    in_X = g.get_tensor_by_name('input:0')\n    # get the model ouput tensor, shape is []\n    out_Y = g.get_tensor_by_name('MobilenetV1/Predictions/Reshape_1:0')\n    out_K = tf.nn.top_k(out_Y, k=3, sorted=True)\n\n    label = load_label()\n    images_path = [dir_path+'/'+n for n in os.listdir(dir_path)]\n\n    with tf.Session(graph=g) as sess:\n        for path in images_path:\n            img = read_img(path, (224, 224))[0]\n            classes, scores = sess.run([out_K.indices, out_K.values], feed_dict={in_X: img[np.newaxis, :, :, :]})  # type:np.ndarray\n            print('识别图像 '+path+' :')\n            for j in range(3):  # top 3\n                idx = classes[0][j]\n                score = scores[0][j]\n                print('\\tNo.', j, '类别:', label[idx], '概率:', score)\n            print()"
  },
  {
    "objectID": "posts/mobilenet-test.html#utils.py",
    "href": "posts/mobilenet-test.html#utils.py",
    "title": "mobilenet测试",
    "section": "utils.py",
    "text": "utils.py\nimport os\nimport numpy as np\nimport skimage  # ! skimage 0.14 not have rectangle_perimeter\nimport cv2\nfrom PIL import ImageFile\nimport tensorflow as tf\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\ndef read_img(image_path: str, input_size: tuple, resize=True)-&gt;tuple:\n    \"\"\"read image from the filesystem\n\n    Parameters\n    ----------\n    image_path : str\n        image path\n    input_size : tuple\n        the input size (height,width)\n    resize : bool, optional\n        set to resize the image (the default is True)\n\n    Returns\n    -------\n    tuple\n        out image source,output height, output width,input height,input width\n    \"\"\"\n    orig_img = skimage.io.imread(image_path)\n    if len(orig_img.shape) == 2:  # avoid input the gray image\n        orig_img = skimage.color.gray2rgb(orig_img)\n    orig_h, orig_w, _ = orig_img.shape\n    if resize:\n        out_img = skimage.transform.resize(orig_img, input_size, mode='reflect')\n    else:\n        out_img = orig_img\n    out_h, out_w, _ = out_img.shape\n    return out_img, out_h, out_w, orig_h, orig_w"
  },
  {
    "objectID": "posts/mlc-tutorial.html",
    "href": "posts/mlc-tutorial.html",
    "title": "机器学习编译概念科普",
    "section": "",
    "text": "带大家建立一个对机器学习编译的基本概念."
  },
  {
    "objectID": "posts/mlc-tutorial.html#机器学习编译中的关键要素",
    "href": "posts/mlc-tutorial.html#机器学习编译中的关键要素",
    "title": "机器学习编译概念科普",
    "section": "机器学习编译中的关键要素",
    "text": "机器学习编译中的关键要素\n让我们首先回顾一个两层神经网络模型执行的例子。\n在这个特定的模型中，我们通过展平输入图像中的像素来获取向量 (Vector)；然后，我们应用线性变换，将输入图像投影到长度为 200 的向量上，并运行ReLU 激活函数。最后，我们将其映射到长度为 10 的向量，向量的每个元素对应于图像属于该特定类别的可能性大小。\n张量 (Tensor) 是执行中最重要的元素。张量是表示神经网络模型执行的输入、输出和中间结果的多维数组。\n张量函数 (Tensor functions) 神经网络的“知识”被编码在权重和接受张量和输出张量的计算序列中。我们将这些计算称为张量函数。值得注意的是，张量函数不需要对应于神经网络计算的单个步骤。部分计算或整个端到端计算也可以看作张量函数。\n\n\n\nmlc-elem-transform\n\n\n我们有多种方法可以在特定环境中实现模型执行。 上面的例子展示了一个例子。 值得注意的是，有两个区别： 首先，第一个linear层和relu计算被折叠成一个 linear_relu 函数，这需要有一个特定的linear_relu的详细实现。 当然，现实世界的用例，linear_relu 可以通过各种代码优化技术来实现，其中一些技术在的后面的课程中会进行介绍。 机器学习编译的过程就是是将上图左侧的内容转换为右侧的过程。在不同的场景中，这个过程可以是手动完成的，也可以使用一些自动转换工具，或两者兼而有之。"
  },
  {
    "objectID": "posts/mlc-tutorial.html#元张量函数",
    "href": "posts/mlc-tutorial.html#元张量函数",
    "title": "机器学习编译概念科普",
    "section": "元张量函数",
    "text": "元张量函数\n在上一章的概述中，我们介绍到机器学习编译的过程可以被看作张量函数之间的变换。一个典型的机器学习模型的执行包含许多步将输入张量之间转化为最终预测的计算步骤，其中的每一步都被称为元张量函数 (primitive tensor function)。\n\n\n\n元张量函数\n\n\n在上面这张图中，张量算子 linear, add, relu 和 softmax 均为元张量函数。特别的是，许多不同的抽象能够表示（和实现）同样的元张量函数（正如下图所示）。我们可以选择调用已经预先编译的框架库（如 torch.add 和 numpy.add）并利用在 Python 中的实现。在实践中，元张量函数被例如 C 或 C++ 的低级语言所实现，并且在一些时候会包含一些汇编代码。\n\n\n\n同一个元张量函数的不同形式\n\n\n许多机器学习框架都提供机器学习模型的编译过程，以将元张量函数变换为更加专门的、针对特定工作和部署环境的函数。\n\n\n\n元张量函数间的变换\n\n\n上面这张图展示了一个元张量函数 add 的实现被变换至另一个不同实现的例子，其中在右侧的代码是一段表示可能的组合优化的伪代码：左侧代码中的循环被拆分出长度为 4 的单元，f32x4.add 对应的是一个特殊的执行向量加法计算的函数。"
  },
  {
    "objectID": "posts/mlc-tutorial.html#张量程序抽象",
    "href": "posts/mlc-tutorial.html#张量程序抽象",
    "title": "机器学习编译概念科普",
    "section": "张量程序抽象",
    "text": "张量程序抽象\n上一节谈到了对元张量函数变换的需要。为了让我们能够更有效地变换元张量函数，我们需要一个有效的抽象来表示这些函数。\n通常来说，一个典型的元张量函数实现的抽象包含了以下成分：存储数据的多维数组，驱动张量计算的循环嵌套以及计算部分本身的语句。\n\n\n\n元张量函数中的典型成分\n\n\n我们称这类抽象为张量程序抽象。张量程序抽象的一个重要性质是，他们能够被一系列有效的程序变换所改变。\n\n\n\n一个元张量函数的序列变换\n\n\n例如，我们能够通过一组变换操作（如循环拆分、并行和向量化）将上图左侧的一个初始循环程序变换为右侧的程序。\n\n张量程序抽象中的其它结构\n重要的是，我们不能任意地对程序进行变换，比方说这可能是因为一些计算会依赖于循环之间的顺序。但幸运的是，我们所感兴趣的大多数元张量函数都具有良好的属性（例如循环迭代之间的独立性）。\n张量程序可以将这些额外的信息合并为程序的一部分，以使程序变换更加便利。\n\n\n\n循环迭代作为张量程序的额外信息\n\n\n举个例子，上面图中的程序包含额外的 T.axis.spatial 标注，表明 vi 这个特定的变量被映射到循环变量 i，并且所有的迭代都是独立的。这个信息对于执行这个程序而言并非必要，但会使得我们在变换这个程序时更加方便。在这个例子中，我们知道我们可以安全地并行或者重新排序所有与 vi 有关的循环，只要实际执行中 vi 的值按照从 0 到 128 的顺序变化。"
  },
  {
    "objectID": "posts/mlc-tutorial.html#总结",
    "href": "posts/mlc-tutorial.html#总结",
    "title": "机器学习编译概念科普",
    "section": "总结",
    "text": "总结\n\n元张量函数表示机器学习模型计算中的单个单元计算。\n\n一个机器学习编译过程可以有选择地转换元张量函数的实现。\n\n张量程序是一个表示元张量函数的有效抽象。\n\n关键成分包括: 多维数组，循环嵌套，计算语句。\n程序变换可以被用于加速张量程序的执行。\n张量程序中额外的结构能够为程序变换提供更多的信息。"
  },
  {
    "objectID": "posts/mlc-tutorial.html#优化内存访问",
    "href": "posts/mlc-tutorial.html#优化内存访问",
    "title": "机器学习编译概念科普",
    "section": "优化内存访问",
    "text": "优化内存访问\n考虑matmul的native实现时, 在机器上是这样执行的:\n\n通常，处理器使用固定大小的cache line（通常为 64 字节）从内存加载数据。当迭代 A 的行时，我们在第一个条目上发生了缓存未命中。处理器的高速缓存行提取也将在其中保存接下来的 15 个浮点数，这是对高速缓存的良好利用。 然而，对于矩阵 B，我们沿着列走，每一步都会发生cache miss, 这个就产生了严重的内存开销.\n\n\n1. loop reorder\n为了解决对于B矩阵的非连续访问导致的cache miss问题, 我们重新排序了两个矩阵乘的循环, 改变对于B矩阵的访问顺序.\ntir_sch: te.Schedule = te.create_schedule([C.op, D.op, F.op])\nm,l = C.op.axis\n(k,) = C.op.reduce_axis\ntir_sch[C].reorder(m,k,l)\nm,n = F.op.axis\n(l,) = F.op.reduce_axis\ntir_sch[F].reorder(m,l,n)\n\nrt_lib = tvm.build(tir_sch, [A, B, E], target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd).mean} sec\")\ntvm.lower(tir_sch, [A, B, E]).show()\nTime cost of transformed sch.mod 0.4165279375 sec\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        C = T.allocate([1048576], \"float32\", \"global\")\n        F = T.allocate([3145728], \"float32\", \"global\")\n        C_1 = T.Buffer((1048576,), data=C)\n        for m in range(1024):\n            for l_init in range(1024):\n                C_1[m * 1024 + l_init] = T.float32(0.0)\n            for rk, l in T.grid(2048, 1024):\n                cse_var_1: T.int32 = m * 1024 + l\n                A_1 = T.Buffer((2097152,), data=A.data)\n                B_1 = T.Buffer((2097152,), data=B.data)\n                C_1[cse_var_1] = C_1[cse_var_1] + A_1[m * 2048 + rk] * B_1[rk * 1024 + l]\n        C_2 = T.Buffer((1048576,), data=C)\n        for m, l in T.grid(1024, 1024):\n            cse_var_2: T.int32 = m * 1024 + l\n            C_2[cse_var_2] = T.exp(C_1[cse_var_2])\n        for m in range(1024):\n            F_1 = T.Buffer((3145728,), data=F)\n            for n_init in range(3072):\n                F_1[m * 3072 + n_init] = T.float32(0.0)\n            for rl, n in T.grid(1024, 3072):\n                cse_var_3: T.int32 = m * 3072 + n\n                E_1 = T.Buffer((3145728,), data=E.data)\n                F_1[cse_var_3] = F_1[cse_var_3] + C_2[m * 1024 + rl] * E_1[rl * 3072 + n]\n\n\n\n\n2. tiling\n可以发现仅通过loop reorder就可以让执行速度提升10倍. 虽然现在每一次load的cache miss减少了, 但是考虑整个的矩阵乘执行过程:\n\n目前的计算模式是固定A矩阵的一行, 然后反复加载B矩阵的所有行, 但是考虑到cache如果为32kb时, 在这个例子中只能缓存2=32*1024/4/4096行的B矩阵, 那么也就是最多两行之后就后来的B矩阵数据就会将之前cache中存储的数据驱除出去, 为了解决这个问题, 我们的方法就是通过切分K循环, 让他分为多个block, 使得每个block内部的B矩阵将会被缓存在cache中:\n\ntir_sch: te.Schedule = te.create_schedule([C.op, D.op, F.op])\nm, l = C.op.axis\n(k,) = C.op.reduce_axis\nko, ki = tir_sch[C].split(k, 8)\ntir_sch[C].reorder(ko, m, ki, l)\n\nm, n = F.op.axis\n(l,) = F.op.reduce_axis\nlo, li = tir_sch[F].split(l, 8)\ntir_sch[F].reorder(lo, m, li, n)\n\nrt_lib = tvm.build(tir_sch, [A, B, E], target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd).mean} sec\")\ntvm.lower(tir_sch, [A, B, E]).show()\nTime cost of transformed sch.mod 0.3838193083 sec\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        C = T.allocate([1048576], \"float32\", \"global\")\n        F = T.allocate([3145728], \"float32\", \"global\")\n        C_1 = T.Buffer((1048576,), data=C)\n        for m_init, l_init in T.grid(1024, 1024):\n            C_1[m_init * 1024 + l_init] = T.float32(0.0)\n        for rk_outer, m, rk_inner, l in T.grid(256, 1024, 8, 1024):\n            cse_var_1: T.int32 = m * 1024 + l\n            A_1 = T.Buffer((2097152,), data=A.data)\n            B_1 = T.Buffer((2097152,), data=B.data)\n            C_1[cse_var_1] = C_1[cse_var_1] + A_1[m * 2048 + rk_outer * 8 + rk_inner] * B_1[rk_outer * 8192 + rk_inner * 1024 + l]\n        C_2 = T.Buffer((1048576,), data=C)\n        for m, l in T.grid(1024, 1024):\n            cse_var_2: T.int32 = m * 1024 + l\n            C_2[cse_var_2] = T.exp(C_1[cse_var_2])\n        F_1 = T.Buffer((3145728,), data=F)\n        for m_init, n_init in T.grid(1024, 3072):\n            F_1[m_init * 3072 + n_init] = T.float32(0.0)\n        for rl_outer, m, rl_inner, n in T.grid(128, 1024, 8, 3072):\n            cse_var_3: T.int32 = m * 3072 + n\n            E_1 = T.Buffer((3145728,), data=E.data)\n            F_1[cse_var_3] = F_1[cse_var_3] + C_2[m * 1024 + rl_outer * 8 + rl_inner] * E_1[rl_outer * 24576 + rl_inner * 3072 + n]\n\n\n可以发现虽然对reduce维度添加了tiling,但是最终的性能没有什么提升. 那是因为要考虑A/B/C矩阵都存储在cache中时, 很有可能B矩阵的一行都无法完全缓存起来, 因此实施tiling优化时通常是多个维度同时进行的:\n\ntir_sch: te.Schedule = te.create_schedule([C.op, D.op, F.op])\nm, l = C.op.axis\n(k,) = C.op.reduce_axis\nmo, mi = tir_sch[C].split(m, 16)\nko, ki = tir_sch[C].split(k, 8)\nlo, li = tir_sch[C].split(l, 8)\ntir_sch[C].reorder(mo, lo, ko, mi, ki, li)\n\nm, n = F.op.axis\n(l,) = F.op.reduce_axis\nmo, mi = tir_sch[F].split(m, 16)\nlo, li = tir_sch[F].split(l, 8)\nno, ni = tir_sch[F].split(n, 8)\ntir_sch[F].reorder(mo, no, lo, mi, li, ni)\n\nrt_lib = tvm.build(tir_sch, [A, B, E], target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd).mean} sec\")\ntvm.lower(tir_sch, [A, B, E]).show()\nTime cost of transformed sch.mod 0.3760715791 sec\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        C = T.allocate([1048576], \"float32\", \"global\")\n        F = T.allocate([3145728], \"float32\", \"global\")\n        C_1 = T.Buffer((1048576,), data=C)\n        for m_outer, l_outer in T.grid(64, 128):\n            for m_inner_init, l_inner_init in T.grid(16, 8):\n                C_1[m_outer * 16384 + m_inner_init * 1024 + l_outer * 8 + l_inner_init] = T.float32(0.0)\n            for rk_outer, m_inner, rk_inner, l_inner in T.grid(256, 16, 8, 8):\n                cse_var_2: T.int32 = l_outer * 8\n                cse_var_1: T.int32 = m_outer * 16384 + m_inner * 1024 + cse_var_2 + l_inner\n                A_1 = T.Buffer((2097152,), data=A.data)\n                B_1 = T.Buffer((2097152,), data=B.data)\n                C_1[cse_var_1] = C_1[cse_var_1] + A_1[m_outer * 32768 + m_inner * 2048 + rk_outer * 8 + rk_inner] * B_1[rk_outer * 8192 + rk_inner * 1024 + cse_var_2 + l_inner]\n        C_2 = T.Buffer((1048576,), data=C)\n        for m, l in T.grid(1024, 1024):\n            cse_var_3: T.int32 = m * 1024 + l\n            C_2[cse_var_3] = T.exp(C_1[cse_var_3])\n        for m_outer, n_outer in T.grid(64, 384):\n            F_1 = T.Buffer((3145728,), data=F)\n            for m_inner_init, n_inner_init in T.grid(16, 8):\n                F_1[m_outer * 49152 + m_inner_init * 3072 + n_outer * 8 + n_inner_init] = T.float32(0.0)\n            for rl_outer, m_inner, rl_inner, n_inner in T.grid(128, 16, 8, 8):\n                cse_var_5: T.int32 = n_outer * 8\n                cse_var_4: T.int32 = m_outer * 49152 + m_inner * 3072 + cse_var_5 + n_inner\n                E_1 = T.Buffer((3145728,), data=E.data)\n                F_1[cse_var_4] = F_1[cse_var_4] + C_2[m_outer * 16384 + m_inner * 1024 + rl_outer * 8 + rl_inner] * E_1[rl_outer * 24576 + rl_inner * 3072 + cse_var_5 + n_inner]\n\n\n\n\n3. loop fusion\nloop fusion是将两个循环中的语句放到一起, 减少对于同一个数据的重用距离, 增加了数据的局部性, 考虑我们的例子中, 当矩阵C计算结束后才能开始下一个exp的计算, 这个时候原本访问到最后一行的数据又被推出cache中重新加载第一行的数据. 此时时候可以把exp的计算放在第一个matmul的tile计算结束后立即计算.\n但是因为tvm的中缺少更加详细的分析, 导致某些合法的调度并没有被tvm te(tensor expression)的调度原语所支持, 因此接下来采用tvm后来提出Tensor IR抽象, 在tir中提供了比te(tensor expression)中更多的调度方式, 比如reindex/cache_index/merge/decompose_reduction等方法, 当无法通过调度原语来达到想要的优化变化时甚至可以直接通过手写的方法来支持.\n接下来先将前面的te抽象转换为tir抽象:\nprim_func = te.create_prim_func([A, B, E, F])\nprim_func.show()\n\n# from tvm.script import tir as T\n\n@T.prim_func\ndef main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\"), F: T.Buffer((1024, 3072), \"float32\")):\n    T.func_attr({\"tir.noalias\": T.bool(True)})\n    # with T.block(\"root\"):\n    C = T.alloc_buffer((1024, 1024))\n    D = T.alloc_buffer((1024, 1024))\n    for m, l, rk in T.grid(1024, 1024, 2048):\n        with T.block(\"C\"):\n            v_m, v_l, v_rk = T.axis.remap(\"SSR\", [m, l, rk])\n            T.reads(A[v_m, v_rk], B[v_rk, v_l])\n            T.writes(C[v_m, v_l])\n            with T.init():\n                C[v_m, v_l] = T.float32(0.0)\n            C[v_m, v_l] = C[v_m, v_l] + A[v_m, v_rk] * B[v_rk, v_l]\n    for m, l in T.grid(1024, 1024):\n        with T.block(\"D\"):\n            v_m, v_l = T.axis.remap(\"SS\", [m, l])\n            T.reads(C[v_m, v_l])\n            T.writes(D[v_m, v_l])\n            D[v_m, v_l] = T.exp(C[v_m, v_l])\n    for m, n, rl in T.grid(1024, 3072, 1024):\n        with T.block(\"F\"):\n            v_m, v_n, v_rl = T.axis.remap(\"SSR\", [m, n, rl])\n            T.reads(D[v_m, v_rl], E[v_rl, v_n])\n            T.writes(F[v_m, v_n])\n            with T.init():\n                F[v_m, v_n] = T.float32(0.0)\n            F[v_m, v_n] = F[v_m, v_n] + D[v_m, v_rl] * E[v_rl, v_n]\n\n\ntir中以block为单位来调度源代码, 这里通过tir来实现和之前te中相同的调度策略:\nf_nd = tvm.nd.empty((M, N), dtype=\"float32\")\ntir_sch = tir.Schedule(prim_func)\nm, l, k = tir_sch.get_loops('C')\nmo, mi = tir_sch.split(m, [M // 16, 16])\nko, ki = tir_sch.split(k, [K // 8, 8])\nlo, li = tir_sch.split(l, [L // 8, 8])\ntir_sch.reorder(mo, lo, ko, mi, ki, li)\n\nm, n, l = tir_sch.get_loops('F')\nmo, mi = tir_sch.split(m, [M // 16, 16])\nlo, li = tir_sch.split(l, [L // 8, 8])\nno, ni = tir_sch.split(n, [N // 8, 8])\ntir_sch.reorder(mo, no, lo, mi, li, ni)\n\nrt_lib = tvm.build(tir_sch.mod, target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd, f_nd).mean} sec\")\ntir_sch.mod.show()\nTime cost of transformed sch.mod 1.0223547624999998 sec\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\"), F: T.Buffer((1024, 3072), \"float32\")):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        # with T.block(\"root\"):\n        C = T.alloc_buffer((1024, 1024))\n        D = T.alloc_buffer((1024, 1024))\n        for m_0, l_0, rk_0, m_1, rk_1, l_1 in T.grid(64, 128, 256, 16, 8, 8):\n            with T.block(\"C\"):\n                v_m = T.axis.spatial(1024, m_0 * 16 + m_1)\n                v_l = T.axis.spatial(1024, l_0 * 8 + l_1)\n                v_rk = T.axis.reduce(2048, rk_0 * 8 + rk_1)\n                T.reads(A[v_m, v_rk], B[v_rk, v_l])\n                T.writes(C[v_m, v_l])\n                with T.init():\n                    C[v_m, v_l] = T.float32(0.0)\n                C[v_m, v_l] = C[v_m, v_l] + A[v_m, v_rk] * B[v_rk, v_l]\n        for m, l in T.grid(1024, 1024):\n            with T.block(\"D\"):\n                v_m, v_l = T.axis.remap(\"SS\", [m, l])\n                T.reads(C[v_m, v_l])\n                T.writes(D[v_m, v_l])\n                D[v_m, v_l] = T.exp(C[v_m, v_l])\n        for m_0, n_0, rl_0, m_1, rl_1, n_1 in T.grid(64, 384, 128, 16, 8, 8):\n            with T.block(\"F\"):\n                v_m = T.axis.spatial(1024, m_0 * 16 + m_1)\n                v_n = T.axis.spatial(3072, n_0 * 8 + n_1)\n                v_rl = T.axis.reduce(1024, rl_0 * 8 + rl_1)\n                T.reads(D[v_m, v_rl], E[v_rl, v_n])\n                T.writes(F[v_m, v_n])\n                with T.init():\n                    F[v_m, v_n] = T.float32(0.0)\n                F[v_m, v_n] = F[v_m, v_n] + D[v_m, v_rl] * E[v_rl, v_n]\n\n\n这里可以发现相同调度下, tir速度慢了许多, 这是由于tir的函数默认将init操作放到循环内部导致的, 我们暂时忽略这个问题. 只关注通过fusion之后是否能产生性能提升:\nmo, lo, ko, mi, ki, li = tir_sch.get_loops('C')\n\nm1, l1 = tir_sch.get_loops('D')\nm1o, m1i = tir_sch.split(m1, [M // 16, 16])\nl1o, l1i = tir_sch.split(l1, [L // 8, 8])\ntir_sch.reorder(m1o, l1o, m1i, l1i)\n\ntir_sch.merge(lo, l1o)\nrt_lib = tvm.build(tir_sch.mod, target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd, f_nd).mean} sec\")\ntir_sch.mod.show()\nTime cost of transformed sch.mod 0.8789533792 sec\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\"), F: T.Buffer((1024, 3072), \"float32\")):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        # with T.block(\"root\"):\n        C = T.alloc_buffer((1024, 1024))\n        D = T.alloc_buffer((1024, 1024))\n        for m_0_m, l_0_m in T.grid(64, 128):\n            for rk_0, m_1, rk_1, l_1 in T.grid(256, 16, 8, 8):\n                with T.block(\"C\"):\n                    v_m = T.axis.spatial(1024, m_0_m * 16 + m_1)\n                    v_l = T.axis.spatial(1024, l_0_m * 8 + l_1)\n                    v_rk = T.axis.reduce(2048, rk_0 * 8 + rk_1)\n                    T.reads(A[v_m, v_rk], B[v_rk, v_l])\n                    T.writes(C[v_m, v_l])\n                    with T.init():\n                        C[v_m, v_l] = T.float32(0.0)\n                    C[v_m, v_l] = C[v_m, v_l] + A[v_m, v_rk] * B[v_rk, v_l]\n            for m_1, l_1 in T.grid(16, 8):\n                with T.block(\"D\"):\n                    v_m = T.axis.spatial(1024, m_0_m * 16 + m_1)\n                    v_l = T.axis.spatial(1024, l_0_m * 8 + l_1)\n                    T.reads(C[v_m, v_l])\n                    T.writes(D[v_m, v_l])\n                    D[v_m, v_l] = T.exp(C[v_m, v_l])\n        for m_0, n_0, rl_0, m_1, rl_1, n_1 in T.grid(64, 384, 128, 16, 8, 8):\n            with T.block(\"F\"):\n                v_m = T.axis.spatial(1024, m_0 * 16 + m_1)\n                v_n = T.axis.spatial(3072, n_0 * 8 + n_1)\n                v_rl = T.axis.reduce(1024, rl_0 * 8 + rl_1)\n                T.reads(D[v_m, v_rl], E[v_rl, v_n])\n                T.writes(F[v_m, v_n])\n                with T.init():\n                    F[v_m, v_n] = T.float32(0.0)\n                F[v_m, v_n] = F[v_m, v_n] + D[v_m, v_rl] * E[v_rl, v_n]\n\n\n可以发现通过fusion elemwise的操作有微弱的速度提升. 其实还有更加激进的fusion, 这个例子中还可以合并两个matmul的m循环. 虽然合并两个循环的m循环是一个合法的优化, 但通过tir去merge是会触发错误的, 因此这里通过手动修改代码的方式来实现这个调度:\nfrom tvm.script import ir as I\nfrom tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n  @T.prim_func\n  def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\"), F: T.Buffer((1024, 3072), \"float32\")):\n    T.func_attr({\"tir.noalias\": T.bool(True)})\n    # with T.block(\"root\"):\n    C = T.alloc_buffer((1024, 1024))\n    D = T.alloc_buffer((1024, 1024))\n    for m_0_m in T.grid(64):\n      for l_0_m in T.grid(128):\n        for rk_0, m_1, rk_1, l_1 in T.grid(256, 16, 8, 8):\n          with T.block(\"C\"):\n            v_m = T.axis.spatial(1024, m_0_m * 16 + m_1)\n            v_l = T.axis.spatial(1024, l_0_m * 8 + l_1)\n            v_rk = T.axis.reduce(2048, rk_0 * 8 + rk_1)\n            T.reads(A[v_m, v_rk], B[v_rk, v_l])\n            T.writes(C[v_m, v_l])\n            with T.init():\n              C[v_m, v_l] = T.float32(0.0)\n            C[v_m, v_l] = C[v_m, v_l] + A[v_m, v_rk] * B[v_rk, v_l]\n        for m_1, l_1 in T.grid(16, 8):\n          with T.block(\"D\"):\n            v_m = T.axis.spatial(1024, m_0_m * 16 + m_1)\n            v_l = T.axis.spatial(1024, l_0_m * 8 + l_1)\n            T.reads(C[v_m, v_l])\n            T.writes(D[v_m, v_l])\n            D[v_m, v_l] = T.exp(C[v_m, v_l])\n      for n_0, rl_0, m_1, rl_1, n_1 in T.grid(384, 128, 16, 8, 8):\n        with T.block(\"F\"):\n          v_m = T.axis.spatial(1024, m_0_m * 16 + m_1)\n          v_n = T.axis.spatial(3072, n_0 * 8 + n_1)\n          v_rl = T.axis.reduce(1024, rl_0 * 8 + rl_1)\n          T.reads(D[v_m, v_rl], E[v_rl, v_n])\n          T.writes(F[v_m, v_n])\n          with T.init():\n            F[v_m, v_n] = T.float32(0.0)\n          F[v_m, v_n] = F[v_m, v_n] + D[v_m, v_rl] * E[v_rl, v_n]\n\n\nrt_lib = tvm.build(Module, target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd, f_nd).mean} sec\")\nTime cost of transformed sch.mod 0.792521475 sec\n其实还有更多的优化手段, 但是在tensor ir不方便实现, 比如通过lift allocation + stage memory + add guard的方式来实现在外层循环申请buffer,并在内循环中逐步填充,并在此后的迭代中重复使用, 但目前还是只能通过手动修改tir代码来实现. 最近有一另个调度抽象exo-lang提供了这些调度手段."
  },
  {
    "objectID": "posts/mlc-tutorial.html#提高并行性",
    "href": "posts/mlc-tutorial.html#提高并行性",
    "title": "机器学习编译概念科普",
    "section": "提高并行性",
    "text": "提高并行性\n现在只考虑单核的程序, 提升并行性通常就使用simd加速:\ntir_sch = tir.Schedule(Module)\n(*_, li) = tir_sch.get_loops('C')\ntir_sch.vectorize(li)\n(*_, ni) = tir_sch.get_loops('F')\ntir_sch.vectorize(ni)\n\nrt_lib = tvm.build(tir_sch.mod, target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd, f_nd).mean} sec\")\ntir_sch.mod.show()\nTime cost of transformed sch.mod 0.2236641083 sec\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\"), F: T.Buffer((1024, 3072), \"float32\")):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        # with T.block(\"root\"):\n        C = T.alloc_buffer((1024, 1024))\n        D = T.alloc_buffer((1024, 1024))\n        for m_0_m in range(64):\n            for l_0_m in range(128):\n                for rk_0, m_1, rk_1 in T.grid(256, 16, 8):\n                    for l_1 in T.vectorized(8):\n                        with T.block(\"C\"):\n                            v_m = T.axis.spatial(1024, m_0_m * 16 + m_1)\n                            v_l = T.axis.spatial(1024, l_0_m * 8 + l_1)\n                            v_rk = T.axis.reduce(2048, rk_0 * 8 + rk_1)\n                            T.reads(A[v_m, v_rk], B[v_rk, v_l])\n                            T.writes(C[v_m, v_l])\n                            with T.init():\n                                C[v_m, v_l] = T.float32(0.0)\n                            C[v_m, v_l] = C[v_m, v_l] + A[v_m, v_rk] * B[v_rk, v_l]\n                for m_1, l_1 in T.grid(16, 8):\n                    with T.block(\"D\"):\n                        v_m = T.axis.spatial(1024, m_0_m * 16 + m_1)\n                        v_l = T.axis.spatial(1024, l_0_m * 8 + l_1)\n                        T.reads(C[v_m, v_l])\n                        T.writes(D[v_m, v_l])\n                        D[v_m, v_l] = T.exp(C[v_m, v_l])\n            for n_0, rl_0, m_1, rl_1 in T.grid(384, 128, 16, 8):\n                for n_1 in T.vectorized(8):\n                    with T.block(\"F\"):\n                        v_m = T.axis.spatial(1024, m_0_m * 16 + m_1)\n                        v_n = T.axis.spatial(3072, n_0 * 8 + n_1)\n                        v_rl = T.axis.reduce(1024, rl_0 * 8 + rl_1)\n                        T.reads(D[v_m, v_rl], E[v_rl, v_n])\n                        T.writes(F[v_m, v_n])\n                        with T.init():\n                            F[v_m, v_n] = T.float32(0.0)\n                        F[v_m, v_n] = F[v_m, v_n] + D[v_m, v_rl] * E[v_rl, v_n]"
  },
  {
    "objectID": "posts/mlc-tutorial.html#随机调度变换",
    "href": "posts/mlc-tutorial.html#随机调度变换",
    "title": "机器学习编译概念科普",
    "section": "随机调度变换",
    "text": "随机调度变换\n假设我们知道想要对原始 TensorIR 程序进行哪些变换, 并且其中一些变化的参数基于我们对底层环境的理解,例如缓存和硬件单元. 因此, 我们想指定调度的方式, 但是选择调度的一些参数这样省略一些细节. 一种自然方法是在我们的变换中添加一些随机元素, 比如下面的代码通过sample_perfect_tile来采样可能的tile size:\ndef stochastic_schedule_mm(s: tvm.tir.Schedule):\n  m, l, k = s.get_loops('C')\n  m_factors = s.sample_perfect_tile(loop=m, n=2)\n  mo, mi = s.split(m, m_factors)\n  k_factors = s.sample_perfect_tile(loop=k, n=2)\n  ko, ki = s.split(k, k_factors)\n  l_factors = s.sample_perfect_tile(loop=l, n=2)\n  lo, li = s.split(l, l_factors)\n  s.reorder(mo, lo, ko, mi, ki, li)\n  return s\n可以多次执行下面这段代码, 每次都会采样出一个随机的tile size, 在操作上可以采用多次实验然后保存下最优性能下的调度方式, 但因为过于耗时这里就不尝试了:\nprim_func = te.create_prim_func([A, B, E, F])\ntir_sch = tvm.tir.Schedule(prim_func)\ntir_sch = stochastic_schedule_mm(tir_sch)\nrt_lib = tvm.build(tir_sch.mod, target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd, f_nd).mean} sec\")\ntir_sch.mod.show()\nTime cost of transformed sch.mod 5.9412203167 sec\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\"), F: T.Buffer((1024, 3072), \"float32\")):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        # with T.block(\"root\"):\n        C = T.alloc_buffer((1024, 1024))\n        D = T.alloc_buffer((1024, 1024))\n        for m_0, l_0, rk_0, m_1, rk_1, l_1 in T.grid(128, 128, 512, 8, 4, 8):\n            with T.block(\"C\"):\n                v_m = T.axis.spatial(1024, m_0 * 8 + m_1)\n                v_l = T.axis.spatial(1024, l_0 * 8 + l_1)\n                v_rk = T.axis.reduce(2048, rk_0 * 4 + rk_1)\n                T.reads(A[v_m, v_rk], B[v_rk, v_l])\n                T.writes(C[v_m, v_l])\n                with T.init():\n                    C[v_m, v_l] = T.float32(0.0)\n                C[v_m, v_l] = C[v_m, v_l] + A[v_m, v_rk] * B[v_rk, v_l]\n        for m, l in T.grid(1024, 1024):\n            with T.block(\"D\"):\n                v_m, v_l = T.axis.remap(\"SS\", [m, l])\n                T.reads(C[v_m, v_l])\n                T.writes(D[v_m, v_l])\n                D[v_m, v_l] = T.exp(C[v_m, v_l])\n        for m, n, rl in T.grid(1024, 3072, 1024):\n            with T.block(\"F\"):\n                v_m, v_n, v_rl = T.axis.remap(\"SSR\", [m, n, rl])\n                T.reads(D[v_m, v_rl], E[v_rl, v_n])\n                T.writes(F[v_m, v_n])\n                with T.init():\n                    F[v_m, v_n] = T.float32(0.0)\n                F[v_m, v_n] = F[v_m, v_n] + D[v_m, v_rl] * E[v_rl, v_n]"
  },
  {
    "objectID": "posts/mlc-tutorial.html#meta-schedule",
    "href": "posts/mlc-tutorial.html#meta-schedule",
    "title": "机器学习编译概念科普",
    "section": "Meta Schedule",
    "text": "Meta Schedule\n在实践中, 需要使用更快速更智能的算法. tvm的meta schedule是支持搜索可能变换空间的命名空间, 他实现了如下功能\n\n并行搜索\n使用cost model来避免每次都进行benchmark\n基于历史轨迹进行遗传搜索evolutionary search, 而不是每次都随机采样\n\n尽管工具变了, 但关键思想是保持不变的: 使用随机变换在指定的程序搜索空间中找到最优的调度方式.\nfrom tvm import meta_schedule as ms\n\n# disable parallel when num cores = 1.\nrules = ms.ScheduleRule.create('llvm')\nnewrules = []\nfor rule in rules:\n  if isinstance(rule, ms.schedule_rule.ParallelizeVectorizeUnroll):\n    newrules.append(ms.schedule_rule.ParallelizeVectorizeUnroll(-1, 64, [0, 16, 64, 512], True))\n  else:\n    newrules.append(rule.clone())\nmutators = ms.Mutator.create('llvm')\nnewmutators = []\nfor m in mutators:\n  if isinstance(m, ms.mutator.MutateParallel):\n    newmutators.append(ms.mutator.MutateParallel(-1))\n  else:\n    newmutators.append(m.clone())\nsg = ms.space_generator.PostOrderApply(sch_rules=newrules)\n\ndatabase = ms.tune_tir(\n    mod=prim_func,\n    target=\"llvm --num-cores=1\",\n    max_trials_global=64,\n    num_trials_per_iter=64,\n    work_dir=\"./tune_tmp\",\n    num_tuning_cores=4,\n    space=sg,\n)\n\nsch = ms.tir_integration.compile_tir(database, prim_func, \"llvm --num-cores=1\")\n2024-08-08 15:05:06 [INFO] Logging directory: ./tune_tmp/logs\n2024-08-08 15:05:06 [INFO] LocalBuilder: max_workers = 4\n2024-08-08 15:05:06 [INFO] LocalRunner: max_workers = 1\n2024-08-08 15:05:07 [INFO] [task_scheduler.cc:159] Initializing Task #0: \"main\"\n\n\n\n\n\n\n\n\nName\n\n\nFLOP\n\n\nWeight\n\n\nSpeed (GFLOPS)\n\n\nLatency (us)\n\n\nWeighted Latency (us)\n\n\nTrials\n\n\nDone\n\n\n\n\n\n\n0\n\n\nmain\n\n\n10737418240\n\n\n1\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n0\n\n\n\n\n\n\n\n2024-08-08 15:05:07 [DEBUG] [task_scheduler.cc:318] \n ID | Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n----------------------------------------------------------------------------------------------------------\n  0 | main | 10737418240 |      1 |            N/A |          N/A |                   N/A |      0 |      \n----------------------------------------------------------------------------------------------------------\nTotal trials: 0\nTotal latency (us): 0\n\n\nTotal trials: 0\nTotal latency (us): 0\n\n2024-08-08 15:05:07 [INFO] [task_scheduler.cc:180] TaskScheduler picks Task #0: \"main\"\n2024-08-08 15:05:11 [INFO] [task_scheduler.cc:193] Sending 64 sample(s) to builder\n2024-08-08 15:05:27 [INFO] [task_scheduler.cc:195] Sending 64 sample(s) to runner\n2024-08-08 15:09:35 [DEBUG] XGB iter   0: tr-p-rmse: 0.549968   tr-a-peak@32: 0.877909  tr-rmse: 0.348221   tr-rmse: 0.348221\n2024-08-08 15:09:35 [DEBUG] XGB iter  25: tr-p-rmse: 0.070855   tr-a-peak@32: 1.000000  tr-rmse: 0.397401   tr-rmse: 0.397401\n2024-08-08 15:09:35 [DEBUG] XGB iter  50: tr-p-rmse: 0.070855   tr-a-peak@32: 1.000000  tr-rmse: 0.397401   tr-rmse: 0.397401\n2024-08-08 15:09:35 [DEBUG] XGB stopped. Best iteration: [14] tr-p-rmse:0.07086 tr-a-peak@32:1.00000    tr-rmse:0.39740 tr-rmse:0.39740 \n2024-08-08 15:09:35 [INFO] [task_scheduler.cc:237] [Updated] Task #0: \"main\"\n\n\n\n\n\n\n\n\nName\n\n\nFLOP\n\n\nWeight\n\n\nSpeed (GFLOPS)\n\n\nLatency (us)\n\n\nWeighted Latency (us)\n\n\nTrials\n\n\nDone\n\n\n\n\n\n\n0\n\n\nmain\n\n\n10737418240\n\n\n1\n\n\n64.5722\n\n\n166285.3613\n\n\n166285.3613\n\n\n64\n\n\n\n\n\n\n\nTotal trials: 64\nTotal latency (us): 166285\n\n2024-08-08 15:09:35 [DEBUG] [task_scheduler.cc:318] \n ID | Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n----------------------------------------------------------------------------------------------------------\n  0 | main | 10737418240 |      1 |        64.5722 |  166285.3613 |           166285.3613 |     64 |      \n----------------------------------------------------------------------------------------------------------\nTotal trials: 64\nTotal latency (us): 166285\n\n2024-08-08 15:09:35 [INFO] [task_scheduler.cc:260] Task #0 has finished. Remaining task(s): 0\n\n\n\n\n\n\n\n\nName\n\n\nFLOP\n\n\nWeight\n\n\nSpeed (GFLOPS)\n\n\nLatency (us)\n\n\nWeighted Latency (us)\n\n\nTrials\n\n\nDone\n\n\n\n\n\n\n0\n\n\nmain\n\n\n10737418240\n\n\n1\n\n\n64.5722\n\n\n166285.3613\n\n\n166285.3613\n\n\n64\n\n\nY\n\n\n\n\n\nTotal trials: 64\nTotal latency (us): 166285\n\n2024-08-08 15:09:35 [DEBUG] [task_scheduler.cc:318] \n ID | Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n----------------------------------------------------------------------------------------------------------\n  0 | main | 10737418240 |      1 |        64.5722 |  166285.3613 |           166285.3613 |     64 |    Y \n----------------------------------------------------------------------------------------------------------\nTotal trials: 64\nTotal latency (us): 166285\n尝试执行一下自动搜索出的最终程序, 可以发现性能提升了非常多倍, 从7s到了0.16s:\nrt_lib = tvm.build(sch.mod, target=\"llvm\")\nf_timer = rt_lib.time_evaluator(\"__tvm_main__\", tvm.cpu())\n\nprint(f\"Time cost of transformed sch.mod {f_timer(a_nd, b_nd, e_nd, f_nd).mean} sec\")\nsch.mod.show()\nTime cost of transformed sch.mod 0.16998505 sec\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 1024), \"float32\"), E: T.Buffer((1024, 3072), \"float32\"), F: T.Buffer((1024, 3072), \"float32\")):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        # with T.block(\"root\"):\n        C = T.alloc_buffer((1024, 1024))\n        D = T.alloc_buffer((1024, 1024))\n        F_global = T.alloc_buffer((1024, 3072))\n        for m_0 in T.serial(32, annotations={\"pragma_auto_unroll_max_step\": 512, \"pragma_unroll_explicit\": 1}):\n            for l_0, m_1, l_1 in T.grid(2, 1, 32):\n                for m_2_init, l_2_init, m_3_init in T.grid(32, 2, 1):\n                    for l_3_fused_init in T.vectorized(8):\n                        with T.block(\"C_init\"):\n                            v_m = T.axis.spatial(1024, m_0 * 32 + m_1 * 32 + m_2_init + m_3_init)\n                            v_l = T.axis.spatial(1024, l_0 * 512 + l_1 * 16 + l_2_init * 8 + l_3_fused_init)\n                            T.reads()\n                            T.writes(C[v_m, v_l])\n                            T.block_attr({\"meta_schedule.tiling_structure\": \"SSRSRS\"})\n                            C[v_m, v_l] = T.float32(0.0)\n                for rk_0, m_2, l_2, rk_1, m_3 in T.grid(128, 32, 2, 16, 1):\n                    for l_3_fused in T.vectorized(8):\n                        with T.block(\"C_update\"):\n                            v_m = T.axis.spatial(1024, m_0 * 32 + m_1 * 32 + m_2 + m_3)\n                            v_l = T.axis.spatial(1024, l_0 * 512 + l_1 * 16 + l_2 * 8 + l_3_fused)\n                            v_rk = T.axis.reduce(2048, rk_0 * 16 + rk_1)\n                            T.reads(C[v_m, v_l], A[v_m, v_rk], B[v_rk, v_l])\n                            T.writes(C[v_m, v_l])\n                            T.block_attr({\"meta_schedule.tiling_structure\": \"SSRSRS\"})\n                            C[v_m, v_l] = C[v_m, v_l] + A[v_m, v_rk] * B[v_rk, v_l]\n        for m_0 in T.serial(4, annotations={\"pragma_auto_unroll_max_step\": 512, \"pragma_unroll_explicit\": 1}):\n            for ax0, ax1 in T.grid(256, 1024):\n                with T.block(\"D\"):\n                    v_m = T.axis.spatial(1024, m_0 * 256 + ax0)\n                    v_l = T.axis.spatial(1024, ax1)\n                    T.reads(C[v_m, v_l])\n                    T.writes(D[v_m, v_l])\n                    D[v_m, v_l] = T.exp(C[v_m, v_l])\n            for n_0, m_1, n_1 in T.grid(1, 4, 16):\n                for m_2_init, n_2_init, m_3_init in T.grid(32, 6, 2):\n                    for n_3_fused_init in T.vectorized(32):\n                        with T.block(\"F_init\"):\n                            v_m = T.axis.spatial(1024, m_0 * 256 + m_1 * 64 + m_2_init * 2 + m_3_init)\n                            v_n = T.axis.spatial(3072, n_0 * 3072 + n_1 * 192 + n_2_init * 32 + n_3_fused_init)\n                            T.reads()\n                            T.writes(F_global[v_m, v_n])\n                            T.block_attr({\"meta_schedule.tiling_structure\": \"SSRSRS\"})\n                            F_global[v_m, v_n] = T.float32(0.0)\n                for rl_0, m_2, n_2, rl_1, m_3 in T.grid(128, 32, 6, 8, 2):\n                    for n_3_fused in T.vectorized(32):\n                        with T.block(\"F_update\"):\n                            v_m = T.axis.spatial(1024, m_0 * 256 + m_1 * 64 + m_2 * 2 + m_3)\n                            v_n = T.axis.spatial(3072, n_0 * 3072 + n_1 * 192 + n_2 * 32 + n_3_fused)\n                            v_rl = T.axis.reduce(1024, rl_0 * 8 + rl_1)\n                            T.reads(F_global[v_m, v_n], D[v_m, v_rl], E[v_rl, v_n])\n                            T.writes(F_global[v_m, v_n])\n                            T.block_attr({\"meta_schedule.tiling_structure\": \"SSRSRS\"})\n                            F_global[v_m, v_n] = F_global[v_m, v_n] + D[v_m, v_rl] * E[v_rl, v_n]\n                for ax0, ax1 in T.grid(64, 192):\n                    with T.block(\"F_global\"):\n                        v0 = T.axis.spatial(1024, m_0 * 256 + m_1 * 64 + ax0)\n                        v1 = T.axis.spatial(3072, n_1 * 192 + ax1)\n                        T.reads(F_global[v0, v1])\n                        T.writes(F[v0, v1])\n                        F[v0, v1] = F_global[v0, v1]\n\n\n但是…, 如果我们使用预先手写好的高度优化的算子, 比如通过numpy来测试一下性能:\ndef numpy_fn(a, b, e, f):\n  c = np.matmul(a, b)\n  np.exp(c, out=c)\n  np.matmul(c, e, out=f)\n\n\ndef numpy_benchmark():\n  a_np = np.random.randn(M, K).astype(np.float32)\n  b_np = np.random.randn(K, L).astype(np.float32)\n  e_np = np.random.randn(L, N).astype(np.float32)\n  f_np = np.empty((M, N), np.float32)\n  times = 20\n  time = np.testing.measure('numpy_fn(a_np, b_np, e_np, f_np)', times)\n  print(f\"Time cost of numpy {time/times} sec\")\n\n\nfrom threadpoolctl import threadpool_limits\nwith np.testing.suppress_warnings() as sup:\n  sup.filter(RuntimeWarning)\n  with threadpool_limits(limits=1, user_api=None):\n    numpy_benchmark()\nTime cost of numpy 0.10500000000000001 sec\n可以发现虽然没有任何的fusion, 但手写算子还是优于自动搜索的结果. 这其实就是体现了目前的自动搜索方法所存在的局限性, 高度优化的gemm库中为了减少加载A/B矩阵的cache miss会进行online packing的操作, 比如blis的论文中的gebp策略:\n\ngebp策略的本质其实是fuse了pack(A),pack(B),matmul(Packed(A),Packed(B),C)这三个操作, 而tvm只专注于matmul(A,B,C)内部的变化策略, 所以他难以达到最优. 不过blis所提出的策略也只对支持simd指令的cpu有效, 对于dsa架构的tensor core来说, 就需要重新基于优化的原则来设计分块策略, 这是一个复杂的过程."
  },
  {
    "objectID": "posts/minmax.html",
    "href": "posts/minmax.html",
    "title": "最大最小聚类",
    "section": "",
    "text": "我写的最大最小聚类.但是我写的时候只写了依据最小进行聚类 233\n\n\n算法流程\n\n随机找一个点为第一个聚类中心\n找距离这个点最远的点为第二个聚类中心\n遍历所有的欧式距离,以其中较小的距离为准,如果这个距离大于第一第二聚类点的\\(\\theta\\)倍,那么这个点就是新的聚类点.重复直到没有新的聚类点.\n分类即可\n\n\n\n代码\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef createDataSet():\n    \"\"\"\n    创建测试的数据集，里面的数值中具有连续值\n    :return:\n    \"\"\"\n    dataSet = [\n        [0.697, 0.460], [0.774, 0.376], [0.634, 0.264],\n        [0.608, 0.318], [0.556, 0.215],\n        [0.403, 0.237], [0.481, 0.149], [0.437, 0.211],\n        [0.666, 0.091], [0.243, 0.267],\n        [0.245, 0.057], [0.343, 0.099], [0.639, 0.161],\n        [0.657, 0.198], [0.360, 0.370],\n        [0.593, 0.042], [0.719, 0.103], [0.359, 0.188],\n        [0.339, 0.241], [0.282, 0.257],\n        [0.748, 0.232], [0.714, 0.346], [0.483, 0.312],\n        [0.478, 0.437], [0.525, 0.369],\n        [0.751, 0.489], [0.532, 0.472], [0.473, 0.376],\n        [0.725, 0.445], [0.446, 0.459],\n    ]\n\n    # 特征值列表\n\n    labels = ['密度', '含糖率']\n\n    # 特征对应的所有可能的情况\n    labels_full = {}\n\n    for i in range(len(labels)):\n        labelList = [example[i] for example in dataSet]\n        uniqueLabel = set(labelList)\n        labels_full[labels[i]] = uniqueLabel\n\n    return dataSet, labels, labels_full\n\n\n# 计算两点之间的欧式距离\ndef one_one_dict(a: list, b: list)-&gt;float:\n    return pow(a[0]-b[0], 2)+pow(a[1]-b[1], 2)\n\n\n# 计算一个点与所有聚类中心的欧式距离\ndef one_n_dict(sample, centers: list)-&gt;np.ndarray:\n    sample = np.array(sample)\n    centers = np.array(centers)\n    # 将sample复制k次 与 centers 矩阵维度相同\n    samples = np.tile(sample, (centers.shape[0], 1))\n    # print(samples)\n    # 矩阵相减然后平方，对每一列求和　distances＝[1 x k]\n    distances = np.power(samples - centers, 2).sum(axis=1)\n    return distances\n\n\n# 计算n个点到一个中心点的欧式距离\ndef n_one_dist(X, c):\n    c = np.array(c, ndmin=2)\n    c_s = np.tile(c, (X.shape[0], 1))\n    # 利用矩阵乘法求出欧式距离矩阵\n    distances = np.power(X-c_s, 2).sum(axis=1)\n    return distances\n\n\nclass Maxmin(object):\n    def __init__(self, x, theta):\n        self.x = x\n        self.centers = []  # 聚类中心矩阵\n        self.theta = theta  # 生成新聚类中心可能性\n\n    # 确定第一个聚类中心\n    def init_center(self, cent=None):\n        \"\"\"\n        cent=None 时 随机生成初始聚类中心 否则指定点\n        \"\"\"\n        if cent == None:\n            cent_index = np.random.choice(np.shape(self.x)[0])\n        else:\n            cent_index = cent\n        self.centers.append(self.x[cent_index])\n\n    # 确定第二个聚类中心\n    def second_center(self):\n        dists = n_one_dist(self.x, self.centers[0])\n        cent_index = np.argmax(dists)\n        self.centers.append(self.x[cent_index])\n\n    # 计算各个样本与中心点的距离,返回距离矩阵\n    def calc_dist(self)-&gt;np.ndarray:\n        dists = np.zeros((self.x.shape[0], len(self.centers)))\n        for i in range(self.x.shape[0]):\n            dist = one_n_dict(self.x[i],  self.centers)\n            # 选距离中心点小的距离作为结果\n            dists[i] = dist\n        return dists\n\n    # 生成新的聚类中心,返回是否成功生成\n    def new_center(self, dists: np.ndarray)-&gt;bool:\n        # 按列取最小值\n        dists = np.min(dists, axis=1)\n        # 最大距离大于两个中心点之间的距离就生成新中心点\n        # 这里加pow是因为我前面算距离都没有开平方\n        if pow(max(dists), 0.5) &gt;\\\n                self.theta*pow(one_one_dict(self.centers[0], self.centers[1]), 0.5):\n            self.centers.append(self.x[np.argmax(dists)])\n            return True\n        else:\n            return False\n\n    # 当没有新的聚类中心之后,生成对应类别\n    def make_ylabel(self, dists: np.ndarray)-&gt;list:\n        index = np.argmin(dists, axis=1)\n        return index\n\n    def fit(self, cent=None):\n        \"\"\"\n        cent=None 时 随机生成初始聚类中心 否则指定点\n        \"\"\"\n        # 得到第一个聚类中心\n        self.init_center(cent)\n        # 得到第二个聚类中心\n        self.second_center()\n        # 迭代计算新的聚类中心\n        distances = self.calc_dist()\n        while self.new_center(distances) == True:\n            distances = self.calc_dist()\n        # 根据距离获得label\n        return self.make_ylabel(distances)\n\n\nif __name__ == \"__main__\":\n    # 书上的例子\n    data = [[0, 0], [3, 8], [2, 2], [1, 1], [5, 3],\n            [4, 8], [6, 3], [5, 4], [6, 4], [7, 5]]\n    data = np.array(data)\n    mm = Maxmin(data, 0.5)\n    y = mm.fit()\n    plt.scatter(x=data[:, 0], y=data[:, 1], c=y)\n    plt.show()\n\n    # 西瓜书中的数据\n    data, _, _ = createDataSet()\n    data = np.array(data)\n    mm1 = Maxmin(data, 0.5)\n    y = mm1.fit()\n    plt.scatter(x=data[:, 0], y=data[:, 1], c=y)\n    plt.show()\n\n\n效果"
  },
  {
    "objectID": "posts/mindspore-detail.html",
    "href": "posts/mindspore-detail.html",
    "title": "关于mindspore",
    "section": "",
    "text": "记录一些关于mindspore代码的东西。"
  },
  {
    "objectID": "posts/mindspore-detail.html#算子调用流程",
    "href": "posts/mindspore-detail.html#算子调用流程",
    "title": "关于mindspore",
    "section": "算子调用流程",
    "text": "算子调用流程\n例如GPU算子Argmax，他的调用顺序是： Argmax&lt;-Primitive.Argmax&lt;-ccsrc/backend/kernel_compiler/gpu/arrays/argmax_gpu_kernel.cc&lt;-ccsrc/backend/kernel_compiler/gpu/cuda_impl/argmax_impl.cu\n对于batchnorm这类操作，因为cudnn中有预先写好的，因此可以不用手动写GPU impl。"
  },
  {
    "objectID": "posts/mindspore-detail.html#batchnorm",
    "href": "posts/mindspore-detail.html#batchnorm",
    "title": "关于mindspore",
    "section": "batchnorm",
    "text": "batchnorm\nbatchnorm属实比较复杂，我看了一下ccsrc里面有4种batchnorm：[batchnorm,fused_batch_norm_ex,fused_batch_norm,batchnorm_fold,batchnorm_fold_2]。其中batchnorm_fold,batchnorm_fold_2是量化时使用的。batchnorm,fused_batch_norm_ex,fused_batch_norm是根据当前的运行模式决定的，如果是graph_mode执行P.FusedBatchNormEx进行训练，而测试时均用P.BatchNorm。\n目前的问题是P.FusedBatchNormEx和P.BatchNorm的GPU后端都不支持2D输入。。但我看代码其实也不复杂，因为对于GPU后端其实都是调用的cudnn库，因此需要看看为何cudnn不支持2D输入，先看看pytorch的底层是怎么搞的。\n\npytorch的batchnorm实现\n\n因为pytorch时间久，因此aten/src/ATen/native/cudnn/BatchNorm.cpp中包含了很多版本适配。看起来比较复杂。\n对于2D的输入他默认的mode=CUDNN_BATCHNORM_PER_ACTIVATION，介绍里写的是bnScale, bnBias tensor dims are 1xCxHxWx.. (one value per CHW...-slice, normalized over N slice)，然后：\n\nTensorDescriptor idesc{ *input, 4 };  // input descriptor\nTensorDescriptor wdesc{ expandScale(*weight, input-&gt;dim()), 4 };  // descriptor for weight, bias, running_mean, etc.\n我认为他的想法应该是将2D输入扩展为4D然后调用cudnn的batchnorm。然后做了下测试，发现他们的梯度的确是相同的：\nimport torch\nimport torch.nn.modules as m\nimport numpy as np\n\nn1 = m.BatchNorm1d(100).to('cuda')\nn1.train(True)\nx = torch.randn(32, 100, device='cuda', requires_grad=True)\ny = torch.randn(32, 100, device='cuda', requires_grad=True)\ncriterion = m.MSELoss()\ny_pred = n1(x)\nloss = criterion(y_pred, y)\nloss.backward()\nxg_1 = x.grad.cpu().numpy().copy()\nx.grad.zero_()\nprint(xg_1)\n\nn2 = m.BatchNorm2d(100).to('cuda')\nn2.train(True)\nnx = torch.unsqueeze(torch.unsqueeze(x, -1), -1)\nny = torch.unsqueeze(torch.unsqueeze(y, -1), -1)\ny_pred = n2(nx)\nloss = criterion(y_pred, ny)\nloss.backward()\nxg_2 = x.grad.cpu().numpy().copy()\nx.grad.zero_()\nprint(xg_2)\nprint(np.allclose(xg_1, xg_2))\n\n\n编译代码\n编译代码首先需要按官方配置设置docker，属实麻烦，因为需要下载很多依赖，必须得有vpn不然不行。\n\n配置docker使用主机的代理：我用的是vscode，因此在devcontainer中添加：\n\n    \"runArgs\": [\n        \"--runtime=nvidia\",\n        \"--network=host\"\n    ],\n然后dockerfile添加代理地址，或者手动在终端设置：\nRUN echo -e \"\\nexport https_proxy=http://127.0.0.1:8888\\nexport http_proxy=http://127.0.0.1:8889\" &gt;&gt; ~/.bashrc && source ~/.bashrc\n\n开始编译，icu4j校验不匹配\n\n我切换到0.7.0版本，然后./build.sh -e gpu -j 8，一开始构建很正常，到icu4j就奇怪了，这个包我下载了不下5次，然后看了一下mindspore的源码记录：https://gitee.com/mindspore/mindspore/commit/094bdbe253a328baee394922aeb54389ca07d563，发现他的md5写错了。。。按他的更新即可。\n\n出现No rule to make target '/usr/local/cuda/lib64/libcudnn.so', needed by 'mindspore/ccsrc/CMakeFiles/_c_expression.dir/cmake_device_link.o'.  Stop.\n\n这个原因是官方给的devel与release版的镜像配置不太一样，导致libcudnn.so没有在正确位置，执行：\nln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5 /usr/local/cuda/lib64/libcudnn.so\nln -s /usr/include/x86_64-linux-gnu/cudnn_v7.h /usr/local/cuda/include/cudnn.h"
  },
  {
    "objectID": "posts/mesh-matmul.html",
    "href": "posts/mesh-matmul.html",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "",
    "text": "分布式内存计算机的出现主要是为了满足大规模计算任务对计算能力和内存容量的需求, 但是由于物理限制与成本考虑, 单处理器的性能提升存在极限, 而分布式内存计算机通过使用多个相对简单/成本较低的处理器组成集群, 可以在不突破物理限制的情况下, 以较低的成本实现更高的计算性能.\n分布式内存架构是一个多处理器计算机系统, 其中每个处理器都有自己的私有内存, 以及处理器之间某种形式的互联. 计算任务只能对本地数据进行操作, 如果需要远程数据, 则计算任务必须与一个或多个远程处理器通信.\n在分布式内存架构下, 数据可以静态分布, 也可以通过节点移动; 既可以按需移动数据, 也可以提前将数据推送到新节点. 因此他的编程模型更复杂, 实现高性能计算任务时, 除了通常数据局部性还需要考虑通信开销, 也就是需要在考虑拓扑结构的同时设计数据的存储,通信的策略等. 仅仅是矩阵乘的计算就有非常多种方式, 接下来我将逐一介绍."
  },
  {
    "objectID": "posts/mesh-matmul.html#原理",
    "href": "posts/mesh-matmul.html#原理",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "2.1 原理",
    "text": "2.1 原理\nCannon算法来自于1969年的论文A cellular computer to implement the Kalman Filter Algorithm, 它适用在均匀的2D mesh上实现分布式矩阵乘, 他的优势在于完全对三个维度进行切分, 不会增加内存占用:\n\n它将K维度的reduce依旧放在同一个core上执行, 但是同时需要把K维度进行切分, 通过时间上的迭代逐渐shift需要的k到目标core上. 因此需要对矩阵A,B进行预先分布, 并且此时的切分状态就无法使用类似SBP的形式表示, 需要用映射的方式来表示: \\[\n\\begin{aligned}\n  [j, (i+j) \\% \\sqrt{P}] \\rightarrow [m,k] \\\\\n  [(i+j) \\% \\sqrt{P}, i] \\rightarrow [k,n] \\\\\n\\end{aligned}\n\\]\n下图展示了在二维mesh的第一行处理器上所分布的数据状态:\n\n接下来分别是矩阵乘累加以及shift, shift表示在x和y方向上对A,B的数据分片进行ring的传输, 下图展示的是shift后数据分片变化情况:"
  },
  {
    "objectID": "posts/mesh-matmul.html#实现",
    "href": "posts/mesh-matmul.html#实现",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "2.2 实现",
    "text": "2.2 实现\n下面是一个基于mpi4py实现的cannon算法实例:\nfrom mpi4py import MPI\nimport numpy as np\nfrom viztracer import log_sparse\nnp.set_printoptions(suppress=True)\n\ndef matmul(c: np.ndarray, a, b):\n  c += a @ b\n\n@log_sparse(stack_depth=3)\ndef cannon(A: np.ndarray, B: np.ndarray, C: np.ndarray, P, comm2d: MPI.Cartcomm):\n  (M, K) = A.shape\n  N = B.shape[1]\n  (mTile, nTile, kTile) = (M // P, N // P, K // P)\n\n  (j, i) = comm2d.Get_coords(rank)  # topology is row major\n  # align data\n  k = (i + j) % P\n  a = np.ascontiguousarray(A[j * mTile:(j + 1) * mTile, k * kTile:(k + 1) * kTile].copy())\n  b = np.ascontiguousarray(B[k * kTile:(k + 1) * kTile, i * nTile:(i + 1) * nTile].copy())\n\n  # compute and shift\n  c = np.empty([mTile, nTile], np.float32)\n  for t in range(P):\n    matmul(c.view(), a, b)\n    if t == P - 1: continue\n    # top right is (0,0)\n    right, left = comm2d.Get_cart_rank([j, (i + 1) % P]), comm2d.Get_cart_rank([j, (i - 1) % P])\n    comm2d.Sendrecv_replace(a, dest=left, source=right)\n    top, down = comm2d.Get_cart_rank([(j - 1) % P, i]), comm2d.Get_cart_rank([(j + 1) % P, i])\n    comm2d.Sendrecv_replace(b, dest=top, source=down)\n\n  # compare result\n  ref = C[j * mTile:(j + 1) * mTile, i * nTile:(i + 1) * nTile]\n  assert np.allclose(c, ref)\n\n\nif __name__ == '__main__':\n  A, B, C = np.load('A.npy', 'r'), np.load('B.npy', 'r'), np.load('C.npy', 'r')\n  P = 3\n  comm = MPI.COMM_WORLD\n  rank = comm.Get_rank()\n  comm2d = comm.Create_cart([P, P], [False, False])\n  cannon(A, B, C, P, comm2d)\n我这里构造了M:11520,K:7680,N:12288大小的矩阵, 并使用9个处理器执行:\nmpiexec -n 9 python cannon.py               \n此代码的profiling结果如下:"
  },
  {
    "objectID": "posts/mesh-matmul.html#分析",
    "href": "posts/mesh-matmul.html#分析",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "2.3 分析",
    "text": "2.3 分析\n首先我们使用此处所提出的alpha-beta模型分析代价:\n\n对于Cannon算法, 假设处理器个数为\\(P\\), 并且矩阵三个维度均为\\(n\\), 在忽略初始分布开销的情况下, 通信代价下限为: \\[\n\\begin{aligned}\n  Cost = 2(\\sqrt{P}-1)(\\alpha + \\frac{n^2}{P} \\beta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/mesh-matmul.html#原理-1",
    "href": "posts/mesh-matmul.html#原理-1",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "3.1 原理",
    "text": "3.1 原理\n他是典型的计算的存储分离的模式, 他同样将数据进行了完全切分, 每个节点上没有重复存储, 此时可以使用类似SBP的方式来表示: \\[\n\\begin{aligned}\n    (split(0), split(1)) \\times (split(0), split(1)) = (split(0), split(1))\n\\end{aligned}\n\\]\n对应的示意图如下:\n\n计算和使用SIMD指令进行外积形式的矩阵乘一致, 在K维度迭代, 只是数据需要从存储它的节点broadcast到其他节点:"
  },
  {
    "objectID": "posts/mesh-matmul.html#实现-1",
    "href": "posts/mesh-matmul.html#实现-1",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "3.2 实现",
    "text": "3.2 实现\n为了简单起见, 依旧使用M:11520,K:7680,N:12288大小的矩阵, 并使用9个处理器执行:\nfrom mpi4py import MPI\nfrom viztracer import log_sparse\nimport numpy as np\nnp.set_printoptions(suppress=True)\n\ndef matmul(c: np.ndarray, a, b):\n  c += a @ b\n\n@log_sparse(stack_depth=5)\ndef summa(A: np.ndarray, B: np.ndarray, C: np.ndarray, P, comm2d: MPI.Cartcomm):\n  col_comm = comm2d.Sub([True, False])\n  row_comm = comm2d.Sub([False, True])\n\n  (M, K) = A.shape\n  N = B.shape[1]\n  (mTile, nTile, kTile) = (M // P, N // P, K // P)\n  (j, i) = comm2d.Get_coords(rank)  # topology is row major\n  # align data\n  a = np.ascontiguousarray(A[j * mTile:(j + 1) * mTile, i * kTile:(i + 1) * kTile].copy())\n  b = np.ascontiguousarray(B[j * kTile:(j + 1) * kTile, i * nTile:(i + 1) * nTile].copy())\n\n  # compute and broadcast\n  c = np.empty([mTile, nTile], np.float32)\n  for k in range(P):\n    Atemp = a if k == i else np.empty_like(a)\n    Btemp = b if k == j else np.empty_like(b)\n    row_comm.Bcast(Atemp, root=k)\n    col_comm.Bcast(Btemp, root=k)\n    matmul(c.view(), Atemp, Btemp)\n\n  # compare result\n  ref = C[j * mTile:(j + 1) * mTile, i * nTile:(i + 1) * nTile]\n  assert np.allclose(c, ref)\n\n\nif __name__ == '__main__':\n  A, B, C = np.load('A.npy', 'r'), np.load('B.npy', 'r'), np.load('C.npy', 'r')\n  P = 3\n  comm = MPI.COMM_WORLD\n  rank = comm.Get_rank()\n  comm2d = comm.Create_cart([P, P], [False, False])\n  summa(A, B, C, P, comm2d)\n此代码的profiling结果如下:"
  },
  {
    "objectID": "posts/mesh-matmul.html#分析-1",
    "href": "posts/mesh-matmul.html#分析-1",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "3.3 分析",
    "text": "3.3 分析\n同样保持与之前类似的假设, 计算得通信代价下界为:\n\\[\n\\begin{aligned}\n  Cost =  2 \\sqrt{P} (\\alpha\\log \\sqrt{P} + \\frac{n^2}{P}\\beta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/mesh-matmul.html#原理-2",
    "href": "posts/mesh-matmul.html#原理-2",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "4.1 原理",
    "text": "4.1 原理\n它首先把处理器节点构造为一个立方体拓扑结构, 然后将数据尽量切分映射到不同的节点上:\n\n使用SBP的表示方式来描述: \\[\n\\begin{aligned}\n  &(split(1), split(1), split(0)) \\times  (split(1), split(0), split(0)) = (split(1), split(1), split(0)) \\\\\n  &A:[i,j+l] \\rightarrow [m,k]\\\\\n  &B:[l,j+i] \\rightarrow [k,n]\\\\\n  &C:[i,j+l] \\rightarrow [m,n]\n\\end{aligned}\n\\]\n为了尽量减少存储开销, 对于A,B都在三个维度进行了切分, 同时其中两个维度切的是同一个轴. 先通过Allgather将A,B收集并广播,然后计算矩阵乘得到\\(D_{ijl}\\), 然后通过一个Alltoall将z轴的k换成了n的切分, 然后每个core单独执行reduce操作即可:"
  },
  {
    "objectID": "posts/mesh-matmul.html#实现-2",
    "href": "posts/mesh-matmul.html#实现-2",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "4.2 实现",
    "text": "4.2 实现\n为了简单起见, 依旧使用M:11520,K:7680,N:12288大小的矩阵, 并使用8个处理器执行:\nimport sys\nfrom mpi4py import MPI\nfrom viztracer import log_sparse\nimport numpy as np\nnp.set_printoptions(suppress=True)\n\n@log_sparse(stack_depth=3)\ndef summa_3d(A: np.ndarray, B: np.ndarray, C: np.ndarray, p, comm3d: MPI.Cartcomm):\n  x_comm = comm3d.Sub([False, False, True])\n  y_comm = comm3d.Sub([False, True, False])\n  z_comm = comm3d.Sub([True, False, False])\n\n  (M, K) = A.shape\n  N = B.shape[1]\n  (MTile, NTile, KTile) = (M // p, N // p, K // p)\n  (mTile, nTile, kTile) = (MTile // p, NTile // p, KTile // p)\n  (l, j, i) = comm3d.Get_coords(rank)  # topology is row major\n  # align data\n  a = np.ascontiguousarray(A[i * MTile:(i + 1) * MTile, l * KTile:(l + 1) * KTile]\n                           [:, j * kTile: (j + 1) * kTile].copy())\n  b = np.ascontiguousarray(B[l * KTile:(l + 1) * KTile, j * NTile:(j + 1)\n                           * NTile][:, i * nTile: (i + 1) * nTile].copy())\n\n  # compute and passing data\n  c = np.zeros([MTile, nTile], np.float32)\n  Atemp = np.empty([p, MTile, kTile], np.float32)\n  Btemp = np.empty([p, KTile, nTile], np.float32)\n  y_comm.Allgather(a, Atemp)\n  x_comm.Allgather(b, Btemp)\n  Atemp = Atemp.transpose([1, 0, 2]).reshape(MTile, KTile)\n  Btemp = Btemp.transpose([1, 0, 2]).reshape(KTile, NTile)\n  Dl = np.dot(Atemp, Btemp)  # [MTile, NTile]\n  Dr = np.empty([p, MTile, nTile], np.float32)\n  # Note that mpi4py will send data sequentially. So, we want to resplit on N. We had to split it first.\n  Dsend = np.ascontiguousarray(np.stack(np.split(Dl, p, axis=-1)))\n  z_comm.Alltoall(Dsend, Dr)\n  c += np.sum(Dr, axis=0, keepdims=False)\n\n  # compare result\n  ref = C[i * MTile:(i + 1) * MTile, j * NTile:(j + 1) * NTile][:, l * nTile:(l + 1) * nTile]\n  assert np.allclose(c, ref)\n\n\nif __name__ == '__main__':\n  A, B, C = np.load('A.npy', 'r'), np.load('B.npy', 'r'), np.load('C.npy', 'r')\n  p = 2\n  comm = MPI.COMM_WORLD\n  rank = comm.Get_rank()\n  comm3d = comm.Create_cart([p, p, p], [False, False, False])\n  summa_3d(A, B, C, p, comm3d)\n注意由于mpi4py所提供一些接口限制, 中间引入了一些必要的数据重排操作来保证通信的正确性, 此代码的profiling结果如下:"
  },
  {
    "objectID": "posts/mesh-matmul.html#分析-2",
    "href": "posts/mesh-matmul.html#分析-2",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "4.3 分析",
    "text": "4.3 分析\n同样保持与之前类似的假设, 计算得通信代价下界为: \\[\n\\begin{aligned}\n  Cost &= 2(\\alpha\\log\\sqrt[3]{P} + \\frac{(\\sqrt[3]{P}-1)}{\\sqrt[3]{P}}\\frac{N^2}{P}\\beta) + (\\alpha\\log\\sqrt[3]{P} + \\frac{(\\sqrt[3]{P}-1)}{\\sqrt[3]{P}} \\frac{N^2}{P^{\\frac{2}{3}}} \\beta ) \\\\\n      &= 3\\alpha\\log\\sqrt[3]{P} + \\frac{(\\sqrt[3]{P}-1)}{\\sqrt[3]{P}}(\\frac{2N^2}{P} + \\frac{N^2}{P^{\\frac{2}{3}}})\\beta\n\\end{aligned}\n\\]\n总的来说, 3D算法可以比2D算法少传输\\(P^{\\frac{1}{6}}\\)倍的数据."
  },
  {
    "objectID": "posts/mesh-matmul.html#原理-3",
    "href": "posts/mesh-matmul.html#原理-3",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "5.1 原理",
    "text": "5.1 原理\n支持可以变化的拓扑结构, d可以降低到1退化为2D并行, 也可以提高到p进化为3d并行. 使用SBP的表示方式来描述: \\[\n\\begin{aligned}\n  &(broadcast, split(1), split(0)) \\times (broadcast, split(1), split(0)) = (partialsum, split(1), split(0)) \\\\\n  &A:[i,j] \\rightarrow [m,k]\\\\\n  &B:[i,j] \\rightarrow [k,n]\\\\\n  &C:[i,j,l] \\rightarrow [m,n,k^*]\n\\end{aligned}\n\\]\n首先在z轴为0处分布A,B矩阵, 然后broadcast到整个阵列. 接着是横向纵向重新分布a,b, 实现在z轴重新切分K, 最后矩阵乘并在z轴reduce得到结果:"
  },
  {
    "objectID": "posts/mesh-matmul.html#实现-3",
    "href": "posts/mesh-matmul.html#实现-3",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "5.2 实现",
    "text": "5.2 实现\n为了简单起见, 依旧使用M:11520,K:7680,N:12288大小的矩阵, 并使用18个处理器执行:\nimport math\nfrom mpi4py import MPI\nfrom viztracer import log_sparse\nimport numpy as np\nnp.set_printoptions(suppress=True)\n\n@log_sparse(stack_depth=3)\ndef summa_2_5d(A: np.ndarray, B: np.ndarray, C: np.ndarray, p, d, comm3d: MPI.Cartcomm):\n  x_comm = comm3d.Sub([False, False, True])\n  y_comm = comm3d.Sub([False, True, False])\n  z_comm = comm3d.Sub([True, False, False])\n\n  (M, K) = A.shape\n  N = B.shape[1]\n  (MTile, NTile, KPTile, KDTile) = (M // p, N // p, K // p, K // d)\n  (l, j, i) = comm3d.Get_coords(rank)  # topology is row major\n  # align data\n  a = np.ascontiguousarray(A[i * MTile:(i + 1) * MTile, j * KPTile:(j + 1)\n                           * KPTile]) if l == 0 else np.ones([MTile, KPTile], np.float32)\n  b = np.ascontiguousarray(B[i * KPTile:(i + 1) * KPTile, j * NTile:(j + 1)\n                           * NTile]) if l == 0 else np.ones([KPTile, NTile], np.float32)\n  c = np.zeros([MTile, NTile], dtype=float)\n  # compute and passing data\n\n  z_comm.Bcast(a, root=0)\n  z_comm.Bcast(b, root=0)\n  ktile = math.gcd(KPTile, KDTile)\n  for k in range((l * KDTile) // ktile, ((l + 1) * KDTile) // ktile):\n    aroot = ((k * ktile) // KPTile)\n    Atemp = np.copy(a[:, (k * ktile) - (aroot * KPTile):\n                      ((k + 1) * ktile) - (aroot * KPTile)]) if aroot == j else np.empty([MTile, ktile], np.float32)\n    y_comm.Bcast(Atemp, root=aroot)\n\n    broot = ((k * ktile) // KPTile)\n    Btemp = np.copy(b[(k * ktile) - (broot * KPTile):\n                      ((k + 1) * ktile) - (broot * KPTile), :]) if broot == i else np.empty([ktile, NTile], np.float32)\n    x_comm.Bcast(Btemp, root=broot)\n    np.add(c, np.dot(Atemp, Btemp), out=c)\n\n  cr = np.empty([MTile, NTile], dtype=float) if l == 0 else None\n  z_comm.Reduce(c, cr)\n\n  # compare result\n  if l == 0:\n    ref = C[i * MTile:(i + 1) * MTile, j * NTile:(j + 1) * NTile]\n    assert np.allclose(cr, ref)\n\n\nif __name__ == '__main__':\n  A, B, C = np.load('A.npy', 'r'), np.load('B.npy', 'r'), np.load('C.npy', 'r')\n  p = 3\n  d = 2\n  comm = MPI.COMM_WORLD\n  rank = comm.Get_rank()\n  comm3d = comm.Create_cart([d, p, p], [False, False, False])\n  summa_2_5d(A, B, C, p, d, comm3d)\n实现的时候需要考虑跟多的细节, 因为他这里实际上重新对K进行了切分, 所以迭代k的时候需要按更小的块来广播, 此代码的profiling结果如下:"
  },
  {
    "objectID": "posts/mesh-matmul.html#分析-3",
    "href": "posts/mesh-matmul.html#分析-3",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "5.3 分析",
    "text": "5.3 分析\n假设处理器维度为\\(p,p,d\\), 并且忽略第一次的分布的代价, 计算通信代价为: \\[\n\\begin{aligned}\n  Cost & = 2\\alpha \\log p  + 2\\frac{n^2}{pd} \\beta +  \\alpha \\log d+ 2\\frac{(d-1)n^2}{d}  \\beta \\\\\n\\end{aligned}\n\\]\n注意到SUMMA 2.5D实际上在d上多存储A,B矩阵."
  },
  {
    "objectID": "posts/mesh-matmul.html#tesseract",
    "href": "posts/mesh-matmul.html#tesseract",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "6.1 Tesseract",
    "text": "6.1 Tesseract\nTesseract实际上就是在大模型训练中被大家所熟知的2.5D并行, 他注意到了SUMMA 2.5D算法额外的内存开销问题, 提出进一步切分A矩阵:\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n\n\n\n\n\n\n并且把拓扑结构构造为\\(p,p,d\\)的形式来更有效的分配数据和计算.\n可以在结果中发现2.5D在进化到3D并行时取得了最好的效果."
  },
  {
    "objectID": "posts/mesh-matmul.html#nus版3d并行",
    "href": "posts/mesh-matmul.html#nus版3d并行",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "6.2 NUS版3D并行",
    "text": "6.2 NUS版3D并行\n这篇论文有三个改进, 分别是负载均衡, 优化矩阵矩阵乘, 优化矩阵向量乘. 但他这里的提到原始的3D并行只在两个维度切分A,B矩阵这是不对的, 本文第四节严格按照3D并行原论文进行了实现, 实际上是对于A矩阵的K是在j,l,以及B矩阵的N在j,i都做了进一步的切分的.\n不过NUS版对于A矩阵的进一步切分维度确实与原论文不同: \\[\n\\begin{align}\n  A:[i+j,l] \\rightarrow [M,K]\\\\\n  B:[l, j+i] \\rightarrow [K,N]\n\\end{align}\n\\]\n\n最终取得了良好的效果, 但是他这里并没有和Tesseract来对比:"
  },
  {
    "objectID": "posts/mesh-matmul.html#rtensor抽象",
    "href": "posts/mesh-matmul.html#rtensor抽象",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "7.1 rTensor抽象",
    "text": "7.1 rTensor抽象\n为了在编译器中表达这样一种计算模式, 因此他提出了rTensor的抽象:\nclass RotatingTensor {\n  vector &lt; size_t &gt; shape ; \n  DataType type ;\n  vector &lt; size_t &gt; spatial_partition_factor ;\n  vector &lt; size_t &gt; temporal_partition_factor ; \n  vector &lt; size_t &gt; rotating_pace ;\n};\nrTensor中的参数解释:\n\\[\n\\begin{aligned}\n  &f_s^X : \\textbf{Spatial Partition Factor},\\ \\text{Spatially partitions a tensor X into sub-tensors.} \\\\\n  &f_t^X : \\textbf{Temporal Partition Factor},\\ \\text{Temporally partitions a sub-tensor of X into sub-tensor partitions.} \\\\\n  &rp : \\textbf{Rotating Pace},\\ \\text{Specifies how sub-tensor partitions are shifted among cores.}\\\\\n  &F_{op}:  \\textbf{ Operator Partition Factor},\\ \\text{Spatially partitions an entire operator into sub-operators.}\n\\end{aligned}\n\\]\n下面给定一个rTensor具体实例, 通过执行示意图来理解每个参数的含义:\n\n上图中首先通过\\(f_s\\)在空间上切分矩阵, 然后由于矩阵在计算上的数据依赖, 是无法再使用空间上的并行, 因此采用\\(f_t\\)在时间维度上切分矩阵, 最后使用\\(rp\\)选择每次传输的数据块大小. 并且看到右下角的例子中\\(rp\\)可以小于时间上的分块, 对应到实际计算中就可以用于平衡计算和通信的时间."
  },
  {
    "objectID": "posts/mesh-matmul.html#自动搜索rtensor",
    "href": "posts/mesh-matmul.html#自动搜索rtensor",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "7.2 自动搜索rTensor",
    "text": "7.2 自动搜索rTensor\n目前有了形式化的描述方法, 那么就可以考虑如何构造搜索域并自动化搜索最优执行方式. T10并没有采用随机添加rTensor的配置来构造搜索域, 而是通过对原始计算语义上的切分来先行构造rTensor可选的配置域. 这也就是上一小节中\\(F_{op}\\)的作用, 他根据当前op的计算定义来对矩阵计算进行切分:\n\\[\n\\begin{aligned}\n  &C[M,N] \\mathrel{+}= A[M,K] * B[K,N] \\\\\n  \\text{if } &F_{op}[m,k,n] :\\\\\n  \\text{each core: } &C[\\frac{M}{m},\\frac{N}{n}] \\mathrel{+}= A[\\frac{M}{m},\\frac{K}{k}] * B[\\frac{K}{k},\\frac{N}{n}] \\\\\n  &\\text{total cores} = \\prod_{i=0}^{2} F_{op}[i]\n\\end{aligned}\n\\]\n假设\\(F_{op}\\)确定时, 就可以确定总core数, 以及每个core上的子任务所依赖的数据量: \\[\n\\begin{aligned}\n  &  C[2,3] \\mathrel{+}= A[2,6] * B[6,3] \\\\\n  &F_{op} = [2,1,3] :\\\\\n  \\text{each core: } & C[1,1] \\mathrel{+}= A[1,6] * B[6,1] \\\\\n  &\\text{total cores} = 6\n\\end{aligned}\n\\]\n通过子任务的数据量, 就可以推导出三个矩阵的空间切分参数: \\[\n\\begin{aligned}\n\\begin{matrix}\n   & \\textbf{m} & \\textbf{k} & \\textbf{n} \\\\\n  F_{op} & 2 & 1 & 3 \\\\\n  f_s^A & 2 & 1 & \\\\\n  f_s^B &  & 1 & 3 \\\\\n  f_s^C & 2 &  & 3\n\\end{matrix}\n\\end{aligned}\n\\]\n观察上表可以发现A,B,C矩阵都有一个维度不参与他们的空间切分, 但实际上为了计算出最终结果, 这个不参与空间切分的维度是被当前矩阵完全依赖的, T10称这个维度为missing axis, 因此采用时间切分来分割missing axis. 比如当前一共6个core, B矩阵在空间上被切分了3组, 那么还剩下\\(P = \\frac{6}{3} = 2\\)组可以用于切分, 因此按这个逻辑推导时间切分:\n\\[\n\\begin{aligned}\n\\begin{matrix}\n   & \\textbf{m} & \\textbf{k} & \\textbf{n} \\\\\n  F_{op} & 2 & 1 & 3 \\\\\n  f_s^A & 2 & 1 & \\\\\n  f_t^A & \\color{blue}1 & \\color{red}3 & \\\\\n  f_s^B &  & 1 & 3 \\\\\n  f_t^B & & \\color{red}2 & \\color{blue}1 \\\\\n  f_s^C & 2 &  & 3 \\\\\n  f_t^C & \\color{blue}1 & & \\color{blue}1 \\\\\n\\end{matrix}\n\\end{aligned}\n\\]\n注意, T10限制了\\(\\frac{P}{\\prod f_t}\\)必须是整数, 因为它其实就是ring的圈数, 如果非整数就无法支持. 当\\(\\frac{P}{\\prod f_t}\\)大于1时, 则表明ring的圈不止一个, 那么为了保证每个圈都可以得到正确的数据, 就需要把子张量复制\\(\\frac{P}{\\prod f_t}\\)次.\n最后进行旋转参数对齐, 按照上面的切分, 可以发现A的K维度在分为3组, 而B的K维度被分为2组, 那么每个节点上所拥有的K分块大小并不一样, 这个时候只能按小的K分块进行计算和旋转:\n\\[\n\\begin{aligned}\n  rp^A &= [0,k^A]\\\\\n  rp^B &= [k^B,0] \\\\\n  k^A , k^B &\\in [1,\\min(2,3)]\n\\end{aligned}\n\\]\n整体的流程如图所示:"
  },
  {
    "objectID": "posts/mesh-matmul.html#算子内与算子间trade-off",
    "href": "posts/mesh-matmul.html#算子内与算子间trade-off",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "7.3 算子内与算子间trade-off",
    "text": "7.3 算子内与算子间trade-off\n对于每个算子,都存在大量涉及不同空间和时间以及旋转因子确定的执行计划. 此外一个端到端的模型由众多算子组成, 这就产生了一个巨大的组合优化空间. T10采用的是两级权衡, 首先对每个算子搜索执行时间和内存消耗的最优平衡, 再在不同算子间最优计划之上优化全局内存分配, 这里就不进一步展开了."
  },
  {
    "objectID": "posts/mesh-matmul.html#子张量放置",
    "href": "posts/mesh-matmul.html#子张量放置",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "7.4 子张量放置",
    "text": "7.4 子张量放置\n优化内存分配之后, 还需要考虑数据放置问题, 因为ring shift只能在相邻节点之间传输数据, 如果初始数据放置的位置不合理, 会导致计算错误, 这在Cannon算法中称为align data过程. 而T10可以从编译器的角度, 看到上下算子的执行计算, 这给他们提供了全局张量放置优化机会. 比如一个算子的计算依赖于另一个算子的输出, T10会将这些相关的子张量放置在合适的核上, 减少数据准备过程的开销. 并且T10还总是按照阵列轴的方向升序排列数据, 避免出现乱序带来的错误.\n在这个3x3阵列上, T10所放置的子张量如图所示:\n\n可以回到第一节查看Cannon算法的初始放置, 两者是一模一样的."
  },
  {
    "objectID": "posts/mesh-matmul.html#分析-4",
    "href": "posts/mesh-matmul.html#分析-4",
    "title": "分布式存储架构下的矩阵乘与编译器",
    "section": "7.5 分析",
    "text": "7.5 分析\n因此T10的本质是形式化与泛化了Cannon算法, 并将其用于核间互联架构. 那么他的问题也与Cannon一样, 即通信量会比SUMMA 3D等方法大, 比SUMMA 2D方法少\\(\\frac{1}{\\sqrt{P}}\\).\n\\[\n\\begin{aligned}\n  Cost = 2(\\sqrt{P}-1)(\\alpha + \\frac{n^2}{P} \\beta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/matlab svm使用.html",
    "href": "posts/matlab svm使用.html",
    "title": "Matlab svm使用",
    "section": "",
    "text": "这里是对svm的函数做一个使用的总结，为了以后便于翻看。\n\n\nfitcsvm函数\nfitcsvm这个函数是用于训练分类模型的。主要用法有: 1. Mdl = fitcsvm(___,Name,Value) 这个用法就比较容易理解，例子如下： matlab     SVMmodel = fitcsvm(data3,theclass,'KernelFunction','rbf','BoxConstraint',Inf,'ClassNames',[-1,1]);\n**data3**：\n是一组用于训练的模型数据，一行作为一组数据。\n\n**theclass**：\n是训练数据的标签数据，每一行对应一行数据。\n\n**Name**：\n这后面的KernelFunction、BoxConstraint等等都是变量名。\n\n**Value**：\n这里的值就为变量的值，比如 `'KernelFunction','rbf'`就代表了核函数选择的是rbf核。\n\n**SVMmodel**：\n这个函数的返回值就是一个训练好的模型。\n\n\nfitrgp函数\nfitrgp函数是高斯过程拟合函数，我发现这个函数的效果比较不错，所以介绍一下：\n1.gprMdl = fitrgp(X,y) 例子如下： matlab     regressionGP = fitrgp(trian_x,trian_y,'BasisFunction', 'constant','KernelFunction','matern52','Standardize',true);\n**trian_x**:\n训练数据，一行作为一组数据。\n\n**trian_y**：\n测试数据：每一行对应一行数据。\n后面依旧是NAME：Value。再此不赘述。\n\n\npredict函数\npredict函数是进行预测的。主要用法有：\n\nyp = predict(sys,data,K) 这个用法的例子如下： yp = predict(datatest,testlabel)\n\n**datatest**：\n为用于测试的数据。\n\n**testlabel**：\n是测试数据的标签。\n\n**K**：\n这里K没有用到，这个K就是预测地平线，现在我还不知道什么意思，他的默认是1。所以这里没有用到。\n\n**yp**:\n预测的输出响应。"
  },
  {
    "objectID": "posts/makemynn.html",
    "href": "posts/makemynn.html",
    "title": "Python神经网络编程",
    "section": "",
    "text": "这两天看了这本书,非常好.一下就可以把人讲懂.我试着写了他的例子\n\n\n程序\nimport scipy.special\nimport numpy\nimport matplotlib.pyplot as plt\nimport pickle\n\n\nclass neuralNetwork:\n\n    def __init__(self, inputnodes: int, hiddennodes: int, outputnodes: int, learningrate: int):\n        # set the node number\n        self.inodes = inputnodes\n        self.hnodes = hiddennodes\n        self.onodes = outputnodes\n        # set the learn rate\n        self.lr = learningrate\n        # generate the W_input_hidden and W_hidden_output matrix\n\n        # hidden = w_ih * innodes  =&gt;   w_ih is [hidden x input]\n        self.w_ih = numpy.random.normal(scale=pow(self.hnodes, -0.5),\n                                        size=(self.hnodes, self.inodes))\n        # out = w_ho * outnodes  =&gt;   w_ho is [out x hidden]\n        self.w_ho = numpy.random.normal(scale=pow(self.onodes, -0.5),  # 节点数的 -1/2 次\n                                        size=(self.onodes, self.hnodes))\n        # set the active function\n        self.active_fuc = lambda x: scipy.special.expit(x)\n        self.re_active_fuc = lambda x: scipy.special.logit(x)\n    # we need set the target to train\n\n    def train(self, input_list, target_list):\n        # convert list to 2d array\n        inputs = numpy.array(input_list, ndmin=2).T\n        targets = numpy.array(target_list, ndmin=2).T\n\n        # calculate signals into hidden layer\n        hidden_inputs = numpy.dot(self.w_ih, inputs)\n        hidden_outputs = self.active_fuc(hidden_inputs)\n        # calculate signals of the output layer\n        final_inputs = numpy.dot(self.w_ho, hidden_outputs)\n        final_outputs = self.active_fuc(final_inputs)\n\n        # hidden_error = output -target\n        output_errors = targets-final_outputs\n        hidden_errors = numpy.dot(self.w_ho.T, output_errors)\n\n        # input_error = hidden_error - hidden_output\n        # update the W_ho\n        self.w_ho += self.lr * \\\n            numpy.dot((output_errors*final_outputs*(1.0-final_outputs)),\n                      numpy.transpose(hidden_outputs))\n\n        self.w_ih += self.lr * \\\n            numpy.dot((hidden_errors*hidden_outputs*(1.0-hidden_outputs)),\n                      numpy.transpose(inputs))\n        pass\n\n    def query(self, input_list: numpy.ndarray)-&gt;numpy.ndarray:\n        # convt  input_list to 2d array\n        inputs = numpy.array(input_list, ndmin=2).T\n        # X_hidden = W_ih · inputs\n        hidden_input = numpy.dot(self.w_ih, inputs)\n        hidden_output = self.active_fuc(hidden_input)\n\n        final_input = numpy.dot(self.w_ho, hidden_output)\n        final_output = self.active_fuc(final_input)\n\n        return final_output\n\n    def backquery(self, targets_list: list)-&gt;numpy.ndarray:\n        # transpose the targets list to a vertical array\n        final_outputs = numpy.array(targets_list, ndmin=2).T\n\n        # calculate the signal into the final output layer\n        final_inputs = self.re_active_fuc(final_outputs)\n\n        # 权重矩阵可能无法求逆，所以直接乘转置\n        hidden_outputs = numpy.dot(self.w_ho.T, final_inputs)\n        # scale them back to 0.01 to .99\n        hidden_outputs -= numpy.min(hidden_outputs)\n        hidden_outputs /= numpy.max(hidden_outputs)\n        hidden_outputs *= 0.98\n        hidden_outputs += 0.01\n\n        # calculate the signal into the hidden layer\n        hidden_inputs = self.re_active_fuc(hidden_outputs)\n\n        # calculate the signal out of the input layer\n        inputs = numpy.dot(self.w_ih.T, hidden_inputs)\n        # scale them back to 0.01 to .99\n        inputs -= numpy.min(inputs)\n        inputs /= numpy.max(inputs)\n        inputs *= 0.98\n        inputs += 0.01\n\n        return inputs\n\n\nif __name__ == \"__main__\":\n    input_nodes = 784\n    hidden_nodes = 100\n    output_nodes = 10\n\n    learn_rate = 0.3\n\n    n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learn_rate)\n\n    # # read the data set\n    # train_file = open(\"./mnist_train.csv\")\n    # train_data = train_file.readlines()  # type:list\n    # train_file.close()\n\n    # for record in train_data:\n    #     all_value = record.split(',')\n    #     # set inputs data to 0.01 ~ 0.99\n    #     inputs = (numpy.asfarray(all_value[1:])/255.0*0.99)+0.01\n    #     # set target data\n    #     targets = numpy.zeros(output_nodes)+0.01  # set the false is 0.01\n    #     targets[int(all_value[0])] = 0.99  # set the right is 0.99\n\n    #     n.train(inputs, targets)\n\n    # save the model !\n    # savefile = open('nnmodel', 'wb')\n    # pickle.dump(n.w_ih, savefile, 1)\n    # pickle.dump(n.w_ho, savefile, 1)\n    # savefile.close()\n    # read the model !\n    savefile = open('/home/zqh/Program/Python_study/NN/nnmodel', 'rb')\n    n.w_ih = pickle.load(savefile)\n    n.w_ho = pickle.load(savefile)\n    savefile.close()\n    # read the test data\n    test_file = open(\"./mnist_test.csv\")\n    test_data = test_file.readlines()  # type:list\n    test_file.close()\n    scorecard = []\n    for record in test_data:\n        test_value = record.strip().split(',')\n        inputs = (numpy.asfarray(test_value[1:]) / 255.0 * 0.99) + 0.01\n        outputs = n.query(inputs)\n        label = numpy.argmax(outputs)\n        # print(\"原始：{} 预测：{}\".format(test_value[0], label))\n        if int(test_value[0]) == label:\n            scorecard.append(1)\n        else:\n            scorecard.append(0)\n\n    print(\"准确率 = {:.2f}%\".format(\n        sum(scorecard) / len(scorecard)*100.0))\n    print('反向查询：')\n    image_array = n.backquery(\n        [0.01, 0.01, 0.99, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01])\n    plt.imshow(image_array.reshape(28, 28), cmap='Greys')\n    plt.show()\n\n\n运行结果\n➜  NN /usr/bin/python3 /home/zqh/Program/Python_study/NN/nn.py\n准确率 = 94.68%\n反向查询：\n\n\n\n数字2的图像"
  },
  {
    "objectID": "posts/macos-bundle.html",
    "href": "posts/macos-bundle.html",
    "title": "macos中bundle的使用",
    "section": "",
    "text": "研究一下在macos中如何编译bundle文件并动态加载并运行.\n使用gcc编译macos程序的时候, 可以通过-bundle选项来指示, 编译出来就是一个包含资源和可执行代码的包. 使用 -bundle 选项时, 还需要指定一个入口点, 通常是 main 函数. 它的好处是使得开发者能够创建动态加载的代码模块, 这些模块可以包含可执行代码和资源, 并且可以被其他应用程序在运行时加载和使用."
  },
  {
    "objectID": "posts/macos-bundle.html#简单例子",
    "href": "posts/macos-bundle.html#简单例子",
    "title": "macos中bundle的使用",
    "section": "简单例子",
    "text": "简单例子\n来自苹果开源库的dyld测试例子\nbundle.c\n#include &lt;stdbool.h&gt;\n#include &lt;stdio.h&gt;\n\n// test to see if bss section is properly expanded \n\nstatic int mydata[1000000];\n\nbool checkdata()\n{\n  printf(\"check data!\\n\");\n    return ( mydata[500000] == 0 );\n}\nmain.c\n#include &lt;Availability.h&gt;\n#include &lt;mach-o/dyld.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;stdio.h&gt;\n\ntypedef bool (*CheckFunc)();\n\nint main() {\n// these APIs are only available on Mac OS X - not iPhone OS\n#if __MAC_OS_X_VERSION_MIN_REQUIRED\n    NSObjectFileImage ofi;\n    if (NSCreateObjectFileImageFromFile(\"test.bundle\", &ofi) !=\n        NSObjectFileImageSuccess) {\n        // FAIL(\"NSCreateObjectFileImageFromFile failed\");\n        return 1;\n    }\n\n    NSModule mod = NSLinkModule(ofi, \"test.bundle\", NSLINKMODULE_OPTION_NONE);\n    if (mod == NULL) {\n        // FAIL(\"NSLinkModule failed\");\n        return 1;\n    }\n\n    NSSymbol sym = NSLookupSymbolInModule(mod, \"_checkdata\");\n    if (sym == NULL) {\n        // FAIL(\"NSLookupSymbolInModule failed\");\n        return 1;\n    }\n\n    CheckFunc func = NSAddressOfSymbol(sym);\n    if (!func()) {\n        // FAIL(\"NSAddressOfSymbol failed\");\n        return 1;\n    }\n\n    if (!NSUnLinkModule(mod, NSUNLINKMODULE_OPTION_NONE)) {\n        // FAIL(\"NSUnLinkModule failed\");\n        return 1;\n    }\n\n    if (!NSDestroyObjectFileImage(ofi)) {\n        // FAIL(\"NSDestroyObjectFileImage failed\");\n        return 1;\n    }\n#endif\n    printf(\"funck\\n\");\n    // PASS(\"bundle-basic\");\n    return 0;\n}\n执行\nclang bundle.c -bundle -o test.bundle\nclang main.c\n./a.out"
  },
  {
    "objectID": "posts/macos-bundle.html#使用cmake",
    "href": "posts/macos-bundle.html#使用cmake",
    "title": "macos中bundle的使用",
    "section": "使用cmake",
    "text": "使用cmake\n注意cmake的MACOS_BUNDLE和链接选项的bundle并不是一个东西. 所以用那个选项是无法得到正确的结果的.\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\n\n# 设置项目名称和版本\nproject(test.bundle VERSION 1.0)\n\n# 指定 C++ 标准\n# set(CMAKE_CXX_STANDARD 11)\n# set(CMAKE_CXX_STANDARD_REQUIRED True)\n\n# 添加一个 macOS bundle 可执行文件\nadd_executable(test.bundle bundle.c)\ntarget_link_options(test.bundle PUBLIC -bundle)\n# 如果有需要，可以链接库\n# target_link_libraries(test.bundle SomeLibrary)\n\n# 包含 CMake 的 macOS 应用程序 bundle 模块\n# include(CMakeOSXBundleInfo)"
  },
  {
    "objectID": "posts/macos-bundle.html#一些问题",
    "href": "posts/macos-bundle.html#一些问题",
    "title": "macos中bundle的使用",
    "section": "一些问题",
    "text": "一些问题\n\n1. link问题\n我在正式的项目本来是使用nostdlib static来编译的, 然后现在链接选项添加了-bundle后就遇到这个问题:\nld: warning: ignoring -e, not used for output type\n0  0x1041d2f2c  __assert_rtn + 72\n1  0x1041a7984  ld::AtomFileConsolidator::addAtomFile(ld::AtomFile const*, ld::AtomFile const*, bool) + 4952\n2  0x1041b34e4  ld::AtomFileConsolidator::addAtomFile(ld::AtomFile const*) + 148\n3  0x1041d1430  ld::pass::stubs(ld::Options const&, ld::AtomFileConsolidator&) + 1464\n4  0x1041bad60  ld::AtomFileConsolidator::resolve() + 12744\n5  0x104142b40  main + 9308\nld: Assertion failed: (slot &lt; _sideTableBuffer.size()), function addAtom, file AtomFileConsolidator.cpp, line 2278.\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\n搜索之后发现是linker升级的原因, 可以通过-ld_classic来避免.\n\n\n2. 全局变量冲突的问题\n我现在是同时添加多个bundle的模块, 然后调用每个模块中的函数, 目前不同的模板都是存在相同名字的全局变量, 这导致在执行的过程中同一个程序的不同的函数中取到的全局变量的地址是不同的, 从而出现问题. 我改成dylib的形式, 然后使用dlopen去加载函数, 也会出现同样的问题, 所以我怀疑这个问题就是和mac os的底层实现有关. 但是又不能一次只加载一个代码, 这样的话每个kernel启动前都需要load一次可执行文件, 性能就炸了.\n尝试使用类似下面这种静态函数的方式来规避全局变量, 发现也不行:\nnncase_runtime_cpu_mt_t *g_cpu_mt(nncase_runtime_cpu_mt_t *from) {\n    static nncase_runtime_cpu_mt_t *g_mt;\n    if (from != NULL) {\n        g_mt = from;\n    }\n    return g_mt;\n}\n目前的总结:\n\n非extern\n\n加static, 两个cpp中的全局变量不是同一个.\n不加static, 两个cpp中的会自己定义两次全局变量, 然后编译的过程中报错多重定义.\n\nextern\n\n不论是在哪个cpp里面实现, 加载多个程序之后都会出现冲突.\n\n同名函数中静态变量, 同样他也会被优化成类似全局变量的形式然后出现混乱.\n给全局变量进行编号也无效, 实际上可能是因为实例化出来的c++函数在全局符号表中发生了冲突, 导致kernel0中执行了kernel7的tensor copy."
  },
  {
    "objectID": "posts/mac-amx.html",
    "href": "posts/mac-amx.html",
    "title": "探索AMX: 解锁Apple Silicon隐藏性能",
    "section": "",
    "text": "自从2020年Apple发布的芯片M1/M2/M3, 至少提供了四种不同的方式可以执行高负载的计算任务:\n在M1 Max上单核计算单精度浮点矩阵乘法时, 使用SIMD指令集可达到102 GFLOPS左右的性能, 而使用AMX指令集最多可达到1475 GFLOPS! 本文就来带领大家一同探索AMX指令集, 学习如何解锁这剩下的14倍算力."
  },
  {
    "objectID": "posts/mac-amx.html#理论计算性能",
    "href": "posts/mac-amx.html#理论计算性能",
    "title": "探索AMX: 解锁Apple Silicon隐藏性能",
    "section": "3.1 理论计算性能",
    "text": "3.1 理论计算性能\n根据上一节我们清楚了Z寄存器组在float32模式下一共被分为了16组, 一次计算在每一组中得到一列结果, 因此ALU实际上被分为了4组. 这里就来验证使用不同个数的ALU组的峰值性能:\nperf_func = [&z_nums]() {\n  constexpr uint64_t a = matfp().dtype_mode(matfp_dtype_t::f32f32).z_row(0);\n  constexpr uint64_t b = matfp().dtype_mode(matfp_dtype_t::f32f32).z_row(1);\n  constexpr uint64_t c = matfp().dtype_mode(matfp_dtype_t::f32f32).z_row(2);\n  constexpr uint64_t d = matfp().dtype_mode(matfp_dtype_t::f32f32).z_row(3);\n  AMX_MATFP(a);\n  if (z_nums &gt; 1)\n    AMX_MATFP(b);\n  if (z_nums &gt; 2)\n    AMX_MATFP(c);\n  if (z_nums &gt; 3)\n    AMX_MATFP(d);\n};\n得到结果如下:\n\n\n\nALU Nums\nGflop/s\n\n\n\n\n1\n405.530\n\n\n2\n826.912\n\n\n3\n1244.570\n\n\n4\n1666.952\n\n\n\n这表明虽然每组ALU是单独配置和发射, 但是他们之间是可以并行执行的."
  },
  {
    "objectID": "posts/mac-amx.html#理论数据加载性能",
    "href": "posts/mac-amx.html#理论数据加载性能",
    "title": "探索AMX: 解锁Apple Silicon隐藏性能",
    "section": "3.2 理论数据加载性能",
    "text": "3.2 理论数据加载性能\n这里是我设计的性能测试用例, 其中reg nums表示X/Y寄存器池. near表示是否读取内存中连续的地址,在M2 Pro中l1 Dcache 大小为65536, 因此将K设计为数倍大于l1 cache size, 当near == 0时需要跨过两倍大小的cache size去读取. width表示一次读取几个寄存器个数, 最大为4.\nconstexpr size_t K = (65536 / 4 / (16 * 4)) * 4;\nfloat M[K * 2][16 * 4]{};\nfloat N[K * 2][16 * 4]{};\nperf_func = [&M, &N, &near, &reg_num, &x_width, &y_width]() {\n  auto ldx = ldxy().register_index(0);\n  auto ldy = ldxy().register_index(0);\n  if (x_width &gt;= 2)\n    ldx = ldx.multiple();\n  if (x_width &gt;= 4)\n    ldx = ldx.multiple_four();\n  if (reg_num &gt; 1) {\n    if (y_width &gt;= 2)\n      ldy = ldy.multiple();\n    if (y_width &gt;= 4)\n      ldy = ldy.multiple_four();\n  }\n\n  if (near) {\n    for (size_t i = 0; i &lt; K; i++) {\n      AMX_LDX(ldx.bind(M[i]));\n      if (reg_num &gt; 1) {\n        AMX_LDY(ldy.bind(N[i]));\n      }\n    }\n  } else {\n    for (size_t i = 0; i &lt; K / 2; i++) {\n      for (size_t j = 0; j &lt; 2; j++) {\n        AMX_LDX(ldx.bind(M[j * K + i]));\n        if (reg_num &gt; 1) {\n          AMX_LDY(ldy.bind(N[j * K + i]));\n        }\n      }\n    }\n  }\n};\n接下来我们计算数据加载时间, 得到下表:\n\n\n\nReg Nums\nNear\nX Width\nY Width\nGB/s\n\n\n\n\n1\n1\n1\n0\n87.1489\n\n\n1\n1\n2\n0\n213.164\n\n\n1\n1\n4\n0\n456.332\n\n\n1\n0\n1\n0\n120.796\n\n\n1\n0\n2\n0\n260.115\n\n\n1\n0\n4\n0\n483.285\n\n\n2\n1\n1\n1\n134.33\n\n\n2\n1\n1\n2\n162.084\n\n\n2\n1\n1\n4\n297.15\n\n\n2\n1\n2\n1\n201.658\n\n\n2\n1\n2\n2\n214.772\n\n\n2\n1\n2\n4\n350.554\n\n\n2\n1\n4\n1\n384.614\n\n\n2\n1\n4\n2\n349.528\n\n\n2\n1\n4\n4\n476.722\n\n\n2\n0\n1\n1\n130.604\n\n\n2\n0\n1\n2\n163.91\n\n\n2\n0\n1\n4\n254.922\n\n\n2\n0\n2\n1\n195.612\n\n\n2\n0\n2\n2\n213.61\n\n\n2\n0\n2\n4\n298.603\n\n\n2\n0\n4\n1\n310.308\n\n\n2\n0\n4\n2\n302.767\n\n\n2\n0\n4\n4\n325.193\n\n\n\n可以发现加大读取宽度是可以翻倍带宽的, 因此这是无成本的. 读取连续摆放的数据相比于非连续读取略有提升, 这表明需要对数据摆放进行优化. 同时加载两个寄存器中也同样不会导致带宽降低, 表明可以同时加载A/B矩阵, 但需要注意尽量cache起来."
  },
  {
    "objectID": "posts/mac-amx.html#理论数据存储性能",
    "href": "posts/mac-amx.html#理论数据存储性能",
    "title": "探索AMX: 解锁Apple Silicon隐藏性能",
    "section": "3.3 理论数据存储性能",
    "text": "3.3 理论数据存储性能\n同样在store时设计了reg_num,near,width等选项, 值的注意的是, 因为Z寄存器池被分为了16组, 所以每一次store时都将16组全部输出:\nconstexpr size_t K = (65536 / 4 / (16 * 4)) * 2;\nfloat CNear[16][16 * 4]{};\nfloat C[16][K]{};\nperf_func = [&C, &CNear, &near, &z_num, &width]() {\n  auto ldst = width == 2 ? ldstz().multiple() : ldstz();\n  for (size_t z = 0; z &lt; z_num; z++) {\n    for (size_t m = 0; m &lt; 16; m++) {\n      AMX_STZ(ldst.row_index(m * 4 + z * width)\n                  .bind(near ? CNear[m] + 16 * z * width\n                              : C[m] + 16 * z * width));\n    }\n  }\n};\n测试结果如下:\n\n\n\nReg Nums\nNear\nWidth\nGB/s\n\n\n\n\n1\n1\n1\n10.3769\n\n\n1\n1\n2\n8.93052\n\n\n1\n0\n1\n12.9423\n\n\n1\n0\n2\n12.3377\n\n\n2\n1\n1\n5.69731\n\n\n2\n1\n2\n12.3658\n\n\n2\n0\n1\n7.55092\n\n\n2\n0\n2\n13.0133\n\n\n3\n1\n1\n6.58085\n\n\n3\n0\n1\n11.4118\n\n\n4\n1\n1\n8.8847\n\n\n4\n0\n1\n9.85956\n\n\n\n可以发现使用多个寄存器并无法提升带宽, 说明基本上是串行输出, 但开启两倍宽度还是可以有效提升带宽. 然而整体速度相比于计算和加载慢了数倍, 这表明我们不能频繁地加载与存储Z."
  },
  {
    "objectID": "posts/mac-amx.html#计算策略1-加载多组n",
    "href": "posts/mac-amx.html#计算策略1-加载多组n",
    "title": "探索AMX: 解锁Apple Silicon隐藏性能",
    "section": "3.1 计算策略1: 加载多组N",
    "text": "3.1 计算策略1: 加载多组N\n在我的M2 Pro中, AMX最大一次可以加载4*64 bytes, 因此可以加载1组M以及4组的N, 利用满计算单元得到M*4N, 然后在下一次循环中切换不同的K. 总的来说是加载一次, 计算四次, 由于X寄存器池大小限制只能多缓存一次. \n查表可知,加载带宽为254.922 GB/s,得到计算时间与数据加载时间分别为: \\[\n\\begin{aligned}\nFLOPs &= 2 * M * N * 4 = 2048 \\\\\nCompute Time &= FLOPs / 1666  = 1.229~NanoSeconds \\\\\nBytes &= (1 * M + 4 * N) * 4 \\\\\nLoad Time &= Bytes / 297.15 = 1.076~NanoSeconds\n\\end{aligned}\n\\]\n两者时间基本差不多, 但他们之间的差只有0.15ns, 这点时间不足以在一次缓存中流水起来."
  },
  {
    "objectID": "posts/mac-amx.html#计算策略2-加载多组m",
    "href": "posts/mac-amx.html#计算策略2-加载多组m",
    "title": "探索AMX: 解锁Apple Silicon隐藏性能",
    "section": "3.2 计算策略2: 加载多组M",
    "text": "3.2 计算策略2: 加载多组M\n同上一种策略类似, 同样也是加载一次计算四次, 计算时间与加载时间和上一小节类似, 但是这里存在一个额外问题则是对于A矩阵的列方向是不连续的, 需要先通过CPU进行转置4倍的M后才可以加载, 因此不考虑."
  },
  {
    "objectID": "posts/mac-amx.html#计算策略3-加载两组mn",
    "href": "posts/mac-amx.html#计算策略3-加载两组mn",
    "title": "探索AMX: 解锁Apple Silicon隐藏性能",
    "section": "3.3 计算策略3: 加载两组M/N",
    "text": "3.3 计算策略3: 加载两组M/N\n为了均匀X/Y寄存器的存储, 可以考虑加载同样大小的M/N, 也就是加载2组, 此时可以计算4次得到2M*2N大小的输出, 考虑到最大可以加载4*64 bytes, 那么可以加载不同K两组M/N, 这样一次加载实际可以计算8次, 最大限度的利用算力. 此时对于X/Y寄存器池使用量是一样的, 也就是他们还可以多缓存一次: \n来动手计算一下,加载带宽为254.922 GB/s,得到计算时间与数据加载时间分别为: \\[\n\\begin{aligned}\nFLOPs &= 2 * M * N * 4 * 2 = 4096 \\\\\nCompute Time &= FLOPs / 1666  = 2.458~NanoSeconds \\\\\nBytes &= (4 * M + 4 * N) * 4 \\\\\nLoad Time &= Bytes / 476.722 = 1.074~NanoSeconds\n\\end{aligned}\n\\]\n此时计算时间减去数据加载时间为1.384 ns, 大于下一次的计算时间1.074 ns, 那么也就表明我们可以在下一次缓存中完美的流水起来, 从而发挥出极限算力."
  },
  {
    "objectID": "posts/mac-amx.html#验证计算策略3",
    "href": "posts/mac-amx.html#验证计算策略3",
    "title": "探索AMX: 解锁Apple Silicon隐藏性能",
    "section": "3.4 验证计算策略3",
    "text": "3.4 验证计算策略3\n为了达到峰值的数据加载性能, 我们需要对矩阵进行布局优化, 也就是将A/B矩阵分别以32为宽度进行pack, 这样加载时是满足内存连续读取, 同时一次可以计算得到2*M*N的结果, 其具体迭代流程图如下: \n为了简单起见我假设M/K/N都满足最小计算单元的倍数, 同时要求输入A/B矩阵都是经过布局优化的, 下面则是具体的代码:\ntemplate &lt;bool LoadC, bool StoreC, size_t KTile, size_t M, size_t N&gt;\nvoid gepdot(size_t curM, size_t curK, size_t curN,\n            const float packedA[KTile][32], const float packedB[KTile][32],\n            float C[M][N]) {\n  static_assert(KTile % 4 == 0, \"not support k%4!=0\");\n\n  if constexpr (LoadC) {\n    // load acc value.\n    for (size_t om = 0; om &lt; 2; om++) {\n      for (size_t m = 0; m &lt; 16; m++) {\n        AMX_STZ(\n            ldstz().row_index(m * 4 + om * 2).multiple().bind(C[om * 16 + m]));\n      }\n    }\n  }\n\n  for (size_t k = curK; k &lt; curK + KTile; k += 4) {\n    for (size_t ok = k; ok &lt; k + 4; ok += 2) {\n      // load [m0,k0], [m1,k0], [m0,k1], [m1,k1]\n      AMX_LDY(ldxy().register_index(0).multiple().multiple_four().bind(\n          (void *)packedA[ok]));\n      // load [n0,k0], [n1,k0], [n0,k1], [n1,k1]\n      AMX_LDX(ldxy().register_index(0).multiple().multiple_four().bind(\n          (void *)packedB[ok]));\n      // compute 8 times.\n      // [[m0,n0],[m0,n1],\n      //  [m1,n0],[m1,n1]]\n      for (size_t ik = ok; ik &lt; ok + 2; ik++) {\n        auto fma = ik == 0 ? fma32().skip_z() : fma32();\n        for (size_t m = 0; m &lt; 2; m++) {\n          for (size_t n = 0; n &lt; 2; n++) {\n            AMX_FMA32(fma.z_row(m * 2 + n)\n                          .y_offset((ik - ok) * 2 * 16 * 4 + m * 16 * 4)\n                          .x_offset((ik - ok) * 2 * 16 * 4 + n * 16 * 4));\n          }\n        }\n      }\n    }\n  }\n\n  // last time need store C.\n  if constexpr (StoreC) {\n    for (size_t om = 0; om &lt; 2; om++) {\n      for (size_t m = 0; m &lt; 16; m++) {\n        AMX_STZ(\n            ldstz().row_index(m * 4 + om * 2).multiple().bind(C[om * 16 + m]));\n      }\n    }\n  }\n}\n在M = 16 * 2, K = 8192, N = 16 * 2的情况下测试, 得到1632.13 Gflop/s的性能, 达到了峰值性能的0.979%.:\n[----------] 1 test from test_amx\n[ RUN      ] test_amx.test_gepdot\n             Gflop/s: 1632.13\n[       OK ] test_amx.test_gepdot (1032 ms)\n[----------] 1 test from test_amx (1032 ms total)"
  },
  {
    "objectID": "posts/linuxblue6.html",
    "href": "posts/linuxblue6.html",
    "title": "Raspi蓝牙:编程实现录音",
    "section": "",
    "text": "经过之前一系列复杂的配置,我搭建好了交叉编译pulseaudio的环境,现在我们可以调用相应的api去进行录音.\n\n\n使用\n我这里使用的例程为parec-simple.c\n当配置好交叉编译环境之后直接执行arm-linux-gnueabihf-gcc parec-simple.c -lpulse -lpulse-simple -o record即可.\n在树莓派中执行,并通过ctrl+c退出录制,接着开始播放~:\npi@raspberrypi:~ $ ./record &gt; test\n^C\npi@raspberrypi:~ $ pacat -pv test --format=s16le\nOpening a playback stream with sample specification 's16le 2ch 44100Hz' and channel map 'front-left,front-right'.\nConnection established.\nStream successfully created.\nBuffer metrics: maxlength=4194304, tlength=352800, prebuf=349276, minreq=3528\nUsing sample spec 's16le 2ch 44100Hz', channel map 'front-left,front-right'.\nConnected to device bluez_sink.E8_07_BF_E1_1B_02.headset_head_unit (index: 5, suspended: no).\nGot EOF.\nStream started.\nPlayback stream drained.: 55360 usec.          \nDraining connection to server.\n\n\n解析\n程序的基本结构如下:\n\n创建一个新的采样流(pa_simple_new)\n读取音频流(pa_simple_read)\n写出输出(write)\n释放流资源(pa_simple_free)"
  },
  {
    "objectID": "posts/linuxblue4.html",
    "href": "posts/linuxblue4.html",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "",
    "text": "上一篇文章描述了如何与串口蓝牙进行通信，这一次记录如何使用蓝牙耳机。"
  },
  {
    "objectID": "posts/linuxblue4.html#配对蓝牙耳机",
    "href": "posts/linuxblue4.html#配对蓝牙耳机",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "配对蓝牙耳机",
    "text": "配对蓝牙耳机\n首先我按照archlinux中的配对方法来进行配对。\nroot@H5:~# bluetoothctl\n[bluetooth]# power on\n[bluetooth]# agent on\n[bluetooth]# default-agent\n[bluetooth]# scan on\n[NEW] Device 1C:52:16:02:40:9F 小米蓝牙耳机青春版\n[bluetooth]# scan off\n[bluetooth]# pair 1C:52:16:02:40:9F\nAttempting to pair with 1C:52:16:02:40:9F\n[CHG] Device 1C:52:16:02:40:9F Connected: yes\n[CHG] Device 1C:52:16:02:40:9F UUIDs: 00001101-0000-1000-8000-00805f9b34fb\n[CHG] Device 1C:52:16:02:40:9F UUIDs: 00001108-0000-1000-8000-00805f9b34fb\n[CHG] Device 1C:52:16:02:40:9F UUIDs: 0000110b-0000-1000-8000-00805f9b34fb\n[CHG] Device 1C:52:16:02:40:9F UUIDs: 0000110c-0000-1000-8000-00805f9b34fb\n[CHG] Device 1C:52:16:02:40:9F UUIDs: 0000110e-0000-1000-8000-00805f9b34fb\n[CHG] Device 1C:52:16:02:40:9F UUIDs: 0000111e-0000-1000-8000-00805f9b34fb\n[CHG] Device 1C:52:16:02:40:9F ServicesResolved: yes\n[CHG] Device 1C:52:16:02:40:9F Paired: yes\nPairing successful\n[CHG] Device 1C:52:16:02:40:9F ServicesResolved: no\n[CHG] Device 1C:52:16:02:40:9F Connected: no\n到这里就出现问题了。"
  },
  {
    "objectID": "posts/linuxblue4.html#检查问题",
    "href": "posts/linuxblue4.html#检查问题",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "检查问题",
    "text": "检查问题\n首先查看错误日志\nsystemctl status bluetooth\n错误日志如下：\n● bluetooth.service - Bluetooth service\n   Loaded: loaded (/lib/systemd/system/bluetooth.service; enabled; vendor preset\n: enabled)\n   Active: active (running) since Fri 2018-06-22 11:02:23 UTC; 12min ago\n     Docs: man:bluetoothd(8)\n Main PID: 933 (bluetoothd)\n   Status: \"Running\"\n    Tasks: 1 (limit: 4915)\n   CGroup: /system.slice/bluetooth.service\n           └─933 /usr/lib/bluetooth/bluetoothd\n\nJun 22 11:02:23 H5 systemd[1]: Starting Bluetooth service...\nJun 22 11:02:23 H5 bluetoothd[933]: Bluetooth daemon 5.43\nJun 22 11:02:23 H5 systemd[1]: Started Bluetooth service.\nJun 22 11:02:23 H5 bluetoothd[933]: Starting SDP server\nJun 22 11:02:23 H5 bluetoothd[933]: Bluetooth management interface 1.14 initiali\nzed\nJun 22 11:02:23 H5 bluetoothd[933]: Failed to obtain handles for \"Service Change\nd\" characteristic\nJun 22 11:02:23 H5 bluetoothd[933]: Sap driver initialization failed.\nJun 22 11:02:23 H5 bluetoothd[933]: sap-server: Operation not permitted (1)\nJun 22 11:02:23 H5 bluetoothd[933]: Failed to set privacy: Rejected (0x0b)\nJun 22 11:11:01 H5 bluetoothd[933]: a2dp-sink profile connect failed for 1C:52:1\n6:02:40:9F: Protocol not available\n这里发现问题出现在a2dp-sink profile connect failed for 1C:52:1 6:02:40:9F: Protocol not available所以需要查找一波资料。"
  },
  {
    "objectID": "posts/linuxblue4.html#解决方法",
    "href": "posts/linuxblue4.html#解决方法",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "解决方法",
    "text": "解决方法\n先试试安装软件包,这里都是root权限下使用\napt-get install pulseaudio-module-bluetooth\nkillall pulseaudio\npulseaudio --start --log-target=syslog\nsystemctl restart bluetooth"
  },
  {
    "objectID": "posts/linuxblue4.html#出现问题",
    "href": "posts/linuxblue4.html#出现问题",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "出现问题",
    "text": "出现问题\n● bluetooth.service - Bluetooth service\n   Loaded: loaded (/lib/systemd/system/bluetooth.service; enabled; vendor preset: enabled)\n   Active: active (running) since Mon 2018-07-09 02:27:15 UTC; 8min ago\n     Docs: man:bluetoothd(8)\n Main PID: 3733 (bluetoothd)\n   Status: \"Running\"\n    Tasks: 1 (limit: 4915)\n   CGroup: /system.slice/bluetooth.service\n           └─3733 /usr/lib/bluetooth/bluetoothd\n\nJul 09 02:27:14 H5 systemd[1]: Starting Bluetooth service...\nJul 09 02:27:14 H5 bluetoothd[3733]: Bluetooth daemon 5.43\nJul 09 02:27:15 H5 systemd[1]: Started Bluetooth service.\nJul 09 02:27:15 H5 bluetoothd[3733]: Starting SDP server\nJul 09 02:27:15 H5 bluetoothd[3733]: Bluetooth management interface 1.14 initialized\nJul 09 02:27:15 H5 bluetoothd[3733]: Failed to obtain handles for \"Service Changed\" characteristic\nJul 09 02:27:15 H5 bluetoothd[3733]: Sap driver initialization failed.\nJul 09 02:27:15 H5 bluetoothd[3733]: sap-server: Operation not permitted (1)\nJul 09 02:27:15 H5 bluetoothd[3733]: Endpoint registered: sender=:1.17 path=/MediaEndpoint/A2DPSink"
  },
  {
    "objectID": "posts/linuxblue4.html#解决问题",
    "href": "posts/linuxblue4.html#解决问题",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "解决问题",
    "text": "解决问题\n这里的sap-server: Operation not permitted (1)是SIM Access Profile,需要禁用他。\n\n打开/etc/systemd/system/bluetooth.target.wants/bluetooth.service\n改变：ExecStart=/usr/lib/bluetooth/bluetoothd为 ExecStart=/usr/lib/bluetooth/bluetoothd --noplugin=sap\n重载systemctl systemctl daemon-reload\n重启蓝牙服务 systemctl restart bluetooth\n查看状态\n● bluetooth.service - Bluetooth service\n Loaded: loaded (/lib/systemd/system/bluetooth.service; enabled; vendor preset\n Active: active (running) since Mon 2018-07-09 03:08:12 UTC; 3s ago\n   Docs: man:bluetoothd(8)\nMain PID: 1768 (bluetoothd)\n Status: \"Running\"\n  Tasks: 1 (limit: 4915)\n CGroup: /system.slice/bluetooth.service\n         └─1768 /usr/lib/bluetooth/bluetoothd --noplugin=sap"
  },
  {
    "objectID": "posts/linuxblue4.html#连接成功",
    "href": "posts/linuxblue4.html#连接成功",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "连接成功",
    "text": "连接成功\n现在打开bluetoothctl就可以连接了\nbluetoothctl\n[NEW] Controller 43:29:B1:55:01:01 H5 [default]\n[NEW] Device 30:22:00:00:8F:67 porbox wireless\n开启代理\nagent on\ndefault-agent\n连接蓝牙耳机\nconnect 30:22:00:00:8F:67\nAttempting to connect to 30:22:00:00:8F:67\n[CHG] Device 30:22:00:00:8F:67 Connected: yes\nConnection successful\n[CHG] Device 30:22:00:00:8F:67 ServicesResolved: yes"
  },
  {
    "objectID": "posts/linuxblue4.html#配置输出",
    "href": "posts/linuxblue4.html#配置输出",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "配置输出",
    "text": "配置输出\n\n查看声卡\n\npactl list cards\n出现了如下\nCard #0\n        Name: bluez_card.30_22_00_00_8F_67\n        Driver: module-bluez5-device.c\n        Owner Module: 22\n        Properties:\n                device.description = \"porbox wireless\"\n                device.string = \"30:22:00:00:8F:67\"\n                device.api = \"bluez\"\n                device.class = \"sound\"\n                device.bus = \"bluetooth\"\n                device.form_factor = \"hands-free\"\n                bluez.path = \"/org/bluez/hci0/dev_30_22_00_00_8F_67\"\n                bluez.class = \"0x240408\"\n                bluez.alias = \"porbox wireless\"\n                device.icon_name = \"audio-handsfree-bluetooth\"\n                device.intended_roles = \"phone\"\n        Profiles:\n                headset_head_unit: Headset Head Unit (HSP/HFP) (sinks: 1, sources: 1, priority: 20, available: yes)\n                a2dp_sink: High Fidelity Playback (A2DP Sink) (sinks: 1, sources: 0, priority: 10, available: yes)\n                off: Off (sinks: 0, sources: 0, priority: 0, available: yes)\n        Active Profile: off\n        Ports:\n                handsfree-output: Handsfree (priority: 0, latency offset: 0 usec)\n                        Part of profile(s): headset_head_unit, a2dp_sink\n                handsfree-input: Handsfree (priority: 0, latency offset: 0 usec)\n                        Part of profile(s): headset_head_unit\n\n单声音配置\n\n查看节点\npacmd list-sinks\n出现\n1 sink(s) available.\n* index: 0\n      name: &lt;auto_null&gt;\n      driver: &lt;module-null-sink.c&gt;\n      flags: DECIBEL_VOLUME LATENCY FLAT_VOLUME DYNAMIC_LATENCY\n      state: SUSPENDED\n      suspend cause: IDLE\n      priority: 1000\n      volume: front-left: 65536 / 100% / 0.00 dB,   front-right: 65536 / 100% / 0.00 dB\n              balance 0.00\n      base volume: 65536 / 100% / 0.00 dB\n      volume steps: 65537\n      muted: no\n      current latency: 0.00 ms\n      max request: 344 KiB\n      max rewind: 344 KiB\n      monitor source: 0\n      sample spec: s16le 2ch 44100Hz\n      channel map: front-left,front-right\n                   Stereo\n      used by: 0\n      linked by: 0\n      configured latency: 0.00 ms; range is 0.50 .. 2000.00 ms\n      module: 13\n      properties:\n              device.description = \"Dummy Output\"\n              device.class = \"abstract\"\n              device.icon_name = \"audio-card\"\n设置默认连接\npacmd set-default-sink 0\npacmd set-card-profile 0 a2dp_sink\n\n查看信息\n\npacmd info\n出现\nMemory blocks currently allocated: 1, size: 63.9 KiB.\nMemory blocks allocated during the whole lifetime: 121, size: 2.6 MiB.\nMemory blocks imported from other processes: 0, size: 0 B.\nMemory blocks exported to other processes: 0, size: 0 B.\nTotal sample cache size: 0 B.\nDefault sample spec: s16le 2ch 44100Hz\nDefault channel map: front-left,front-right\nDefault sink name: bluez_sink.30_22_00_00_8F_67.a2dp_sink\nDefault source name: bluez_sink.30_22_00_00_8F_67.a2dp_sink.monitor\nMemory blocks of type POOL: 1 allocated/75 accumulated.\nMemory blocks of type POOL_EXTERNAL: 0 allocated/0 accumulated.\nMemory blocks of type APPENDED: 0 allocated/0 accumulated.\nMemory blocks of type USER: 0 allocated/0 accumulated.\nMemory blocks of type FIXED: 0 allocated/0 accumulated.\nMemory blocks of type IMPORTED: 0 allocated/46 accumulated.\n23 module(s) loaded.\n    index: 0\n        name: &lt;module-device-restore&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Automatically restore the volume/mute state of devices\"\n                module.version = \"10.0\"\n    index: 1\n        name: &lt;module-stream-restore&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Automatically restore the volume/mute/device state of streams\"\n                module.version = \"10.0\"\n    index: 2\n        name: &lt;module-card-restore&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Automatically restore profile of cards\"\n                module.version = \"10.0\"\n    index: 3\n        name: &lt;module-augment-properties&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Augment the property sets of streams with additional static information\"\n                module.version = \"10.0\"\n    index: 4\n        name: &lt;module-switch-on-port-available&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: no\n        properties:\n\n    index: 5\n        name: &lt;module-udev-detect&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Detect available audio hardware and load matching drivers\"\n                module.version = \"10.0\"\n    index: 6\n        name: &lt;module-bluetooth-policy&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Frédéric Dalleau, Pali Rohár\"\n                module.description = \"Policy module to make using bluetooth devices out-of-the-box easier\"\n                module.version = \"10.0\"\n    index: 7\n        name: &lt;module-bluetooth-discover&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"João Paulo Rechi Vita\"\n                module.description = \"Detect available Bluetooth daemon and load the corresponding discovery module\"\n                module.version = \"10.0\"\n    index: 8\n        name: &lt;module-bluez5-discover&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"João Paulo Rechi Vita\"\n                module.description = \"Detect available BlueZ 5 Bluetooth audio devices and load BlueZ 5 Bluetooth audio drivers\"\n                module.version = \"10.0\"\n    index: 9\n        name: &lt;module-native-protocol-unix&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: no\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Native protocol (UNIX sockets)\"\n                module.version = \"10.0\"\n    index: 10\n        name: &lt;module-default-device-restore&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Automatically restore the default sink and source\"\n                module.version = \"10.0\"\n    index: 11\n        name: &lt;module-rescue-streams&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"When a sink/source is removed, try to move its streams to the default sink/source\"\n                module.version = \"10.0\"\n    index: 12\n        name: &lt;module-always-sink&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Colin Guthrie\"\n                module.description = \"Always keeps at least one sink loaded even if it's a null one\"\n                module.version = \"10.0\"\n    index: 14\n        name: &lt;module-intended-roles&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Automatically set device of streams based on intended roles of devices\"\n                module.version = \"10.0\"\n    index: 15\n        name: &lt;module-suspend-on-idle&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"When a sink/source is idle for too long, suspend it\"\n                module.version = \"10.0\"\n    index: 16\n        name: &lt;module-console-kit&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Create a client for each ConsoleKit session of this user\"\n                module.version = \"10.0\"\n    index: 17\n        name: &lt;module-systemd-login&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Create a client for each login session of this user\"\n                module.version = \"10.0\"\n    index: 18\n        name: &lt;module-position-event-sounds&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Position event sounds between L and R depending on the position on screen of the widget triggering them.\"\n                module.version = \"10.0\"\n    index: 19\n        name: &lt;module-role-cork&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Mute & cork streams with certain roles while others exist\"\n                module.version = \"10.0\"\n    index: 20\n        name: &lt;module-filter-heuristics&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Colin Guthrie\"\n                module.description = \"Detect when various filters are desirable\"\n                module.version = \"10.0\"\n    index: 21\n        name: &lt;module-filter-apply&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: yes\n        properties:\n                module.author = \"Colin Guthrie\"\n                module.description = \"Load filter sinks automatically when needed\"\n                module.version = \"10.0\"\n    index: 22\n        name: &lt;module-bluez5-device&gt;\n        argument: &lt;path=/org/bluez/hci0/dev_30_22_00_00_8F_67&gt;\n        used: 0\n        load once: no\n        properties:\n                module.author = \"João Paulo Rechi Vita\"\n                module.description = \"BlueZ 5 Bluetooth audio sink and source\"\n                module.version = \"10.0\"\n    index: 23\n        name: &lt;module-cli-protocol-unix&gt;\n        argument: &lt;&gt;\n        used: -1\n        load once: no\n        properties:\n                module.author = \"Lennart Poettering\"\n                module.description = \"Command line interface protocol (UNIX sockets)\"\n                module.version = \"10.0\"\n1 sink(s) available.\n  * index: 1\n        name: &lt;bluez_sink.30_22_00_00_8F_67.a2dp_sink&gt;\n        driver: &lt;module-bluez5-device.c&gt;\n        flags: HARDWARE DECIBEL_VOLUME LATENCY FLAT_VOLUME\n        state: SUSPENDED\n        suspend cause: IDLE\n        priority: 9030\n        volume: front-left: 65536 / 100% / 0.00 dB,   front-right: 65536 / 100% / 0.00 dB\n                balance 0.00\n        base volume: 65536 / 100% / 0.00 dB\n        volume steps: 65537\n        muted: no\n        current latency: 0.00 ms\n        max request: 4 KiB\n        max rewind: 0 KiB\n        monitor source: 1\n        sample spec: s16le 2ch 44100Hz\n        channel map: front-left,front-right\n                    Stereo\n        used by: 0\n        linked by: 0\n        fixed latency: 48.22 ms\n        card: 0 &lt;bluez_card.30_22_00_00_8F_67&gt;\n        module: 22\n        properties:\n                bluetooth.protocol = \"a2dp_sink\"\n                device.description = \"porbox wireless\"\n                device.string = \"30:22:00:00:8F:67\"\n                device.api = \"bluez\"\n                device.class = \"sound\"\n                device.bus = \"bluetooth\"\n                device.form_factor = \"hands-free\"\n                bluez.path = \"/org/bluez/hci0/dev_30_22_00_00_8F_67\"\n                bluez.class = \"0x240408\"\n                bluez.alias = \"porbox wireless\"\n                device.icon_name = \"audio-handsfree-bluetooth\"\n                device.intended_roles = \"phone\"\n        ports:\n                handsfree-output: Handsfree (priority 0, latency offset 0 usec, available: unknown)\n                        properties:\n\n        active port: &lt;handsfree-output&gt;\n1 source(s) available.\n  * index: 1\n        name: &lt;bluez_sink.30_22_00_00_8F_67.a2dp_sink.monitor&gt;\n        driver: &lt;module-bluez5-device.c&gt;\n        flags: DECIBEL_VOLUME LATENCY\n        state: SUSPENDED\n        suspend cause: IDLE\n        priority: 1030\n        volume: front-left: 65536 / 100% / 0.00 dB,   front-right: 65536 / 100% / 0.00 dB\n                balance 0.00\n        base volume: 65536 / 100% / 0.00 dB\n        volume steps: 65537\n        muted: no\n        current latency: 0.00 ms\n        max rewind: 0 KiB\n        sample spec: s16le 2ch 44100Hz\n        channel map: front-left,front-right\n                    Stereo\n        used by: 0\n        linked by: 0\n        fixed latency: 48.22 ms\n        monitor_of: 1\n        card: 0 &lt;bluez_card.30_22_00_00_8F_67&gt;\n        module: 22\n        properties:\n                device.description = \"Monitor of porbox wireless\"\n                device.class = \"monitor\"\n                device.string = \"30:22:00:00:8F:67\"\n                device.api = \"bluez\"\n                device.bus = \"bluetooth\"\n                device.form_factor = \"hands-free\"\n                bluez.path = \"/org/bluez/hci0/dev_30_22_00_00_8F_67\"\n                bluez.class = \"0x240408\"\n                bluez.alias = \"porbox wireless\"\n                device.icon_name = \"audio-handsfree-bluetooth\"\n                device.intended_roles = \"phone\"\n2 client(s) logged in.\n    index: 0\n        driver: &lt;module-systemd-login.c&gt;\n        owner module: 17\n        properties:\n                application.name = \"Login Session 2\"\n                systemd-login.session = \"2\"\n    index: 28\n        driver: &lt;cli.c&gt;\n        owner module: 23\n        properties:\n                application.name = \"UNIX socket client\"\n1 card(s) available.\n    index: 0\n        name: &lt;bluez_card.30_22_00_00_8F_67&gt;\n        driver: &lt;module-bluez5-device.c&gt;\n        owner module: 22\n        properties:\n                device.description = \"porbox wireless\"\n                device.string = \"30:22:00:00:8F:67\"\n                device.api = \"bluez\"\n                device.class = \"sound\"\n                device.bus = \"bluetooth\"\n                device.form_factor = \"hands-free\"\n                bluez.path = \"/org/bluez/hci0/dev_30_22_00_00_8F_67\"\n                bluez.class = \"0x240408\"\n                bluez.alias = \"porbox wireless\"\n                device.icon_name = \"audio-handsfree-bluetooth\"\n                device.intended_roles = \"phone\"\n        profiles:\n                headset_head_unit: Headset Head Unit (HSP/HFP) (priority 20, available: unknown)\n                a2dp_sink: High Fidelity Playback (A2DP Sink) (priority 10, available: unknown)\n                off: Off (priority 0, available: yes)\n        active profile: &lt;a2dp_sink&gt;\n        sinks:\n                bluez_sink.30_22_00_00_8F_67.a2dp_sink/#1: porbox wireless\n        sources:\n                bluez_sink.30_22_00_00_8F_67.a2dp_sink.monitor/#1: Monitor of porbox wireless\n        ports:\n                handsfree-output: Handsfree (priority 0, latency offset 0 usec, available: unknown)\n                        properties:\n\n                handsfree-input: Handsfree (priority 0, latency offset 0 usec, available: unknown)\n                        properties:\n\n0 sink input(s) available.\n0 source output(s) available.\n0 cache entrie(s) available.\n查看状态\npacmd stat\n出现\nMemory blocks currently allocated: 1, size: 63.9 KiB.\nMemory blocks allocated during the whole lifetime: 121, size: 2.6 MiB.\nMemory blocks imported from other processes: 0, size: 0 B.\nMemory blocks exported to other processes: 0, size: 0 B.\nTotal sample cache size: 0 B.\nDefault sample spec: s16le 2ch 44100Hz\nDefault channel map: front-left,front-right\nDefault sink name: bluez_sink.30_22_00_00_8F_67.a2dp_sink\nDefault source name: bluez_sink.30_22_00_00_8F_67.a2dp_sink.monitor\nMemory blocks of type POOL: 1 allocated/75 accumulated.\nMemory blocks of type POOL_EXTERNAL: 0 allocated/0 accumulated.\nMemory blocks of type APPENDED: 0 allocated/0 accumulated.\nMemory blocks of type USER: 0 allocated/0 accumulated.\nMemory blocks of type FIXED: 0 allocated/0 accumulated.\nMemory blocks of type IMPORTED: 0 allocated/46 accumulated."
  },
  {
    "objectID": "posts/linuxblue4.html#测试蓝牙耳机",
    "href": "posts/linuxblue4.html#测试蓝牙耳机",
    "title": "OrangePI蓝牙：蓝牙耳机",
    "section": "测试蓝牙耳机",
    "text": "测试蓝牙耳机\n此时先查看连接\npactl list sinks\n出现\nSink #1\n        State: SUSPENDED\n        Name: bluez_sink.30_22_00_00_8F_67.a2dp_sink\n        Description: porbox wireless\n        Driver: module-bluez5-device.c\n        Sample Specification: s16le 2ch 44100Hz\n        Channel Map: front-left,front-right\n        Owner Module: 22\n        Mute: no\n        Volume: front-left: 65536 / 100% / 0.00 dB,   front-right: 65536 / 100% / 0.00 dB\n                balance 0.00\n        Base Volume: 65536 / 100% / 0.00 dB\n        Monitor Source: bluez_sink.30_22_00_00_8F_67.a2dp_sink.monitor\n        Latency: 0 usec, configured 0 usec\n        Flags: HARDWARE DECIBEL_VOLUME LATENCY\n        Properties:\n                bluetooth.protocol = \"a2dp_sink\"\n                device.description = \"porbox wireless\"\n                device.string = \"30:22:00:00:8F:67\"\n                device.api = \"bluez\"\n                device.class = \"sound\"\n                device.bus = \"bluetooth\"\n                device.form_factor = \"hands-free\"\n                bluez.path = \"/org/bluez/hci0/dev_30_22_00_00_8F_67\"\n                bluez.class = \"0x240408\"\n                bluez.alias = \"porbox wireless\"\n                device.icon_name = \"audio-handsfree-bluetooth\"\n                device.intended_roles = \"phone\"\n        Ports:\n                handsfree-output: Handsfree (priority: 0)\n        Active Port: handsfree-output\n        Formats:\n                pcm\n查看声卡\npactl list cards\n出现\nCard #0\n        Name: bluez_card.30_22_00_00_8F_67\n        Driver: module-bluez5-device.c\n        Owner Module: 22\n        Properties:\n                device.description = \"porbox wireless\"\n                device.string = \"30:22:00:00:8F:67\"\n                device.api = \"bluez\"\n                device.class = \"sound\"\n                device.bus = \"bluetooth\"\n                device.form_factor = \"hands-free\"\n                bluez.path = \"/org/bluez/hci0/dev_30_22_00_00_8F_67\"\n                bluez.class = \"0x240408\"\n                bluez.alias = \"porbox wireless\"\n                device.icon_name = \"audio-handsfree-bluetooth\"\n                device.intended_roles = \"phone\"\n        Profiles:\n                headset_head_unit: Headset Head Unit (HSP/HFP) (sinks: 1, sources: 1, priority: 20, available: yes)\n                a2dp_sink: High Fidelity Playback (A2DP Sink) (sinks: 1, sources: 0, priority: 10, available: yes)\n                off: Off (sinks: 0, sources: 0, priority: 0, available: yes)\n        Active Profile: a2dp_sink\n        Ports:\n                handsfree-output: Handsfree (priority: 0, latency offset: 0 usec)\n                        Part of profile(s): headset_head_unit, a2dp_sink\n                handsfree-input: Handsfree (priority: 0, latency offset: 0 usec)\n                        Part of profile(s): headset_head_unit\n现在使用以下命令即可\naplay xxx.wav"
  },
  {
    "objectID": "posts/linkedlist.html",
    "href": "posts/linkedlist.html",
    "title": "单链表",
    "section": "",
    "text": "最近开始准备白天画画板子，写写程序，晚上给自己打点数据结构的基础。首先先实现一下最简单的的单链表。\n\n\nmain.c\n/*\n * @Author: Zheng Qihang \n * @Date: 2018-07-02 21:17:17 \n * @Last Modified by: Zheng Qihang\n * @Last Modified time: 2018-07-02 21:55:11\n */\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include \"mylist.h\"\n\nint main(int argc, char const *argv[])\n{\n    Node *List = NULL;\n    List = InitList(9);//init the list\n    Node * pos =NULL;\n    PrintList(List);\n\n    /* printf(\"Let us test FindPos!\\n\");\n    FindPos(3, List); */\n\n   /*  printf(\"Let us test AddNode!\\n\");\n    AddNode(11,List);\n    PrintList(List); */\n\n/*     printf(\"Let us test DeleteNode!\\n\");\n    DeleteNode(11,List);\n    PrintList(List);\n    DeleteNode(9,List);\n    PrintList(List);\n    DeleteNode(5,List);\n    PrintList(List); */\n    \n/*     printf(\"Let us test ListLength!\\n\");\n    printf(\"List length %d\\n\",ListLength(List)); */\n    \n/*     printf(\"Let us test FindValue!\\n\");\n    if((pos=FindValue(12,List))!=NULL)\n    {\n    printf(\"Node number %d\\n\",pos-&gt;number);\n    }\n    else\n    {\n        printf(\"no value\\n\");\n    } */\n    \n    return 0;\n}\n\n\nmylist.c\n/*\n * @Author: Zheng Qihang \n * @Date: 2018-07-02 21:14:47 \n * @Last Modified by: Zheng Qihang\n * @Last Modified time: 2018-07-02 21:49:15\n */\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include \"mylist.h\"\n#include &lt;time.h&gt;\n#include &lt;stdint.h&gt;\n\n/**\n* description  创建长度为n的链表\n* @param[in]   长度\n* @retval      链表头节点\n**/\nNode *InitList(uint8_t n)\n{\n    Node *Head = NULL; //首先为链表设定一个头节点\n    Node *p = NULL;    //中间变量\n    Node *Last = NULL; //链表的最后一个节点\n    uint8_t cnt = 0;\n    srand(time(0)); //随机种子\n\n    p = (Node *)malloc(sizeof(Node)); //为链表申请内存\n    Head = p;                         //第一个为头\n    Last = Head;\n    Head-&gt;value = rand() % 50;\n    Head-&gt;number = cnt;\n    while (n--)\n    {\n        cnt++;\n        Last = (Node *)malloc(sizeof(Node)); //每次申请的都指向尾节点\n        Last-&gt;value = rand() % 50;\n        Last-&gt;number = cnt;\n        p-&gt;next = Last;\n        p = Last;\n    }\n    Last-&gt;next = NULL;\n    return Head;\n}\n\n/**\n* description  find postion in all list\n* @param[in]   pos list\n* @retval      node address pointer\n**/\nNode *FindPos(uint8_t pos, Node *list)\n{\n    Node *pnode;\n    pnode = list-&gt;next;\n    /* old is  while ((pnode-&gt;next != NULL) && (pos != 0)) \n    I find the x-- to 0 have a x+1 times\n    */\n    while ((pnode-&gt;next != NULL) && (pos != 1))\n    {\n        pos--;\n        pnode = pnode-&gt;next;\n        // printf(\"pos=%2d\\tpos-&gt;number=%d\\n\", pos, pnode-&gt;number);\n    }\n\n    if (pos == 1)\n    {\n        return pnode;\n    }\n    else\n    {\n        return NULL;\n    }\n}\n/**\n* description  find the value first time int List\n* @param[in]   value List\n* @retval      node *\n**/\nNode *FindValue(uint8_t value, Node *list)\n{\n\n    while (list-&gt;next != NULL)\n    {\n        \n        if (list-&gt;value==value) {\n            return list;\n        }\n        list=list-&gt;next;   \n    }\n    return NULL;\n}\n/**\n* description  在指定位置增加节点\n* @param[in]   pos  insert address\n* @retval      void\n**/\nvoid AddNode(uint8_t pos, Node *list)\n{\n    srand(time(0));\n    Node *pnew = NULL;\n    Node *ppos = NULL;\n    Node *pnext = NULL;\n    pnew = (Node *)malloc(sizeof(Node));\n    pnew-&gt;value = rand() % 50;\n    pnew-&gt;next = NULL;\n    ppos = FindPos(pos, list);\n\n    if (ppos != NULL)\n    {\n        if ((pnext = ppos-&gt;next) != NULL)\n        {\n            ppos-&gt;next = pnew;\n            pnew-&gt;next = pnext;\n        }\n        else\n        {\n            ppos-&gt;next = pnew;\n        }\n    }\n    else\n    {\n        printf(\"Add Node failed\\n\");\n    }\n}\n/**\n* description  ret the linked list length\n* @param[in]   list\n* @retval      uint8_t\n**/\nuint8_t ListLength(Node *list)\n{\n    uint8_t cnt = 0;\n\n    while (list-&gt;next != NULL)\n    {\n        cnt++;\n        list = list-&gt;next;\n    }\n    return cnt;\n}\n/**\n* description  delete node in postion\n* @param[in]   postion list header\n* @retval      void\n**/\nvoid DeleteNode(uint8_t pos, Node *list)\n{\n    Node *ppre = FindPos(pos - 1, list); //find a previous node\n    Node *ppos = NULL;                   //will be delete node\n    if ((ppre != NULL) && ((ppos = ppre-&gt;next) != NULL))\n    {\n        ppre-&gt;next = ppos-&gt;next;\n        free(ppos);\n    }\n    else\n    {\n        printf(\"delete node error\\n\");\n    }\n}\n\n/**\n* description  打印整个列表\n* @param[in]   列表头节点\n* @retval      \n**/\nvoid PrintList(Node *list)\n{\n    Node *p = list;\n    int cnt = 0;\n    while (p)\n    {\n        printf(\"addr:%p\\tvlaue:%2d\\tnext:%14p\\tnumber=%d\\n\", p, p-&gt;value, p-&gt;next, p-&gt;number);\n        p = p-&gt;next;\n    }\n    printf(\"\\n\");\n}\n/**\n* description  判断链表是否为空\n* @param[in]   链表头地址\n* @retval      真假\n**/\nint IsEmpty(Node *list)\n{\n    return (list-&gt;next) == NULL;\n}\n\n/**\n* description  判断位置是否为链表尾部\n* @param[in]   位置 链表头地址\n* @retval      真假\n**/\n\nint IsLast(char pos, Node *list)\n{\n    Node *p = list;\n\n    while (p-&gt;next != NULL)\n    {\n        pos--;\n        p = p-&gt;next;\n    }\n    if (pos != 0)\n    {\n        // printf(\"not last\\n\");\n        return -1;\n    }\n    else\n    {\n        // printf(\"is last\\n\");\n        return 0;\n    }\n}\n\n\nmylist.h\n```c /  @Author: Zheng Qihang * @Date: 2018-07-02 21:56:01 * @Last Modified by: Zheng Qihang * @Last Modified time: 2018-07-02 21:56:01 */ #ifndef __MYLIST_H #define __MYLIST_H #include &lt;stdint.h&gt; typedef struct _Node { int value;//链表中的数据项 int number;//计数项 struct _Node * next;//这里还没有定义Node,要加Struct } Node;\nNode InitList(uint8_t n); Node FindPos(uint8_t pos, Node list); void AddNode(uint8_t pos, Node list); void PrintList(Node list); int IsEmpty(Node list); int IsLast(char pos,Node list); void DeleteNode(uint8_t pos, Node list); uint8_t ListLength(Node * list); Node FindValue(uint8_t value, Node list); #endif\n``\n`"
  },
  {
    "objectID": "posts/leetcode.html",
    "href": "posts/leetcode.html",
    "title": "leetcode刷题总结",
    "section": "",
    "text": "唠唠叨叨的刷题总结,不定期更新.\n\n\n栈\n\n[1673] 找出最具竞争力的子序列\n和最大序列差不多，入栈之后检测新元素大小，否则\n[232] 用栈实现队列\n用两个栈进行模拟，pop的时候将暂存区中的数据pop到另一个栈中。\n[42] 接雨水\n想到了用栈，但是怎么用的递推逻辑没想出来。。其实一个栈维护数列然后递归消除。\n[84] 柱状图中最大的矩形\n我觉得比接雨水还难一点，最优解法需要设置一个哨兵，因为每次将大于当前值的数据出栈了，没有考虑到出栈后的元素和后续元素中存在着联系，亦或者说是当前的元素如果是整个数列的最小值，那么需要考虑到\\(nums.size()*min\\_value\\)的情况，所以利用哨兵的机制去计算。\n[85] 最大矩形\n这题属实恶心,我一开始以为和最大正方形面积一样的求法,然后发现并不行…是84题的纵向扩展..\n[32] 最长有效括号\n本来我还想用回文串。。不过一看数据规模就知道gg。正确做法是用栈模拟，加哨兵可以更加保持规律。或者用计数器就无法知道什么时候分段比如 “()((())”，所以从两边进行遍历得到最大结果。\n\n\n\n递归\n\n[95] 不同的二叉搜索树 II\n需要了解如何建树，递归构建不同的左子树和右子树然后合并。\n[37] 解数独 吐了,模板题做了老半天,最后发现是x和y有个地方写反了.\n\n\n\n动态规划\n\n[486] 预测赢家\ndp记录当前选择下的相对分数.\n[264] 丑数 II\n想到了dp,但是没想到那样dp.每个数字都是2,3,5的倍数,循环乘上去放入dp数组,要注意放入时的大小,以及去除重复项.\n[518] 零钱兑换 II\n需要理解动态规划下的组合去重方法。\n[474] 一和零\n二维背包问题，从大到小更新dp数组是关键。\n[89] 格雷编码\n找规律，类似dp，实际上下一个数的编码的多余部分就是上一个编码的镜像后前缀加1\n[740] 删除与获得点数\n将问题转换为打家劫舍。\n[1388] 3n 块披萨\n打家劫舍的另外一种版本，核心都还是序列中非连续数的最大和。推导出3n中取数的实际可能情况即为 在3n的数列中取不相邻元素的最大和。 利用2维dp进行求解。\n[873] 最长的斐波那契子序列的长度\n设置2维dp，由于数列是强递增的因此可以利用坐标确定子序列的最后两个元素，然后利用二分或者字典找到有没有x1。\\(dp[x2][x3]=dp[x1][x2]+1\\)\n[1027] 最长等差数列\n用873一样的套路会超时，而且这个重复问题也非常头疼。以后遇到会重复的直接暴力开最大容量的dp把。\n[813] 最大平均值和的分组\n带状态的一维dp，\\(dp[i][k]\\)是第i个位置切第k刀。比如我们要分三组得到最大值，假设从\\([0~i]\\)是1,2组，从\\([i~n]\\)为第三组。 满足 \\(mean([0~j])+mean([j~k])\\)最大, 那么从\\([0~j] [j~i]\\)分别为1，2组，他们也满足\\(mean([0~j])+mean([j~i])\\)最大，那么就分解成子问题了，即我们每个位置的最大值是,切上一刀的位置到这一刀的位置的,两组和最大. 特殊情况： 对于切一刀的情况，永远都是均值。\n[188] 买卖股票的最佳时机 IV\n对于股票买卖问题，如果是限制次数的问法，那么就一个套路：\n\n当前持有 最大收益\n\n\n保持不变\n上一次卖出后+今天买入\n\n\n当前不持有 最大收益\n\n\n保持不变\n这次买入后+今天卖出\n\n[309] 最佳买卖股票时机含冷冻期\n和188一样的套路，但是因为包含冷冻期，实际上购买n次的纵向扩展，购买n次的时候用的是一个变量进行反复更新，这里需要将一个变量扩展为3天进行滚动更新。\n[72] 编辑距离\n\\(dp[y][x]\\) 表示 \\(word1[:y]\\) 修改为\\(word2[:x]\\) 的最大编辑距离。\n\n修改: \\(aa\\rightarrow ab = a\\rightarrow a + 1\\)。\n\n\\(dp[y][x]=dp[y-1][x-1]+1\\);\n\n添加: \\(aa\\rightarrow ab = a\\rightarrow ab + 1\\)。\n\n\\(dp[y][x]=dp[y-1][x]+1\\);\n\n删除: \\(aa\\rightarrow ab = aa\\rightarrow a + 1\\)。\n\n\\(dp[y][x]=dp[y][x-1]+1\\);\n\n当\\(s1[y]==s2[x]\\)时不需要操作\n\n\n\n\n数组\n\n[9] 回文数 这题最好要想到可以部分翻转。\n[992] K 个不同整数的子数组\n求解最多K个不同数字组成的数组,通过累加的方式我们将[2,1]这种情况累积到结果中,最终我们只需要删除 [1],[2] 这些值就能获得恰好K个不同数字组成的数组个数.\n\n假设 1:\n\nans + 1 = ans + [1]\n\n假设 12:\n\nans + 2 = ans + [2] , [1,2]\n\n假设 121:\n\nans + 3 = ans + [1] , [2,1] , [1,2,1]\n\n\n[214] 最短回文串\n这题利用kmp算法是极好的.kmp算法可以将找到前缀和后缀中的相同部分.对于一个回文串,我们将其分为两部分,把后面一部分反向之后就可以发现他的前缀和后缀从中心开始匹配:abfda b adfba-&gt;adfba b adfba.\n对于一个半回文串abadfba,我们找他最短的回文串,那么就是要找到他的重复部分: \\[\n\\begin{aligned}\n  &\\mathbf{aba}dfba \\\\\n  abfd&\\mathbf{aba} \\\\\n  &\\downarrow \\\\\n  abfd&\\mathbf{aba}dfba\n\\end{aligned}\n\\]\n那么我们现在有kmp算法可以算前缀和后缀中重复部分,那么我们把可能出现重复的部分分别安排到前缀和后缀开始执行kmp算法,利用kmp算法求解到next数组,这时候next数组的最后一位恰巧指向翻转字符串的重复部分的起始位置,截断后加上原字符串即可: \\[\n\\begin{aligned}\n  &\\mathbf{aba}dfba \\\\\n  abfd&\\mathbf{aba} \\\\\n  &\\downarrow \\\\\n  \\mathbf{aba}dfba&\\$abfd\\mathbf{aba} \\\\\n  &\\downarrow\\ (kmp) \\\\\n   , a, b, a, d, f, b, a, &\\$, a, b, f, d, a, b, a\\\\\n  -1, 0, 0, 1, 0, 0, 0, 1, &0, 1, 2, 0, 0, 1, 2, 3(\\text{next数组})\\\\\n  &\\downarrow\\ (merge) \\\\\n  abfd &+\\\\\n  &\\mathbf{aba}dfba\\\\\n  &\\downarrow\\ (merge) \\\\\n  abfd&\\mathbf{aba}dfba\n\\end{aligned}\n\\]\n最后附上kmp中计算next数组的一个巧妙实现,来自b站:\nint findMaxCommon(const string& pattern) {\n  int n = pattern.size();\n  vector&lt;int&gt; next(n + 1, 0);\n  for (int i = 0, j = next[0] = -1; i &lt; n; next[++i] = ++j) {\n    while ((j != -1) and pattern[j] != pattern[i]) {\n      j = next[j];\n    }\n  }\n  return next[n];\n}\n\n\n\n并查集\n\n[1489] 找到最小生成树里的关键边和伪关键边\n利用kruskal算法找到最小生成树,然后根据题意暴力寻找.\n[1319] 连通网络的操作次数\n直接并查集,记录无法merge的次数,最终集合与次数比较一下即可.\n[959] 由斜杠划分区域\n一般想法是把每个区域划分四个格子，然后并查。巧妙的方法是为每个连接线进行并查，如果成环了，那么分割区域加1.\n[128] 最长连续序列 将数组全部放到set中然后寻找\n\n\n\n树\n\n[114] 二叉树展开为链表\n找到root左边的右边前驱，将root右边给右边前驱。递归的方法真的挺难想的。\n[117] 填充每个节点的下一个右侧节点指针 II\n难度不算高，需要注意先递归右侧，否则左侧无法连接到右侧。\n[440] 字典序的第K小数字\n这题真的有点难,需要想到字典树,然后对分情况对字典树的间隔元素数量进行统计,我就是半天都没想好怎么去统计间隔数量,下面的代码能很好的解决这个问题,我开始是考虑分层,第一层10个,第二层100个.但是下面的代码很巧妙直接分层加统计一步到位.太妙了.\nint getsteps(long pre, long n) {\n  long cur = pre;\n  long next = pre + 1;\n  long step = 0;\n  while (cur &lt;= n) {\n    step += min(next, n + 1) - cur;\n    cur *= 10;\n    next *= 10;\n  }\n  return step;\n}\n\n\n\n哈希表\n\n\n链表\n\n\n字符串\n\n[424] 替换后的最长重复字符\n这种类型用滑动窗口，我做过一次第二次还是一直出错。写的时候需要注意滑动窗口自带剪枝的功能，对于右侧只需要一直扩展即可，左侧看时机进行收缩，并不需要收缩到特定长度，因为小于当前窗体长度的解被剪枝了。\n[76] 最小覆盖子串 滑动窗口模板题,算是较为简单的hard了.\n\n\n\n贪心\n\n[763] 划分字母区间\n每一步都更新当前区间的最大范围,直到最大范围就截取\n[621] 任务调度器\n通过贪心构造结果的公式，直接计算。否则需要进行模拟。\n[45] 跳跃游戏 II\n我觉得应该算动态规划的优化版，动态规划超时思考了一番我才写出贪心。\n[376] 摆动序列\n用两个状态的dp，或者用贪心去找峰值，如果找到峰值就更新。\n[406] 根据身高重建队列\n又是做过一次还是没想出来，按身高从大到小，排列从小到大排序。然后根据排序进行插入就能构建成功。我觉得重点在于身高高的人插入时不需要考虑身高低的，亦或者说每个人的排列去除矮的人后即按位排序。\n\n\n\n二分查找\n\n[153] 寻找旋转排序数组中的最小值\n简单二分,比较mid和right的大小调整区间即可.\n[81] 搜索旋转排序数组 II\n不简单二分，太考验细节，首先需要加上\\(left=n[left]==n[mid]?left++:left\\)消除重复项，并且如果数组全部重复，时间复杂度为\\(O(n)\\)。接下来判断前面部分有序还是后面部分有序，再判断target是不是在有序部分。\n[35] 搜索插入位置 二分的模板用这个比较好,最后结果的l位置是第一个不小于target值的位置,可以理解为以target的下界.\nint l = 0, r = nums.size(), mid;\nwhile (l &lt; r) {\n  mid = (l + r) / 2;\n  if (target == nums[mid]) {\n    return mid;\n  } else if (target &lt; nums[mid]) {\n    r = mid;\n  } else if (target &gt; nums[mid]) {\n    l = mid + 1;\n  }\n}\nreturn l;\n\n\n\n概率论\n\n[470] 用 Rand7() 实现 Rand10()\n\n需要理解\\(Rand7()+7*(Rand7()-1)\\)可以实现在\\([1\\sim49]\\)中随机采样，那么我们可以从\\(40\\%10+1\\)得到\\([1\\sim10]\\)的随机采样。\n取巧方法是\\(Rand7()-1\\)生成奇数偶数，\\(Rand7()&lt;6\\)生成\\([1\\sim5]\\)，然后根据\\(\\frac{1}{2}\\times\\frac{1}{5}\\)得到\\(\\frac{1}{5}\\)\n\n\n\n\nBFS\n\n[1631] 最小体力消耗路径\n这题看起来像dp，其实不能直接dp。做法有很多，可以并查集、二分+dfs。我觉得dp+bfs的方法比较好。\n\n\n\n数字\n\n[231] 2的幂\n方法有两种，方法一 : \\((2&lt;&lt;30 \\% n) ==0\\)，因为2的幂次的倍数都是最大幂次的因子，因此肯定为0。 方法二：\\((n \\& -n) == n\\) ， 在计算机里面 \\(-n = ~n + 1\\)，且\\(n \\& -n\\)得到结果是\\(n\\)的二进制表示中最靠右的第一个1。那如果他只有一个1构成，就必然等于自身。\n[326] 3的幂\n可以连续除3进行判断。或者利用\\(\\log_3 \\text{MAX_INT}\\)找到最大的3的幂次，然后利用2的幂的方法进行计算。\n[342] 4的幂\n这题就不好用上面的方法了，因为4的因子有2。但4也是2的倍数，通过\\((n \\& -n) == n\\)加上\\(n\\%3==1\\)的条件可以进行判断。\n还有一个常用技巧是\\(n \\& (n-1)==0\\)， \\(n-1\\)可以将最右边第一个1置零，后序设置为1，可以用来快速找到第一个1的位置或者判断是不是只有一个1. 或者利用 \\(n \\&= (n-1)\\)来消除1，可以统计1出现的次数(比循环右移更快)。\n[260] 只出现一次的数字 III\n用异或找到两个不同数字的异或结果，然后利用\\(n \\& (n-1)\\)找到他们第一个不同的位，根据这mask将数组分为两组进行单独异或。\n[201] 数字范围按位与\n找到两个数的公共前缀即可。\n[477] 汉明距离总和\n我们直接统计每一个位上面的差异个数，那么每个位的汉明距离为\\(k*(n-k)\\)，累加即可。\n[421] 数组中两个数的最大异或值\n字典树+dfs"
  },
  {
    "objectID": "posts/kmeans.html",
    "href": "posts/kmeans.html",
    "title": "kmeans 实现",
    "section": "",
    "text": "kmeans是最基础的聚类算法,我这里写一下流程就直接上我写的代码了.\n\n\n流程\n\n从n个数据中随机选择k个点作为聚类中心\n遍历n个数据,此点离哪一个聚类中心最近就分配到哪一类中\n求出每一类的均值,作为新的聚类中心\n重复第2步直到新的聚类中心与之前聚类中心之差小于阈值e\n输出\n\n\n\n程序\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef createDataSet():\n    \"\"\"\n    创建测试的数据集，里面的数值中具有连续值\n    :return:\n    \"\"\"\n    dataSet = [\n        [0.697, 0.460], [0.774, 0.376], [0.634, 0.264],\n        [0.608, 0.318], [0.556, 0.215],\n        [0.403, 0.237], [0.481, 0.149], [0.437, 0.211],\n        [0.666, 0.091], [0.243, 0.267],\n        [0.245, 0.057], [0.343, 0.099], [0.639, 0.161],\n        [0.657, 0.198], [0.360, 0.370],\n        [0.593, 0.042], [0.719, 0.103], [0.359, 0.188],\n        [0.339, 0.241], [0.282, 0.257],\n        [0.748, 0.232], [0.714, 0.346], [0.483, 0.312],\n        [0.478, 0.437], [0.525, 0.369],\n        [0.751, 0.489], [0.532, 0.472], [0.473, 0.376],\n        [0.725, 0.445], [0.446, 0.459],\n    ]\n\n    # 特征值列表\n\n    labels = ['密度', '含糖率']\n\n    # 特征对应的所有可能的情况\n    labels_full = {}\n\n    for i in range(len(labels)):\n        labelList = [example[i] for example in dataSet]\n        uniqueLabel = set(labelList)\n        labels_full[labels[i]] = uniqueLabel\n\n    return dataSet, labels, labels_full\n\n\n# 计算一个点与所有聚类中心的欧式距离\ndef dist_eclud(sample, centers):\n    # sample=[1 x n] n是特征数\n    sample = np.array(sample)\n    # 将sample复制k次 与 centers 矩阵维度相同\n    samples = np.tile(sample, (centers.shape[0], 1))\n    # 矩阵相减然后平方，对每一列求和　distances＝[1 x k]\n    distances = np.power(samples - centers, 2).sum(axis=1)\n    return distances\n\n\nclass Kmeans():\n    \"\"\"Kmeans聚类算法.\n\n    Parameters:\n    -----------\n    k: int\n        聚类的数目.\n    max_iterations: int\n        最大迭代次数.\n    varepsilon: float\n        判断是否收敛, 如果上一次的所有k个聚类中心与本次的所有k个聚类中心的差都小于varepsilon,\n        则说明算法已经收敛\n    \"\"\"\n    # 从所有样本中随机选取self.k样本作为初始的聚类中心\n\n    def __init__(self, k=2, max_iterations=500, varepsilon=0.0001):\n        self.k = k\n        self.max_iterations = max_iterations\n        self.varepsilon = varepsilon\n\n    def init_rand_center(self, data)-&gt;np.ndarray:\n        # 生成k行的中心点\n        centroids = np.zeros((self.k, np.shape(data)[1]))\n        # 从data中随机选择一个\n        row = np.random.choice(np.shape(data)[0], size=self.k)\n        # print('初始随机点坐标为:', row)\n        for i in range(self.k):\n            centroids[i] = data[row[i]]\n        return centroids\n\n    # 返回距离该样本最近的一个中心索引[0, k)\n    def _closest_centroids(self, sample, centroids):\n        # 计算点sample 到 几个聚类中心的距离\n        distance = dist_eclud(sample, centroids)\n        # 找的最小的距离下标 并返回\n        closest_i = np.argmin(distance)\n        return closest_i\n\n    # 分类,将样本分类到距离他最近的中心\n    def create_clusters(self, centroids, X):\n        # 生成 [k x 1] 的二维list\n        clusters = [[] for i in range(self.k)]\n        # 获得X中的每一个元素及其下标\n        for index, sample in enumerate(X):\n            # 求得距离sample最近的距离中心\n            centroid_i = self._closest_centroids(sample, centroids)\n            # 将sample的下标放入对应的簇中\n            clusters[centroid_i].append(index)\n        # 返回该簇\n        return clusters\n\n    # 对中心进行更新\n    def update_centroids(self, clusters, X):\n        # 获得特征数\n        n_features = np.shape(X)[1]\n        # 新的聚类中心\n        centroids = np.zeros((self.k, n_features))\n        # 遍历簇\n        for i, cluster in enumerate(clusters):\n            # 将每一簇的元素提出,并按行求平均  x平均  y平均\n            centroid = np.mean(X[cluster], axis=0)\n            # 平均值就作为新的聚类中心\n            centroids[i] = centroid\n        return centroids\n\n    # 将所有样本进行归类，其所在的类别的索引就是其类别标签\n    def get_cluster_labels(self, clusters, X):\n        y_pred = np.zeros(np.shape(X)[0])\n        # 遍历簇 构造标签矩阵\n        for cluster_i, cluster in enumerate(clusters):\n            for sample_i in cluster:\n                y_pred[sample_i] = cluster_i\n        return y_pred\n\n\nif __name__ == \"__main__\":\n    # 步骤一（替换sans-serif字体）\n    plt.rcParams['font.sans-serif'] = ['YaHei Consolas Hybrid']\n    # 步骤二（解决坐标轴负数的负号显示问题）\n    plt.rcParams['axes.unicode_minus'] = False\n\n    dataset, labels, label_full = createDataSet()\n    X = np.array(dataset)  # type:np.ndarray\n\n    km = Kmeans(3, 100, 0.001)  # 设置参数\n    center = km.init_rand_center(X)  # 随机初始聚类中心\n    # 迭代到最大次数或聚类中心不变\n    for _ in range(km.max_iterations):\n        clusters = km.create_clusters(center, X)\n        # 存放上一次中心点值\n        former_centroids = center\n        # 计算新的聚类中心\n        center = km.update_centroids(clusters, X)\n        diff = center - former_centroids\n        # 如果聚类中心几乎没有变化，说明算法已经收敛，退出迭代\n        if diff.any() &lt; km.varepsilon:\n            break\n    y_pred = km.get_cluster_labels(clusters, X)\n\n    C = np.c_[X, y_pred]\n    # 画出聚类\n    plt.scatter(C[:, 0], C[:, 1], c=C[:, 2])\n    # 画出当前聚类中心\n    c = [float(i) for i in range(center.shape[0])]\n    plt.scatter(center[:, 0], center[:, 1], c=c, marker='s')\n    plt.xlabel('密度')\n    plt.ylabel('含糖量')\n    plt.title('k-means聚类')\n    plt.show()\n\n\n运行效果"
  },
  {
    "objectID": "posts/keras-outshape.html",
    "href": "posts/keras-outshape.html",
    "title": "tf.keras.Model.outputs隐藏问题",
    "section": "",
    "text": "今天发现tf.keras.Model.outputs的隐藏问题(feature),我居然之前都没有发现233\n\n\n描述\n其实就是我这次的模型输出的是List[Tuple]的方式,然后我以为keras的模型输出还是List的形式,算loss的时候就一直出错.\ntrain_model.outputs\n[&lt;tf.Tensor 'MConv_Stage1_L1_5_bn/Identity:0' shape=(None, 80, 120, 38) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage1_L2_5_bn/Identity:0' shape=(None, 80, 120, 19) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage2_L1_5_bn/Identity:0' shape=(None, 80, 120, 38) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage2_L2_5_bn/Identity:0' shape=(None, 80, 120, 19) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage3_L1_5_bn/Identity:0' shape=(None, 80, 120, 38) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage3_L2_5_bn/Identity:0' shape=(None, 80, 120, 19) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage4_L1_5_bn/Identity:0' shape=(None, 80, 120, 38) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage4_L2_5_bn/Identity:0' shape=(None, 80, 120, 19) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage5_L1_5_bn/Identity:0' shape=(None, 80, 120, 38) dtype=float32&gt;,\n &lt;tf.Tensor 'MConv_Stage5_L2_5_bn/Identity:0' shape=(None, 80, 120, 19) dtype=float32&gt;]\n然后我发现实际上模型的输出就是按原本的形式:\ntrain_model.output_shape\n[((None, 80, 120, 38), (None, 80, 120, 19)),\n ((None, 80, 120, 38), (None, 80, 120, 19)),\n ((None, 80, 120, 38), (None, 80, 120, 19)),\n ((None, 80, 120, 38), (None, 80, 120, 19)),\n ((None, 80, 120, 38), (None, 80, 120, 19))]\n结论就是tf.keras比我想的完善多了233"
  },
  {
    "objectID": "posts/keras-loss-error.html",
    "href": "posts/keras-loss-error.html",
    "title": "Tensorflow中的错误记录",
    "section": "",
    "text": "以后这篇文章就来记录tensorflow中遇到的问题与解决方式。\n\n\n1. 自定义loss中的reshape问题\n我想在loss函数中对tensor进行reshape，在model.compile的时候，keras会生成两个虚placeholder来进行尺寸检查，比如我的yolo中的y_true会生成为(?, ?, ?, ?, ?)，y_pred会按照tf.dataset来生成(?, 7, 10, 5, 16)。\n这个时候我对标签reshape给的参数为tf.TensorShape(None, 7, 10, 5, 8, 2)，但是报错如下：\nValueError: Tried to convert 'shape' to a tensor and failed. Error: Cannot convert a partially known TensorShape to a Tensor: (?, 7, 10, 5, 5, 2)\n解决方式：\n咋一看这个出错好像很蠢，但其实是因为在尺寸检查的时候不接受未知的尺寸None，所以把上面修改为：tf.TensorShape(batch_size, 7, 10, 5, 8, 2)即可。\n\n\n2. Map_fn或者While_Loop速度很慢\n这个问题的确很蛋疼，我看了github的issue，这两个函数都不能有效的进行GPU加速，但是我又需要对一个batch中的每个样本对进行单独处理，这就很难受。\n解决方式：\n还好tensorflow的构建可以是静态图的方式，像我这样知道batch size的情况下，就可以使用在构建graph的时候循环构建一波。如：\nmasks = []\nfor bc in range(helper.batch_size):\n    vaild_xy = tf.boolean_mask(t_xy_A[bc], obj_mask[bc])\n    vaild_wh = tf.boolean_mask(t_wh_A[bc], obj_mask[bc])\n    iou_score = tf_iou(pred_xy[bc], pred_wh[bc], vaild_xy, vaild_wh)\n    best_iou = tf.reduce_max(iou_score, axis=-1, keepdims=True)\n    masks.append(tf.cast(best_iou &lt; iou_thresh, tf.float32))\ntf.parallel_stack(masks)\n\n\n3. 使用tf.keras构建模型时Tensorboard无法显示graph\n之前我写yolo的时候，使用Tensorboard去查看图形时，一直显示如下\nGraph visualization failed\nError: The graph is empty. This can happen when TensorFlow could not trace any graph. Please refer to https://github.com/tensorflow/tensorboard/issues/1961 for more information.\n然后我看了issue，全是因为tf2的eager的原因，我这里又没有用这个模式，怎么会出这个问题呢。\n解决方式：\n找了半天解决方式，就是没找到，我本来想按照以前的方式做，忽然发现就可以了，在callback之后加一句话即可，如下：\ncbs.append(TensorBoard(str(log_dir), update_freq='batch', profile_batch=3))\nfile_writer = tf.summary.FileWriter(str(log_dir), sess.graph)  # NOTE avoid can't write graph, I don't now why..\n\n\n4. tf.keras中Model复用\n这个其实不算问题，只不过我不太清楚，就做了个测试来验证一下。就是比如我们用Sequential构建了一个body部分，然后用这个body产生多个输出，我一开始不知道他这样使用是否是公用参数了，然后我就写了个函数测试了下：\ninput_sim = k.Input((113))\ninput_measre = k.Input((113))\n\nbodymodel = k.Sequential([\n    kl.Dense(64, activation=tf.nn.leaky_relu),\n    kl.Dense(32, activation=tf.nn.leaky_relu),\n    kl.Dense(1),\n])\n\nout_1 = bodymodel(input_sim)\nout_2 = bodymodel(input_measre)\n\nmodel = k.Model([input_sim, input_measre], [out_1, out_2])\nk.utils.plot_model(model, show_shapes=True, to_file='two_in.png')\nfwriter = tf.summary.FileWriter('logs', graph=sess.graph)\n结果：\n这样的复用方式是共享参数的，可以看到，两个sequential，一个含有kernel，另一个没有，或者说他们公用一个kernel。\n\n\n\n5. Error while reading resource variable xxx from Container: localhost. This could mean that the variable was uninitialized.\n我想在tf.keras里面使用苏神的Lookahead，他的代码是用于纯keras的，但是我现在用tf.keras，虽然表层使用看起来差不多，但是核心代码我发现还是很多都不一样。我的问题出现在这里:\nfast_params = model._collected_trainable_weights\n\nwith K.name_scope('training'):\n    with K.name_scope(model.optimizer.__class__.__name__):\n        training_updates = model.optimizer.get_updates(\n            params=fast_params,\n            loss=model.total_loss)\n        slow_params = [K.variable(p) for p in fast_params]\n使用K.variable转换参数的时候出错了，说我的变量没有被初始化。\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable batch_normalization/gamma from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/batch_normalization/gamma/N10tensorflow3VarE does not exist.\n         [[{{node training/RAdam/Variable_274/Initializer/ReadVariableOp}}]]\n解决方案：\ngoogle了一下也没看到有人有相同的问题，我抱着试试看的心态写了如下代码：\nsess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\ntrain_model.compile(optimizer, loss=losses, metrics=metrics)\n在model.compile之前全局初始化，然后就完事了？然后就可以用上最新的优化算法RAdam和Lookahead咯。\n\n\n6. tf.data对于多输入多输出模型时的操作\n我现在的模型是3输入,2输出的,tf.data输出的应该为( (a,b,c) , (label_a,label_b) ),然后我原本代码如下:\nreturn [a_img, p_img, n_img], [1., 1.]\n然后dataset对象就是这样:\n&lt;DatasetV1Adapter shapes: ((32, 3, 96, 96, 3), (32, 2)), types: (tf.float32, tf.float32)&gt;\n解决方案:\n用元组即可,不然默认是一个张量对象,会把我们的结构破坏掉.\nreturn (a_img, p_img, n_img), (1., 1.)\n然后dataset对象就是这样:\n&lt;DatasetV1Adapter shapes: (((32, 96, 96, 3), (32, 96, 96, 3), (32, 96, 96, 3)), ((32,), (32,))), types: ((tf.float32, tf.float32, tf.float32), (tf.float32, tf.float32))&gt;"
  },
  {
    "objectID": "posts/k210img-test.html",
    "href": "posts/k210img-test.html",
    "title": "k210图片测试",
    "section": "",
    "text": "马上要开始移植模型到k210中,首先我需要知道k210中的图像数据是如何排列的.所以需要做一个测试."
  },
  {
    "objectID": "posts/k210img-test.html#jpg转img_buf",
    "href": "posts/k210img-test.html#jpg转img_buf",
    "title": "k210图片测试",
    "section": "jpg转img_buf",
    "text": "jpg转img_buf\n这里发现要把两个rgb拼成一个32位的数据,图片的显示效果会更好.\ndef test_jpg_to_array():\n    img = fddb._read_img('data/2.jpeg', True)\n    img = img.astype('uint8')\n    with open('tmp/img.h', 'w') as f:\n        f.write('#ifndef _IMG_H_ \\n\\\n#define _IMG_H_ \\n\\\n#include &lt;stdint.h&gt; \\n\\\nuint32_t rgb_image[38400] __attribute__((aligned(64))) = {')\n        for i in range(img.shape[0]):\n            for j in range(0, img.shape[1], 2):\n                rR = img[i, j, 0] &gt;&gt; 3\n                rG = img[i, j, 1] &gt;&gt; 2\n                rB = img[i, j, 2] &gt;&gt; 3\n                rrgb = (rR &lt;&lt; 11) | (rG &lt;&lt; 5) | rB\n                lR = img[i, j+1, 0] &gt;&gt; 3\n                lG = img[i, j+1, 1] &gt;&gt; 2\n                lB = img[i, j+1, 2] &gt;&gt; 3\n                lrgb = (lR &lt;&lt; 11) | (lG &lt;&lt; 5) | lB\n                f.write(str(hex(rrgb &lt;&lt; 16 | lrgb))+',')\n            f.write('\\n')\n        f.write('};\\n\\\n#endif')"
  },
  {
    "objectID": "posts/k210img-test.html#jpg转ai_buf",
    "href": "posts/k210img-test.html#jpg转ai_buf",
    "title": "k210图片测试",
    "section": "jpg转ai_buf",
    "text": "jpg转ai_buf\n这里就是把hwc移动成chw数据,然后写入即可.\ndef test_jpeg_to_ai_array():\n    img = fddb._read_img('data/2.jpeg', True)\n    img = img.astype('uint8')\n    img = np.rollaxis(img, 2, 0)\n    with open('tmp/aiimg.h', 'w') as f:\n        f.write('#ifndef _AIIMG_H_ \\n\\\n#define _AIIMG_H_ \\n\\\n#include &lt;stdint.h&gt; \\n\\\nuint8_t ai_image[] __attribute__((aligned(64))) = {')\n        for c in range(img.shape[0]):\n            for i in range(img.shape[1]):\n                for j in range(img.shape[2]):\n                    f.write(str(hex(img[c, i, j]))+',')\n                f.write('\\n')\n        f.write('};\\n\\\n#endif')"
  },
  {
    "objectID": "posts/k210img-test.html#显示图片部分",
    "href": "posts/k210img-test.html#显示图片部分",
    "title": "k210图片测试",
    "section": "显示图片部分",
    "text": "显示图片部分\n只需要修改成对应的buf即可\n    lcd_draw_picture(0, 0, 320, 240, rgb_image);"
  },
  {
    "objectID": "posts/k210img-test.html#kpu部分",
    "href": "posts/k210img-test.html#kpu部分",
    "title": "k210图片测试",
    "section": "kpu部分",
    "text": "kpu部分\n只需要把task.src改成对应的buf即可.\n    /* init face detect model */\n    kpu_task_gencode_output_init(&task);\n    task.src= (uint64_t *)ai_image;\n    task.dma_ch= 5;\n    task.callback= ai_done;\n    kpu_single_task_init(&task);"
  },
  {
    "objectID": "posts/k210conv.html",
    "href": "posts/k210conv.html",
    "title": "使k210支持Tensorflow卷积",
    "section": "",
    "text": "我昨天咋编译模型的时候,碰到k210 model-compiler提示ValueError: conv2d MobilenetV1/Conv2d_12_depthwise/depthwise:0 should use padding=SAME.他说我的卷积输入不正确,让我使用samepadding.但是我查看了代码,的确使用的same卷积.所以今天就来解决下这个问题.\n\n\n1. 查看代码\n首先我看了下出错的代码:\nif self.input_shape[1:3] != self.output_shape[1:3]:\n    print(self.input_shape, self.output_shape)\n    raise ValueError('conv2d {} should use padding=SAME'.format(tensor_info.get('name', 'noname')))\n上面的意思是要卷积输入输出中间两维形状要相等.然后我print了一下我的shape:\n[layer 23]: MobilenetV1/Conv2d_12_depthwise/Relu6:0\n           shape(HWC): 15x20x256 ==&gt; 16x20x256\n           scale,bias: (0.023529411764705882,0.0) ==&gt; (0.023529411764705882,0.0)\n[5, 15, 20, 256] [5, 16, 20, 256]\n可以看到,他这里提示15与16不匹配了.\n\n\n2. 查看原图\n因为我是强行把mobilenet的224输入改成了(240,320)的输入.所以会有一些维度上的冲突.\n我查看原图的节点时发现:\n[?,15,20,256]=== stride=2 padding='same' ===&gt;[?,8,10,256]\n\n\n3. 原因\n这里我就想通了,在Tensorflow中,对于samepadding输出为: \\[ new\\_height=new\\_weight=\\lceil\\frac{W}{S}\\rceil \\] 对于vaild输出为: \\[ new\\_height=new\\_weight=\\lceil\\frac{(W-F+1)}{S}\\rceil \\]\n所以\\(\\lceil15/2\\rceil=8\\) 但是在k210中应该是不支持这个操作,应该是内部操作只支持整数的操作.所以我们需要修改代码.\n\n\n4. 解决方案\n\n首先尝试padding之后用same卷积. 因为这里的使用的是depthwise_conv2d_native卷积.所以不知道k210中是否支持这个卷积的same操作.所以先试试. 我使用类似于下面的操作进行padding: python     a = tf.constant(np.zeros((1, 15, 20, 256)), dtype=tf.float32)     b = tf.space_to_batch(a, [[1, 0], [0, 0]], block_size=1)     c = tf.nn.depthwise_conv2d_native(b, tf.ones((3, 3, 256, 1)), strides=[1, 2, 2, 1], padding='SAME')     print(a)     print(b)     print(c)     \"\"\"     Tensor(\"Const_3:0\", shape=(1, 15, 20, 256), dtype=float32)     Tensor(\"SpaceToBatchND_7:0\", shape=(1, 16, 20, 256), dtype=float32)     Tensor(\"DepthwiseConv2dNative_6:0\", shape=(1, 8, 10, 256), dtype=float32)     \"\"\" 经过尝试之后,我发现在k210 model-compiler报错: sh     [layer 23]: MobilenetV1/Conv2d_12_depthwise/Relu6:0            shape(HWC): 14x18x256 ==&gt; 8x10x256            scale,bias: (0.023529411764705882,0.0) ==&gt; (0.023529411764705882,0.0)     [5, 14, 18, 256] [5, 8, 10, 256] 他居然把我的输入识别成了[14,18].问了群里的人,他们说space to batch nd就是为了做padding的 后面的卷积当然不需要再次padding，所以要用valid 我还是不太理解为什么维度会和Tensorflow board里面不相同.\n现在尝试valid卷积 那么我给height和widthpadding[3,2]. \\[ \\lceil\\frac{15+3-3+1}{2}\\rceil=8 \\] \\[ \\lceil\\frac{20+2-3+1}{2}\\rceil=10 \\] 对应代码为: python     tf.space_to_batch(a, [[2, 1], [1, 1]], block_size=1) 现在来尝试一下编译.就显示编译成功了."
  },
  {
    "objectID": "posts/k210-tool-chains.html",
    "href": "posts/k210-tool-chains.html",
    "title": "k210-tool-chains mac m1编译",
    "section": "",
    "text": "关于如何在apple m1 上编译k210 toolchains"
  },
  {
    "objectID": "posts/k210-tool-chains.html#readline编译问题",
    "href": "posts/k210-tool-chains.html#readline编译问题",
    "title": "k210-tool-chains mac m1编译",
    "section": "readline编译问题",
    "text": "readline编译问题\n发现是readline编译出现错误\nconfigure: creating ./config.status\nconfig.status: creating Makefile\nconfig.status: creating po/Makefile.in\nconfig.status: creating config.h\nconfig.status: executing depfiles commands\nconfig.status: executing libtool commands\nconfig.status: executing default-1 commands\nconfig.status: executing default commands\nmake[1]: *** [all] Error 2\nmake: *** [stamps/build-binutils-newlib] Error 2\n出错具体信息如下\n/Users/lisa/Documents/kendryte-gnu-toolchain/riscv-binutils-gdb/readline/rltty.c:83:7: error: implicit declaration of function 'ioctl' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n  if (ioctl (tty, TIOCGWINSZ, &w) == 0)\n      ^\n/Users/lisa/Documents/kendryte-gnu-toolchain/riscv-binutils-gdb/readline/rltty.c:720:3: error: implicit declaration of function 'ioctl' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n  ioctl (fildes, TIOCSTART, 0);\n  ^\n/Users/lisa/Documents/kendryte-gnu-toolchain/riscv-binutils-gdb/readline/rltty.c:759:3: error: implicit declaration of function 'ioctl' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n  ioctl (fildes, TIOCSTOP, 0);\n一番查看发现虽然外面的config都设置了-disable-werror,但是readline里面的编译选项并没有继承.所以需要单独配置一下. 去kendryte-gnu-toolchain/riscv-binutils-gdb/readline/Makefile.in中把79行修改为如下,避免这个waring即可.\nLOCAL_DEFS = -Wno-implicit-function-declaration @LOCAL_DEFS@"
  },
  {
    "objectID": "posts/k210-tool-chains.html#backend错误",
    "href": "posts/k210-tool-chains.html#backend错误",
    "title": "k210-tool-chains mac m1编译",
    "section": "backend错误",
    "text": "backend错误\n链接的时候报错,这个问题好像是gcc不支持arm64的mac系统,因为那时候都还没出m1..\nclang: warning: argument unused during compilation: '-no-pie' [-Wunused-command-line-argument]\nUndefined symbols for architecture arm64:\n  \"_host_hooks\", referenced from:\n      c_common_no_more_pch() in c-pch.o\n      toplev::main(int, char**) in libbackend.a(toplev.o)\n      gt_pch_save(__sFILE*) in libbackend.a(ggc-common.o)\n      gt_pch_restore(__sFILE*) in libbackend.a(ggc-common.o)\nld: symbol(s) not found for architecture arm64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\n参考这个方案,在riscv-gcc/gcc/config/host-darwin.c中添加两行代码,以根据当前host重新生成."
  },
  {
    "objectID": "posts/k210-gpio.html",
    "href": "posts/k210-gpio.html",
    "title": "k210_GPIO使用",
    "section": "",
    "text": "听说学会了点灯就学会了一切2333\n\n\n1.看原理图\n我的是绿色板子，观察原理图。LED0对应IO12，LED1对应IO12.\n\n\n2.看手册\n我看了一会，发现这个芯片有个很牛逼的东西现场可编程 IO 阵列 (Field programmable IO Array)，他可以自由映射内部的255个功能到外部48个io！真的强\n但是这个芯片的缺憾是通用GPIO只有8个，从FUNC_GPIO0到FUNC_GPIO7,但是～他还有更强的32个高速GPIO # 3.coding\n\n功能映射\n因为有FPIOA的存在，所以我们需要先将io映射到想要的功能：\n    /* 绿色板子IO12--&gt;LED0 IO13--&gt;LED1 */\n    fpioa_set_function(12, FUNC_GPIO1);\n    fpioa_set_function(13, FUNC_GPIO2);\n初始化GPIO\ngpio_init();/* 初始化gpio */\n设置GPIO模式\n这里的1就是对应了FUNC_GPIO1～\n/* typedef enum _gpio_drive_mode\n{\n    GPIO_DM_INPUT,\n    GPIO_DM_INPUT_PULL_DOWN,\n    GPIO_DM_INPUT_PULL_UP,\n    GPIO_DM_OUTPUT,\n} gpio_drive_mode_t; */\ngpio_set_drive_mode(1, GPIO_DM_OUTPUT); \n设置状态 这里没有什么好说的，HIGH对应1，LOW对应0\ngpio_pin_value_t value1= GPIO_PV_HIGH, value2= GPIO_PV_LOW;\ngpio_set_pin(1, value1);\n完整程序 这里的sleep函数是sdk中自带的，这个自带的sleep有三种，完全满足我们的一般要求。\n#include \"fpioa.h\"\n#include \"gpio.h\"\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\n\nint main(void) {\n    /* 老板子io 11 12 是led */\n    fpioa_set_function(12, FUNC_GPIO1);\n    fpioa_set_function(13, FUNC_GPIO2);\n\n    gpio_init();/* 初始化gpio */\n    gpio_set_drive_mode(1, GPIO_DM_OUTPUT);\n    gpio_set_drive_mode(2, GPIO_DM_OUTPUT);\n    gpio_pin_value_t value1= GPIO_PV_HIGH, value2= GPIO_PV_LOW;\n    gpio_set_pin(1, value1);\n    gpio_set_pin(2, value2);\n    while (1) {\n        sleep(1);\n        gpio_set_pin(1, value1= !value1);\n        gpio_set_pin(2, value2= !value2);\n    }\n    return 0;\n}\n\n\n\n4.编译运行\n➜  build cmake .. -DPROJ=gpio_led && make\n➜  build python3 isp.py -p /dev/ttyUSB0 -b 115200 gpio_led.bin\n上电即可看到现象咯。"
  },
  {
    "objectID": "posts/k210-cutecom-error.html",
    "href": "posts/k210-cutecom-error.html",
    "title": "k210与cutecom冲突解决",
    "section": "",
    "text": "我昨天在做k210的双核测试的时候，想给大家看看两个核printf的时间差，以此作为对比。但是我发现只要打开cutecom进行串口通信，就会导致单片机死机，所以我寻找了方法对他进行解决。\n\n\n方法1\n我在群里面问了大佬，简单粗暴的方式就是直接吧ch340上面的两个跳线帽拔掉，即可解决，但是这也会导致一个问题，就是无法一键下载程序了。\n\n\n方法2\n安装最新的cutecom\n➜  ~ cd Downloads/\n➜  Downloads git clone https://gitlab.com/cutecom/cutecom.git\n➜  cutecom git:(master) ✗ mkdir build\n➜  cutecom git:(master) ✗ cd build   \n➜  build git:(master) ✗ sudo apt-get install libqt5serialport5-dev \n➜  build git:(master) ✗ sudo apt-get install qtbase5-dev\n➜  build git:(master) ✗ cmake ..\n➜  build git:(master) ✗ make\n➜  build git:(master) ✗ ./cutecom\n\n\n解决后\n可以看到两个核几乎是同时输出："
  },
  {
    "objectID": "posts/infor-max.html",
    "href": "posts/infor-max.html",
    "title": "互信息：无监督提取特征",
    "section": "",
    "text": "本文是对苏剑林的深度学习的互信息：无监督提取特征的学习总结,主要是关于Deep INFOMAX的论文复现."
  },
  {
    "objectID": "posts/infor-max.html#关于重构",
    "href": "posts/infor-max.html#关于重构",
    "title": "互信息：无监督提取特征",
    "section": "关于重构",
    "text": "关于重构\n传统思想是使用解码器将去重构原始图像，但使用低维编码重构原图的结果通常是很模糊的。比如我们都能轻松辨认一个物体是什么，但是让我们将他完整地画出来是很难的，这就说明对于任务来说，最合理的特征并不能一定能完成图像重构。"
  },
  {
    "objectID": "posts/infor-max.html#互信息引出",
    "href": "posts/infor-max.html#互信息引出",
    "title": "互信息：无监督提取特征",
    "section": "互信息引出",
    "text": "互信息引出\n既然重构不是好特征的必要条件。那好特征的基本原则应当是能够从整个数据集中辨别出该样本出来，也就是说，提取出该样本（最）独特的信息。如何衡量提取出来的信息是该样本独特的呢？我们用互信息来衡量。\n用\\(X\\)表示原始图像的集合，\\(x\\in X\\)表示其中一图像，\\(\\tilde{p}(x)\\)表示采样所得\\(x\\)的分布，\\(Z\\)为编码向量集合，\\(z\\in Z\\)表示其中一编码向量，\\(p(z|x)\\)表示\\(x\\)所产生的编码向量\\(z\\)的分布，我们假设它为正态分布且\\(\\tilde{p}(x)\\)和\\(p(z)\\)相互独立，即 \\[\n\\begin{aligned}\n    p(z) = \\int p(z|x)\\tilde{p}(x)\\ dx\n\\end{aligned}\n\\]\n接下来使用互信息来表示\\(X，Z\\)的相关性： \\[\n\\begin{aligned}\nI(X,Z) &= \\iint p(z,x)\\log \\frac{p(z,x)}{\\tilde{p}(x)p(z)}\\ dx\\ dz \\\\\n&= \\iint p(z,x)\\log \\frac{p(z|x)}{p(z)}\\ dx\\ dz\\\\\n&= \\iint p(z|x)\\tilde{p}(x)\\log \\frac{p(z|x)}{p(z)}\\ dx\\ dz\n\\end{aligned}\n\\]\n那么接下来我们就要使互信息尽可能的大，来得到一个好的特征编码器： \\[\n\\begin{aligned}\np(z|x) = -\\min_{p(z|x)} I(X,Z)\n\\end{aligned}\n\\]\n互信息越大就代表\\(\\log \\frac{p(z|x)}{p(z)}\\)越大，说明\\(p(z|x)\\gg p(x)\\),则此时的解码出向量\\(p(z|x)\\)和\\(p(x)\\)同时出现的概率越大，那就代表编码器找到了专属于\\(x\\)的那个\\(z\\)，这不就是我们所需要的独特信息么。"
  },
  {
    "objectID": "posts/infor-max.html#先验分布",
    "href": "posts/infor-max.html#先验分布",
    "title": "互信息：无监督提取特征",
    "section": "先验分布",
    "text": "先验分布\n前面提到，相对于自编码器，变分自编码器同时还希望隐变量服从标准正态分布的先验分布，这有利于使得编码空间更加规整，甚至有利于解耦特征，便于后续学习。因此，在这里我们同样希望加上这个约束。\n关于服从于标准正态就可以有利于解耦特征，我暂时没有找到相关的资料，这里是摘录的原文。\n利用在VAE中思路(参考VAE直观推导)添加KL散度约束来实现，假设\\(q(z)\\sim N(0,1)\\),则： \\[\n    KL(q(z)\\|p(z))=\\int p(z)\\log\\frac{p(z)}{q(z)}\\ dz\n\\]"
  },
  {
    "objectID": "posts/infor-max.html#化简先验分布",
    "href": "posts/infor-max.html#化简先验分布",
    "title": "互信息：无监督提取特征",
    "section": "化简先验分布",
    "text": "化简先验分布\n将KL散度约束加权并与互信息约束混合得： \\[\n\\begin{aligned}\n    p(z|x) &= \\min_{p(z|x)}\\left\\{- I(X,Z) + \\lambda KL(p(z)\\Vert q(z))\\right\\}\\\\\n    =\\min_{p(z|x)}&\\left\\{- I(X,Z)+ \\lambda\\int p(z)\\log \\frac{p(z)}{q(z)}\\ dz\\right\\} \\\\\n    =\\min_{p(z|x)}&\\left\\{- I(X,Z)+ \\lambda\\left(\\int p(z)\\log \\left(\\frac{p(z|x)}{q(z)}\\times\\frac{p(z)}{p(z|x)}\\right)\\ dz\\right)\\right\\} \\\\\n    =\\min_{p(z|x)}&\\left\\{- I(X,Z)+ \\lambda\\left(\\int p(z)\\log\\frac{p(z|x)}{q(z)} \\ dz-\\int p(z)\\log\\frac{p(z|x)}{p(z)} \\ dz\\right)\\right\\} \\\\\n    =\\min_{p(z|x)}&\\left\\{- \\iint p(z)\\log \\frac{p(z|x)}{p(z)}\\ dx\\ dz+ \\lambda\\left(\\int p(z)\\log\\frac{p(z|x)}{q(z)} \\ dz-\\int p(z)\\log\\frac{p(z|x)}{p(z)} \\ dz\\right)\\right\\} \\\\\n    =\\min_{p(z|x)}&\\left\\{- (1+\\lambda)\\iint p(z)\\log \\frac{p(z|x)}{p(z)}\\ dx\\ dz+ \\lambda\\int p(z)\\log\\frac{p(z|x)}{q(z)} \\ dz\\right\\} \\\\\n    =\\min_{p(z|x)}&\\left\\{\\iint p(z|x)\\tilde{p}(x) \\left[- (1+\\lambda)\\log\\frac{p(z|x)}{p(z)}+\\lambda\\log\\frac{p(z|x)}{q(x)}\\right]\\ dx\\ dz \\right\\} \\\\\n    =\\min_{p(z|x)}&\\left\\{-(1+\\lambda)\\cdot I(X,Z)+\\lambda\\cdot \\mathbb{E}_{x\\sim\\tilde{p}(x)}\\left[KL(p(z|x)||q(z))\\right] \\right\\} \\\\\n    =\\min_{p(z|x)}&\\left\\{-(1+\\lambda)\\cdot KL(p(z|x)\\tilde{p}(x)\\| p(z)\\tilde{p}(x)) +\\lambda\\cdot \\mathbb{E}_{x\\sim\\tilde{p}(x)}\\left[KL(p(z|x)||q(z))\\right] \\right\\}\n\\end{aligned}\n\\]\n现在发现先验分布在化简后被去除了，并且约束函数中后面一部分就是VAE中的约束，比较容易实现，接下来考虑如何构建互信息约束。"
  },
  {
    "objectID": "posts/infor-max.html#互信息转化",
    "href": "posts/infor-max.html#互信息转化",
    "title": "互信息：无监督提取特征",
    "section": "互信息转化",
    "text": "互信息转化\n在第一部分提到了互信息可以化为KL散度的形式：\n\\[\n\\begin{aligned}\n    I(X,Z) =& \\iint p(z|x)\\tilde{p}(x)\\log \\frac{p(z|x)\\tilde{p}(x)}{p(z)\\tilde{p}(x)}dxdz\\\\\n    =& KL(p(z|x)\\tilde{p}(x)\\Vert p(z)\\tilde{p}(x))\n\\end{aligned}\n\\]\n这个形式展示了互信息的本质含义：\\(p(z|x)\\tilde{p}(x)\\)表示变量\\(x,z\\)的联合分布，\\(p(z)\\tilde{p}(x)\\)表示随机抽取一个\\(x\\)和一个\\(z\\)时的分布(假设不相关)，而互信息则是这两个分布的KL散度，那么最大化互信息意思就是拉大这两个分布间的距离。\n一个严重问题是KL散度理论上是无上界的，我们不能去最大化一个无上界的量。为了有效优化，抓住\n\n最大化互信息就是拉大\\(p(z|x)\\tilde{p}{x}\\)和\\(p(z)\\tilde{p}(x)\\)之间的距离\n\n这个核心要点，我们可以选择其他的度量函数来进行约束，可以选择JS散度和Hellinger距离。这里使用JS散度：\n\\[\n\\begin{aligned}\n    JS(P,Q) = \\frac{1}{2}KL\\left(P\\left\\Vert\\frac{P+Q}{2}\\right.\\right)+\\frac{1}{2}KL\\left(Q\\left\\Vert\\frac{P+Q}{2}\\right.\\right)\n\\end{aligned}   \n\\]\nJS散度同样衡量了两个分布的距离，但他具备上界\\(\\frac{1}{2}\\log2\\)，我们最大化他的时候，也可以起到最大化互信息的效果，下面将JS散度代入约束函数。\n\\[\n\\begin{aligned}\n    p(z|x) =\\min_{p(z|x)}&\\left\\{-(1+\\lambda)\\cdot JS(p(z|x)\\tilde{p}(x)\\| p(z)\\tilde{p}(x)) +\\lambda\\cdot \\mathbb{E}_{x\\sim\\tilde{p}(x)}\\left[KL(p(z|x)||q(z))\\right] \\right\\}\n\\end{aligned}\n\\]\n但是还是存在一个\\(p(z)\\)让我们无法解决，下面就要把他解决："
  },
  {
    "objectID": "posts/infor-max.html#变分推断",
    "href": "posts/infor-max.html#变分推断",
    "title": "互信息：无监督提取特征",
    "section": "变分推断",
    "text": "变分推断\n参考f-GAN简介：GAN模型的生产车间中的f散度局部变分推断中的距离定义为：\n\\[\n\\begin{aligned}\\mathcal{D}_f(P\\Vert Q) =& \\max_{T}\\int q(x) \\left[\\frac{p(x)}{q(x)}T\\left(\\frac{p(x)}{q(x)}\\right)-g\\left(T\\left(\\frac{p(x)}{q(x)}\\right)\\right)\\right]\\ dx\\\\\n=& \\max_{T}\\int\\left[p(x)\\cdot T\\left(\\frac{p(x)}{q(x)}\\right)-q(x)\\cdot g\\left(T\\left(\\frac{p(x)}{q(x)}\\right)\\right)\\right]\\ dx \\\\\n\\text{记}T\\left(\\frac{p(x)}{q(x)}\\right)\\text{为}&T(x)\\text{得}\\\\\n\\mathcal{D}_f(P\\Vert Q) =& \\max_{T}\\Big(\\mathbb{E}_{x\\sim p(x)}[T(x)]-\\mathbb{E}_{x\\sim q(x)}[g(T(x))]\\Big)\n\\end{aligned}\n\\]\n利用上面这个公式，我们可以进行f散度估计，意思是：分别从两个分布中采样，然后分别计算\\(T(x)\\)和\\(g(T(x))\\)的均值，优化函数\\(T(x)\\)使得他们的差值尽可能的大，最终的结果即为f散度的近似值了。显然\\(T(x)\\)可以用神经网络进行拟合，我们优化此函数就是优化神经网络参数。\n将上述公式代入JS散度中得： \\[\n\\begin{aligned}\n    &JS(P,Q) \\\\\n    =& \\max_{T}\\Big(\\mathbb{E}_{x\\sim p(x)}[\\log \\sigma(T(x))] + \\mathbb{E}_{x\\sim q(x)}[\\log(1-\\sigma(T(x))]\\Big) \\\\\n    \\\\\n    &JS\\big(p(z|x)\\tilde{p}(x), p(z)\\tilde{p}(x)\\big)\\\\\n    =& \\max_{T}\\Big(\\mathbb{E}_{(x,z)\\sim p(z|x)\\tilde{p}(x)}[\\log \\sigma(T(x,z))] + \\mathbb{E}_{(x,z)\\sim p(z)\\tilde{p}(x)}[\\log(1-\\sigma(T(x,z))]\\Big)\n\\end{aligned}\n\\]\n注意:这里的\\(1-\\sigma(T(x))\\)是根据此前文章中的变分推断所计算的激活函数\\(g\\)，具体请参考文章\n并且现在这个约束函数可以看成负采样估计：引入一个判别网络\\(\\sigma(T(x,z))\\)，\\(x\\)和对应的\\(z\\)看成一个正样本对，\\(x\\)与其他一个随机抽取的\\(z\\)作为负样本对，然后最大化似然函数，等价于最小化交叉熵。终于通过负采样的方式，得到了一种估计JS散度的方案，从而解决了互信息的最大化问题，得到具体的loss为：\n\\[\n\\begin{aligned}\np(z|x),T(x,z)=\\\\\n\\min_{p(z|x),T(x,z)}\\Big\\{-&(1+\\lambda)\\cdot\\Big(\\mathbb{E}_{(x,z)\\sim p(z|x)\\tilde{p}(x)}[\\log \\sigma(T(x,z))] + \\mathbb{E}_{(x,z)\\sim p(z)\\tilde{p}(x)}[\\log(1-\\sigma(T(x,z))]\\Big)\\\\\n+&\\lambda\\cdot \\mathbb{E}_{x\\sim\\tilde{p}(x)}[KL(p(z|x)\\Vert q(z))]\\Big\\}\n\\end{aligned}\n\\]\n下面开始实际实验。"
  },
  {
    "objectID": "posts/infor-max.html#batch内负采样",
    "href": "posts/infor-max.html#batch内负采样",
    "title": "互信息：无监督提取特征",
    "section": "batch内负采样",
    "text": "batch内负采样\n我们可以对这个batch中样本对顺序进行shuffle，shuffle前的样本对为正样本，shuffle之后的样本对为负样本。"
  },
  {
    "objectID": "posts/infor-max.html#局部负采样",
    "href": "posts/infor-max.html#局部负采样",
    "title": "互信息：无监督提取特征",
    "section": "局部负采样",
    "text": "局部负采样\n上面的做法，实际上就是考虑了整张图片之间的关联，但是我们知道，图片的相关性更多体现在局部中（也就是因为这样所以我们才可以对图片使用CNN）。换言之，图片的识别、分类等应该是一个从局部到整体的过程。因此，有必要把“局部互信息”也考虑进来。\n一般的CNN编码过程： \\[\n\\text{原始图片}x\\xrightarrow{\\text{多个卷积层}} h\\times w\\times c\\text{的特征} \\xrightarrow{\\text{卷积和全局池化}} \\text{固定长度的向量}z\n\\]\n我们已经考虑到了\\(x\\)和\\(z\\)的关联，那么中间层特征(feature map)和\\(z\\)的关联呢？把中间层向量记为\\(\\{C_{ij}(x)|i=1,2,\\dots,h;j=1,2,\\dots,w\\}\\),一共\\(h\\times w\\)个向量与\\(z_x\\)的互信息，称为局部互信息。\n那么我们要构建出一个局部的互信息估算网络，首先把每个通道的向量与\\(z_x\\)拼接得到\\([C_{ij}(x),z_x]\\),相当于得到了一个更大的feature map，然后对这个feature map用多个1x1的卷积层来作为局部互信息的估算网络\\(T_{local}\\)。负样本的选取方法也是用在batch内随机打算的方案。\n加入局部互信息的总loss为： \\[\n\\begin{aligned}\n&p(z|x),T_1(x,z),T_2(C_{ij}, z) \\\\\n=&\\min_{p(z|x),T_1,T_2}\\Big\\{-\\alpha\\cdot\\Big(\\mathbb{E}_{(x,z)\\sim p(z|x)\\tilde{p}(x)}[\\log \\sigma(T_1(x,z))] + \\mathbb{E}_{(x,z)\\sim p(z)\\tilde{p}(x)}[\\log(1-\\sigma(T_1(x,z))]\\Big)\\\\\n&\\qquad-\\frac{\\beta}{hw}\\sum_{i,j}\\Big(\\mathbb{E}_{(x,z)\\sim p(z|x)\\tilde{p}(x)}[\\log \\sigma(T_2(C_{ij},z))] + \\mathbb{E}_{(x,z)\\sim p(z)\\tilde{p}(x)}[\\log(1-\\sigma(T_2(C_{ij},z))]\\Big)\\\\\n&\\qquad+\\gamma\\cdot \\mathbb{E}_{x\\sim\\tilde{p}(x)}[KL(p(z|x)\\Vert q(z))]\\Big\\}\n\\end{aligned}\n\\]\n注意：上面的几部分的损失权重分别设置为\\(\\alpha,\\beta,\\gamma\\)。"
  },
  {
    "objectID": "posts/infor-max.html#余弦距离knn采样",
    "href": "posts/infor-max.html#余弦距离knn采样",
    "title": "互信息：无监督提取特征",
    "section": "余弦距离KNN采样",
    "text": "余弦距离KNN采样"
  },
  {
    "objectID": "posts/infor-max.html#欧式距离knn采样",
    "href": "posts/infor-max.html#欧式距离knn采样",
    "title": "互信息：无监督提取特征",
    "section": "欧式距离KNN采样",
    "text": "欧式距离KNN采样"
  },
  {
    "objectID": "posts/i2candspi.html",
    "href": "posts/i2candspi.html",
    "title": "i2c之总结",
    "section": "",
    "text": "最近在linux下移植i2c的传感器驱动。移植了才发现各个设备的i2c读写都不太一样，对于这几个方式我做一个小总结。 首先我使用的是linux应用层通用的i2c读写，我的读写默认是使用smbus协议进行读写的。\n\n\nsmbus协议\n先放一张图片合集，接下来就逐一看看这几种方式。 \n\n\nsmbus Send Byte\n这个方式比较少用到，一般我们的传感器都有地址位以及数据位，所以至少传输两位。但是我在使用SHT31这个传感器的就需要用到此方式。先看看SHT31寄存器的读写：\n\n这里发现这个传感器没有寄存器地址，但是也要发两位数据，而且是高位先行。 所以这个时候就需要改写对应的读写函数如下：\n直接按高低位写两位数据即可。\n当然这里我也存在一个疑问，直接调用write函数，对应的i2c输出应该是一个地址位，一个数据位，但是我两次调用write函数，应该会有四个byte信号输出，但是按照读写情况，还是十分正常的。\nDrvStatus_t SHT31_writeCommand(uint16_t cmd) {\n    uint8_t data[2];\n    data[0] = cmd &gt;&gt; 8, data[1] = cmd & 0x00FF;\n    if (write(sht31Fd, data, 2) &lt; 0)\n        return COMPONENT_ERROR;\n    return COMPONENT_OK;\n}\n\n\nsmbus Write Byte\n这个应该是使用最多的的了。写一个数据到对应的寄存器中。这个就不赘述了。\n\n\nsmbus Write Word\n对于smbus的读取和写入16位都应该算是上一操作的扩展，但是有时候也有不按常理出牌的地方。 因为标准的smbus协议读取16位数据是，都是默认先读到的为低8位，之后读取到的是高8位。 在使用smbus协议读取fdc2214传感器设备id就会出现问题。\nfdc2214传感器读取协议：\n\n可以发现他的读取是默认先高位，再低位。\n所以我们在使用smbus协议读取到数据后要做好数据高低转换的措施：\n/* 读取16位 先读高8位再读低8位 */\nstatic uint16_t read16(uint8_t reg) {\n    /* 读取两字节 bmp280默认先输出高位再输出低位  */\n    /* smbus协议默认先低位再高位 所以需要转换  */\n    uint16_t temp = (uint16_t)i2c_smbus_read_word_data(bmp280fd, reg);\n    return temp &gt;&gt; 8 | temp &lt;&lt; 8;\n}\n\n\ni2c传输位顺序\n值得一提的是i2c传输中的每一位都是高位先行的,也只有这样我们接收到msb的数据后直接8位交换位置即可。\n比如设备id为 0x3055 其二进制为:0011 0000 0101 0101 发送顺序为:0011 0000 0101 0101\nsmbus接收到后为: 高八位 0011 0000 -&gt; 低八位 0011 0000\n低八位 0101 0101 -&gt; 高八位 0101 0101\n拼接==&gt; 0101 0101 0011 0000 = 0x5530\n交换==&gt; 0x5530 = 0x3055"
  },
  {
    "objectID": "posts/heap.html",
    "href": "posts/heap.html",
    "title": "堆",
    "section": "",
    "text": "堆其实是也是一种二叉树，不过他的排序方式更加舒服，对于需要升序或者降序排列的数据非常有用。"
  },
  {
    "objectID": "posts/heap.html#binheap.cpp",
    "href": "posts/heap.html#binheap.cpp",
    "title": "堆",
    "section": "binheap.cpp",
    "text": "binheap.cpp\n#include \"binheap.h\"\n#include &lt;cstdint&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstring&gt;\n/**\n * @brief 优先队列-堆的数组实现\n **/\nPriorityQueue Initialize(int MaxElements) {\n    PriorityQueue H = nullptr;\n    if (MaxElements &lt; 0) {\n        return nullptr;\n    }\n    /* 为队列结构体申请内存 */\n    H = (PriorityQueue)malloc(sizeof(struct HeapStruct));\n\n    /* 为队列数组申请内存 */\n    H-&gt;Elements =\n        (ElementType *)malloc((MaxElements + 1) * sizeof(ElementType));\n\n    H-&gt;Capacity = MaxElements;\n    H-&gt;Size = 0;\n    H-&gt;Elements[0] = 0; /* scout element */\n    return H;\n}\nvoid Destroy(PriorityQueue H) {}\nvoid MakeEmpty(PriorityQueue H) {}\n/**\n * @brief 插入的元素都会塞满数组\n **/\nvoid Insert(ElementType X, PriorityQueue H) {\n    int i;\n    if (IsFull(H)) {\n        return;\n    }\n    /*---&gt;1.指向数组的下一个元素\n    |   | 2.判断当前元素是否小于其母节点\n    |   | 3.小于:将母节点元素移动到此处 大于:X赋值到此处\n    ----v 4.指向母节点的位置 */\n    for (i = ++H-&gt;Size; H-&gt;Elements[i / 2] &gt; X; i /= 2) {\n        H-&gt;Elements[i] = H-&gt;Elements[i / 2];\n    }\n    H-&gt;Elements[i] = X;\n}\n/**\n * @brief 只需将较小的元素上移即可\n **/\nElementType DeleteMin(PriorityQueue H) {\n    int i, Child;\n    ElementType MinElement, LastElement;\n    if (IsEmpty(H)) {\n        return H-&gt;Elements[0];\n    }\n    /* 最小元素 */\n    MinElement = H-&gt;Elements[1];\n    /* 倒数第二个元素 */\n    LastElement = H-&gt;Elements[H-&gt;Size--];\n\n    for (i = 1; i * 2 &lt; H-&gt;Size; i = Child) {\n        /* 子节点位置为当前节点的2倍 */\n        Child = i * 2;\n        /* 选择左右子节点中小的那个 */\n        if (Child != H-&gt;Size && (H-&gt;Elements[Child] &gt; H-&gt;Elements[Child + 1])) {\n            Child++;\n        }\n        /* 没有到倒数第二个元素的右边 */\n        if (LastElement &gt; H-&gt;Elements[Child]) {\n            /* 上浮 */\n            H-&gt;Elements[i] = H-&gt;Elements[Child];\n        } else {\n            break;\n        }\n    }\n    /* 交换最后一个元素 */\n    H-&gt;Elements[i] = LastElement;\n    /* 返回删去的元素 */\n    return MinElement;\n}\nElementType FindMin(PriorityQueue H) {\n    if (IsEmpty(H)) {\n        return 0;\n    } else {\n        return H-&gt;Elements[1];\n    }\n}\nint IsEmpty(PriorityQueue H) {\n\n    if (H-&gt;Size == 0) {\n        return true;\n    } else {\n        return false;\n    }\n}\nint IsFull(PriorityQueue H) {\n    if (H-&gt;Size == H-&gt;Capacity) {\n        return true;\n    } else {\n        return false;\n    }\n}\n\nvoid Traversal(PriorityQueue H) {\n\n    if (IsEmpty(H)) {\n        printf(\"Empty\\n\");\n    } else {\n        for (int i = 1; i &lt;= H-&gt;Size; i++) {\n            printf(\"%d \", H-&gt;Elements[i]);\n        }\n        printf(\"\\n\");\n    }\n}"
  },
  {
    "objectID": "posts/heap.html#binheap.h",
    "href": "posts/heap.html#binheap.h",
    "title": "堆",
    "section": "binheap.h",
    "text": "binheap.h\n#ifndef _BinHeap_H\n#define _BinHeap_H\n\n#define ElementType int\n\nstruct HeapStruct;\ntypedef struct HeapStruct *PriorityQueue;\n\nPriorityQueue Initialize(int MaxElements);\nvoid Destroy(PriorityQueue H);\nvoid MakeEmpty(PriorityQueue H);\nvoid Insert(ElementType X, PriorityQueue H);\nElementType DeleteMin(PriorityQueue H);\nElementType FindMin(PriorityQueue H);\nint IsEmpty(PriorityQueue H);\nint IsFull(PriorityQueue H);\nvoid Traversal(PriorityQueue H);\nstruct HeapStruct {\n    int Capacity;\n    int Size;\n    ElementType *Elements;\n};\n\n#endif"
  },
  {
    "objectID": "posts/heap.html#main.cpp",
    "href": "posts/heap.html#main.cpp",
    "title": "堆",
    "section": "main.cpp",
    "text": "main.cpp\n#include \"binheap.h\"\n#include &lt;cstdint&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstring&gt;\n\nvoid printUseage(void) {\n    printf(\"\\\n    二叉堆操作：\\n\\\n    (1):初始化\\n\\\n    (2):插入元素\\n\\\n    (3):删除最小值\\n\\\n    (4):遍历元素\\n\\\n    (q):退出\\n\\\n    \");\n}\n\nint main(int argc, char const *argv[]) {\n    printUseage();\n    PriorityQueue heap;\n    ElementType tempint;\n    while (true) {\n        switch (getchar()) {\n        case '1':\n            printf(\"初始化大小为10的堆\");\n            heap = Initialize(10);\n            printf(\"\\n\");\n            break;\n        case '2':\n            printf(\"请输入插入元素:\");\n            scanf(\"%d\", &tempint);\n            Insert(tempint, heap);\n            printf(\"\\n\");\n            break;\n        case '3':\n            printf(\"删除:%d\", DeleteMin(heap));\n            printf(\"\\n\");\n            break;\n        case '4':\n            printf(\"遍历堆:\");\n            Traversal(heap);\n            printf(\"\\n\");\n            break;\n        case 'q':\n            exit(EXIT_SUCCESS);\n            break;\n        default:\n            break;\n        }\n    }\n\n    return 0;\n}"
  },
  {
    "objectID": "posts/heap.html#makefile",
    "href": "posts/heap.html#makefile",
    "title": "堆",
    "section": "Makefile",
    "text": "Makefile\nCC = g++\nCFLAGS = #-g \nCLIBS = #-lpthread\n \nINCLUDE = $(wildcard ./*.h) # INCLUDE = a.h b.h ... can't be defined like \"INCLUDE = ./*.h\"\nSOURCES = $(wildcard ./*.cpp)\n \nTARGET = BinHeap\nOBJECTS = $(patsubst %.cpp,%.o,$(SOURCES))\n \n$(TARGET) : $(OBJECTS)\n    $(CC) $(CFLAGS) $^ -o $@ $(CLIBS)\n    rm -rf *.o\n\n$(OBJECTS) : %.o : %.cpp\n    $(CC) -c $(CFLAGS) $&lt; -o $@\n \n.PHONY : clean\nclean:\n    rm -rf *.o $(TARGET)"
  },
  {
    "objectID": "posts/heap.html#test",
    "href": "posts/heap.html#test",
    "title": "堆",
    "section": "test",
    "text": "test\n1\n2\n10\n2\n8\n2\n7\n2\n5\n2\n3\n2\n11\n2\n13\n4\n3\n4\n3\n4\n3\n4\nq"
  },
  {
    "objectID": "posts/halide-metal.html",
    "href": "posts/halide-metal.html",
    "title": "halide metal 初体验",
    "section": "",
    "text": "买了m2 mac pro之后, 一直想把m2的计算能力应用起来, 发现还是halide的功能比较完备, 支持metal后端, 所以尝试一下.\n\n\n0. Setup\n我使用的是halide 14.0, 编译好之后配置好python bindings.\n\n\n1. PyHalide CodeGen\n简单写了一个代码, 验证metal后端的指令生成有效且正确.\n⚠️ : halide本身没有导出ParamMap的接口, 我这里简单添加了一下, 后面有机会考虑做一下pr.\n我对于metal gpu架构了解的比较少, 没太理解halide为什么对于metal后端调度也是和cuda gpu类似, 因为Apple的简单介绍里面并没有提到有block的层级.\nimport halide as hl\nimport numpy as np\nhost = hl.get_host_target().with_feature(hl.TargetFeature.Metal).with_feature(hl.TargetFeature.Debug)\nassert hl.host_supports_target_device(host)\n\nbrighter = hl.Func(\"brighter\")\nx, y = hl.Var(\"x\"), hl.Var(\"y\")\n\ninput = hl.ImageParam(hl.Float(32), 2, \"input\")\ninput_value = hl.Buffer(hl.Float(32), [16, 16])\ninput_value.fill(1.0)\n\n# Define the hl.Func.\nbrighter[x, y] = input[x, y] * 3\n\n# Schedule it.\nxo, yo, xi, yi = hl.Var(\"xo\"), hl.Var(\"yo\"), hl.Var(\"xi\"), hl.Var(\"yi\")\n\nbrighter.gpu_tile(x, y, xo, yo, xi, yi, 8, 8)\nbrighter.print_loop_nest()\nbrighter.compile_jit(host)\nbrighter.compile_to_file(\"brighter\", [hl.Argument(input)], \"brighter\", host)\n\n# test the schedule\nreference_output = hl.Buffer(hl.Float(32), [16, 16])\nbrighter.realize([reference_output], host, hl.ParamMap([hl.ParamMapping(input, input_value)]))\nreference_output.copy_to_host()\nfor i in range(16):\n  for j in range(16):\n    assert reference_output[j, i] == 3.0\n输出:\nhalide_metal_device_malloc (user_context: 0x0, buf: 0x16d8e3f48)\n    allocating buffer(0, 0x0, 0x104268800, 1, uint8, {0, 8, 1}, {0, 8, 8}, {0, 3, 64})\nMetal - Allocating: MTLCreateSystemDefaultDevice\nMetal - Allocating: new_command_queue\n    Time: 4.000000e-03 ms\nhalide_metal_copy_to_device dev = 0x124041160 metal_buffer = 0x124058e10 host = 0x104268800\nTime for halide_metal_copy_to_device: 1.070000e-01 ms\nhalide_metal_device_free called on buf 0x16d8e3f48 device is 4899213664\n    Time: 1.883300e-02 ms\nproduce brighter:\n  gpu_block y.yo&lt;Default_GPU&gt;:\n    gpu_block x.xo&lt;Default_GPU&gt;:\n      gpu_thread y.yi in [0, 7]&lt;Default_GPU&gt;:\n        gpu_thread x.xi in [0, 7]&lt;Default_GPU&gt;:\n          brighter(...) = ...\nEntering Pipeline brighter\nTarget: arm-64-osx-debug-jit-metal-user_context\n Input Buffer input: buffer(0, 0x0, 0x114008280, 1, float32, {0, 16, 1}, {0, 16, 16})\n Input (void *) __user_context: 0x16d8e2b40\n Output Buffer brighter: buffer(0, 0x0, 0x127077e80, 0, float32, {0, 16, 1}, {0, 16, 16})\nCaching compiled kernel: 0x133e4dc00 id 2 context 0x1248a6e00\nTime for halide_metal_initialize_kernels: 4.945000e-01 ms\nhalide_metal_device_malloc (user_context: 0x16d8e2b40, buf: 0x112b511a0)\n    allocating buffer(0, 0x0, 0x127077e80, 0, float32, {0, 16, 1}, {0, 16, 16})\n    Time: 2.945900e-02 ms\nhalide_metal_device_malloc (user_context: 0x16d8e2b40, buf: 0x113e307b0)\n    allocating buffer(0, 0x0, 0x114008280, 1, float32, {0, 16, 1}, {0, 16, 16})\n    Time: 6.083000e-03 ms\nhalide_metal_copy_to_device dev = 0x133e52d10 metal_buffer = 0x112b9f2c0 host = 0x114008280\nTime for halide_metal_copy_to_device: 9.870800e-02 ms\nMetal - supports setBytes\nTotal args size is 44 and with padding, size is 44\nSetting shared memory length to 0\nDispatching threadgroups (number 0) blocks(2, 2, 1) threads(8, 8, 1)\nTime for halide_metal_device_run: 6.777500e-01 ms\nExiting Pipeline brighter\nTime for halide_metal_copy_to_host: 6.170410e-01 ms\nhalide_metal_device_free called on buf 0x113e307b0 device is 5165624592\n    Time: 1.229200e-02 ms\nhalide_metal_device_free called on buf 0x112b511a0 device is 5165682976\n    Time: 4.417000e-03 ms\n\n\n2. Cpp Using Generated Kernel\n接下来再写一个cpp代码调用一下生成好的kernel.\n#include \"brighter.h\"\n#include &lt;HalideBuffer.h&gt;\n#include &lt;iostream&gt;\n\nint main(int argc, char **argv) {\n    int dim = 16;\n    size_t length = (size_t)dim * dim;\n    float a[length];\n    for (size_t i = 0; i &lt; length; i++) {\n        a[i] = 1;\n        std::cout &lt;&lt; a[i] &lt;&lt; \", \";\n    }\n    std::cout &lt;&lt; std::endl;\n    float b[length];\n    Halide::Runtime::Buffer&lt;float, 2&gt; input_buffer(a, dim, dim);\n    input_buffer.set_host_dirty(); // for async buffer\n    Halide::Runtime::Buffer&lt;float, 2&gt; output_buffer(b, dim, dim);\n    brighter(input_buffer, output_buffer);\n    output_buffer.copy_to_host(); // copy to host\n    for (size_t i = 0; i &lt; length; i++) {\n        std::cout &lt;&lt; b[i] &lt;&lt; \", \";\n    }\n    std::cout &lt;&lt; std::endl;\n    return 0;\n}\n编译运行:\n❯ clang++ -std=c++17 -stdlib=libc++ -Iout/build/debug/include/ -fno-objc-arc -framework Metal -framework Foundation -framework MetalKit brighter.o main.cpp && ./a.out\n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \nEntering Pipeline brighter\nTarget: arm-64-osx-debug-metal\n Input Buffer input: buffer(0, 0x0, 0x16b35a770, 1, float32, {0, 16, 1}, {0, 16, 16})\n Output Buffer brighter: buffer(0, 0x0, 0x16b35a370, 0, float32, {0, 16, 1}, {0, 16, 16})\nMetal - Allocating: MTLCreateSystemDefaultDevice\nMetal - Allocating: new_command_queue\nCaching compiled kernel: 0x143611bd0 id 2 context 0x14480ca00\nTime for halide_metal_initialize_kernels: 6.449170e-01 ms\nhalide_copy_to_device validating input buffer: buffer(0, 0x0, 0x16b35a370, 0, float32, {0, 16, 1}, {0, 16, 16})\nhalide_device_malloc validating input buffer: buffer(0, 0x0, 0x16b35a370, 0, float32, {0, 16, 1}, {0, 16, 16})\nhalide_device_malloc: target device interface 0x104acb260\nhalide_metal_device_malloc (user_context: 0x0, buf: 0x16b35abe8)\n    allocating buffer(0, 0x0, 0x16b35a370, 0, float32, {0, 16, 1}, {0, 16, 16})\n    Time: 8.459000e-03 ms\nhalide_copy_to_device 0x16b35abe8 skipped (host is not dirty)\nhalide_copy_to_device validating input buffer: buffer(0, 0x0, 0x16b35a770, 1, float32, {0, 16, 1}, {0, 16, 16})\nhalide_device_malloc validating input buffer: buffer(0, 0x0, 0x16b35a770, 1, float32, {0, 16, 1}, {0, 16, 16})\nhalide_device_malloc: target device interface 0x104acb260\nhalide_metal_device_malloc (user_context: 0x0, buf: 0x16b35ac60)\n    allocating buffer(0, 0x0, 0x16b35a770, 1, float32, {0, 16, 1}, {0, 16, 16})\n    Time: 3.375000e-03 ms\nhalide_copy_to_device 0x16b35ac60 host is dirty\nhalide_copy_to_device 0x16b35ac60 calling copy_to_device()\nhalide_metal_copy_to_device dev = 0x1436206f0 metal_buffer = 0x143621d00 host = 0x16b35a770\nTime for halide_metal_copy_to_device: 1.747500e-01 ms\nMetal - supports setBytes\nTotal args size is 44 and with padding, size is 44\nSetting shared memory length to 0\nDispatching threadgroups (number 0) blocks(2, 2, 1) threads(8, 8, 1)\nTime for halide_metal_device_run: 7.601670e-01 ms\nExiting Pipeline brighter\nhalide_copy_to_host validating input buffer: buffer(5425404128, 0x104acb260, 0x16b35a370, 2, float32, {0, 16, 1}, {0, 16, 16})\ncopy_to_host_already_locked 0x16b35abe8 dev_dirty is true\nTime for halide_metal_copy_to_host: 4.510420e-01 ms\n3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, \nhalide_device_free validating input buffer: buffer(5425404128, 0x104acb260, 0x16b35a370, 0, float32, {0, 16, 1}, {0, 16, 16})\nhalide_metal_device_free called on buf 0x16b35abe8 device is 5425404128\n    Time: 1.316700e-02 ms\nhalide_device_free validating input buffer: buffer(5425465072, 0x104acb260, 0x16b35a770, 0, float32, {0, 16, 1}, {0, 16, 16})\nhalide_metal_device_free called on buf 0x16b35ac60 device is 5425465072\n    Time: 4.042000e-03 ms\n\n\n3. PyHalide with numpy\n如果把halide升级到15.x,那么可以无缝的python利用使用halide jit的kernel了. 这里说的无缝就是不需要对numpy的array做任何操作就可以直接传入halide kernel. 本来halide buffer是最内层为最低维度, 而numpy是最外层是最低维度, 现在在调用PyCallable的时候他已经可以做到自动转换了.\nimport halide as hl\nimport numpy as np\nhost = hl.get_host_target().with_feature(hl.TargetFeature.Metal).with_feature(hl.TargetFeature.Debug)\nassert hl.host_supports_target_device(host)\n\n\nclass MatMulPipeLine:\n  def __init__(self) -&gt; None:\n    self.M, self.K, self.N = 4096, 4096, 2\n    inputLhs = hl.ImageParam(hl.Float(32), 2, \"inputLhs\")\n    inputRhs = hl.ImageParam(hl.Float(32), 2, \"inputRhs\")\n    output = hl.Func(\"output\")\n    (m, n) = hl.Var(\"m\"), hl.Var(\"n\")\n    k = hl.RDom([hl.Range(0, self.K)], \"k\")\n\n    output[n, m] = 0.0\n    output[n, m] += inputLhs[k.x, m] * inputRhs[n, k.x]\n\n    self.matmul = output.compile_to_callable([inputLhs, inputRhs], target=host)\n\n  def test_correctness(self):\n    a = np.random.rand(self.M, self.K).astype(np.float32)\n    b = np.random.rand(self.K, self.N).astype(np.float32)\n    reference_output = np.matmul(a, b)\n    halide_output = np.zeros_like(reference_output, dtype=np.float32)\n    self.matmul(a, b, halide_output)\n    np.allclose(reference_output, halide_output)\n\n\nmatmul = MatMulPipeLine()\nmatmul.test_correctness()\n\n\n3. Metal 算子调度\n对于metal的架构我并不太熟悉,比较熟悉的是npu架构, 我测试了一个简单的schedule之后, 发现性能比不调度还慢3倍. 后面发现是gpu这种架构没法在kernel里面load, 必须得在一开始的时候设定好buffer.\n再然后我仔细调研了一下metal编程, 发现metal其实比较想象中的复杂, 主要是halide其实没有提供足够多个schedule api让每个循环映射到不同的硬件层次上, 所以想要高性能还是得自己来, 或者魔改halide. 对于cpu这种内存层次不多的硬件, halide所提供的描述能力倒是足够的.\nkernel void inspector(\n                  device const float* X                        [[buffer(0)]],\n                  device float* result                         [[buffer(1)]],\n                  device uint* store                           [[buffer(2)]],\n                  uint thread_position_in_grid             ****    [[thread_position_in_grid]],\n                  uint threads_per_grid                        [[threads_per_grid]],\n                  uint dispatch_quadgroups_per_threadgroup     [[dispatch_quadgroups_per_threadgroup]],\n                  uint dispatch_simdgroups_per_threadgroup     [[dispatch_simdgroups_per_threadgroup]], \n                  uint dispatch_threads_per_threadgroup        [[dispatch_threads_per_threadgroup]], \n                  uint grid_origin                             [[grid_origin]], \n                  uint grid_size                               [[grid_size]], \n                  uint quadgroup_index_in_threadgroup          [[quadgroup_index_in_threadgroup]], \n                  uint quadgroups_per_threadgroup              [[quadgroups_per_threadgroup]], \n                  uint simdgroup_index_in_threadgroup          [[simdgroup_index_in_threadgroup]], \n                  uint simdgroups_per_threadgroup              [[simdgroups_per_threadgroup]], \n                  uint thread_execution_width                  [[thread_execution_width]], \n                  uint thread_index_in_quadgroup               [[thread_index_in_quadgroup]], \n                  uint thread_index_in_simdgroup               [[thread_index_in_simdgroup]], \n                  uint thread_index_in_threadgroup             [[thread_index_in_threadgroup]], \n                  uint thread_position_in_threadgroup          [[thread_position_in_threadgroup]], \n                  uint threadgroup_position_in_grid            [[threadgroup_position_in_grid]],\n                  uint threadgroups_per_grid                   [[threadgroups_per_grid]], \n                  uint threads_per_simdgroup                   [[threads_per_simdgroup]], \n                  uint threads_per_threadgroup                 [[threads_per_threadgroup]])"
  },
  {
    "objectID": "posts/hafuman.html",
    "href": "posts/hafuman.html",
    "title": "哈夫曼树",
    "section": "",
    "text": "其实早就应该写完这个哈夫曼树，只不过最近有点没有心情学习。对于哈夫曼树的构造，我总结了以下几步："
  },
  {
    "objectID": "posts/hafuman.html#main.cpp",
    "href": "posts/hafuman.html#main.cpp",
    "title": "哈夫曼树",
    "section": "main.cpp",
    "text": "main.cpp\n#include \"hafuman.cpp\"\n#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n#include &lt;functional&gt;\n#include &lt;queue&gt;\nusing namespace std;\n\n/* 运算符重载  */\nstruct mycmp {\n    bool operator()(int a, int b) { //通过传入不同类型来定义不同类型优先级\n        return a &gt; b;               //最小值优先\n    }\n};\n\nint TestNum[] = {5, 4, 3, 2, 1};\n\nint main(int argc, char const *argv[]) {\n    /* 最小堆用于存放数据 */\n    priority_queue&lt;int, vector&lt;int&gt;, mycmp&gt; MyHeap;\n    /* 队列于存放临时节点 */\n    queue&lt;HAFUNode_t&lt;int&gt;&gt; TempQueue;\n    /* 哈夫曼树根节点 */\n    HAFUNode_t&lt;int&gt; HaFuTree = nullptr;\n    /* 创建临时变量 */\n    int tright, tleft;\n    /* 赋值 */\n    for (int i = 0; i &lt; 5; i++) {\n        MyHeap.push(TestNum[i]);\n    }\n\n    printf(\"测试哈夫曼\\n\");\n    while (!MyHeap.empty()) {               /* 优先队列非空 */\n        tleft = MyHeap.top(), MyHeap.pop(); /* 输出左值并释放 */\n        if (MyHeap.empty()) {               /* 判断是否为空 */\n            break;\n        }\n        tright = MyHeap.top(), MyHeap.pop(); /* 输出右值并释放 */\n        /* 构造子树 */\n        HaFuTree = CreatSubTree(tleft, tright, tleft + tright);\n        if (MyHeap.empty()) { /* 若为最后一个节点 */\n            break;\n        } else {\n            AutoJoin(HaFuTree, TempQueue);\n        }\n        /* 再加入元素 */\n        MyHeap.push(tleft + tright);\n    }\n    /* 将队列中所有的子树合并成一颗 */\n    while (!TempQueue.empty()) {\n        FinalJoin(HaFuTree, TempQueue);\n    }\n\n    DrawTree(HaFuTree);\n    return 0;\n}"
  },
  {
    "objectID": "posts/hafuman.html#hafuman.cpp",
    "href": "posts/hafuman.html#hafuman.cpp",
    "title": "哈夫曼树",
    "section": "hafuman.cpp",
    "text": "hafuman.cpp\n#include \"hafuman.h\"\n#include &lt;cmath&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstring&gt;\n#include &lt;iostream&gt;\n\n/**\n * @brief  构造一个子树\n * @param[in]  类型T：左节点 右节点\n * @param[out]\n * @return\n **/\ntemplate &lt;typename T&gt; HAFUNode_t&lt;T&gt; CreatSubTree(T Left, T Right, T Root) {\n    /* set the root  */\n    HAFUTree&lt;T&gt; *pH = (HAFUNode_t&lt;T&gt;)malloc(sizeof(struct HAFUTree&lt;T&gt;));\n    pH-&gt;data = Root;\n\n    /* set the left child */\n    pH-&gt;left = (HAFUNode_t&lt;T&gt;)malloc(sizeof(struct HAFUTree&lt;T&gt;));\n    pH-&gt;left-&gt;data = Left;\n    pH-&gt;left-&gt;left = nullptr;\n    pH-&gt;left-&gt;right = nullptr;\n\n    /* set the right child */\n    pH-&gt;right = (HAFUNode_t&lt;T&gt;)malloc(sizeof(struct HAFUTree&lt;T&gt;));\n    pH-&gt;right-&gt;data = Right;\n    pH-&gt;right-&gt;left = nullptr;\n    pH-&gt;right-&gt;right = nullptr;\n    return pH;\n}\n\n/**\n * @brief 将一个根挂载在另一根的左侧\n **/\ntemplate &lt;typename T&gt;\nstatic HAFUNode_t&lt;T&gt; LeftJoin(HAFUNode_t&lt;T&gt; LEFT, HAFUNode_t&lt;T&gt; ROOT) {\n    free(ROOT-&gt;left);\n    ROOT-&gt;left = LEFT;\n    return ROOT;\n}\n/**\n * @brief 将一个根挂载在另一根的右侧\n **/\ntemplate &lt;typename T&gt;\nstatic HAFUNode_t&lt;T&gt; RightJoin(HAFUNode_t&lt;T&gt; LEFT, HAFUNode_t&lt;T&gt; ROOT) {\n    free(ROOT-&gt;right);\n    ROOT-&gt;right = LEFT;\n    return ROOT;\n}\n\n/**\n * @brief 将队中的子树插入新的子树，成功后保存至队列\n **/\ntemplate &lt;typename T&gt;\nvoid AutoJoin(HAFUNode_t&lt;T&gt; NEW, std::queue&lt;HAFUNode_t&lt;T&gt;&gt; &Q) {\n    HAFUNode_t&lt;T&gt; temp = nullptr;\n    if (Q.empty()) {\n        Q.push(NEW);\n    } else { /* Queue not empty then start insert */\n        // printf(\"NEW-&gt;data:%d Q.front()-&gt;left-&gt;data:%d\\n\", NEW-&gt;data,\n        //        Q.front()-&gt;left-&gt;data);\n        if (Q.front()-&gt;data == NEW-&gt;left-&gt;data) { /* left join */\n            temp = LeftJoin(Q.front(), NEW);\n            Q.pop();      /* pop the old  */\n            Q.push(temp); /* push the new root */\n        } else if (Q.front()-&gt;data == NEW-&gt;right-&gt;data) {\n            temp = RightJoin(Q.front(), NEW);\n            Q.pop();\n            Q.push(temp);\n        } else { /* can't insert */\n            Q.push(NEW);\n        }\n    }\n}\n/**\n * @brief 最终插入，将队列中的元素插入到根中\n **/\ntemplate &lt;typename T&gt;\nvoid FinalJoin(HAFUNode_t&lt;T&gt; ROOT, std::queue&lt;HAFUNode_t&lt;T&gt;&gt; &Q) {\n    //        Q.front()-&gt;left-&gt;data);\n    if (Q.front()-&gt;data == ROOT-&gt;left-&gt;data) { /* left join */\n        LeftJoin(Q.front(), ROOT);\n        Q.pop(); /* pop the old  */\n    } else if (Q.front()-&gt;data == ROOT-&gt;right-&gt;data) {\n        RightJoin(Q.front(), ROOT);\n        Q.pop();\n    } else { /* can't insert */\n        printf(\"insert error!\\r\\n\");\n    }\n}\n\n/**\n * description  输出二叉树的高度\n * @param[in]   BinTree_t\n * @retval      int\n **/\ntemplate &lt;typename T&gt; static int FindTreeHeight(HAFUNode_t&lt;T&gt; BT) {\n    int rightlen, leftlen, maxlen;\n    if (BT) {\n        rightlen = FindTreeHeight(BT-&gt;right);\n        leftlen = FindTreeHeight(BT-&gt;left);\n        maxlen = leftlen &gt; rightlen ? leftlen : rightlen;\n        return maxlen + 1;\n    } else {\n        return 0;\n    }\n}\n\n/**\n * @brief 绘出树\n **/\ntemplate &lt;typename T&gt; void DrawTree(HAFUNode_t&lt;T&gt; BT) {\n    int height = FindTreeHeight(BT);\n    int cnt = 0;\n\n    /* 开始层序遍历二叉树并且绘制图形 */\n    std::queue&lt;HAFUNode_t&lt;T&gt;&gt; CurLevelqueue;  /*记录当前层的元素  */\n    std::queue&lt;HAFUNode_t&lt;T&gt;&gt; NextLevelqueue; /*记录下一层的元素  */\n    std::queue&lt;HAFUNode_t&lt;T&gt;&gt; CurTempqueue;   /*当前队列副本  */\n    std::queue&lt;HAFUNode_t&lt;T&gt;&gt; NextTempqueue;  /*下一层队列副本  */\n    HAFUNode_t&lt;T&gt; temp, lasttemp = NULL;\n    int width = 0;\n    CurLevelqueue.push(BT); //将头节点入队s\n    /*现在修改了入队函数,空指针也可以入队\n    所以在出队的时候就需要进行判断  */\n    for (int i = 0; i &lt; height; i++) {\n        cnt = (int)pow(2, i);           //当前行的个数21\n        width = pow(2, height - i + 1); //设置宽度为2^(height-i+1)\n        CurTempqueue = CurLevelqueue;   /* 临时记录 */\n        while (cnt--) {\n            temp = CurLevelqueue.front();\n            if (temp != NULL) {\n                printf(\"%*d%*c\", width, temp-&gt;data, width, ' ');\n                NextLevelqueue.push(temp-&gt;left);\n                NextLevelqueue.push(temp-&gt;right); //将下一层子节点入队\n            } else {\n                printf(\"%*c%*c\", width, ' ', width, ' ');\n                NextLevelqueue.push(nullptr);\n                NextLevelqueue.push(nullptr); //如果此层是空，那么再入两个空指针\n            }\n            CurLevelqueue.pop(); //将上一层节点出队\n        }\n        printf(\"\\n\");\n        NextTempqueue = NextLevelqueue; /* 记录下一层元素 */\n        /* 接下来打印层间符号 */\n        if (i != height - 1) { /* 若不是最后一行 */\n            /* 先记录下一行元素宽度间隔 */\n            int nextwidth = (int)pow(2, height - i);\n            /* 并且以他下一层元素的个数打印'-' */\n            for (int k = 0; k &lt; (int)pow(2, i + 1); k++) {\n                /* 每隔一位去打印width个'-' */\n                if ((k % 2) == 0) { /* 偶数位打印 空格+`-` */\n                    if (NextTempqueue.front() != nullptr) {\n                        printf(\"%*c\", nextwidth, '-');\n                        for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                            printf(\"-\");\n                        }\n                    } else {\n                        printf(\"%*c\", nextwidth, ' ');\n                        for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                            printf(\" \");\n                        }\n                    }\n                    NextTempqueue.pop();\n                } else { /* 奇数位打印 空格+`-` */\n                    if (NextTempqueue.front() != nullptr) {\n                        for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                            printf(\"-\");\n                        }\n                        printf(\"%*c\", nextwidth, ' ');\n                    } else {\n                        for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                            printf(\" \");\n                        }\n                        printf(\"%*c\", nextwidth, ' ');\n                    }\n                    NextTempqueue.pop();\n                }\n            }\n            printf(\"\\n\");\n        }\n        printf(\"\\r\");\n        swap(CurLevelqueue, NextLevelqueue); /* 交换队列 */\n    }\n}"
  },
  {
    "objectID": "posts/hafuman.html#hafuman.h",
    "href": "posts/hafuman.html#hafuman.h",
    "title": "哈夫曼树",
    "section": "hafuman.h",
    "text": "hafuman.h\n#ifndef _HAFUMAN_H\n#define _HAFUMAN_H\n\n#include &lt;queue&gt;\n/**\n * @brief the defination\n **/\ntemplate &lt;typename T&gt; struct HAFUTree;\n/**\n * @brief 别名\n **/\ntemplate &lt;typename T&gt; using HAFUNode_t = struct HAFUTree&lt;T&gt; *;\n/**\n * @brief 结构体模板类\n **/\ntemplate &lt;typename T&gt; struct HAFUTree {\n    T data;\n    HAFUNode_t&lt;T&gt; left;\n    HAFUNode_t&lt;T&gt; right;\n};\n\n\n#endif"
  },
  {
    "objectID": "posts/hafuman.html#makefile",
    "href": "posts/hafuman.html#makefile",
    "title": "哈夫曼树",
    "section": "makefile",
    "text": "makefile\nCC = clang++\nCFLAGS = -g # debug\nCLIBS = #-lpthread\n \nINCLUDE = $(wildcard ./*.h) # INCLUDE = a.h b.h ... can't be defined like \"INCLUDE = ./*.h\"\nSOURCES = $(wildcard ./*.cpp)\n \nTARGET = hafutree\nOBJECTS = $(patsubst %.cpp,%.o,$(SOURCES))\n \n$(TARGET) : $(OBJECTS)\n    $(CC) $(CFLAGS) $^ -o $@ $(CLIBS)\n\n$(OBJECTS) : %.o : %.cpp\n    $(CC) -c $(CFLAGS) $&lt; -o $@\n \n.PHONY : clean\nclean:\n    rm -rf *.o $(TARGET)"
  },
  {
    "objectID": "posts/glenside.html",
    "href": "posts/glenside.html",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "",
    "text": "这是一篇基于EGraph对Tensor级别的IR进行Term Rewrite的文章."
  },
  {
    "objectID": "posts/glenside.html#machine-learning-accelerators",
    "href": "posts/glenside.html#machine-learning-accelerators",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Machine Learning Accelerators",
    "text": "Machine Learning Accelerators\n对于深度学习加速器来说最麻烦的就是如何自动化的把神经网络操作转化为这种加速器所支持的操作."
  },
  {
    "objectID": "posts/glenside.html#tensor-irs-and-compilers",
    "href": "posts/glenside.html#tensor-irs-and-compilers",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Tensor IRs and Compilers",
    "text": "Tensor IRs and Compilers\nRewriting 和 Polyhedral 虽然做法不一样,但是他们对于编译器来说是互补的."
  },
  {
    "objectID": "posts/glenside.html#term-rewriting-and-equality-saturation",
    "href": "posts/glenside.html#term-rewriting-and-equality-saturation",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "term rewriting and Equality Saturation",
    "text": "term rewriting and Equality Saturation\nEgg 已经被应用在DSP Compiler的自动向量化上了."
  },
  {
    "objectID": "posts/glenside.html#pure-matrix-multiplication",
    "href": "posts/glenside.html#pure-matrix-multiplication",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Pure Matrix Multiplication",
    "text": "Pure Matrix Multiplication\n我们用[A]表示一个由A类型组成的向量. 那么可以表示出内积为 \\([f64] \\cdot [f64] \\rightarrow f64\\)\n然后2D Tanspose表示为 \\([[f64]] \\rightarrow [[f64]]\\) 这里的意思就是一个向量内部由向量组成,那么就是2D矩阵了, 同时输出也是同样的2D矩阵,(可能维度发生了变化)\n2D的矩阵乘的公式如下: \\(R_{ij} = \\sum_k P_{ik}Q_{kj} = P_{i} \\cdot Q_{j}^T\\)\n也就是计算输出\\(ij\\)上每对\\(P\\)的行和\\(Q\\)的列长度为\\(k\\)的向量内积.\n因此我们引入map 操作: \\(map : (A \\rightarrow B) * [A] \\rightarrow [B]\\) \\((A -&gt; B)\\) 表示的就是一个函数,他的计算就是把类型A转换为B.\n笛卡尔积: \\(cardProd : [A] \\times [B] \\rightarrow [A \\times B]\\)\n假设这里的\\(A\\)和\\(B\\)都是一维向量[f64],这里\\(A \\times B\\)就是表示的是[[f64]],其中里面的维度是2, 外面维度和\\(A\\)相同, 最后外面再加一个维度得到\\([A \\times B]\\).\n\\(matmul(P,Q) = map(dotProd, cartProd(P, trans2(Q)))\\)\n这里的思路就是\\(P\\)和\\(Q\\)的转置 每个元素组合, 也就是\\(P\\)的行和\\(Q\\)的列组合, 组合好后每个数据对都应用内积求结果.\n这个matMul公式实例化就如下所示, 注意到输出的数据就变成了[f64]. 各位也可以自己将P = [[f64]]带入公式中推导一下shape.\nimport numpy as np\n\ndef dotProd(AB):\n  (A, B) = AB\n  assert A.ndim == 1\n  return np.dot(A, B)\n\ndef cartProd(A: np.ndarray, B: np.ndarray):\n  AB = []\n  for a in A.reshape((-1, A.shape[-1])):\n    for b in B.reshape((-1, A.shape[-1])):      \n      AB.append((a, b)) \n  return AB\n\ndef trans2(A: np.ndarray):\n  assert A.ndim == 2\n  return A.transpose()\n\ndef test_cardproduct():\n  P = np.random.rand(3, 4)\n  Q = np.random.rand(4, 5)\n  print(list(map(dotProd, cartProd(P, trans2(Q)))))\n[0.10732114230108192,\n 0.21243371438870884,\n 0.34685428666259904,\n 0.14556577914149274,\n 0.23254688326914144,\n 0.5821735344411842,\n 0.9735256103240557,\n 1.9118977760582447,\n 0.5735451588389484,\n 0.5549736743719554,\n 0.31553182873079905,\n 0.582579830538644,\n 1.1357542180343412,\n 0.20513303615713718,\n 0.3916623321089719]\n上面那个公式的得到的结果是[f64],但是实际上我们的2D矩阵乘就是要得到2D的结果. 经过观察,很明显就是cartProd会将shape给展开, 因此简单的修改方法则是添加一个新的函数.\n\\(cartProd2D : [A] * [B] -&gt; [[A*B]]\\)\n但是如果用这个函数代替上面的cartProd, map时就会出错,他不能把一个[[f64]]的输入传递给dotProd.\n因此添加一个新的mapAt2, 将map作用在指定维度\n\\(mapAt2 : (A \\rightarrow B) * [[A]] \\rightarrow [[B]]\\)\n那么要得到[[f4]]的矩阵乘结果,公式如下:\n\\(matMul(P,Q) = mapAt2(dotProd), cartProd2D(P, trans2(Q))\\)\n对应的代码实现如下:\ndef cartProd2(A: np.ndarray, B: np.ndarray):\n  n, m = len(A), len(B)\n  AB = [[1 for j in range(m)] for i in range(n)]\n  for i in range(n):\n    for j in range(m):\n      AB[i][j] = (A[i], B[j])\n  return AB\n\ndef mapAt2(func, A: list[list[any]]):\n  n, m = len(A), len(A[0])\n  B = [[1 for j in range(m)] for i in range(n)]\n  for i in range(n):\n    for j in range(m):\n      B[i][j] = func(A[i][j])        \n  return B\n\nP = np.random.rand(3, 4)\nQ = np.random.rand(4, 5)\nprint(np.array(mapAt2(dotProd, cartProd2(P, trans2(Q)))))  \n[[1.90265933 1.37014723 1.90525837 2.16506508 0.8182536 ]\n [1.74624439 1.06923152 1.74345372 1.85747233 0.82131666]\n [1.88350644 1.49704492 1.93444511 2.1764349  0.8319122 ]]"
  },
  {
    "objectID": "posts/glenside.html#glenside-design-constraints-and-goals",
    "href": "posts/glenside.html#glenside-design-constraints-and-goals",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Glenside Design Constraints and Goals",
    "text": "Glenside Design Constraints and Goals\n我们根据上面提出的函数就能写出一系列的rewrite规则了.但是有个规则时依赖于特定维度的shape, 如果我们有了更高阶的维度, 我们首先得实现对应的算子(就像刚才需要添加一个cart Product2D),还得在所有的规则上添加新的规则转换,比如1D转换2D,2D转1D. 非常容易就出现组合爆炸的问题.\n一种解决方法是添加lambda函数,通过偏函数的方式解决shape align的问题\n\\[\n\\text{matMul}'\\ P\\ Q\\ :=\\ \\text{map}'(\\lambda\\ \\text{r} \\Rightarrow \\text{map}' (\\text{dotProd}'\\ \\lambda)\\ (\\text{trans2}\\ Q))\\ P\n\\] 或者使用index标记的方式 \\[\n\\text{matMul}(P,Q)[i,j]\\ :=\\  \\text{dotProd}(P[i],\\text{trans2}(Q)[j])\n\\]\n但是上面两种方法实际上都是要添加name binding的,这对term rewriting来说是很困难的,因为你做rewrite的时候需要分析每个表达式上下文,当前的var bind到的是什么.作者利用egg尝试了实现,但是发现潜在的搜索空间膨胀问题还是难以解决.\n以上所有的约束就是Glenside需要解决的问题: 提供一个灵活的IR支持高阶的tensor的操作的同时支持高性能的term rewriting."
  },
  {
    "objectID": "posts/glenside.html#access-patterns",
    "href": "posts/glenside.html#access-patterns",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Access Patterns",
    "text": "Access Patterns\naccess pattern将通用的tensor IR的dimension分成了iterated over 和 computed on两部分. 其中iterated over表示的就是accessed. (这种思路和numpy的universal functions比较类似).比如之前的matMul的例子,就是在dim 0进行迭代,在dim 1 进行计算.\naccess pattern 是被tensor shape所定义为两个tuple组成 paIR \\((S_a,\\ S_c)\\),tensor 的shape 等于两个tuple的concat结果.\n对于一个tensor T, 我们用\\(n_A\\)表示\\(S_A\\)的长度, 此时我们利用语法 \\(\\text{access}\\ T\\ n_A\\)来表示这个tensor的access pattern.\n比如\\(\\text{T.shape} = (m,n)\\)那么\\(\\text{access}\\ T\\ 1\\)就表示\\(((m),(n))\\)"
  },
  {
    "objectID": "posts/glenside.html#access-pattern-transformers",
    "href": "posts/glenside.html#access-pattern-transformers",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Access Pattern Transformers",
    "text": "Access Pattern Transformers\naccess pattern transformer修改一个或多个access pattern生成一个新的access pattern, glenside通过这个可以支持复杂的pattern如slice transpose.\n其实就是把一些tensor的operator仅仅用修改access pattern的进行实现了,比如transpose,其本质就是改变了数据的访问顺序,对于pad就是多访问了一些元素.access pattern的妙处就是把很多复杂的操作都看成了对于tensor的访问这种简单的抽象,同时我们还不需要像TVM/MLIR一样定义一套shape infer的图,因为access pattern原生就能表示tensor的shape.\n下面举个🌰： 比如我们要取tensor \\(Q\\)的每一列进行矩阵乘, 此时使用transpose transformer,把access pattern修改成当前需要的结果.\n比如\\(Q\\)的shape为\\((N,O)\\),\\((\\text{access}\\ Q\\ 1)\\)表示读取每一行进行计算 \\(((N),(O))\\), \\((\\text{transpose}\\ (\\text{access}\\ Q\\ 1)\\ (\\text{list}\\ 1\\ 0))\\)就表示把读取每一行的访问模式变成了读取每一列进行计算即\n\\[\n\\begin{aligned}\n(\\text{access}\\ Q\\ 1) &= ((N),(O)) \\\\\n(\\text{transpose}\\ (\\text{access}\\ Q\\ 1)\\ (\\text{list}\\ 1\\ 0)) &= ((O),(N))\n\\end{aligned}\n\\]\n接下来对于cartProd的access transformer如下:\n\\[\n\\begin{aligned}\n  ((a_0,\\ldots,a_n),\\ (c_0,\\ldots,c_p)),\\ ((b_0,\\ldots,b_n),\\ (c_0, \\ldots,c_p)) \\Rightarrow ((a_0,\\ldots,a_n,\\ b_0,\\ldots,b_n),\\ (2,\\ c_0,\\ldots,c_p))\n\\end{aligned}\n\\]\n其中\\((2,\\ c_0,\\ldots,c_p)\\)表示的就是concat起来的两个子tensor.\n在矩阵乘中, \\(Q = (M,N),\\ P = (N,O)\\), 读取\\(Q\\)的行与\\(P\\)的列\\((((M),\\ (N)),\\ ((O),\\ (N)) )\\),然后带入cartProd的access transformer得到\\(((M,\\ O),\\ (2,\\ N))\\). 那么就表示在\\(Q\\)的行与\\(P\\)的列上每次取两个长度为\\(N\\)的向量进行计算."
  },
  {
    "objectID": "posts/glenside.html#access-pattern-operators",
    "href": "posts/glenside.html#access-pattern-operators",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Access Pattern Operators",
    "text": "Access Pattern Operators\noperator是Glenside中唯一表示计算的IR. 他们只在添加compute前缀时才被invoke（区别于access pattern transformer）, 即把操作映射到access pattern的compute维度上, 最终返回的access pattern中compute维度会被修改为operator所指示的,简单的说就是计算所调用的函数.\n\n\n\n\n\n\n\n\nOperator\nType\nDescription\n\n\n\n\nreduceSum\n\\((\\ldots) \\rightarrow ()\\)\nsum values\n\n\nreduceMax\n\\((\\ldots) \\rightarrow ()\\)\nmax of all values\n\n\ndotProd\n\\((t,s_0,\\ldots,s_n) \\rightarrow ()\\)\neltwise mul ; sum\n\n\n\n通过cartProd之后得到了\\(((M,O),(2,N))\\)的access pattern, 然后应用dotProd之后的得到了\\(((M,O),())\\), 最后一个矩阵乘的Glenside表示·就如下所示：\n\\[\n\\begin{aligned}\n& (\\text{compute}\\ \\text{dotProd}          &;\\ \\ \\ &((M,O), ()) \\\\\n& \\ (\\text{cartProd}                 &;\\ \\ \\ &((M,O), (2, N)) \\\\\n& \\ \\ (\\text{access}\\ \\text{activations}\\ 1)  &;\\ \\ \\ &((M), (N)) \\\\\n& \\ \\ \\ (\\text{transpose}              &;\\ \\ \\ &((O), (N)) \\\\\n& \\ \\ \\ \\ (\\text{access}\\  \\text{weights}\\ 1)    &;\\ \\ \\ &((N), (O)) \\\\\n& \\ \\ \\ \\ \\ (\\text{list}\\ 1\\ 0))))\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/glenside.html#representation-of-common-ml-kernels",
    "href": "posts/glenside.html#representation-of-common-ml-kernels",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "5.1 Representation of Common ML Kernels",
    "text": "5.1 Representation of Common ML Kernels\n\n2D Convolution\n卷积的计算公式如下：\n\\[\n\\begin{aligned}\n&\\operatorname{out}[n, o, x, y]= \\\\\n&\\sum_{d x, d y, c}(A[n, c, S[0] \\cdot x+d x, S[1] \\cdot y+d y] \\cdot W[o, c, d x, d y])\n\\end{aligned}\n\\]\n转换为Glenside表示： \\[\n\\begin{array}{lll}\n\\text { (transpose } & ; & \\left(\\left(N, O, H^{\\prime}, W^{\\prime}\\right),()\\right) \\\\\n\\ \\text { (squeeze } & ; & \\left(\\left(N, H^{\\prime}, W^{\\prime}, O\\right),()\\right) \\\\\n\\ \\ \\text { (compute dotProd } & ; & \\left(\\left(N, 1, H^{\\prime}, W^{\\prime}, O\\right),()\\right) \\\\\n\\ \\ \\ \\text { (cartProd } & ; & \\left(\\left(N, 1, H^{\\prime}, W^{\\prime}, O\\right),\\left(2, C, K_{h}, K_{w}\\right)\\right) \\\\\n\\ \\ \\ \\ \\text { (windows } & ; & \\left(\\left(N, 1, H^{\\prime}, W^{\\prime}\\right),\\left(C, K_{h}, K_{w}\\right)\\right) \\\\\n\\ \\ \\ \\ \\ \\text { (access activations 1) } & ; & ((N),(C, H, W)) \\\\\n\\ \\ \\ \\ \\ \\ \\text { (shape C Kh Kw) } & & \\\\\n\\ \\ \\ \\ \\ \\ \\text { (stride 1 Sh Sw)) } & & \\\\\n\\ \\ \\ \\ \\ \\text { (access weights 1))) } & & ((O),  \\left.\\left(C, K_{h}, K_{w}\\right)\\right) \\\\\n\\ \\ \\ \\ \\text { 1) } & & & \\\\\n\\ \\ \\ \\text { (list } 0 \\text { 3 1 2) ) } & &\n\\end{array}\n\\]\n首先取出weights的\\((C,K_h,K_w)\\),然后使用windows的操作生成新的access pattern \\(((N,1,H’,W’),(C,K_h,K_w))\\). 即对于输出的每一个的像素位置,取一个原始的输入窗口. 最后每个窗口和卷积的 filter 进行外积后计算内积. 再用squeeze和transpose得到输出的结果.\n\n\nMax Pooling\n其数学公式如下：\n\\[\n\\begin{aligned}\n&\\operatorname{out}[n, c, x, y]= \\\\\n&\\max _{d x, d y}(\\text { activations }[n, c, \\text { strides }[0] \\cdot x+d x, \\text { strides }[1] \\cdot y+d y])\n\\end{aligned}\n\\]\n他的Glenside表示与卷积类似,windows之后reduce即可：\n\\[\n\\begin{array}{ll}\n\\text { (compute reduceMax } & ;\\left(\\left(N, C, H^{\\prime}, W^{\\prime}\\right),()\\right) \\\\\n\\ \\text { (windows } & ;\\left(\\left(N, C, H^{\\prime}, W^{\\prime}\\right),\\left(K_{h}, K_{w}\\right)\\right) \\\\\n\\ \\ \\text { (access activations 2) } & ;((N, C),(H, W)) \\\\\n\\ \\ \\ \\text { (shape Kh Kw) } & \\\\\n\\ \\ \\ \\ \\text { (stride Sh Sw))) } &\n\\end{array}\n\\]\n我觉得glenside把访问和计算分离的方式就极大的简化了计算的算子, 因为访问变换的时候其实包含了传统表述中计算的一部分.比如上面的两个例子中, conv2d和maxpool的核心都是取window然后计算,一个是取3d一个取2d, 但是此时取window的并不是在window函数上配参数,而是直接把这个信息附加到tensor自身上了.\n这种表示方法虽然无法和通常的数学计算流程表示一一对应,但是他作为IR就起到了很好的桥梁作用,并且他这个内积外积设计就和很多加速器的核心逻辑一致."
  },
  {
    "objectID": "posts/glenside.html#mapping-matmul-to-accelerators",
    "href": "posts/glenside.html#mapping-matmul-to-accelerators",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Mapping matMul to Accelerators",
    "text": "Mapping matMul to Accelerators\nGlenside所提出的demo是一个weight-stationary的脉动阵列,然后Glenside基于egg的库添加了一系列的规则,下面是将矩阵乘转换为脉动整列计算的规则：\n\\[\n\\begin{aligned}\n&\\text { (compute dotProd (cartProd ?a0 ?a1)) } \\Longrightarrow \\\\\n&\\quad \\text { (systolicArray ?rows ?cols } \\\\\n&\\quad ? a 0 \\text { (access (transpose ?a1 (list } 1\\ 0))\\ 0) \\text { ) } \\\\\n&\\text { where ?a0 is of shape ((?batch), (?rows)) } \\\\\n&\\text { and ?a1 is of shape ((?cols), (?rows)) }\n\\end{aligned}\n\\]\n脉动阵列的形状参数由\\(\\text{rows}\\)和\\(\\text{cols}\\)所决定,同时在接下来的access pattern中更加细致的表示硬件如何访问tensor,首先是读取所有的数据\\((\\text{hence},(\\text{access}\\ \\ldots\\ 0))\\),然后在内存中进行transpose.这种更加细致的表示方法可以提供更加丰富的数据layout信息,对于后续的优化/codegen有潜在的好处."
  },
  {
    "objectID": "posts/glenside.html#flexible-mapping-discovering-im2col",
    "href": "posts/glenside.html#flexible-mapping-discovering-im2col",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Flexible Mapping: Discovering im2col",
    "text": "Flexible Mapping: Discovering im2col\nim2col的布局转换可以提升计算速度,虽然会导致一部分的内存开销. 这种transform涉及直接在内存中对windows操作实例化,虽然会导致额外的数据复制,但是只要这个开销小雨取偏移的开销就是有好处的. 接下来Glenside将展示如何自动发现im2col的transform.\n首先上面提出的脉动整列转换都是只针对单纯两个向量计算的映射,而卷积/矩阵乘最大的问题就是最后的内积/外积操作输入的tensor维度并不确定,所以需要先自动的把access pattern的维度降下来转换到脉动阵列上,不然我们又回到了为每个场景写pass的情况了.\nGlenside提出一个exploratory rewrite,即添加一系列看似无效的操作从而引入潜在的rewrite机会.比如把一个access pattern展平之后并reshape为原样,这样就能解决之前规则中维度不匹配的问题. \\[\n\\begin{aligned}\n?a \\Rightarrow (\\text{reshape}\\ \\ (\\text{flatten}\\ ?a)\\ \\text{?shape})\n\\end{aligned}\n\\]\n不过这样也带来了一个问题,添加了reshape之后还需要消除它才能真正的进行脉动阵列的转换,因此又添加了关于reshape与cartProd/dotProd计算的composition commutativity规则,将reshape操作从表达式中移除（意思就是这里没什么好办法,直接手动加两个规则规避一下比较简单）. \\[\n\\begin{array}{r}\n\\text { (cartProd (reshape ?a0 ?shape0) (reshape ?a1 ?shape } 1) \\text { ) } \\Longrightarrow \\text { (reshape (cartProd ?a0 ?a1) ?newShape) } \\\\\n\\text { (compute dotProd (reshape ?a ?shape)) } \\Longrightarrow \\text { (reshape (compute dotProd ?a) ?newShape) }\n\\end{array}\n\\] 不过最终的结果证明了只需要寥寥几个规则就可以达到传统手写pass的程度,编写的复杂度更低,同时无需考虑pass ordering的问题."
  },
  {
    "objectID": "posts/glenside.html#flexible-mapping-matmul-blocking",
    "href": "posts/glenside.html#flexible-mapping-matmul-blocking",
    "title": "Pure Tensor Program Rewriting via Access Patterns",
    "section": "Flexible Mapping: matMul Blocking",
    "text": "Flexible Mapping: matMul Blocking\n接下来作者探索了用Glenside做tiling,比如把\\(256 \\times 256\\)转换为多个\\(16 \\times 16\\)小矩阵乘. 和脉动阵列一样,作者也是需要一个探索性的rewrite以及一些消除多余operate的rewrite,这里的探索性rewrite那肯定就是slice/concat了: \\[\n\\begin{aligned}\n  ?a \\Rightarrow (\\text{concat}\\ \\ (\\text{slice}\\ ?a\\ ?dim\\ ?b0\\ ?b1) (\\text{slice}\\ ?a\\ ?dim\\ ?b1\\ ?b2)\\ ?dim)\n\\end{aligned}\n\\] 不过这个探索性太强了,如果全部都组合肯定直接爆炸,因此作者设置的每次切一半,保证是2的倍数.然后再添加一些规则消除计算前的concat/slice."
  },
  {
    "objectID": "posts/flashattn.html",
    "href": "posts/flashattn.html",
    "title": "Flash Attention记录",
    "section": "",
    "text": "简单记录一下flash attention的推导和实现。"
  },
  {
    "objectID": "posts/flashattn.html#naive-softmax实现",
    "href": "posts/flashattn.html#naive-softmax实现",
    "title": "Flash Attention记录",
    "section": "naive softmax实现：",
    "text": "naive softmax实现：\n\\[\n\\begin{aligned}\nsoftmax(x_i, \\ldots, x_N) = \\frac{e ^ {x_i}}{\\sum_{j=1}^{N} e^{x_j}}, i \\in [1, N]\n\\end{aligned}\n\\]\n由于\\(e^x\\)会很大，容易出现数值溢出，因此出现safe softmax"
  },
  {
    "objectID": "posts/flashattn.html#safe-softmax实现",
    "href": "posts/flashattn.html#safe-softmax实现",
    "title": "Flash Attention记录",
    "section": "safe softmax实现",
    "text": "safe softmax实现\n\\[\n\\begin{aligned}\n  max_{N} &= max(x_i), \\ i \\in [1, N] \\\\\n  softmax(x_i, \\ldots, x_N) &= \\frac{e ^ {x_i - max_{N}}}{\\sum_{j=1}^{N} e^{x_j - max_{N}}}, i \\in [1, N]\n\\end{aligned}\n\\]\n但是在实现上需要循环三次，因为\\(max_N\\)和\\(x_{sum}\\)都需要单独的循环\n\\[\n\\begin{aligned}\n  max_i &= max(m_{i-1}, x_i)\\\\\n  sum_i &= sum_{i-1} + e^{x_i - max_N} \\\\\n  a_i &= \\frac{e^{x_i - max_N}}{sum_N}, \\ i \\in [1, N]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/flashattn.html#pass-softmax",
    "href": "posts/flashattn.html#pass-softmax",
    "title": "Flash Attention记录",
    "section": "2-pass softmax",
    "text": "2-pass softmax\n说实话，让我是没办法能想出来融合max以及sum的公式，可能作者也是受Welford方法启发的。 首先我们考虑把\\(sum_N\\)的公式展开，通过\\(exp\\)的计算性质把\\(- max_N\\)这个拆分为两个部分 \\[\n\\begin{aligned}\n  sum_{N} &= \\sum_{j = 1} ^ N e^{x_j - max_N} \\\\ \\\\\n    &= \\sum_{j = 1} ^ {N-1} e^{x_j - max_N} + e^{x_N - max_N}  \\\\\n    &= \\sum_{j = 1} ^ {N-1} e^{x_j - max_{N-1} + max_{N-1} - max_N} + e^{x_N - max_N} \\\\\n    &= (\\sum_{j = 1} ^ {N-1} e^{x_j - max_{N-1}}) e^{max_{N-1} - max_N} +e^{x_N - max_N} \\\\\n\\end{aligned}\n\\]\n观察上面的公式，发现如果从另一个视角去定义变量就可以让他们递归起来 \\[\n\\begin{aligned}\n\\text{let}\\ sum_{N}^\\prime &=\\sum_{j=1}^{N} e^{x_j - max_N} = sum_{N} \\\\\n  &= (\\sum_{j = 1} ^ {N-1} e^{x_j - max_{N-1}}) e^{max_{N-1} - max_N} +e^{x_N - max_N}  \\\\\n  &= sum_{N-1}^\\prime e^{max_{N-1} - max_N} +e^{x_N - max_N} \\\\\n  sum_i ^\\prime &= sum_{i-1} ^\\prime e^{max_{i-1} - max_i} +e^{x_i - max_i}\n\\end{aligned}\n\\]\n通过视角的转换将\\(max_N\\)与\\(sum_{i}\\)进行了解耦，并且当迭代到最后时\\(sum_{N}^\\prime = sum_{N}\\)。 虽然2-pass的方式需要在每次迭代添加额外的乘\\(e^{max_{i-1} - max_i}\\)的运算，但显然比访存开销低很多。"
  },
  {
    "objectID": "posts/flashattn.html#pass-attention",
    "href": "posts/flashattn.html#pass-attention",
    "title": "Flash Attention记录",
    "section": "2-pass Attention",
    "text": "2-pass Attention\n首先使用2-pass的softmax来实现一个attention,这里为了不混淆query len和seq len， 分别用k和i来表示。\n\\[\n\\begin{aligned}\n\\text{for i in [1, N]}:&\\\\\n  x_i &= Q[k, :]K^T[:, i]\\\\\n  max_i &= \\max(max_{i-1}, x_i) \\\\\n  sum_i^\\prime &= sum_{i-1} ^\\prime e^{max_{i-1} - max_i} +e^{x_i - max_i}\\\\\n\\text{end} \\qquad \\qquad \\\\\n\\text{for i in [1, N]}:&\\\\\n  a_i &= \\frac{e^{x_i - max_N}}{sum_N^\\prime} \\\\\n  o_i &= o_{i-1} + a_i V[i,:] \\\\\n\\text{end} \\qquad \\qquad \\\\\n  O[k,:] & = o_N\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/flashattn.html#pass-attention-1",
    "href": "posts/flashattn.html#pass-attention-1",
    "title": "Flash Attention记录",
    "section": "1-pass Attention",
    "text": "1-pass Attention\n在和V做矩阵乘时，每一个\\(o_i\\)还是依赖了\\(max_N\\)。 接下来就是找到办法把\\(max_N\\)的依赖消除。参考2-pass softmax的套路先定义： \\[\n\\begin{aligned}\no_N^\\prime &= \\sum_{i = 1} ^ {N} a_i V[i,:] \\\\\n           &= \\sum_{i = 1} ^ {N} \\frac{e^{x_i - max_N}}{sum_N^\\prime} V[i,:] \\\\\n           &= (\\sum_{i = 1} ^ {N-1} \\frac{e^{x_i - max_N}}{sum_N^\\prime} V[i,:]) + \\frac{e^{x_N - max_N}}{sum_N^\\prime} V[N,:] \\\\\n           &= (\\sum_{i = 1} ^ {N-1} \\frac{e^{x_i - max_N}}{sum_N^\\prime} \\frac{sum_{N-1}^\\prime}{sum_{N-1}^\\prime} \\frac{e^{x_i - max_{N-1}}}{e^{x_i - max_{N-1}}}  V[i,:]) + \\frac{e^{x_N - max_N}}{sum_N^\\prime} V[N,:] \\\\\n           &= (\\sum_{i = 1} ^ {N-1} \\frac{e^{x_i - max_{N-1}}}{sum_{N-1}^\\prime} V[i,:]) \\frac{sum_{N-1}^\\prime}{sum_{N}^\\prime}\\frac{e^{x_i - max_N}}{e^{x_i - max_{N-1}}} + \\frac{e^{x_N - max_N}}{sum_N^\\prime} V[N,:] \\\\\n           &= (\\sum_{i = 1} ^ {N-1} \\frac{e^{x_i - max_{N-1}}}{sum_{N-1}^\\prime} V[i,:]) \\frac{sum_{N-1}^\\prime}{sum_{N}^\\prime}e^{max_{N-1} - max_N} + \\frac{e^{x_N - max_N}}{sum_N^\\prime} V[N,:] \\\\\n           &= o_{N-1}^\\prime \\frac{sum_{N-1}^\\prime}{sum_{N}^\\prime}e^{max_{N-1} - max_N} + \\frac{e^{x_N - max_N}}{sum_N^\\prime} V[N,:] \\\\\n\\end{aligned}\n\\] 然后归纳得到不包含\\(max_N\\)的\\(o_i^\\prime\\)公式为： \\[\n\\begin{aligned}\n  o_i^\\prime &= o_{i-1}^\\prime \\frac{sum_{i-1}^\\prime}{sum_{i}^\\prime}e^{max_{i-1} - max_i} + \\frac{e^{x_i - max_i}}{sum_i^\\prime} V[i,:]\n\\end{aligned}\n\\]\n最终列出标量化的1-pass Attention形式： \\[\n\\begin{aligned}\n\\text{for i in [1, N]}:&\\\\\n  x_i &= Q[k, :]K^T[:, i]\\\\\n  max_i &= \\max(max_{i-1}, x_i) \\\\\n  sum_i^\\prime &= sum_{i-1} ^\\prime e^{max_{i-1} - max_i} +e^{x_i - max_i}\\\\\n    o_i^\\prime &= o_{i-1}^\\prime \\frac{sum_{i-1}^\\prime}{sum_{i}^\\prime}e^{max_{i-1} - max_i} + \\frac{e^{x_i - max_i}}{sum_i^\\prime} V[i,:] \\\\\n\\text{end} \\qquad \\qquad\\\\\n  O[k,:] & = o_N\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/flashattn.html#flash-attention-v1",
    "href": "posts/flashattn.html#flash-attention-v1",
    "title": "Flash Attention记录",
    "section": "Flash Attention v1",
    "text": "Flash Attention v1\n上面推导出来的1-pass attention是基于标量循环的，对于flash attention是需要按tile进行计算的，所以具体的公式还需要稍作修改。\n首先列出普通的softmax计算公式：\n\\[\n\\begin{aligned}\nX & =  [x_1, \\ldots , x_N] \\\\\nmax_N & = \\max(X) \\\\\n  &= \\max([x_1, \\ldots , x_N]) \\\\\nf(X) & = [f(x_1), \\ldots , f(x_N)] \\\\\n  &= [e^{x_1 - max_N}, \\ldots, e^{x_N - max_N}] \\\\\nsum_N & = \\sum_{i = 1}^N f(x_i) \\\\\n  & = \\sum_{i = 1}^N e^{x_i - max_N} \\\\\nsoftmax(X) & = \\frac{ f(X)}{sum_N}\n\\end{aligned}\n\\]\n现在来推导tiled softmax的计算公式， 那么假设现在的\\(X\\)是由两个长度为\\(N\\)的子向量组成的, 那么首先把它看成单个向量计算，然后拆分转换为可分治的公式： \\[\n\\begin{aligned}\nX & = [x^1, x^2] \\\\\nmax_{2N} & = \\max([\\max(x^1),\\max(x^2)]) \\\\\n  & = \\max([max_N^1, max_N^2]) \\\\\nf(X) &= \\left[ [e^{x_1^1 - max_{2N}},\\ldots, e^{x_N^1 - max_{2N}}] , [e^{x_1^2 - max_{2N}},\\ldots, e^{x_N^2 - max_{2N}}] \\right] \\\\\n    &= \\left[ e^{max_N^1 - max_{2N}} [e^{x_1^1 - max_N^1},\\ldots, e^{x_N^1 - max_N^1}] , e^{max_N^2 - max_{2N}} [e^{x_1^2 - max_N^2},\\ldots, e^{x_N^2 - max_N^2}] \\right] \\\\\n    &= \\left[ e^{max_N^1 - max_{2N}} f(x^1) , e^{max_N^2 - max_{2N}} f(x^2) \\right] \\\\\nsum_{2N} & = \\sum_{i = 1}^N e^{x_i^1 - max_{2N}} + \\sum_{i = 1}^N e^{x_i^2 - max_{2N}} \\\\\n         & = e^{max_N^1 - max_{2N}} \\sum_{i = 1}^N e^{x_i^1 - max_{N}^1} + e^{max_N^2 - max_{2N}} \\sum_{i = 1}^N e^{x_i^2 - max_{N}^2} \\\\\n         & = e^{max_N^1 - max_{2N}} sum_N^1 + e^{max_N^2 - max_{2N}} sum_N^2 \\\\\nsoftmax(X) &= \\frac{f(X)}{sum_{2N}}\n\\end{aligned}\n\\]\n此时可以发现，除了每个子向量的 \\(max^j\\) 用于计算 \\(f(x^j), sum^j\\)，还需要维护整体的\\(max, sum\\)用于计算最终的结果。\nflash attention的tiling就是将\\(x_i\\)向量化,基于1-pass attention的公式，结合tiled softmax公式，只需要略微修改\\(max_i,sum_i\\)的计算即可得到flash attention的公式： \\[\n\\begin{aligned}\n\\text{for i in [1, N/b]}:&\\\\\n  x_i &= Q[k, :]K^T[:, i:i+b]\\\\\n  max_i^{local} &= max(x_i) \\\\\n  max_i &= \\max(max_{i-1}, max_i^{local}) \\\\\n  sum_i^\\prime &= sum_{i-1} ^\\prime e^{max_{i-1} - max_i} + \\sum_{j=1}^{b} e^{x_i[j] - max_i}\\\\\n  o_i^\\prime &= o_{i-1}^\\prime \\frac{sum_{i-1}^\\prime}{sum_{i}^\\prime}e^{max_{i-1} - max_i} + \\sum_{j=1}^b \\frac{e^{x_i[j] - max_i}}{sum_i^\\prime} V[(i-1)b+j,:] \\\\\n\\text{end}\\qquad \\qquad\\\\\n  O[k,:] & = o_N\n\\end{aligned}\n\\]\n附上一个简易的flash attention实现供参考：\nimport pytest\nimport torch\nimport torch.nn.functional as F\nimport math\nimport numpy as np\n\n\nnp.set_printoptions(suppress=True)\n\n\ndef flash_attn(query: np.ndarray, key: np.ndarray, value: np.ndarray, attn_mask=None, dropout_p=0.0,\n               is_causal=False, scale=None, enable_gqa=False):\n  L, S = query.shape[-2], key.shape[-2]\n  scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n  attn_bias = np.zeros((L, S), dtype=query.dtype)\n  if is_causal:\n    assert attn_mask is None\n    temp_mask = np.tril(np.ones((L, S), dtype=np.bool_), k=0)\n    attn_bias[False == temp_mask] = float(\"-inf\")\n\n  if attn_mask is not None:\n    if attn_mask.dtype == np.bool_:\n      attn_bias[False == attn_mask] = -np.inf\n    else:\n      attn_bias = attn_mask + attn_bias\n\n  assert enable_gqa is False, \"GQA not implemented\"\n\n  for head in range(query.shape[0]):\n    Q = query[head]  # [query_len, dim]\n    K = key[head]  # [seq_len, dim]\n    V = value[head]  # [seq_len, dim]\n    O = np.zeros_like(Q, dtype=np.float32)  # [query_len, dim]\n    Tc = 4\n    Tr = 16\n    assert L % Tr == 0\n    assert S % Tc == 0\n\n    global_maxs = np.zeros([Tr, L // Tr], dtype=np.float32)\n    global_sums = np.zeros([Tr, L // Tr], dtype=np.float32)\n    for j in range(0, S, Tc):\n      # outer loop is seq_len, because seq_len is `K` dimension, we can reuse Kj,Vj `query_len/Tc` times\n      Kj = K[j:j + Tc, :]  # [Tc, dim]\n      Vj = V[j:j + Tc, :]  # [Tc, dim]\n      for (ii, i) in enumerate(range(0, L, Tr)):\n        # load\n        Qi = Q[i:i + Tr, :]  # [Tr, dim]\n        O_last = O[i:i + Tr, :]  # [Tr, dim]\n        max_last = (np.zeros([Tr, 1], dtype=np.float32) - np.inf) if j == 0 else global_maxs[:, ii:ii + 1]\n        sum_last = np.zeros([Tr, 1], dtype=np.float32) if j == 0 else global_sums[:, ii:ii + 1]\n\n        a_ij = (Qi @ Kj.T) * scale_factor  # [Tr, Tc]\n        a_ij += attn_bias[i:i + Tr, j:j + Tc]\n        max_local = np.max(a_ij, axis=1, keepdims=True)  # [Tr, 1]\n        max_i = np.maximum(max_last, max_local)  # [Tr, 1]\n        p_ij = np.exp(a_ij - max_i)  # [Tr, Tc]\n        sum_i = sum_last * np.exp(max_last - max_i) + np.sum(p_ij, axis=1, keepdims=True)  # [Tr, 1]\n        O_i = O_last * (sum_last * np.exp(max_last - max_i)) / sum_i + (p_ij / sum_i) @ Vj  # [Tr, dim]\n\n        # store\n        O[i:i + Tr, :] = O_i\n        global_maxs[:, ii:ii + 1] = max_i\n        global_sums[:, ii:ii + 1] = sum_i\n    return O\n\n\n@pytest.mark.parametrize(\"head_q, head_kv\", [(1, 1)])\n@pytest.mark.parametrize(\"query_len, seq_len\", [(64, 64)])\n@pytest.mark.parametrize(\"dim\", [128])\n@pytest.mark.parametrize(\"is_causal\", [False, True])\n@pytest.mark.parametrize(\"scale\", [1.0])\ndef test_flash_attention(head_q, head_kv, query_len, seq_len, dim, is_causal, scale):\n  query = np.random.rand(head_q, query_len, dim).astype(np.float32)  # [head_q, query_len, dim]\n  key = np.random.rand(head_kv, seq_len, dim).astype(np.float32)  # [head_kv, seq_len, dim]\n  value = np.random.rand(head_kv, seq_len, dim).astype(np.float32)  # [head_kv, seq_len, dim]\n\n  o = F.scaled_dot_product_attention(\n      torch.tensor(query), torch.tensor(key), torch.tensor(value), is_causal=is_causal, scale=scale)\n  o_np = o.numpy()  # [q_head,query,dim]\n\n  o_actual = flash_attn(query, key, value, is_causal=is_causal, scale=scale)\n\n  assert np.allclose(o_np, o_actual, atol=1e-7)\n\n\nif __name__ == \"__main__\":\n  pytest.main([__file__, \"-vvs\"])"
  },
  {
    "objectID": "posts/filebreak.html",
    "href": "posts/filebreak.html",
    "title": "ntfs文件损坏",
    "section": "",
    "text": "今天真的是背，我的一个md文档莫名奇妙损坏了。导致我的hexo无法生成博客。\n\n\n问题出现\n因为的是双系统，所以在两个系统中切换的时候，由于Windows的快速关机的bug特性，会导致在Linux那个硬盘只读，需要使用ntfsfix，我怀疑就是因为经常这样，导致我的那个文件被破坏了，并且在Linux下还无法删除。\n\n\n\n错误\n\n\n\n\n问题解决\n我重启到Windows下，然后删除了这个文件，后来在群里面问了一波，需要再Windows关机的时候选择重启，这样Windows就不会使用快速关机了。我可以在重启的时候去grub关机。真麻烦。"
  },
  {
    "objectID": "posts/fcitxconfig.html",
    "href": "posts/fcitxconfig.html",
    "title": "fcitx配置",
    "section": "",
    "text": "昨天刚刚重新装了Ubuntu，最舒服的一点就是Nvida的显卡驱动分分钟安装。但是也有一些不爽的，像这个fcitx的配置我又找了老半天。\n\n\n配置\n我的都是使用右CTRL切换输入法的，所以我巨讨厌Windows ( 因为我改不了233\n在Ubuntu下也会出现一些问题，不过都还好解决，就是fcitx不保存我的配置，所以需要去强行配置一下～\n修改他的配置文件，把switch_key修改为R_CTRL,并且把权限都改成只读～\n$ vi ~/.config/fcitx/config \n# 修改内容\n$ sudo chmod 444 ~/.config/fcitx/config \n$ ll ~/.config/fcitx/config \n-r--r--r-- 1 zqh zqh 3038 11月 26 15:56 /home/zqh/.config/fcitx/config"
  },
  {
    "objectID": "posts/em-algm.html",
    "href": "posts/em-algm.html",
    "title": "EM算法与EM路由",
    "section": "",
    "text": "搞定了yolo，终于可以学习点新的东西了。今天就学习一波胶囊网络中的EM路由。 先推荐一个课程资料，杜克大学的统计学课程，从python讲到c++，从矩阵计算讲到概率统计，从jit讲到cuda编程。看看人家本科生学的东西。。。"
  },
  {
    "objectID": "posts/em-algm.html#詹森不等式",
    "href": "posts/em-algm.html#詹森不等式",
    "title": "EM算法与EM路由",
    "section": "詹森不等式",
    "text": "詹森不等式\n对于一个凸函数\\(f\\),\\(E[f(x)]\\geq f(E[x])\\),将其反转获得一个凹函数。 如果一个函数\\(f(x)\\)在区间内都有\\(f''(x)\\geq 0\\)，那么它是一个凸函数。例如\\(f(x)=log\\ x\\),\\(f''(x)=-\\frac{1}{x^2}\\),说明他在\\(x\\in (0,+ \\infty]\\)是一个凸函数，詹森不等式的直观说明如下。\n\n其实只有当\\(f(x)\\)为常数时，詹森不等式才会相等。\n这里使用概率论表述的\\(E\\)，其实际就是积分。也就是说先经过凸函数的期望值必然大于等于期望的凸函数值。"
  },
  {
    "objectID": "posts/em-algm.html#完整信息的最大似然",
    "href": "posts/em-algm.html#完整信息的最大似然",
    "title": "EM算法与EM路由",
    "section": "完整信息的最大似然",
    "text": "完整信息的最大似然\n设置一个实验，硬币A向上的概率为\\(\\theta_A\\)的，硬币B向上的概率为\\(\\theta_B\\),接着一共做m此实验，每次实验随机选择一个硬币，投掷n次，并记录向下和向下的次数。如果我们记录了每个样本所使用的硬币，那我们就有完整的信息可以估计\\(\\theta_A\\)和\\(\\theta_B\\)。\n假设我们做了5次实验，每次向上的次数记录成向量\\(x\\)，并且使用的硬币顺序为\\(A,A,B,A,B\\)。他的似然函数为： \\[\n\\begin{aligned}\n    L(\\theta_A,\\theta_B)&= p(x_1; \\theta_A)\\cdot p(x_2; \\theta_A)\\cdot p(x_3; \\theta_B) \\cdot  p(x_4; \\theta_A) \\cdot p(x_5; \\theta_B) \\\\\n    &=B(n,\\theta_A).pmf(x_1)\\cdot B(n,\\theta_A).pmf(x_2)\\cdot B(n,\\theta_B).pmf(x_3)\\cdot B(n,\\theta_A).pmf(x_4)\\cdot B(n,\\theta_B).pmf(x_5)\n\\end{aligned}\n\\] 其中二项分布的概率计算方式为： \\[\n\\begin{aligned}\n    \\because X & \\sim B(n,p)\\\\\n    \\therefore  P(X=k) &= \\binom{n}{k} p^k (1-p)^{n-k} \\\\\n        &=  C_n^k p^{k}(1-p)^{n-k}\n\\end{aligned}\n\\]\n将似然函数对数化：\n\\[\n\\begin{aligned}\n    L(\\theta_A,\\theta_B)=\\log p(x_1; \\theta_A) + \\log p(x_2; \\theta_A) +\\log p(x_3; \\theta_B) + \\log p(x_4; \\theta_A) +\\log p(x_5; \\theta_B)\n\\end{aligned}\n\\]\n\\(p(x_i; \\theta)\\)是二项式分布的概率质量函数，其中\\(n=m,p=\\theta\\)。我们会使用\\(z_i\\)来表示第\\(i\\)个硬币的标签。\n\n使用最小化函数求解似然\nfrom numpy.core.umath_tests import matrix_multiply as mm\nfrom scipy.optimize import minimize\nfrom scipy.stats import bernoulli, binom\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1234)\n\n\n\"\"\" 做五次实验 \"\"\"\nn = 10  # 每次实验投掷10次\ntheta_A = 0.8\ntheta_B = 0.3\ntheta_0 = [theta_A, theta_B]\n# 两个硬币对应两个不同thta的二项分布\ncoin_A = bernoulli(theta_A)\ncoin_B = bernoulli(theta_B)\n\nzs = [0, 0, 1, 0, 1]  # 代表使用的硬币为哪个\n# 得到实验结果\nxs = np.array([np.sum(coin.rvs(n)) for coin in [coin_A, coin_A, coin_B, coin_A, coin_B]])\nprint(xs) # [7 9 2 6 0]\n\n\"\"\" 精确求解 \"\"\"\n# 计算所有的硬币A朝上的比例作为概率\nml_A = np.sum(xs[[0, 1, 3]]) / (3.0 * n)\n# 计算所有的硬币B朝上的比例作为概率\nml_B = np.sum(xs[[2, 4]]) / (2.0 * n)\n\nprint(ml_A, ml_B)  # 0.7333333333333333 0.1\n\n\n\"\"\" 数值估计 \"\"\"\n\n\ndef neg_loglik(thetas, n, xs, zs):\n    \"\"\" 对数似然计算函数 \n        这里的的二项分布是次数为n，概率可能为 theta_A 或 theta_B。\n        将每个二项分布对应x的对数概率密度函数求和后取相反数，接下来就是最小化此函数即可。\n    \"\"\"\n\n    logpmf = -np.sum([binom(n, thetas[z]).logpmf(x) for (x, z) in zip(xs, zs)])\n    return logpmf\n\n\nbnds = [(0, 1), (0, 1)]\n# 使用优化策略进行最小化求解\nres = minimize(neg_loglik, [0.5, 0.5], args=(n, xs, zs),\n               bounds=bnds, method='tnc', options={'maxiter': 100})\nprint(res)\n\"\"\" fun: 7.655267754139319\n     jac: array([-7.31859018e-05, -7.58504370e-05])\n message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n    nfev: 17\n     nit: 6\n  status: 1\n success: True\n       x: array([0.73333285, 0.09999965]) \"\"\"\n可以发现似然估计的估计值相当近似与精确计算所得到的结论。"
  },
  {
    "objectID": "posts/em-algm.html#当信息缺失时的最大似然",
    "href": "posts/em-algm.html#当信息缺失时的最大似然",
    "title": "EM算法与EM路由",
    "section": "当信息缺失时的最大似然",
    "text": "当信息缺失时的最大似然\n当我们没有记录实验所使用的硬币种类时，这个问题的解决就开始困难起来了。有一种解决问题的方式就是我们根据这个样本是由\\(A\\)或\\(B\\)产生的来给每个样本假设权重\\(\\boldsymbol{w}_i\\)。直觉上来想，权重应该是\\(\\boldsymbol{z}_i\\)的后验分布： \\[\n\\begin{aligned}\n    w_i = p(z_i \\ | \\ x_i; \\theta)\n\\end{aligned}\n\\]\n假设我们有\\(\\theta\\)的一些估计值，如果我们知道\\(z_i\\)，那我们就可以估计出\\(\\theta\\)，因为我们相当于拥有了全部的似然信息。EM算法的基本思想就是先猜测\\(\\theta\\)，然后计算出\\(z_i\\)，接着继续更新\\(\\theta\\)再计算\\(z_i\\)，重复数次直到收敛。\n非书面的表述：首先考虑一个对数似然函数作为曲线(曲面)，他的\\(x\\)轴表示\\(\\theta\\)。找到另一个\\(\\theta\\)的函数\\(\\boldsymbol{Q}\\)，他是对数似然函数的下界，在特定的\\(\\theta\\)值时对数似然函数与函数\\(\\boldsymbol{Q}\\)接触。接下来找到这个\\(\\theta\\)的值，并且使函数\\(\\boldsymbol{Q}\\)最大化，重复这个过程，是下界函数\\(\\boldsymbol{Q}\\)和对数似然函数的最大值相同，那么我们就得到了最大对数似然～\n从下图可以理解，每次迭代都能找到新的下界函数\\(\\boldsymbol{Q}\\)和当前最大的对数似然，等到迭代到一定程度时，就找到了全局的最大的对数似然。\n\n当然，还有个问题就是如何找到这个对数似然函数的下界函数\\(\\boldsymbol{Q}\\)，这就需要使用詹森不等式来进行数学推理。\n\n推导\n在EM算法的E-step中，我们确定一个函数，他是对数似然的下界。 \\[\n\\begin{align}\nl &= \\sum_i{\\log p(x_i; \\theta)} && \\text{定义对数似然函数} \\\\\n&= \\sum_i \\log \\sum_{z_i}{p(x_i, z_i; \\theta)} && \\text{在函数中加入隐变量$z$} \\\\\n&= \\sum_i \\log \\sum_{z_i} Q_i(z_i) \\frac{p(x_i, z_i; \\theta)}{Q_i(z_i)} && \\text{贝叶斯定理得$Q_i$为$z_i$分布} \\\\\n&= \\sum_i \\log E_{z_i}[\\frac{p(x_i, z_i; \\theta)}{Q_i(z_i)}] && \\text{得到期望 - 就是EM中的E} \\\\\n&\\geq \\sum E_{z_i}[\\log \\frac{p(x_i, z_i; \\theta)}{Q_i(z_i)}] && \\text{使用詹森不等式计算凹的对数似然函数} \\\\\n&\\geq \\sum_i \\sum_{z_i} Q_i(z_i) \\log \\frac{p(x_i, z_i; \\theta)}{Q_i(z_i)} && \\text{得到期望的定义}\n\\end{align}\n\\]\n我们如何确定分布\\(\\boldsymbol{Q_i}\\)？我们想要\\(\\boldsymbol{Q}\\)函数与对数似然函数进行接触，并且也知道了詹森不等式相等的充要条件就是这个函数为常数，因此：\n\\[\n\\begin{align}\n\\frac{p(x_i, z_i; \\theta)}{Q_i(z_i)} =& c \\\\\n\\implies Q_i(z_i) &\\propto p(x_i, z_i; \\theta)\\\\\n\\implies Q_i(z_i) &= \\frac{p(x_i, z_i; \\theta) }{\\sum_{z_i}{p(x_i, z_i; \\theta) } } &&\\text{因为 $\\boldsymbol{Q}$ 是个分布且和为1} \\\\\n\\implies Q_i(z_i) &= \\frac{p(x_i, z_i; \\theta) }{ {p(x_i, \\theta) } } && \\text{边缘化 $z_i$}\\\\\n\\implies Q_i(z_i) &= p(z_i | x_i; \\theta) && \\text{根据条件概率定义}\n\\end{align}\n\\]\n因此得到\\(\\boldsymbol{Q_i}\\)就是\\(z_i\\)的后验概率，这就完成了E-step。\n在M-step中，我找到最大化对数似然函数下界时的\\(\\theta\\)值，然后我们迭代E和M即可。\n所以EM算法是在缺少信息的情况下最大化似然的优化算法，或者说他可以很方便的添加隐变量来简化最大似然计算。\n\n\n例子\n现在如果我们忘记记录了投掷硬币的顺序，那我们来求解一波A、B硬币向上的概率。 对于E-step,我们：\n\\[\n\\begin{align}\nw_j &= Q_i(z_i = j) \\\\\n&= p(z_i = j \\mid x_i; \\theta) \\\\\n&= \\frac{p(x_i \\mid z_i = j; \\theta) p(z_i = j; \\phi)}  {\\sum_{l=1}^k{p(x_i \\mid z_i = l; \\theta) p(z_i = l; \\phi) } }  && \\text{贝叶斯准则} \\\\\n&= \\frac{\\theta_j^h(1-\\theta_j)^{n-h} \\phi_j}{\\sum_{l=1}^k \\theta_l^h(1-\\theta_l)^{n-h} \\phi_l} && \\text{代入二项分布公式} \\\\\n&= \\frac{\\theta_j^h(1-\\theta_j)^{n-h} }{\\sum_{l=1}^k \\theta_l^h(1-\\theta_l)^{n-h} } && \\text{为了简单起见使 $\\phi$ 为常数}\n\\end{align}\n\\]\n对于M-step,我们需要找到最大时的\\(\\theta\\)值。\n\\[\n\\begin{align}\n& \\sum_i \\sum_{z_i} Q_i(z_i) \\log \\frac{p(x_i, z_i; \\theta)}{Q_i(z_i)} \\\\\n&= \\sum_{i=1}^m \\sum_{j=1}^k w_j \\log \\frac{p(x_i \\mid z_i=j; \\theta) \\, p(z_i = j; \\phi)}{w_j} \\\\\n&= \\sum_{i=1}^m \\sum_{j=1}^k w_j \\log \\frac{\\theta_j^h(1-\\theta_j)^{n-h} \\phi_j}{w_j} \\\\\n&= \\sum_{i=1}^m \\sum_{j=1}^k w_j \\left( h \\log \\theta_j + (n-h) \\log (1-\\theta_j) + \\log \\phi_j - \\log w_j \\right)\n\\end{align}\n\\]\n我们使用区间的方式解决\\(\\theta_s\\)导数消失的问题： \\[\n\\begin{align}\n\\sum_{i=1}^m w_s \\left( \\frac{h}{\\theta_s} - \\frac{n-h}{1-\\theta_s} \\right) &= 0  \\\\\n\\implies \\theta_s &= \\frac {\\sum_{i=1}^m w_s h}{\\sum_{i=1}^m w_s n}\n\\end{align}\n\\]\n\n\n第一种直接的代码\nxs = np.array([(5, 5), (9, 1), (8, 2), (4, 6), (7, 3)])\nthetas = np.array([[0.6, 0.4], [0.5, 0.5]])  # 初始化参数A B (向上概率，向下概率)\n\ntol = 0.01  # 变化容忍度\nmax_iter = 100  # 迭代次数\n\nll_old = 0\nfor i in range(max_iter):\n    exp_A = []\n    exp_B = []\n\n    ll_new = 0\n\n    # ! E-step: 计算可能的概率分布\n    for x in xs:\n        # 求解当前theta下两个分布的对数似然\n        ll_A = np.sum(x * np.log(thetas[0]))  # 多项式分布的对数似然函数 (忽略常数).\n        ll_B = np.sum(x * np.log(thetas[1]))\n\n        w_A = np.exp(ll_A) / (np.exp(ll_A) + np.exp(ll_B))  # 求出概率A的权重\n        w_B = np.exp(ll_B) / (np.exp(ll_A) + np.exp(ll_B))  # 求出概率B的权重\n\n        exp_A.append(w_A * x)  # 概率A权重乘上样本\n        exp_B.append(w_B * x)  # 概率B权重乘上样本\n\n        ll_new += w_A * ll_A + w_B * ll_B  # 计算当前的theta值对应的似然值\n\n    # ! M-step: 为给定的分布更新当前参数\n    thetas[0] = np.sum(exp_A, 0) / np.sum(exp_A)  # 利用更新之后的样本值计算出当前A的theta值\n    thetas[1] = np.sum(exp_B, 0) / np.sum(exp_B)  # 利用更新之后的样本值计算出当前B的theta值\n\n    # 输出每个x和当前参数估计z的分布\n    print(\"Iteration: %d\" % (i + 1))\n    print(\"theta_A = %.2f, theta_B = %.2f, ll = %.2f\" % (thetas[0, 0], thetas[1, 0], ll_new))\n\n    if np.abs(ll_new - ll_old) &lt; tol:\n        break\n    ll_old = ll_new\n\n\n结果\nIteration: 1\ntheta_A = 0.71, theta_B = 0.58, ll = -32.69\nIteration: 2\ntheta_A = 0.75, theta_B = 0.57, ll = -31.26\nIteration: 3\ntheta_A = 0.77, theta_B = 0.55, ll = -30.76\nIteration: 4\ntheta_A = 0.78, theta_B = 0.53, ll = -30.33\nIteration: 5\ntheta_A = 0.79, theta_B = 0.53, ll = -30.07\nIteration: 6\ntheta_A = 0.79, theta_B = 0.52, ll = -29.95\nIteration: 7\ntheta_A = 0.80, theta_B = 0.52, ll = -29.90\nIteration: 8\ntheta_A = 0.80, theta_B = 0.52, ll = -29.88\nIteration: 9\ntheta_A = 0.80, theta_B = 0.52, ll = -29.87\nNOTE： 他这里的对数似然函数是根据上面的公式推导之后简化而来的，所以直接对\\(\\theta\\)做对数即可，下面我按最基本的思路来写了一个例程。\n\n\n我的代码\nn = 10  # 实验次数\nm_xs = np.array([5, 9, 8, 4, 7])  # 向上次数\ntheta_A = 0.6\ntheta_B = 0.5\n\n\ntol = 0.01  # 变化容忍度\nmax_iter = 100  # 迭代次数\nloglike_old = 0  # 初始对数似然值\nfor i in range(max_iter):\n    cnt_A = []\n    cnt_B = []\n    loglike_new = 0  # 新的对数似然值\n    # ! E-step\n    for x in m_xs:\n        pmf_A = binom(n, theta_A).pmf(x)  # 当前theta下 A的概率\n        pmf_B = binom(n, theta_B).pmf(x)  # 当前theta下 B的概率\n\n        logpmf_A = binom(n, theta_A).logpmf(x)  # 当前theta下 A的对数概率\n        logpmf_B = binom(n, theta_B).logpmf(x)  # 当前theta下 B的对数概率\n\n        weight_A = pmf_A / (pmf_A + pmf_B)  # 求得权重\n        weight_B = pmf_B / (pmf_A + pmf_B)  # 求得权重\n\n        cnt_A.append(weight_A * np.array([x, n - x]))  # 概率A权重乘上样本，得到新的硬币次数统计\n        cnt_B.append(weight_B * np.array([x, n - x]))  # 概率B权重乘上样本，得到新的硬币次数统计\n\n        loglike_new += weight_A * logpmf_A + weight_B * logpmf_B  # 计算当前的theta值对应的似然值\n    # ! M-step\n    theta_A = np.sum(cnt_A, 0)[0] / np.sum(cnt_A)  # 硬币A向上的次数除以总次数即为theta_A\n    theta_B = np.sum(cnt_B, 0)[0] / np.sum(cnt_B)  # 硬币B向上的次数除以总次数即为theta_B\n\n    # 输出每个x和当前参数估计z的分布\n    print(\"Iteration: %d\" % (i + 1))\n    print(\"theta_A = %.2f, theta_B = %.2f, ll = %.2f\" % (theta_A, theta_B, loglike_new))\n\n    if np.abs(loglike_new - loglike_old) &lt; tol:\n        break\n    loglike_old = loglike_new\n\n\n结果\nIteration: 1\ntheta_A = 0.71, theta_B = 0.58, ll = -10.91\nIteration: 2\ntheta_A = 0.75, theta_B = 0.57, ll = -9.49\nIteration: 3\ntheta_A = 0.77, theta_B = 0.55, ll = -8.99\nIteration: 4\ntheta_A = 0.78, theta_B = 0.53, ll = -8.56\nIteration: 5\ntheta_A = 0.79, theta_B = 0.53, ll = -8.30\nIteration: 6\ntheta_A = 0.79, theta_B = 0.52, ll = -8.18\nIteration: 7\ntheta_A = 0.80, theta_B = 0.52, ll = -8.13\nIteration: 8\ntheta_A = 0.80, theta_B = 0.52, ll = -8.11\nIteration: 9\ntheta_A = 0.80, theta_B = 0.52, ll = -8.10\nNOTE： 就是对数似然值和例程计算的不一样。。"
  },
  {
    "objectID": "posts/em-algm.html#混合模型",
    "href": "posts/em-algm.html#混合模型",
    "title": "EM算法与EM路由",
    "section": "混合模型",
    "text": "混合模型\n从一个简单的混合模型开始，即k-means，k-means不使用EM算法，但是可以结合EM算法帮助理解EM算法如何用于高斯混合模型。\n\nK-means\n这个算法比较简单，初始化选择\\(k\\)个中心点，然后做如下：\n\n找到每个点与中心点的距离\n给每个点分配最近的中心点\n根据所分配的点来更新中心点位置\n\n下面给出一段程序示例：\nfrom numpy.core.umath_tests import inner1d\nimport numpy as np\nimport seaborn as sns\n\n\ndef kmeans(xs, k, max_iter=10):\n    \"\"\"K-means 算法.\"\"\"\n    idx = np.random.choice(len(xs), k, replace=False)\n    cs = xs[idx]\n    for n in range(max_iter):\n        ds = np.array([inner1d(xs - c, xs - c) for c in cs])\n        zs = np.argmin(ds, axis=0)\n        cs = np.array([xs[zs == i].mean(axis=0) for i in range(k)])\n    return (cs, zs)\n\n\niris = sns.load_dataset('iris')\ndata = iris.iloc[:, :4].values\ncs, zs = kmeans(data, 3)\niris['cluster'] = zs\nsns.pairplot(iris, hue='cluster', diag_kind='kde', vars=iris.columns[:4])\nplt.show()\n\n\n结果"
  },
  {
    "objectID": "posts/em-algm.html#高斯混合模型",
    "href": "posts/em-algm.html#高斯混合模型",
    "title": "EM算法与EM路由",
    "section": "高斯混合模型",
    "text": "高斯混合模型\n\\(k\\)个高斯分布混合具有以下的概率密度函数： \\[\n\\begin{align}\np(x) = \\sum_{j=1}^k \\pi_j \\phi(x; \\mu_j, \\sigma_j)\n\\end{align}\n\\]\n\\(\\pi_j\\)是第\\(j\\)个高斯分布的权重，且：\n\\[\n\\begin{align}\n\\phi(x; \\mu, \\sigma) = \\frac{1}{(2 \\pi)^{d/2}|\\sigma|^{1/2 } } \\exp \\left( -\\frac{1}{2}(x-\\mu)^T\\sigma^{-1}(x-\\mu) \\right)\n\\end{align}\n\\]\n假设我们观察\\(y_1, y2, \\ldots, y_n\\)作为来自高斯混合模型的样本，那么对数似然为： \\[\n\\begin{align}\nl(\\theta) = \\sum_{i=1}^n \\log \\left( \\sum_{j=1}^k \\pi_j \\phi(y_i; \\mu_j, \\sigma_j) \\right)\n\\end{align}\n\\]\n其中\\(\\theta = (\\pi, \\mu, \\sigma)\\)，很难拟合这种对数似然最大值的参数，因为他们要在对数函数中求和。\n\n使用EM算法\n假设我们增加潜在变量\\(z\\)，它表明我们观察到的\\(y\\)来自于第\\(k\\)个高斯分布。其中E-step和M-step的推导与之前的例子类似，只是变量更多。\n在E-step我们想要计算样本\\(x_i\\)输入第\\(j\\)个类别的后验概率，给定参数\\(\\theta =(\\pi,\\mu,\\sigma)\\)\nNOTE： 这里后验概率有的地方使用公式\\(p(j|x_i)\\),这里使用\\(w_j^i\\)来表示。\n\\[\n\\begin{align}\nw_j^i &= Q_i(z^i = j) \\\\\n&= p(z^i = j \\mid y^i; \\theta) \\\\\n&= \\frac{p(x^i \\mid z^i = j; \\mu, \\sigma) p(z^i = j; \\pi)}  {\\sum_{l=1}^k{p(y^i \\mid z^i = l; \\mu, \\sigma) p(z^i = l; \\pi) } }  && \\text{贝叶斯定理} \\\\\n&= \\frac{\\phi(x^i; \\mu_j, \\sigma_j) \\pi_j}{\\sum_{l=1}^k \\phi(x^i; \\mu_l, \\sigma_l) \\pi_l}\n\\end{align}\n\\]\n在M-step中，我们要找到\\(\\theta = (w, \\mu, \\sigma)\\)来最大化\\(\\boldsymbol{Q}\\)函数，对应与真实对数似然函数的下界。\n\\[\n\\begin{align}\n\\sum_{i=1}^{m}\\sum_{j=1}^{k} Q(z^i=j) \\log \\frac{p(x^i \\mid z^i= j; \\mu, \\sigma) p(z^i=j; \\pi)}{Q(z^i=j)}\n\\end{align}\n\\]\n通过分别取\\((w, \\mu, \\sigma)\\)的导数并求解（使用拉格朗日乘数构造约束\\(\\sum_{j=1}^k w_j = 1\\)来求解），我们得到：\n\\[\n\\begin{align}\n\\pi_j &= \\frac{1}{m} \\sum_{i=1}^{m} w_j^i \\\\\n\\mu_j &= \\frac{\\sum_{i=1}^{m} w_j^i x^i}{\\sum_{i=1}^{m} w_j^i} \\\\\n\\sigma_j &= \\frac{\\sum_{i=1}^{m} w_j^i (x^i - \\mu)(x^i - \\mu)^T}{\\sum_{i1}^{m} w_j^i}\n\\end{align}\n\\]\n\n\n代码\nfrom scipy.stats import multivariate_normal\n\n\ndef normalize(xs, axis=None):\n    \"\"\"Return normalized marirx so that sum of row or column (default) entries = 1.\"\"\"\n    if axis is None:\n        return xs / xs.sum()\n    elif axis == 0:\n        return xs / xs.sum(0)\n    else:\n        return xs / xs.sum(1)[:, None]\n\n\ndef mix_mvn_pdf(xs, pis, mus, sigmas):\n    return np.array([pi * multivariate_normal(mu, sigma).pdf(xs) for (pi, mu, sigma) in zip(pis, mus, sigmas)])\n\n\ndef em_gmm_orig(xs, pis, mus, sigmas, tol=0.01, max_iter=100):\n\n    n, p = xs.shape\n    k = len(pis)\n\n    ll_old = 0\n    for i in range(max_iter):\n        exp_A = []\n        exp_B = []\n        ll_new = 0\n\n        # ！ E-step\n        ws = np.zeros((k, n))\n        for j in range(len(mus)):\n            for i in range(n):\n                # 遍历所有的 mu，sigma，pi 来计算概率密度\n                ws[j, i] = pis[j] * multivariate_normal(mus[j], sigmas[j]).pdf(xs[i])\n        ws /= ws.sum(0)  # 根据概率密度求权值\n\n        # M-step\n        # NOTE 下面的更新过程是根据公式来计算的！\n        pis = np.zeros(k)\n        for j in range(len(mus)):\n            for i in range(n):\n                pis[j] += ws[j, i]  # 使用权值更新pi\n        pis /= n\n\n        mus = np.zeros((k, p))\n        for j in range(k):\n            for i in range(n):\n                mus[j] += ws[j, i] * xs[i]  # 使用权值更新mu\n            mus[j] /= ws[j, :].sum()\n\n        sigmas = np.zeros((k, p, p))\n        for j in range(k):\n            for i in range(n):\n                ys = np.reshape(xs[i] - mus[j], (2, 1))\n                sigmas[j] += ws[j, i] * np.dot(ys, ys.T)  # 使用权值更新sigma\n            sigmas[j] /= ws[j, :].sum()\n\n        # 更新对数似然函数\n        ll_new = 0.0\n        for i in range(n):\n            s = 0\n            for j in range(k):\n                s += pis[j] * multivariate_normal(mus[j], sigmas[j]).pdf(xs[i])\n            ll_new += np.log(s)\n\n        if np.abs(ll_new - ll_old) &lt; tol:\n            break\n        ll_old = ll_new\n\n    return ll_new, pis, mus, sigmas\n\n\n# 构建数据集\nn = 1000\n_mus = np.array([[0, 4], [-2, 0]])\n_sigmas = np.array([[[3, 0], [0, 0.5]], [[1, 0], [0, 2]]])\n_pis = np.array([0.6, 0.4])\nxs = np.concatenate([np.random.multivariate_normal(mu, sigma, int(pi * n))\n                     for pi, mu, sigma in zip(_pis, _mus, _sigmas)])\n\n# 初始化预测值\npis = normalize(np.random.random(2))  # pi 要经过归一化\nmus = np.random.random((2, 2))\nsigmas = np.array([np.eye(2)] * 2)\n\n# 使用EM算法拟合\nll1, pis1, mus1, sigmas1 = em_gmm_orig(xs, pis, mus, sigmas)\n\n# 绘图\nintervals = 101\nys = np.linspace(-8, 8, intervals)\nX, Y = np.meshgrid(ys, ys)\n_ys = np.vstack([X.ravel(), Y.ravel()]).T\n\nz = np.zeros(len(_ys))\nfor pi, mu, sigma in zip(pis1, mus1, sigmas1):\n    z += pi * multivariate_normal(mu, sigma).pdf(_ys)\nz = z.reshape((intervals, intervals))\n\nax = plt.subplot(111)\nplt.scatter(xs[:, 0], xs[:, 1], alpha=0.2)\nplt.contour(X, Y, z, N=10)\nplt.axis([-8, 6, -6, 8])\nax.axes.set_aspect('equal')\nplt.tight_layout()\nplt.show()\n\n\n结果"
  },
  {
    "objectID": "posts/em-algm.html#矩阵胶囊",
    "href": "posts/em-algm.html#矩阵胶囊",
    "title": "EM算法与EM路由",
    "section": "矩阵胶囊",
    "text": "矩阵胶囊\n现在的矩阵胶囊与之前的胶囊不是很一样了，因为胶囊是矩阵，就不能把模长作为激活了。所以多加一个标量\\(a\\)来表示激活值。\n矩阵胶囊的主体，名字也变了一下，叫做姿态矩阵，这里是\\(4\\times4\\)的矩阵。"
  },
  {
    "objectID": "posts/em-algm.html#胶囊投票机制",
    "href": "posts/em-algm.html#胶囊投票机制",
    "title": "EM算法与EM路由",
    "section": "胶囊投票机制",
    "text": "胶囊投票机制\n在胶囊网络中，通过寻找底层胶囊投票之间的一致性来检测一个高层特征(比如一张脸)，对于来自胶囊\\(i\\)对父胶囊\\(j\\)的投票\\(\\boldsymbol{V}_{ij}\\)，通过将胶囊\\(i\\)的姿态矩阵\\(\\boldsymbol{P}_i\\)和视角不变矩阵\\(W_{ij}\\)相乘得到。\n\\[\n\\begin{split}\n&\\boldsymbol{V}_{ij} =\\boldsymbol{P}_i W_{ij} \\quad\n\\end{split}\n\\]\n基于投票\\(\\boldsymbol{V}_{ij}\\)和其他投票\\((\\boldsymbol{V}_{o_{1}j} \\ldots \\boldsymbol{V}_{o_{k}j})\\)相似度来计算胶囊\\(i\\)被分组到胶囊\\(j\\)中的概率。"
  },
  {
    "objectID": "posts/em-algm.html#胶囊分配",
    "href": "posts/em-algm.html#胶囊分配",
    "title": "EM算法与EM路由",
    "section": "胶囊分配",
    "text": "胶囊分配\nEM算法对低层的胶囊进行聚类，也计算底层胶囊与上层胶囊间分配概率\\(r_{ij}\\)。例如一个表示手的胶囊不属于脸的胶囊，他们的分配概率就被抑止到0，\\(r_{ij}\\)也受到胶囊激活的影响。"
  },
  {
    "objectID": "posts/em-algm.html#计算胶囊激活和姿态矩阵",
    "href": "posts/em-algm.html#计算胶囊激活和姿态矩阵",
    "title": "EM算法与EM路由",
    "section": "计算胶囊激活和姿态矩阵",
    "text": "计算胶囊激活和姿态矩阵\n在EM聚类中，我们将数据使用正态分布来表现。在EM路由中，我们也对父胶囊使用正态分布来建模，姿态矩阵\\(M\\)是\\(4\\times4\\)的矩阵，即16个分量。我们对姿态矩阵建立16个\\(\\mu\\),16个\\(\\sigma\\)的高斯分布，每个\\(\\mu\\)就表征了姿态矩阵的每个分量。注意为了避免在概率密度函数中求逆，胶囊网络中的\\(\\sigma\\)是被固定成一个对角矩阵的。\n让\\(v_{ij}\\)作为底层胶囊\\(i\\)对父胶囊\\(j\\)的投票，\\(v^{n}_{ij}\\)代表着第n个分量。我们使用高斯概率密度函数\n\\[\n\\begin{split}\nP(x) & = \\frac{1}{\\sigma \\sqrt{2\\pi } }e^{-(x - \\mu)^{2}/2\\sigma^{2} } \\\\\n\\end{split}\n\\]\n来计算属于胶囊\\(j\\)正态分布的投票\\(v^{n}_{ij}\\)概率。 \\[\n\\begin{split}\np^n_{i \\vert j} & = \\frac{1}{\\sqrt{2 \\pi ({\\sigma^n_j})^2 } } \\exp{(- \\frac{(v^n_{ij}-\\mu^n_j)^2}{2 ({\\sigma^n_j})^2})} \\\\\n\\end{split}\n\\]\n取其对数：\n\\[\n\\begin{split}\n\\ln(p^n_{i \\vert j}) &= \\ln \\frac{1}{\\sqrt{2 \\pi ({\\sigma^n_j})^2 } } \\exp{(- \\frac{(v^n_{ij}-\\mu^n_j)^2}{2 ({\\sigma^n_j})^2})} \\\\\n\\\\\n&= - \\ln(\\sigma^n_j) - \\frac{\\ln(2 \\pi)}{2} - \\frac{(v^n_{ij}-\\mu^n_j)^2}{2 ({\\sigma^n_j})^2}\\\\\n\\end{split}\n\\]\n接下来估计胶囊激活的损失值，越低的损失值，代表胶囊越有可能被激活。如果损失值较大，就代表投票与父胶囊的高斯分布不匹配，因此激活概率较小。\n让\\(cost_{ij}\\)作为胶囊\\(i\\)激活胶囊\\(j\\)的损失，是负的对数似然也可以称之为熵(熵越小，那就意味似然估计越准)： \\[\ncost^n_{ij} = - \\ln(P^n_{i \\vert j})\n\\]\n由于胶囊\\(i\\)与胶囊\\(j\\)之间的联系并不相同，因此我们将在运行时按比例\\(r_{ij}\\)来分配\\(cost\\)，那下层胶囊的\\(cost\\)计算为：\n\\[\n\\begin{split}\ncost^n_j &= \\sum_i  r_{ij} cost^n_{ij} \\\\\n&= \\sum_i - r_{ij} \\ln(p^n_{i \\vert j}) \\\\\n&= \\sum_i r_{ij}  \\big( \\frac{(v^n_{ij}-\\mu^n_j)^2}{2 ({\\sigma^n_j})^2} + \\ln(\\sigma^n_j) + \\frac{\\ln(2 \\pi)}{2} \\big)\\\\\n&= \\frac{\\sum_i r_{ij} (\\sigma^n_j)^2}{2 (\\sigma^n_j)^2} + (\\ln(\\sigma^n_j) + \\frac{\\ln(2 \\pi)}{2}) \\sum_i r_{ij} \\\\\n&= \\big(\\ln(\\sigma^n_j) + \\beta_v \\big) \\sum_i r_{ij}  \\quad \\beta_v\\text{是待优化参数}\n\\end{split}\n\\]\n有了\\(cost\\)值，使用\\(a_j\\)来决定胶囊\\(j\\)是否被激活： \\[\na_j = sigmoid(\\lambda(\\beta_a - \\sum_h cost^n_j))\n\\]\n这里面的\\(\\beta_a，\\beta_v\\)是根据反向传播进行优化的，所以并不需要直接计算。\n其中的\\(r_{ij},\\mu,\\sigma,a_j\\)是根据EM路由来计算的。\\(\\lambda\\)是根据温度参数\\(\\frac{1}{temperature}\\)来计算(退火策略)，随着\\(r_{ij}\\)越来越好，就降低temperature以获得更大的\\(\\lambda\\)，也就是增加\\(sigmoid\\)的陡度，这样就更加在更加小的范围内微调\\(r_{ij}\\)。"
  },
  {
    "objectID": "posts/em-algm.html#em路由",
    "href": "posts/em-algm.html#em路由",
    "title": "EM算法与EM路由",
    "section": "EM路由",
    "text": "EM路由\n这一块是重点，所以我要仔细看每个公式细节。并且要一步一步的渐进的看。\n\n新动态路由1\n首先用一个矩阵\\(\\boldsymbol{P}_{i}， i=1,\\ldots,n\\)来表示第\\(l\\)层的胶囊，用矩阵\\(\\boldsymbol{M}_j， j=1,\\ldots,k\\)来表示第\\(l+1\\)层的胶囊。在做EM路由的过程中，他将\\(P_i\\)变成长16的向量，且协方差矩阵为对角阵。\n\\[\n\\begin{aligned}\n    &p_{ij} \\leftarrow N(\\boldsymbol{P}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\sigma}^2_j) && \\text{计算概率密度} \\\\  \n    &R_{ij} \\leftarrow \\frac{\\pi_j p_{ij} }{\\sum\\limits_{j=1}^k\\pi_j p_{ij} } && \\text{计算后验概率} \\\\\n    &r_{ij}\\leftarrow \\frac{R_{ij } }{\\sum\\limits_{i=1}^n R_{ij } } && \\text{计算归一化后验概率} \\\\\n    &\\boldsymbol{M}_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}\\boldsymbol{P}_i && \\text{计算新样本} \\\\\n    &\\boldsymbol{\\sigma}^2_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}(\\boldsymbol{P}_i-\\boldsymbol{M}_j)^2 && \\text{更新}\\sigma \\\\\n    &\\pi_j \\leftarrow \\frac{1}{n}\\sum\\limits_{i=1}^n R_{ij} && \\text{更新聚类中心}\\pi\n\\end{aligned}\n\\]\n这里的\\(R_{ij}\\)就是后验概率，也就是在EM算法中使用的\\(w_j\\)。\n\n\n新动态路由2\n前面讲道理有一个激活标量\\(a_j\\)来衡量胶囊单元的显著程度，根据EM算法计算我们可以得到\\(\\pi_j\\)为第\\(l+1\\)层胶囊的聚类中心，但我们不能选择\\(\\pi\\)因为：\n\n\\(\\pi_j\\)是归一化的，就是我们只想得到他的显著程度，而不是显著程度的概率。\n\\(\\pi_j\\)不能反映出类内的元素特征是否相似。\n\n现在再看前面所使用的激活标量\\(a_j\\)，他既考虑了似然估计值\\(cost_j^h\\),又考虑了显著程度\\(\\pi_j\\)。所以作者直接将\\(a_j\\)替换了\\(\\pi_j\\)，这样虽然不完全与原始EM算法相同，但是也能收敛，得到新的动态路由：\n\\[\n\\begin{aligned}\n    &p_{ij} \\leftarrow N(\\boldsymbol{P}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\sigma}^2_j) && \\text{计算概率密度} \\\\  \n    &R_{ij} \\leftarrow \\frac{a_j p_{ij} }{\\sum\\limits_{j=1}^k a_j p_{ij} } && \\text{计算后验概率} \\\\\n    &r_{ij}\\leftarrow \\frac{R_{ij } }{\\sum\\limits_{i=1}^n R_{ij } } && \\text{归一化后验概率} \\\\\n    &\\boldsymbol{M}_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}\\boldsymbol{P}_i && \\text{计算新样本} \\\\\n    &\\boldsymbol{\\sigma}^2_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}(\\boldsymbol{P}_i-\\boldsymbol{M}_j)^2 && \\text{更新}\\sigma \\\\\n   & cost_j \\leftarrow \\left(\\beta_v+\\sum\\limits_{l=1}^d \\ln \\boldsymbol{\\sigma}_j^l \\right)\\sum\\limits_i r_{ij} && \\text{计算熵} \\\\\n   & a_j \\leftarrow sigmoid(\\lambda(\\beta_a - \\sum_h cost^h_j)) && \\text{更新}a_j\n\\end{aligned}\n\\]\n\n\n新动态路由3\n上面好像没什么问题。但是没有使用底层胶囊的激活值\\(a^{last}_i\\)，作者在计算归一化后验概率\\(r_{ij}\\leftarrow \\frac{R_{ij } }{\\sum\\limits_{i=1}^n R_{ij } }\\)的时候加入：\n\\[\n\\begin{aligned}\n    &p_{ij} \\leftarrow N(\\boldsymbol{P}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\sigma}^2_j) && \\text{计算概率密度} \\\\  \n    &R_{ij} \\leftarrow \\frac{a_j p_{ij} }{\\sum\\limits_{j=1}^k a_j p_{ij} } && \\text{计算后验概率} \\\\\n    &r_{ij}\\leftarrow \\frac{a_i^{last}R_{ij } }{\\sum\\limits_{i=1}^n a_i^{last} R_{ij } } && \\text{计算归一化后验概率} \\\\\n    &\\boldsymbol{M}_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}\\boldsymbol{P}_i && \\text{计算新样本} \\\\\n    &\\boldsymbol{\\sigma}^2_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}(\\boldsymbol{P}_i-\\boldsymbol{M}_j)^2 && \\text{更新}\\sigma \\\\\n   & cost_j \\leftarrow \\left(\\beta_v+\\sum\\limits_{l=1}^d \\ln \\boldsymbol{\\sigma}_j^l \\right)\\sum\\limits_i r_{ij} && \\text{计算熵} \\\\\n   & a_j \\leftarrow sigmoid(\\lambda(\\beta_a - \\sum_h cost^h_j)) && \\text{更新}a_j\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/em-algm.html#新动态路由4",
    "href": "posts/em-algm.html#新动态路由4",
    "title": "EM算法与EM路由",
    "section": "新动态路由4",
    "text": "新动态路由4\n因为\\(\\sum\\limits_i r_{ij}=1\\)，所以下面还可以化简。\n\\[\n\\begin{aligned}\n    &p_{ij} \\leftarrow N(\\boldsymbol{P}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\sigma}^2_j) && \\text{计算概率密度} \\\\  \n    &R_{ij} \\leftarrow \\frac{a_j p_{ij} }{\\sum\\limits_{j=1}^k a_j p_{ij} } && \\text{计算后验概率} \\\\\n    &r_{ij}\\leftarrow \\frac{a_i^{last}R_{ij } }{\\sum\\limits_{i=1}^n a_i^{last} R_{ij } } && \\text{计算归一化后验概率} \\\\\n    &\\boldsymbol{M}_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}\\boldsymbol{P}_i && \\text{计算新样本} \\\\\n    &\\boldsymbol{\\sigma}^2_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}(\\boldsymbol{P}_i-\\boldsymbol{M}_j)^2 && \\text{更新}\\sigma \\\\\n   & cost_j \\leftarrow \\left(\\beta_v+\\sum\\limits_{l=1}^d \\ln \\boldsymbol{\\sigma}_j^l \\right)\\sum\\limits_i a_i^{last} R_{ij} && \\text{计算熵} \\\\\n   & a_j \\leftarrow sigmoid(\\lambda(\\beta_a - \\sum_h cost^h_j)) && \\text{更新}a_j\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/em-algm.html#最终的动态路由",
    "href": "posts/em-algm.html#最终的动态路由",
    "title": "EM算法与EM路由",
    "section": "最终的动态路由",
    "text": "最终的动态路由\n现在再把投票机制与视角不变矩阵加入进来,投票矩阵与上面描述一样\\(\\boldsymbol{V}_{ij}=\\boldsymbol{P}_{i}\\boldsymbol{W}_{ij}\\)。\n\\[\n\\begin{aligned}\n    &p_{ij} \\leftarrow N(\\boldsymbol{V}_{ij};\\boldsymbol{\\mu}_j,\\boldsymbol{\\sigma}^2_j) && \\text{计算概率密度} \\\\  \n    &R_{ij} \\leftarrow \\frac{a_j p_{ij} }{\\sum\\limits_{j=1}^k a_j p_{ij} } && \\text{计算后验概率} \\\\\n    &r_{ij}\\leftarrow \\frac{a_i^{last}R_{ij } }{\\sum\\limits_{i=1}^n a_i^{last} R_{ij } } && \\text{计算归一化后验概率} \\\\\n    &\\boldsymbol{M}_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}\\boldsymbol{V}_{ij} && \\text{计算新样本} \\\\\n    &\\boldsymbol{\\sigma}^2_j \\leftarrow \\sum\\limits_{i=1}^n r_{ij}(\\boldsymbol{V}_{ij}-\\boldsymbol{M}_j)^2 && \\text{更新}\\sigma \\\\\n   & cost_j \\leftarrow \\left(\\beta_v+\\sum\\limits_{l=1}^d \\ln \\boldsymbol{\\sigma}_j^l \\right)\\sum\\limits_i a_i^{last} R_{ij} && \\text{计算熵} \\\\\n   & a_j \\leftarrow sigmoid(\\lambda(\\beta_a - \\sum_h cost^h_j)) && \\text{更新}a_j\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/em-algm.html#代码-1",
    "href": "posts/em-algm.html#代码-1",
    "title": "EM算法与EM路由",
    "section": "代码",
    "text": "代码\n\n论文中的伪代码\n官方的EM路由的顺序是和之前使用的EM算法不一样的，他先做M-step再做E-step，因为一开始我们是不知道父胶囊的分布，所以我们没有办法通过概率密度函数来计算后验分布，他这里比较简单粗暴，直接先把后验概率\\(R_{ij}\\)分配为1，再来算分配权重矩阵\\(r_{ij}\\)。 \n在M-step中计算出父胶囊正态分布的\\(\\mu,\\sigma\\)。然后计算熵\\(cost\\)，再计算激活。\nNOTE： 在论文或者tensorflow里面，\\(log\\)默认都是以\\(e\\)为底的。并且这里的h就是我上面用的n\n\n在E-step中，我们有了之前的父胶囊的\\(\\mu,\\sigma\\)，那我们根据正态分布公式计算概率密度\\(p_{ij}\\)，然后贝叶斯公式更新后验概率\\(R_{ij}\\)。这样一个循环就形成了。 \n\n\npython实现\n使用Tensorflow 1.14直接运行即可。主要就是看EM算法的实现部分，当然这里在计算\\(a_j\\)的时候，用的是迂回的方式来计算，避免值溢出。我再尝试过这个算法之后，感觉真的不咋地，思路很高深，但是效果没有那些简单粗暴的算法好，我还是喜欢谷歌提出一些网络，虽然没那么多数学原理，但是很直接有效。不过这个笔记对我拿来写论文还是很有帮助的。\nimport numpy as np\nimport tensorflow.python as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.keras.datasets import mnist\nfrom tensorflow.contrib.layers import xavier_initializer\n\nepsilon = 1e-9\n\n\ndef conv2d(inputs, kernel, out_channels, stride, padding, name, is_train=True, activation_fn=None):\n    with slim.arg_scope([slim.conv2d], trainable=is_train):\n        with tf.variable_scope(name) as scope:\n            output = slim.conv2d(inputs,\n                                 num_outputs=out_channels,\n                                 kernel_size=[kernel, kernel], stride=stride, padding=padding,\n                                 scope=scope, activation_fn=activation_fn)\n            tf.logging.info(f\"{name} output shape: {output.get_shape()}\")\n\n    return output\n\n\ndef primary_caps(inputs, kernel_size, out_capsules, stride, padding, pose_shape, name):\n    \"\"\"This constructs a primary capsule layer using regular convolution layer.\n\n    :param inputs: shape (N, H, W, C) (?, 14, 14, 32)\n    :param kernel_size: Apply a filter of [kernel, kernel] [5x5]\n    :param out_capsules: # of output capsule (32)\n    :param stride: 1, 2, or ... (1)\n    :param padding: padding: SAME or VALID.\n    :param pose_shape: (4, 4)\n    :param name: scope name\n\n    :return: (P, a), (P (?, 14, 14, 32, 4, 4), a (?, 14, 14, 32))\n    \"\"\"\n\n    with tf.variable_scope(name) as scope:\n        # Generate the P matrics for the 32 output capsules\n        P = conv2d(\n            inputs,\n            kernel_size,\n            out_capsules * pose_shape[0] * pose_shape[1],\n            stride,\n            padding=padding,\n            name='pose_stacked'\n        )\n\n        input_shape = inputs.get_shape()\n\n        # Reshape 16 scalar values into a 4x4 matrix\n        P = tf.reshape(\n            P, shape=[-1, input_shape[-3], input_shape[-2], out_capsules, pose_shape[0], pose_shape[1]],\n            name='P'\n        )\n\n        # Generate the activation for the 32 output capsules\n        a = conv2d(\n            inputs,\n            kernel_size,\n            out_capsules,\n            stride,\n            padding=padding,\n            activation_fn=tf.sigmoid,\n            name='activation'\n        )\n\n        tf.summary.histogram(\n            'a', a\n        )\n\n    # P (?, 14, 14, 32, 4, 4), a (?, 14, 14, 32)\n    return P, a\n\n\ndef kernel_tile(input, kernel, stride):\n    \"\"\"This constructs a primary capsule layer using regular convolution layer.\n\n    :param inputs: shape (?, 14, 14, 32, 4, 4)\n    :param kernel: 3\n    :param stride: 2\n\n    :return output: (?, 5, 5, 3x3=9, 136)\n    \"\"\"\n\n    # (?, 14, 14, 32x(16)=512)\n    input_shape = input.get_shape()\n    size = input_shape[4] * input_shape[5] if len(input_shape) &gt; 5 else 1\n    input = tf.reshape(input, shape=[-1, input_shape[1], input_shape[2], input_shape[3] * size])\n\n    input_shape = input.get_shape()\n    # (3, 3, 512, 9)\n    tile_filter = np.zeros(shape=[kernel, kernel, input_shape[3],\n                                  kernel * kernel], dtype=np.float32)\n    for i in range(kernel):\n        for j in range(kernel):\n            tile_filter[i, j, :, i * kernel + j] = 1.0  # (3, 3, 512, 9)\n\n    # (3, 3, 512, 9)\n    tile_filter_op = tf.constant(tile_filter, dtype=tf.float32)\n\n    # (?, 6, 6, 4608)\n    output = tf.nn.depthwise_conv2d(input, tile_filter_op, strides=[\n                                    1, stride, stride, 1], padding='VALID')\n\n    output_shape = output.get_shape()\n    output = tf.reshape(output, shape=[-1, output_shape[1], output_shape[2], input_shape[3], kernel * kernel])\n    output = tf.transpose(output, perm=[0, 1, 2, 4, 3])\n\n    # (?, 6, 6, 9, 512)\n    return output\n\n\n# import tensorflow.python as tf\n\n\ndef mat_transform(inputs, output_cap_size, size):\n    \"\"\"Compute the vote.\n\n    :param inputs: shape (size, 288, 16)\n    :param output_cap_size: 32\n\n    :return V: (24, 5, 5, 3x3=9, 136)\n    \"\"\"\n\n    caps_num_i = int(inputs.get_shape()[1])  # 288\n    P = tf.reshape(inputs, shape=[size, caps_num_i, 1, 4, 4])  # (size, 288, 1, 4, 4)\n\n    W = slim.variable('W', shape=[1, caps_num_i, output_cap_size, 4, 4], dtype=tf.float32,\n                      initializer=tf.truncated_normal_initializer(mean=0.0, stddev=1.0))  # (1, 288, 32, 4, 4)\n    W = tf.tile(W, [size, 1, 1, 1, 1])  # (24, 288, 32, 4, 4)\n\n    P = tf.tile(P, [1, 1, output_cap_size, 1, 1])  # (size, 288, 32, 4, 4)\n\n    V = tf.matmul(P, W)  # (24, 288, 32, 4, 4)\n    V = tf.reshape(V, [size, caps_num_i, output_cap_size, 16])  # (size, 288, 32, 16)\n\n    return V\n\n\ndef coord_addition(V, H, W):\n    \"\"\"Coordinate addition.\n\n    :param V: (24, 4, 4, 32, 10, 16)\n    :param H, W: spaital height and width 4\n\n    :return V: (24, 4, 4, 32, 10, 16)\n    \"\"\"\n    coordinate_offset_hh = tf.reshape(\n        (tf.range(H, dtype=tf.float32) + 0.50) / H, [1, H, 1, 1, 1]\n    )\n    coordinate_offset_h0 = tf.constant(\n        0.0, shape=[1, H, 1, 1, 1], dtype=tf.float32\n    )\n    coordinate_offset_h = tf.stack(\n        [coordinate_offset_hh, coordinate_offset_h0] + [coordinate_offset_h0 for _ in range(14)], axis=-1\n    )  # (1, 4, 1, 1, 1, 16)\n\n    coordinate_offset_ww = tf.reshape(\n        (tf.range(W, dtype=tf.float32) + 0.50) / W, [1, 1, W, 1, 1]\n    )\n    coordinate_offset_w0 = tf.constant(\n        0.0, shape=[1, 1, W, 1, 1], dtype=tf.float32\n    )\n    coordinate_offset_w = tf.stack(\n        [coordinate_offset_w0, coordinate_offset_ww] + [coordinate_offset_w0 for _ in range(14)], axis=-1\n    )  # (1, 1, 4, 1, 1, 16)\n\n    # (24, 4, 4, 32, 10, 16)\n    V = V + coordinate_offset_h + coordinate_offset_w\n\n    return V\n\n\ndef conv_capsule(inputs, shape, strides, iterations, batch_size, name):\n    \"\"\" This constructs a convolution capsule layer from a primary or convolution capsule layer.\n        i: input capsules (32)\n        o: output capsules (32)\n        batch size: 24\n        spatial dimension: 14x14\n        kernel: 3x3\n    :param inputs: a primary or convolution capsule layer with poses and a_j\n           pose: (24, 14, 14, 32, 4, 4)\n           activation: (24, 14, 14, 32)\n    :param shape: the shape of convolution operation kernel, [kh, kw, i, o] = (3, 3, 32, 32)\n    :param strides: often [1, 2, 2, 1] (stride 2), or [1, 1, 1, 1] (stride 1).\n    :param iterations: number of iterations in EM routing. 3\n    :param name: name.\n\n    :return: (poses, a_j).\n\n    \"\"\"\n    P, a_last = inputs\n\n    with tf.variable_scope(name) as scope:\n\n        stride = strides[1]  # 2\n        i_size = shape[-2]  # 32\n        o_size = shape[-1]  # 32\n        pose_size = P.get_shape()[-1]  # 4\n\n        # Tile the input capusles' pose matrices to the spatial dimension of the output capsules\n        # Such that we can later multiple with the transformation matrices to generate the V.\n        P = kernel_tile(P, 3, stride)  # (?, 14, 14, 32, 4, 4) -&gt; (?, 6, 6, 3x3=9, 32x16=512)\n\n        # Tile the a_j needed for the EM routing\n        a_last = kernel_tile(a_last, 3, stride)  # (?, 14, 14, 32) -&gt; (?, 6, 6, 9, 32)\n        spatial_size = int(a_last.get_shape()[1])  # 6\n\n        # Reshape it for later operations\n        P = tf.reshape(P, shape=[-1, 3 * 3 * i_size, 16])  # (?, 9x32=288, 16)\n        a_last = tf.reshape(a_last, shape=[-1, spatial_size, spatial_size, 3 * 3 * i_size])  # (?, 6, 6, 9x32=288)\n\n        with tf.variable_scope('V') as scope:\n\n            # Generate the V by multiply it with the transformation matrices\n            V = mat_transform(P, o_size, size=batch_size * spatial_size * spatial_size)  # (864, 288, 32, 16)\n\n            # Reshape the vote for EM routing\n            V_shape = V.get_shape()\n            V = tf.reshape(\n                V,\n                shape=[batch_size, spatial_size,\n                       spatial_size, V_shape[-3],\n                       V_shape[-2], V_shape[-1]])  # (24, 6, 6, 288, 32, 16)\n            tf.logging.info(f\"{name} V shape: {V.get_shape()}\")\n\n        with tf.variable_scope('routing') as scope:\n\n            # beta_v and beta_a one for each output capsule: (1, 1, 1, 32)\n            beta_v = tf.get_variable(\n                name='beta_v', shape=[1, 1, 1, o_size], dtype=tf.float32,\n                initializer=xavier_initializer()\n            )\n            beta_a = tf.get_variable(\n                name='beta_a', shape=[1, 1, 1, o_size], dtype=tf.float32,\n                initializer=xavier_initializer()\n            )\n\n            # Use EM routing to compute the pose and activation\n            # V (24, 6, 6, 3x3x32=288, 32, 16), a_last (?, 6, 6, 288)\n            # poses (24, 6, 6, 32, 16), activation (24, 6, 6, 32)\n            M, a_j = matrix_capsules_em_routing(\n                V, a_last, beta_v, beta_a, iterations, name='em_routing'\n            )\n\n            # Reshape it back to 4x4 pose matrix\n            poses_shape = M.get_shape()\n            # (24, 6, 6, 32, 4, 4)\n            M = tf.reshape(\n                M, [\n                    poses_shape[0], poses_shape[1], poses_shape[2], poses_shape[3], pose_size, pose_size\n                ]\n            )\n\n        tf.logging.info(f\"{name} pose shape: {M.get_shape()}\")\n        tf.logging.info(f\"{name} a_j shape: {a_j.get_shape()}\")\n\n        return M, a_j\n\n\ndef class_capsules(inputs, num_classes, iterations, batch_size, name):\n    \"\"\"\n    :param inputs: ((24, 4, 4, 32, 4, 4), (24, 4, 4, 32))\n    :param num_classes: 10\n    :param iterations: 3\n    :param batch_size: 24\n    :param name:\n    :return poses, a_j: poses (24, 10, 4, 4), activation (24, 10).\n    \"\"\"\n\n    P, a_last = inputs  # (24, 4, 4, 32, 4, 4), (24, 4, 4, 32)\n\n    P_shape = P.get_shape()\n    spatial_size = int(P_shape[1])  # 4\n    pose_size = int(P_shape[-1])    # 4\n    i_size = int(P_shape[3])        # 32\n\n    # P (24*4*4=384, 32, 16)\n    P = tf.reshape(P, shape=[batch_size * spatial_size * spatial_size, P_shape[-3], P_shape[-2] * P_shape[-2]])\n\n    with tf.variable_scope(name) as scope:\n        with tf.variable_scope('V') as scope:\n            # P (384, 32, 16)\n            # V: (384, 32, 10, 16)\n            V = mat_transform(P, num_classes, size=batch_size * spatial_size * spatial_size)\n            tf.logging.info(f\"{name} V shape: {V.get_shape()}\")\n\n            # V (24, 4, 4, 32, 10, 16)\n            V = tf.reshape(V, shape=[batch_size, spatial_size, spatial_size, i_size, num_classes, pose_size * pose_size])\n\n            # (24, 4, 4, 32, 10, 16)\n            V = coord_addition(V, spatial_size, spatial_size)\n\n            tf.logging.info(f\"{name} V shape with coord addition: {V.get_shape()}\")\n\n        with tf.variable_scope('routing') as scope:\n            # beta_v and beta_a one for each output capsule: (1, 10)\n            beta_v = tf.get_variable(\n                name='beta_v', shape=[1, num_classes], dtype=tf.float32,\n                initializer=xavier_initializer()\n            )\n            beta_a = tf.get_variable(\n                name='beta_a', shape=[1, num_classes], dtype=tf.float32,\n                initializer=xavier_initializer()\n            )\n\n            # V (24, 4, 4, 32, 10, 16) -&gt; (24, 512, 10, 16)\n            V_shape = V.get_shape()\n            V = tf.reshape(V, shape=[batch_size, V_shape[1] * V_shape[2] * V_shape[3], V_shape[4], V_shape[5]])\n\n            # a_last (24, 4, 4, 32) -&gt; (24, 512)\n            a_last = tf.reshape(a_last, shape=[batch_size,\n                                               V_shape[1] * V_shape[2] * V_shape[3]])\n\n            # V (24, 512, 10, 16), a_last (24, 512)\n            # poses (24, 10, 16), activation (24, 10)\n            M, a_j = matrix_capsules_em_routing(\n                V, a_last, beta_v, beta_a, iterations, name='em_routing'\n            )\n\n        # M (24, 10, 16) -&gt; (24, 10, 4, 4)\n        M = tf.reshape(M, shape=[batch_size, num_classes, pose_size, pose_size])\n\n        # M (24, 10, 4, 4), activation (24, 10)\n        return M, a_j\n\n\ndef matrix_capsules_em_routing(V, a_last, beta_v, beta_a, iterations, name):\n    \"\"\"The EM routing between input capsules (i) and output capsules (j).\n\n    :param V: (N, OH, OW, kh x kw x i, o, 4 x 4) = (24, 6, 6, 3x3*32=288, 32, 16)\n    :param i_activation: activation from Level L (24, 6, 6, 288)\n    :param beta_v: (1, 1, 1, 32)\n    :param beta_a: (1, 1, 1, 32)\n    :param iterations: number of iterations in EM routing, often 3.\n    :param name: name.\n\n    :return: (pose, activation) of output capsules.\n    \"\"\"\n\n    V_shape = V.get_shape().as_list()\n\n    with tf.variable_scope(name) as scope:\n\n        # Match R_ij (routing assignment) shape, a_last shape with V shape for broadcasting in EM routing\n        # R_ij 就是后验概率\n        # R_ij: [3x3x32=288, 32, 1]\n        # R_ij: routing matrix from each input capsule (i) to each output capsule (o)\n        R_ij = tf.constant(\n            1.0 / V_shape[-2], shape=V_shape[-3:-1] + [1], dtype=tf.float32\n        )\n\n        # a_last: expand_dims to (24, 6, 6, 288, 1, 1)\n        a_last = a_last[..., tf.newaxis, tf.newaxis]\n\n        # beta_v and beta_a: expand_dims to (1, 1, 1, 1, 32, 1]\n        beta_v = beta_v[..., tf.newaxis, :, tf.newaxis]\n        beta_a = beta_a[..., tf.newaxis, :, tf.newaxis]\n\n        def m_step(R_ij, V, a_last, beta_v, beta_a, inverse_temperature):\n            \"\"\"The M-Step in EM Routing from input capsules i to output capsule j.\n            i: input capsules (32)\n            o: output capsules (32)\n            h: 4x4 = 16\n            output spatial dimension: 6x6\n            :param R_ij: routing assignments. shape = (kh x kw x i, o, 1) =(3x3x32, 32, 1) = (288, 32, 1)\n            :param V. shape = (N, OH, OW, kh x kw x i, o, 4x4) = (24, 6, 6, 288, 32, 16)\n            :param a_last: input capsule activation (at Level L). (N, OH, OW, kh x kw x i, 1, 1) = (24, 6, 6, 288, 1, 1)\n               with dimensions expanded to match V for broadcasting.\n            :param beta_v: Trainable parameters in computing cost (1, 1, 1, 1, 32, 1)\n            :param beta_a: Trainable parameters in computing next level activation (1, 1, 1, 1, 32, 1)\n            :param inverse_temperature: lambda, increase over each iteration by the caller.\n\n            :return: (M, sigma, o_activation)\n            \"\"\"\n\n            # 用于计算归一化后验概率的零时变量\n            R_ij_a_last = R_ij * a_last\n            # R_ij_a_last_sum: sum over all input capsule i\n            R_ij_a_last_sum = tf.reduce_sum(R_ij_a_last, axis=-3, keepdims=True, name='R_ij_a_last_sum')\n\n            # 计算聚类中心,也就是新样本\n            M = tf.reduce_sum(R_ij_a_last * V, axis=-3, keepdims=True) / R_ij_a_last_sum\n\n            # 计算输出方差sigma:  sigma (24, 6, 6, 1, 32, 16)\n            sigma = tf.sqrt(\n                tf.reduce_sum(\n                    R_ij_a_last * tf.square(V - M), axis=-3, keepdims=True\n                ) / R_ij_a_last_sum\n            )\n\n            # 计算每component的差异 cost_n: (24, 6, 6, 1, 32, 16)\n            cost_n = (beta_v + tf.log(sigma + epsilon)) * R_ij_a_last_sum\n\n            # cost: (24, 6, 6, 1, 32, 1)\n            # 计算求和得到熵\n            cost = tf.reduce_sum(cost_n, axis=-1, keepdims=True)\n\n            # NOTE 为了数值上的稳定性 计算输出激活值a_j的时候,利用分布之间的差异来算\n            cost_mu = tf.reduce_mean(cost, axis=-2, keepdims=True)\n            # 这是熵的sigma\n            cost_sigma = tf.sqrt(\n                tf.reduce_sum(\n                    tf.square(cost - cost_mu), axis=-2, keepdims=True\n                ) / cost.get_shape().as_list()[-2]\n            )\n\n            # 计算激活值之间的差异性  a_cost = (24, 6, 6, 1, 32, 1)\n            a_cost = beta_a + (cost_mu - cost) / (cost_sigma + epsilon)\n\n            # 归一化激活值 (24, 6, 6, 1, 32, 1)\n            a_j = tf.sigmoid(inverse_temperature * a_cost)\n\n            return M, sigma, a_j\n\n        def e_step(M, sigma, a_j, V):\n            \"\"\"The E-Step in EM Routing.\n\n            :param M: (24, 6, 6, 1, 32, 16)\n            :param sigma: (24, 6, 6, 1, 32, 16)\n            :param a_j: (24, 6, 6, 1, 32, 1)\n            :param V: (24, 6, 6, 288, 32, 16)\n\n            :return: R_ij\n            \"\"\"\n\n            o_p_unit0 = - tf.reduce_sum(tf.square(V - M) / (2 * tf.square(sigma)), axis=-1, keepdims=True)\n            o_p_unit2 = - tf.reduce_sum(tf.log(sigma + epsilon), axis=-1, keepdims=True)\n\n            # 求出概率 p_ij\n            # (24, 6, 6, 1, 32, 16)\n            p_ij = o_p_unit0 + o_p_unit2\n\n            # R_ij: (24, 6, 6, 288, 32, 1)\n            zz = tf.log(a_j + epsilon) + p_ij\n            # 求出后验概率\n            R_ij = tf.nn.softmax(zz, dim=len(zz.get_shape().as_list()) - 2)\n\n            return R_ij\n\n        # inverse_temperature schedule (min, max)\n        it_min = 1.0\n        it_max = min(iterations, 3.0)\n        for it in range(iterations):\n            inverse_temperature = it_min + (it_max - it_min) * it / max(1.0, iterations - 1.0)\n            M, sigma, a_j = m_step(\n                R_ij, V, a_last, beta_v, beta_a, inverse_temperature=inverse_temperature\n            )\n            if it &lt; iterations - 1:\n                R_ij = e_step(\n                    M, sigma, a_j, V\n                )\n\n        # pose: (N, OH, OW, o 4 x 4) via squeeze M (24, 6, 6, 32, 16)\n        M = tf.squeeze(M, axis=-3)\n\n        # activation: (N, OH, OW, o) via squeeze o_activationis [24, 6, 6, 32]\n        a_j = tf.squeeze(a_j, axis=[-3, -1])\n\n    return M, a_j\n\n\ndef capsules_net(inputs, num_classes, iterations, batch_size, name='capsule_em'):\n    \"\"\"Define the Capsule Network model\n    \"\"\"\n\n    with tf.variable_scope(name) as scope:\n        # ReLU Conv1\n        # Images shape (24, 28, 28, 1) -&gt; conv 5x5 filters, 32 output channels, strides 2 with padding, ReLU\n        # nets -&gt; (?, 14, 14, 32)\n        nets = conv2d(\n            inputs,\n            kernel=5, out_channels=32, stride=2, padding='SAME',\n            activation_fn=tf.nn.relu, name='relu_conv1'\n        )\n\n        # PrimaryCaps\n        # (?, 14, 14, 32) -&gt; capsule 1x1 filter, 32 output capsule, strides 1 without padding\n        # nets -&gt; (poses (?, 14, 14, 32, 4, 4), activations (?, 14, 14, 32))\n        nets = primary_caps(\n            nets,\n            kernel_size=1, out_capsules=32, stride=1, padding='VALID',\n            pose_shape=[4, 4], name='primary_caps'\n        )\n\n        # ConvCaps1\n        # (poses, activations) -&gt; conv capsule, 3x3 kernels, strides 2, no padding\n        # nets -&gt; (poses (24, 6, 6, 32, 4, 4), activations (24, 6, 6, 32))\n        nets = conv_capsule(\n            nets, shape=[3, 3, 32, 32], strides=[1, 2, 2, 1], iterations=iterations,\n            batch_size=batch_size, name='conv_caps1'\n        )\n\n        # ConvCaps2\n        # (poses, activations) -&gt; conv capsule, 3x3 kernels, strides 1, no padding\n        # nets -&gt; (poses (24, 4, 4, 32, 4, 4), activations (24, 4, 4, 32))\n        nets = conv_capsule(\n            nets, shape=[3, 3, 32, 32], strides=[1, 1, 1, 1], iterations=iterations,\n            batch_size=batch_size, name='conv_caps2'\n        )\n\n        # Class capsules\n        # (poses, activations) -&gt; 1x1 convolution, 10 output capsules\n        # nets -&gt; (poses (24, 10, 4, 4), activations (24, 10))\n        nets = class_capsules(nets, num_classes, iterations=iterations,\n                              batch_size=batch_size, name='class_capsules')\n\n        # poses (24, 10, 4, 4), activations (24, 10)\n        poses, activations = nets\n\n    return poses, activations\n\n\ndef spread_loss(labels, activations, iterations_per_epoch, global_step, name):\n    \"\"\"Spread loss\n\n    :param labels: (24, 10] in one-hot vector\n    :param activations: [24, 10], activation for each class\n    :param margin: increment from 0.2 to 0.9 during training\n\n    :return: spread loss\n    \"\"\"\n\n    # Margin schedule\n    # Margin increase from 0.2 to 0.9 by an increment of 0.1 for every epoch\n    margin = tf.train.piecewise_constant(\n        tf.cast(global_step, dtype=tf.int32),\n        boundaries=[\n            (iterations_per_epoch * x) for x in range(1, 8)\n        ],\n        values=[\n            x / 10.0 for x in range(2, 10)\n        ]\n    )\n\n    activations_shape = activations.get_shape().as_list()\n\n    with tf.variable_scope(name) as scope:\n        # mask_t, mask_f Tensor (?, 10)\n        mask_t = tf.equal(labels, 1)      # Mask for the true label\n        mask_i = tf.equal(labels, 0)      # Mask for the non-true label\n\n        # Activation for the true label\n        # activations_t (?, 1)\n        activations_t = tf.reshape(tf.boolean_mask(activations, mask_t), shape=(tf.shape(activations)[0], 1))\n\n        # Activation for the other classes\n        # activations_i (?, 9)\n        activations_i = tf.reshape(\n            tf.boolean_mask(activations, mask_i), [tf.shape(activations)[0], activations_shape[1] - 1]\n        )\n\n        l = tf.reduce_sum(tf.square(tf.maximum(0.0, margin - (activations_t - activations_i))))\n        tf.losses.add_loss(l)\n\n        return l\n\n\ndef preprocess_image(image, label):\n    \"\"\" Scale the image value between -1 and 1.\n      :param image: An image in Tensor.\n      :return A scaled image in Tensor.\n    \"\"\"\n\n    image = tf.to_float(image)\n    image = tf.subtract(image, 128.0)\n    image = tf.div(image, 128.0)\n    image = tf.reshape(image, (28, 28, 1))\n    label = tf.one_hot(label, 10)\n    return image, label\n\n\ndef load_batch(batch_size=32) -&gt; [tf.Tensor, tf.Tensor]:\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train))\n               .shuffle(10000, 66)\n               .repeat()\n               .map(preprocess_image)\n               .batch(batch_size))  # type: tf.data.Dataset\n    return dataset._make_one_shot_iterator().get_next()\n\n\ndef main():\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    seed = 66\n    batch_size = 8\n    log_dir = './log/train'\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    np.random.seed(seed)\n    tf.set_random_seed(seed)\n\n    # Slim dataset contains data sources, decoder, reader and other meta-information\n    iterations_per_epoch = 60000 // batch_size  # 60,000/24 = 2500\n\n    # images: Tensor (?, 28, 28, 1)\n    # labels: Tensor (?)\n    next_images, next_labels = load_batch(batch_size)\n\n    images = tf.placeholder_with_default(next_images, (batch_size, 28, 28, 1))\n    labels = tf.placeholder_with_default(next_labels, (batch_size, 10))\n\n    # poses: Tensor(?, 10, 4, 4) activations: (?, 10)\n    poses, activations = capsules_net(images, num_classes=10, iterations=3, batch_size=batch_size, name='capsules_em')\n\n    global_step = tf.train.get_or_create_global_step()\n    loss = spread_loss(labels, activations, iterations_per_epoch, global_step, name='spread_loss')\n    tf.summary.scalar('losses/spread_loss', loss)\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train_tensor = slim.learning.create_train_op(loss, optimizer, global_step=global_step, clip_gradient_norm=4.0)\n\n    slim.learning.train(\n        train_tensor,\n        logdir=log_dir,\n        log_every_n_steps=10,\n        save_summaries_secs=60,\n        saver=tf.train.Saver(max_to_keep=2),\n        save_interval_secs=600,\n        session_config=config\n    )\n\n\nif __name__ == '__main__':\n    main()"
  },
  {
    "objectID": "posts/egg-bad-case.html",
    "href": "posts/egg-bad-case.html",
    "title": "Equality Saturation优化在AI编译器中遇到的挑战",
    "section": "",
    "text": "Egg是一个基于EGraph的程序优化框架, 作者在其中实现基于Equality Saturation概念的优化方法, 简单来说就是通过将所有的表达式保存在EGraph这个数据结构中,可以按任意顺序实施RBO(基于规则的优化), 因为其中同时存储了所有可能的表达式, 所以没有传统优化中phase ordering的问题, 最终可通过CostModel提取出最优的图结构.\nEgg在编译优化方面已经有许多应用了, 比如王润基大佬写的SQL 优化器, 其中也详细解释了Egg的使用, 不了解的朋友可以参考一下.\n在端侧AI编译中,每个阶段都需要大量的优化与trade-off, 比如中端的计算图优化与后端的算子Fusion以及后端算子的量化类型(平衡精度/速度), 如果基于传统优化方式, 可能许多模型最优的Pass顺序,算子Fusion方案都需要编译器工程师手动调试与指定. 这主要就是因为传统优化方式一旦lower之后就丢失了之前的信息, 失去了最优的可能性, 因此考虑采用Equality Saturation技术来将中端优化/后端Fusion/Tiling/算子精度选择都放入其中进行整体性优化,希望可以得到尽量优化的编译结果."
  },
  {
    "objectID": "posts/egg-bad-case.html#问题描述",
    "href": "posts/egg-bad-case.html#问题描述",
    "title": "Equality Saturation优化在AI编译器中遇到的挑战",
    "section": "问题描述",
    "text": "问题描述\n不论是中端优化还是后端Fusion, 都会涉及到算子的折叠与合并. 通常无分支的算子的合并, 那么合并后Cost必然减小, 可以自然的选择当前Cost最小的表达式. 但是如果多分支的情况下就会遇到问题.\n假设我们导入的模型有卷积/激活等算子,在Cpu上我们支持的Relu6/Clamp算子,他们的Cost分别为60,70. 后端支持卷积Conv,通用激活Act,以及卷积+通用激活ConvAct, 设他们的Cost分别为100,50,125. 其中执行ConvAct肯定是快于分别执行Conv和Act.\n考虑如下的模型结构:\n\n\n\nmodel structure\n\n\n同时我们的存在这样一个Rule : rw!(\"fold_conv_act\";  \"(act (conv2d ?x))\" =&gt; \"(conv2dAct ?x)\"), 在经过Egg的Runner实施优化后, 得到了这样的结果:\n\n\n\nmodel structure optimized\n\n\n大家可以发现, 虽然我们合并了一个Act, 但是反而多计算了一次Conv, 最终的计算时间增加了."
  },
  {
    "objectID": "posts/egg-bad-case.html#探究原因",
    "href": "posts/egg-bad-case.html#探究原因",
    "title": "Equality Saturation优化在AI编译器中遇到的挑战",
    "section": "探究原因",
    "text": "探究原因\nEgraph中保存了展平的数据结构, 对于每一个Eclass选择其内部最小Cost的ENode来作为它的Cost. 但是因为EGraph中找不到入口点, 所以是反复遍历所有的EClass, 直到每个Eclass不再减小时退出.\n其核心逻辑如下:\n        let mut did_something = true;\n        while did_something {\n            did_something = false;\n\n            for class in self.egraph.classes() {\n                let pass = self.make_pass(class);\n                match (self.costs.get(&class.id), pass) {\n                    (None, Some(new)) =&gt; {\n                        self.costs.insert(class.id, new);\n                        did_something = true;\n                    }\n                    (Some(old), Some(new)) if new.0 &lt; old.0 =&gt; {\n                        self.costs.insert(class.id, new);\n                        did_something = true;\n                    }\n                    _ =&gt; (),\n                }\n            }\n        }\n    .\n    .\n    .\n    fn make_pass(&mut self, eclass: &EClass&lt;L, N::Data&gt;) -&gt; Option&lt;(CF::Cost, L)&gt; {\n        let (cost, node) = eclass\n            .iter()\n            .map(|n| (self.node_total_cost(n), n))\n            .min_by(|a, b| cmp(&a.0, &b.0))\n            .unwrap_or_else(|| panic!(\"Can't extract, eclass is empty: {:#?}\", eclass));\n        cost.map(|c| (c, node.clone()))\n    }\n问题就在于make_pass的时候他无法得到上下文的信息, 如下图所示:\n\n\n\neclass cost selet\n\n\n在蓝色的EClass中它自然会选择当前的conv2dAct节点,因为它是当前Eclass最小Cost的ENode."
  },
  {
    "objectID": "posts/egg-bad-case.html#方案1",
    "href": "posts/egg-bad-case.html#方案1",
    "title": "Equality Saturation优化在AI编译器中遇到的挑战",
    "section": "方案1",
    "text": "方案1\n简单的方案可以在编写rule的时候判断要折叠的算子的user个数,如果是会引起这种现象的情况, 就不进行折叠. 不过这样总觉得和Equality Saturation的思路相悖, 不是一个很完美的做法."
  },
  {
    "objectID": "posts/egg-bad-case.html#方案2",
    "href": "posts/egg-bad-case.html#方案2",
    "title": "Equality Saturation优化在AI编译器中遇到的挑战",
    "section": "方案2",
    "text": "方案2\n需要记录每个ENode可能的Compute Sequence, 如同上图所展示的那样, 比如对于Add节点左边可能存在x -&gt; conv2d -&gt; relu6 -&gt; conv2d, x -&gt; conv2dAct -&gt; conv2d等4种情况,右边则只有x -&gt; conv2d一种情况, 然后消除两边计算序列的交集, 从而算得正确的cost值. 不过这样存储的Compute Sequence在每经过一个EClass时,都是按EClass.Nodes.Count来翻倍的, 需要一种节省内存的数据结构. 同时因为计算Cost的时候是将所有表达式展平之后处理的, 还需要方便的从中间节点进行替换. 总之不是一个容易实现的方案."
  },
  {
    "objectID": "posts/dynamic-shape.html",
    "href": "posts/dynamic-shape.html",
    "title": "tvm dynamic shape 学习",
    "section": "",
    "text": "探究tvm dynamic shape的实现.\n\n\ntvm ir design\n\n将relax ir的语法dump出来可以知道, 这里与relay那种数据流的ir不同, dataflow中的每个操作使用一个var binding来存储.\n@R.function\ndef fn1(a: R.Tensor((\"n\", 10), 'float32'), b: R.Tensor((1,), 'float32')):\n    with R.dataflow():\n        n = T.int64()\n        c: R.Tensor((n, 10)) = a + b\n        R.output(c)\n    return c\nFunction(\n    params=[\n        Var(\n            name_hint=\"a\",\n            struct_info=TensorStructInfo(\n                dtype=float32,\n                shape=ShapeExpr(\n                    values=[\n                        PrimExpr(value=`n`),\n                        PrimExpr(value=`T.int64(10)`)\n                    ],\n                    struct_info=ShapeStructInfo(\n                        ndim=2,\n                        values=[\n                            PrimExpr(value=`n`),\n                            PrimExpr(value=`T.int64(10)`)\n                        ]\n                    )\n                )\n            )\n        ),\n        Var(\n            name_hint=\"b\",\n            struct_info=TensorStructInfo(\n                dtype=float32,\n                shape=ShapeExpr(\n                    values=[PrimExpr(value=`T.int64(1)`)],\n                    struct_info=ShapeStructInfo(\n                        ndim=1,\n                        values=[PrimExpr(value=`T.int64(1)`)]\n                    )\n                )\n            )\n        )\n    ],\n    body=SeqExpr(\n        blocks=[\n            BindingBlock(\n                bindings=[\n                    VarBinding(\n                        var=Var(\n                            name_hint=\"c\",\n                            struct_info=TensorStructInfo(\n                                dtype=,\n                                shape=ShapeExpr(\n                                    values=[\n                                        PrimExpr(value=`n`),\n                                        PrimExpr(value=`T.int64(10)`)\n                                    ],\n                                    struct_info=ShapeStructInfo(\n                                        ndim=2,\n                                        values=[\n                                            PrimExpr(value=`n`),\n                                            PrimExpr(value=`T.int64(10)`)\n                                        ]\n                                    )\n                                )\n                            )\n                        ),\n                        value=Call(\n                            op=Op(name=\"relax.add\"),\n                            args=[\n                                Var(\n                                    name_hint=\"a\",\n                                    struct_info=TensorStructInfo(\n                                        dtype=float32,\n                                        shape=ShapeExpr(\n                                            values=[\n                                                PrimExpr(value=`n`),\n                                                PrimExpr(value=`T.int64(10)`)\n                                            ],\n                                            struct_info=ShapeStructInfo(\n                                                ndim=2,\n                                                values=[\n                                                    PrimExpr(value=`n`),\n                                                    PrimExpr(value=`T.int64(10)`)\n                                                ]\n                                            )\n                                        )\n                                    )\n                                ),\n                                Var(\n                                    name_hint=\"b\",\n                                    struct_info=TensorStructInfo(\n                                        dtype=float32,\n                                        shape=ShapeExpr(\n                                            values=[PrimExpr(value=`T.int64(1)`)],\n                                            struct_info=ShapeStructInfo(\n                                                ndim=1,\n                                                values=[PrimExpr(value=`T.int64(1)`)]\n                                            )\n                                        )\n                                    )\n                                )\n                            ],\n                            struct_info=TensorStructInfo(\n                                dtype=,\n                                shape=ShapeExpr(\n                                    values=[\n                                        PrimExpr(value=`n`),\n                                        PrimExpr(value=`T.int64(10)`)\n                                    ],\n                                    struct_info=ShapeStructInfo(\n                                        ndim=2,\n                                        values=[\n                                            PrimExpr(value=`n`),\n                                            PrimExpr(value=`T.int64(10)`)\n                                        ]\n                                    )\n                                )\n                            )\n                        )\n                    )\n                ]\n            )\n        ],\n        body=Var(\n            name_hint=\"c\",\n            struct_info=TensorStructInfo(\n                dtype=,\n                shape=ShapeExpr(\n                    values=[\n                        PrimExpr(value=`n`),\n                        PrimExpr(value=`T.int64(10)`)\n                    ],\n                    struct_info=ShapeStructInfo(\n                        ndim=2,\n                        values=[\n                            PrimExpr(value=`n`),\n                            PrimExpr(value=`T.int64(10)`)\n                        ]\n                    )\n                )\n            )\n        ),\n        struct_info=TensorStructInfo(\n            dtype=,\n            shape=ShapeExpr(\n                values=[\n                    PrimExpr(value=`n`),\n                    PrimExpr(value=`T.int64(10)`)\n                ],\n                struct_info=ShapeStructInfo(\n                    ndim=2,\n                    values=[\n                        PrimExpr(value=`n`),\n                        PrimExpr(value=`T.int64(10)`)\n                    ]\n                )\n            )\n        )\n    ),\n    ret_struct_info=TensorStructInfo(\n        dtype=,\n        shape=ShapeExpr(\n            values=[\n                PrimExpr(value=`n`),\n                PrimExpr(value=`T.int64(10)`)\n            ],\n            struct_info=ShapeStructInfo(\n                ndim=2,\n                values=[\n                    PrimExpr(value=`n`),\n                    PrimExpr(value=`T.int64(10)`)\n                ]\n            )\n        )\n    ),\n    is_pure=1,\n    attrs={\"global_symbol\": \"fn1\"},\n    struct_info=FuncStructInfo(\n        params=[\n            TensorStructInfo(\n                dtype=float32,\n                shape=ShapeExpr(\n                    values=[\n                        PrimExpr(value=`n`),\n                        PrimExpr(value=`T.int64(10)`)\n                    ],\n                    struct_info=ShapeStructInfo(\n                        ndim=2,\n                        values=[\n                            PrimExpr(value=`n`),\n                            PrimExpr(value=`T.int64(10)`)\n                        ]\n                    )\n                )\n            ),\n            TensorStructInfo(\n                dtype=float32,\n                shape=ShapeExpr(\n                    values=[PrimExpr(value=`T.int64(1)`)],\n                    struct_info=ShapeStructInfo(\n                        ndim=1,\n                        values=[PrimExpr(value=`T.int64(1)`)]\n                    )\n                )\n            )\n        ],\n        ret=TensorStructInfo(\n            dtype=,\n            shape=ShapeExpr(\n                values=[\n                    PrimExpr(value=`n`),\n                    PrimExpr(value=`T.int64(10)`)\n                ],\n                struct_info=ShapeStructInfo(\n                    ndim=2,\n                    values=[\n                        PrimExpr(value=`n`),\n                        PrimExpr(value=`T.int64(10)`)\n                    ]\n                )\n            )\n        ),\n        purity=True\n    )\n)\n而根据relax shape设计文档下面这种情况应该是无法支持的:\n@R.function\ndef fn2(a: R.Tensor((\"n\", 10), 'float32'), b: R.Tensor((1,), 'float32')):\n    with R.dataflow():\n        n = T.int64()\n        c = a + b\n        cshape: R.Shape() = R.shape_of(c)\n        d = R.reshape(c, [1, cshape[0], cshape[1], 1])\n        R.output(d)\n    return d\n我在思考是不是应该有一种直接基于数据流的方式来添加symbolic shape的信息?"
  },
  {
    "objectID": "posts/dsa-schedule.html",
    "href": "posts/dsa-schedule.html",
    "title": "带宽受限下的DSA后端Compute Schedule",
    "section": "",
    "text": "之前写过一篇带宽受限下的DSA后端优化, 不过主要是针对已经构建好Compute Schedule之后的优化, 今天准备展开讲讲. 从单层卷积到优化计算,再到Layer Fusion,以及后续各种优化,下面将通过一系列的例子来介绍:"
  },
  {
    "objectID": "posts/dsa-schedule.html#尝试将weights-stage到oc循环外",
    "href": "posts/dsa-schedule.html#尝试将weights-stage到oc循环外",
    "title": "带宽受限下的DSA后端Compute Schedule",
    "section": "3.1 尝试将Weights Stage到OC循环外",
    "text": "3.1 尝试将Weights Stage到OC循环外\n这里就是在SRAM中保存所有的weights, 实际上在stage到OC循环外之后, 我们还可以选择在OC循环内逐步的加载weights以进行流水.\ndef demo3_1(imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray):\n  image = TarcedArray(imageArr)\n  weight = TarcedArray(weightArr)\n  output = TarcedArray(outputArr)\n  (B, OC, OH, OW) = outputArr.shape\n  for b in Segments(0, B, 1):\n    with GlobalHierarchy(L2SIZE):\n      reuse = False # 为了简单起见, 添加reuse的参数来避免重复统计load的数据, 其实应该把allocate buffer和load/store的逻辑分离出来.\n      weightTile = weight[Infer.get_w_segment(slice(0, OC))]  # 将weights加载移动到OC循环外 也就是一次加载所有的权重\n      for oc in Segments(0, OC, CORE_OC):\n        for oh in Segments(0, OH, 2):\n          for ow in Segments(0, OW, OW):\n            outputTile = output[b, oc, oh, ow]\n            imageTile = image[Infer.get_input_segment(b, oc, oh, ow), reuse]  # 重用同一份SRAM\n            if not reuse:\n              reuse = True\n            weightSubTile = weightTile[oc]\n            outputTile += TensorCore(imageTile, weightSubTile)\n\n  assert (np.allclose(output._array, targetOutput, atol=1e-5))\n  print(\"demo3-1 total loaded :\", GlobalHierarchy.TotalLoaded)\n  GlobalHierarchy.Reset()\n\ndemo3-1 We can't move load weights statement out of OC!\n不过实际情况会发现因SRAM空间不够而出错. 这就是SRAM大小影响compute schedule."
  },
  {
    "objectID": "posts/dsa-schedule.html#尝试将weights-stage到oc循环内",
    "href": "posts/dsa-schedule.html#尝试将weights-stage到oc循环内",
    "title": "带宽受限下的DSA后端Compute Schedule",
    "section": "3.2 尝试将Weights Stage到OC循环内",
    "text": "3.2 尝试将Weights Stage到OC循环内\n把weights stage在OC循环内部, 这样可以在OH/OW循环中复用.\ndef demo3_2(imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray):\n  image = TarcedArray(imageArr)\n  weight = TarcedArray(weightArr)\n  output = TarcedArray(outputArr)\n  (B, OC, OH, OW) = outputArr.shape\n  for b in Segments(0, B, 1):\n    for oc in Segments(0, OC, CORE_OC):\n      with GlobalHierarchy(L2SIZE):\n        reuse = False\n        weightTile = weight[Infer.get_w_segment(oc)]\n        for oh in Segments(0, OH, 2):\n          for ow in Segments(0, OW, OW):\n            outputTile = output[b, oc, oh, ow]\n            imageTile = image[Infer.get_input_segment(b, oc, oh, ow), reuse]  # 重用同一份SRAM\n            if not reuse:\n              reuse = True\n            outputTile += TensorCore(imageTile, weightTile)\n\n  assert (np.allclose(output._array, targetOutput, atol=1e-5))\n  print(\"demo3-2 total loaded :\", GlobalHierarchy.TotalLoaded)\n  GlobalHierarchy.Reset()\n\ndemo3-2 total loaded : 9586432\n\n虽然这个例子和demo3-1实际上差别不大, 但是主要是用于说明SRAM对于不同的Compute Schedule的限制."
  },
  {
    "objectID": "posts/dsa-schedule.html#尝试新的切分维度",
    "href": "posts/dsa-schedule.html#尝试新的切分维度",
    "title": "带宽受限下的DSA后端Compute Schedule",
    "section": "3.3 尝试新的切分维度",
    "text": "3.3 尝试新的切分维度\n注意这里我们添加一个IC的切分维度, 这样每次内部循环加载的就是不同的weights[oc,ic,:,:]的tile了.\ndef demo3_3(imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray, prefix=\"demo3-3\"):\n  image = TarcedArray(imageArr)\n  weight = TarcedArray(weightArr)\n  output = TarcedArray(outputArr)\n  (B, OC, OH, OW) = outputArr.shape\n  IC = imageArr.shape[1]\n  for b in Segments(0, B, 1):\n    for oc in Segments(0, OC, 8):\n      for oh in Segments(0, OH, OH):\n        for ow in Segments(0, OW, OW):\n          with GlobalHierarchy(L2SIZE):\n            reuse = False\n            outputTile = output[b, oc, oh, ow]\n            for ic in Segments(0, IC, CORE_IC):\n              wSeg = Infer.get_w_segment(oc)\n              wSeg[1] = ic  # add slice in ic\n              weightTile = weight[wSeg, reuse]\n              imageSeg = Infer.get_input_segment(b, oc, oh, ow)\n              imageSeg[1] = ic  # add slice in ic\n              imageTile = image[imageSeg, reuse]\n              outputTile += TensorCore(imageTile, weightTile)\n              if reuse is False:\n                reuse = True  # reuse same buffer.\n\n  assert (np.allclose(output._array, targetOutput, atol=1e-5))\n  print(prefix, \"total loaded :\", GlobalHierarchy.TotalLoaded)\n  GlobalHierarchy.Reset()\n\ndemo3-3 total loaded : 4825088\n此时对于weights的加载比之前减少了一倍, 但是如果此时OH/OW有切分, 那么同一份ic的weights也会被多次加载. 而需要注意的是SRAM的大小有限, 所以必须缩小OC上的tile size, 来保证当前策略较优. 这就是tile size与SRAM大小共同影响compute schedule."
  },
  {
    "objectID": "posts/dsa-schedule.html#单独执行每个算子",
    "href": "posts/dsa-schedule.html#单独执行每个算子",
    "title": "带宽受限下的DSA后端Compute Schedule",
    "section": "5.1 单独执行每个算子",
    "text": "5.1 单独执行每个算子\n以上实验了单层卷积的情况, 我们尝试了调整tile size/调整buffer stage的位置来减少总的数据加载次数. 接下来我们需要考虑多个算子fusion的情况, 假设卷积前面不是一个带有reduction的算子, 比如binary add, 首先单独执行两个算子.\n\ndef demo5_1(imageArr: np.ndarray, biasArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray):\n  image = TarcedArray(imageArr)\n  bias = TarcedArray(biasArr)\n  mid = TarcedArray(np.zeros_like(imageArr))\n  # binary add\n  (B, C, H, W) = imageArr.shape\n  for b in Segments(0, B, 1):\n    for c in Segments(0, C, 8):\n      for h in Segments(0, H, H):\n        for w in Segments(0, W, W):\n          with GlobalHierarchy(L2SIZE):\n            imageTile = image[b, c, h, w]\n            biasTile = bias[b, c, h, w]\n            midTile = mid[b, c, h, w]\n            midTile += imageTile + biasTile\n\n  demo3_3(mid._array, weightArr, outputArr, targetOutput, \"demo5-1\")\n\ndemo5-1 total loaded : 30515200\n那么每次算子执行结束后, 数据需要出DDR再回到SRAM, 这样就消耗了许多带宽."
  },
  {
    "objectID": "posts/dsa-schedule.html#执行fusion后的算子",
    "href": "posts/dsa-schedule.html#执行fusion后的算子",
    "title": "带宽受限下的DSA后端Compute Schedule",
    "section": "5.2 执行Fusion后的算子",
    "text": "5.2 执行Fusion后的算子\n我们可以发现,后面卷积的循环[B,OH,OW,IC]分别可以对应前面binary的[B,H,W,C], 其实即前面binary的H与W是可以依据卷积的tile size来确定, 他的C维度依据卷积的IC维度确定, 并且因为这个binary计算时没有元素依赖关系, 所以简单调整他的循环顺序我们就可以进行算子Fusion了, 也就是在IC的循环中计算elemwise的计算操作.\n\ndef demo5_2(imageArr: np.ndarray, biasArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray):\n  image = TarcedArray(imageArr)\n  bias = TarcedArray(biasArr)\n  weight = TarcedArray(weightArr)\n  output = TarcedArray(outputArr)\n  (B, OC, OH, OW) = outputArr.shape\n  IC = imageArr.shape[1]\n  for b in Segments(0, B, 1):\n    for oc in Segments(0, OC, 8):\n      for oh in Segments(0, OH, OH):\n        for ow in Segments(0, OW, OW):\n          with GlobalHierarchy(L2SIZE):\n            reuse = False\n            outputTile = output[b, oc, oh, ow]\n            for ic in Segments(0, IC, CORE_IC):\n              wSeg = Infer.get_w_segment(oc)\n              wSeg[1] = ic  # add slice in ic\n              weightTile = weight[wSeg, reuse]\n              imageSeg = Infer.get_input_segment(b, oc, oh, ow)\n              imageSeg[1] = ic  # add slice in ic\n              imageTile = image[imageSeg, reuse]\n              biasTile = bias[imageSeg, reuse]\n              outputTile += TensorCore(imageTile + biasTile, weightTile)\n              if reuse is False:\n                reuse = True  # reuse same buffer.\n\n  assert (np.allclose(output._array, targetOutput, atol=1e-5))\n  print(\"demo5-2 total loaded :\", GlobalHierarchy.TotalLoaded)\n  GlobalHierarchy.Reset()\n\ndemo5-2 total loaded : 8036352\n\n可以发现减少了两倍的数据搬运."
  },
  {
    "objectID": "posts/dsa-schedule.html#单独执行每个算子-1",
    "href": "posts/dsa-schedule.html#单独执行每个算子-1",
    "title": "带宽受限下的DSA后端Compute Schedule",
    "section": "6.1 单独执行每个算子",
    "text": "6.1 单独执行每个算子\n首先测试两层卷积单独执行的数据加载.\n\ndef demo6(Infer1: Conv2dBoundsInfer, Infer2: Conv2dBoundsInfer, imageArr: np.ndarray, weightArr1: np.ndarray, weightArr2: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray):\n  image = TarcedArray(imageArr)\n  weight1 = TarcedArray(weightArr1)\n  tempOutput = TarcedArray(np.zeros(Infer2.in_shape).astype(np.float32))\n  (B, OC, OH, OW) = Infer2.in_shape\n  with GlobalHierarchy(L2SIZE):\n    reuse = False\n    weight1Tile = weight1[:, :, :, :]\n    for b in Segments(0, B, 1):\n      for oc in Segments(0, OC, 16):\n        for oh in Segments(0, OH, 48):\n          for ow in Segments(0, OW, OW):\n            outputTile = tempOutput[(b, oc, oh, ow), reuse]\n            imageSeg = Infer1.get_input_segment(b, oc, oh, ow)\n            imageTile = image[imageSeg, reuse]\n            outputTile += TensorCore(imageTile, weight1Tile[oc])\n            if reuse is False:\n              reuse = True  # reuse same buffer.\n\n  weight2 = TarcedArray(weightArr2)\n  output = TarcedArray(outputArr)\n  (B, OC, OH, OW) = outputArr.shape\n  with GlobalHierarchy(L2SIZE):\n    reuse = False\n    weight2Tile = weight2[:, :, :, :]\n    for b in Segments(0, B, 1):\n      for oc in Segments(0, OC, 16):\n        for oh in Segments(0, OH, 48):\n          for ow in Segments(0, OW, OW):\n            outputTile = output[(b, oc, oh, ow), reuse]\n            imageSeg = Infer2.get_input_segment(b, oc, oh, ow)\n            imageTile = tempOutput[imageSeg, reuse]\n            outputTile += TensorCore(imageTile, weight2Tile[oc])\n            if reuse is False:\n              reuse = True  # reuse same buffer.\n\n  assert (np.allclose(output._array, targetOutput, atol=1e-5))\n  print(\"demo6 total loaded :\", GlobalHierarchy.TotalLoaded)\n  GlobalHierarchy.Reset()\n\ndemo6 total loaded : 722528"
  },
  {
    "objectID": "posts/dsa-schedule.html#执行fusion后的算子-1",
    "href": "posts/dsa-schedule.html#执行fusion后的算子-1",
    "title": "带宽受限下的DSA后端Compute Schedule",
    "section": "6.2 执行Fusion后的算子",
    "text": "6.2 执行Fusion后的算子\n假设我们遇到前面一个算子是带有reduction的情况, 比如卷积+卷积. 那么只需要考虑将两层卷积的循环直接合并即可, 同时现在的compute schedule就不能和单层卷积时相同了, 在两层卷积的循环直接合并时, 我们无法在最后一层卷积的input channel上切分, 因为后一个卷积的每一份input channel都依赖前面一个卷积的所有input channel, 这样切分会导致前面的卷积的weights反复加载, 目前我实现的多层卷积的合并必须要在SRAM中可以存下所有的weights才可以.\ndef demo6_1(Infer1: Conv2dBoundsInfer, Infer2: Conv2dBoundsInfer, imageArr: np.ndarray, weightArr1: np.ndarray, weightArr2: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray):\n  image = TarcedArray(imageArr)\n  weight1 = TarcedArray(weightArr1)\n  weight2 = TarcedArray(weightArr2)\n  output = TarcedArray(outputArr)\n  (B, OC, OH, OW) = outputArr.shape\n  IC = imageArr.shape[1]\n  with GlobalHierarchy(L2SIZE):\n    reuse = False\n    weight1Tile = weight1[:, :, :, :]\n    weight2Tile = weight2[:, :, :, :]\n    for b in Segments(0, B, 1):\n      for oc in Segments(0, OC, 16):\n        for oh in Segments(0, OH, 48):\n          for ow in Segments(0, OW, OW):\n            outputTile = output[(b, oc, oh, ow), reuse]\n            imageSeg2 = Infer2.get_input_segment(b, oc, oh, ow)\n            imageSeg1 = Infer1.get_input_segment(\n                imageSeg2[0], imageSeg2[1], imageSeg2[2], imageSeg2[3])\n            imageTile1 = image[imageSeg1, reuse]\n            imageTile2 = TensorCore(imageTile1, weight1Tile)\n            outputTile += TensorCore(imageTile2, weight2Tile[oc])\n            if reuse is False:\n              reuse = True  # reuse same buffer.\n\n  assert (np.allclose(output._array, targetOutput, atol=1e-5))\n  print(\"demo6-1 total loaded :\", GlobalHierarchy.TotalLoaded)\n  GlobalHierarchy.Reset()\n\ndemo6-1 total loaded : 206432"
  },
  {
    "objectID": "posts/docker-remote.html",
    "href": "posts/docker-remote.html",
    "title": "vscode连接远程服务器中docker",
    "section": "",
    "text": "关于如何利用vscode在远程服务器中连接docker并进行开发。"
  },
  {
    "objectID": "posts/docker-remote.html#方法一-挂载多个卷组成multi-container",
    "href": "posts/docker-remote.html#方法一-挂载多个卷组成multi-container",
    "title": "vscode连接远程服务器中docker",
    "section": "方法一： 挂载多个卷组成multi-container",
    "text": "方法一： 挂载多个卷组成multi-container\n\n1. 新建vscode本地仓库，编辑docker配置文件\n我是在Windows中建立一个multi-container的文件夹，用vscode打开然后建立docker配置文件： 在其中建立.devcontainer文件夹，.devcontainer\\devcontainer.json与docker-compose.yml。\n\n\n.devcontainer\\docker-compose.yml\n这里需要注意的就是把远程服务器上面的文件分别映射到容器中，这样才能用vscode分别打开各个文件夹。\nversion: '3'\nservices:\n  # Update this to the name of the service you want to work with in your docker-compose.yml file\n  dev_remote:\n    image: \"compilerteamer/gnne-compiler:nncase\"\n    volumes:\n      # Update this to wherever you want VS Code to mount the folder of your project\n      - /home/zhengqihang/workspace/ncnn:/home/ncnn:cached\n      - /home/zhengqihang/workspace/nncase:/home/nncase:cached\n    # Overrides default command so things don't shut down after the process ends.\n    command: /bin/sh -c \"while sleep 1000; do :; done\"\n\n\n.devcontainer\\devcontainer.json:\n这里的workspaceFolder就是我们要打开的容器内文件夹的位置\n{\n    \"name\": \"Existing Docker Compose (Extend)\",\n    \"dockerComposeFile\": [\n        \"docker-compose.yml\"\n    ],\n    \"service\": \"dev_remote\",\n    \"workspaceFolder\": \"/home/ncnn\",\n    \"settings\": {\n        \"terminal.integrated.shell.linux\": null\n    },\n    \"extensions\": []\n}\n\n\n\n2. 登录远程容器仓库\n在multi-container中点击左下角的蓝色链接标志，然后选择reopen in container，最后当前文件夹被远程容器的repo替换掉。\n\n我们默认打开的是ncnn的文件夹，如果要打开nncase的文件夹需要在remote-container插件中进行attch，然后就默认打开了下一个挂在的卷，也就是ncnn。"
  },
  {
    "objectID": "posts/docker-remote.html#方法2挂载一个卷利用ssh的方式远程开发",
    "href": "posts/docker-remote.html#方法2挂载一个卷利用ssh的方式远程开发",
    "title": "vscode连接远程服务器中docker",
    "section": "方法2：挂载一个卷，利用ssh的方式远程开发",
    "text": "方法2：挂载一个卷，利用ssh的方式远程开发\n经过一番折腾，发现上面的方法在vscode中存在巨大限制，vscode只能同时开两个文件夹，所以只能用ssh的方式进行开发了。\n我已经把模板上传到github了，下载后修改公钥就可以直接利用vscode启动远程容器了。\n进入远程容器之后启动ssh服务：\nservice ssh start\n然后通过ssh即可连接远程服务器进行开发，我的模板仓库中把22端口映射到50008端口，所以ssh的连接命令如下\nssh root@xxxx -p 50008"
  },
  {
    "objectID": "posts/docker-remote.html#总结",
    "href": "posts/docker-remote.html#总结",
    "title": "vscode连接远程服务器中docker",
    "section": "总结",
    "text": "总结\n这样多个repo可以共用一个container就比较方便。不过还是需要寻找一些自动化的Dockerfile去配置容器，否则每次换一个人创建新环境又会浪费大量时间在配置上。"
  },
  {
    "objectID": "posts/docker-remote.html#remote-container-没有glibc-2.28",
    "href": "posts/docker-remote.html#remote-container-没有glibc-2.28",
    "title": "vscode连接远程服务器中docker",
    "section": "remote container 没有glibc 2.28",
    "text": "remote container 没有glibc 2.28\nremote container的node使用的host 上的glibc，但是我的host又没有sudo权限，只能手动编译\nwget -c https://ftp.gnu.org/gnu/glibc/glibc-2.28.tar.gz\ntar -zxvf glibc-2.28.tar.gz\nmkdir glibc-2.28/build\ncd glibc-2.28/build\n../configure --prefix=/opt/glibc\nmake -j\nmake install\n编译安装好之后，删除~/.vscode-remote-containers/，然后重新连接他会下载一个新的node，然后报错得到没有glibc。但是你现在知道了具体的node路径。\n/home/zhengqihang/.local/bin/patchelf --set-interpreter /data/zhengqihang/glibc-install/lib/ld-linux-x86-64.so.2 --set-rpath /data/zhengqihang/glibc-install/lib --force-rpath /home/zhengqihang/.vscode-remote-containers/bin/e249dada235c2083c83813bd65b7f4707fb97b76/node"
  },
  {
    "objectID": "posts/docker-remote.html#手动下载code-server",
    "href": "posts/docker-remote.html#手动下载code-server",
    "title": "vscode连接远程服务器中docker",
    "section": "手动下载code server",
    "text": "手动下载code server\n\n首先根据remote dev启动时下载的文件路径确定commit id\n手动下载 :https://update.code.visualstudio.com/commit:${commit_id}/server-linux-x64/stable,\n然后scp copy vscode-server-linux-x64.tar.gz到server上\n解压vscode-server-linux-x64.tar.gz 到 ~/.vscode-server/bin/\\({commit_id} ，如果是remote container需要用~/.vscode-remote-containers/bin/\\){commit_id}\n\ntar -xzf xx.tar.gz -C /path/to/target_directory\nmv /path/to/target_directory to xxx"
  },
  {
    "objectID": "posts/distal.html",
    "href": "posts/distal.html",
    "title": "DISTAL: The Distributed Tensor Algebra Compiler",
    "section": "",
    "text": "这篇论文主要是介绍了一个分布式张量代数编译器， 它通过自定义的DSL可以帮助我们快速生成分布式计算代码。"
  },
  {
    "objectID": "posts/distal.html#distribute",
    "href": "posts/distal.html#distribute",
    "title": "DISTAL: The Distributed Tensor Algebra Compiler",
    "section": "Distribute",
    "text": "Distribute\ndistal在taco的基础上使用distribute的调度来修改迭代域，从而表示分布式计算。内部的实现如下：\ndistribute(vector&lt;IndexVar&gt; targets, vector&lt;IndexVar&gt; dist, vector&lt;IndexVar&gt; local, Machine m):\n  // Divide each dimension by the corresponding machine dimension.\n  for i in range(0, m.dim):\n    // Reorder loops so each outer divided variable is on the outside.\n    divide(targets[i], dist[i], local[i], m.dims[i]) \n  // Distribute all of the outer divided variables.\n  reorder(dist + local) \n  distribute(dist)\n实际上就是将循环进行split后再进行reorder，再标记外层循环与machine进行对应。\n当distribution notation为\\(\\mathcal{a}_{x} \\mapsto_{x} \\mathcal{M}\\),\\(\\mathcal{b}_{x} \\mapsto_{x} \\mathcal{M}\\), 那么对应的分布式调度为distribute(i,io,ii,M), 经过调度后的迭代域如下：\n\n灰色区域所表示的是单独的分布式节点，在循环i上进行分布式后，所有的循环j都被划分到了不同的处理器节点上。"
  },
  {
    "objectID": "posts/distal.html#communicate",
    "href": "posts/distal.html#communicate",
    "title": "DISTAL: The Distributed Tensor Algebra Compiler",
    "section": "Communicate",
    "text": "Communicate\n当数据分布在不同的处理器上时，在发生计算时就需要将所依赖的数据从别的处理器节点上获取，因此需要进行数据的通信。基于distribution notation以及多面体分析，我们可以计算得到每一次迭代所需要传输的数据。将这种数据依赖以及循环j的前三次迭代绘制出来，如下所示：\n\n可以看到，a,b在循环i上进行分布，那么每个i的迭代对应他们的一部分数据区域，在每次j的迭代中，每个节点会需要从其他节点获取到b的部分数据。当然这是基于数据的分布式情况以及依赖分析所得到的naive 通信方式。\n在上面这种情况下，如果采用一个更大块的通信操作，直接从节点获取多次计算所需要的数据，而不是每次迭代都获取数据，那么性能可能会更好，这就引出了通信频率和内存使用的trade-off。为了在这种trade-off的场景下可以进行调节，distal提供了\\(\\text{communicate}(\\mathcal{T}, i)\\)的调度。他通过指定迭代循环的方式，配合数据分布以及数据依赖分析，把迭代循环内部所有嵌套循环所依赖的数据放到所指定循环的开始来获取。比如执行distribute(b,i)后，计算如图所示：\n\ndistal的communicate调度是进行了简化设计，因为如果要指定send/receive或者设备编号等，整个事情就会变的非常复杂。"
  },
  {
    "objectID": "posts/distal.html#rotate",
    "href": "posts/distal.html#rotate",
    "title": "DISTAL: The Distributed Tensor Algebra Compiler",
    "section": "Rotate",
    "text": "Rotate\n许多分布式算法采用脉动的方式实现，处理器反复将数据shift到相邻节点。脉动算法可以有效利用互连的机器架构，并通过避免对相同数据的竞争来提高性能。\n首先继续从distribute(i)的调度开始，默认可以认为他执行的是broadcast的通信操作，在每个时间片上，都会向所有处理器发送对相同数据的通信请求，如下图所示： \n为了表达脉动计算，distal引入rotate调度, 他是通过引入时间维度的映射，让j的迭代空间在时间上旋转，从而实现脉动式通信模式。如下图所示，实际上现在的横轴已经不是j的迭代空间了，而是时间维度，真正的j的迭代空间在每个时间片上发生了旋转。 如果对于每个i的迭代，j的迭代空间被旋转且j的第i次迭代先发生，那么称整个execution space具备脉动模式： \n给定一组索引变量\\(I\\)，目标索引变量\\(t\\), 以及结果索引变量\\(r\\), rotate(t,I,r)的调度会另迭代空间\\(t\\)根据\\(\\sum_{i\\in I} i \\mod \\text{extent(t)}\\)发生旋转。 旋转后，对于所有的\\(i\\in I\\)，给定其一个固定的剩余迭代\\(i'\\in (I-i)\\), \\(r\\)的相同迭代会在\\(i\\)的不同迭代上发生。假设\\(t\\)在\\(i,j\\)两个维度上进行了rotate, 那么每次\\(i\\)或\\(j\\)的迭代开始时,\\(r\\)的值都是不同的。"
  },
  {
    "objectID": "posts/distal.html#summa",
    "href": "posts/distal.html#summa",
    "title": "DISTAL: The Distributed Tensor Algebra Compiler",
    "section": "Summa",
    "text": "Summa\n首先定义各种分布式tensor以及循环域。\n  int dim = 1024;\n  // Place each tensor onto a processor grid.\n  auto gx = ir::Var::make(\"gridX\", Int32, false, false, true);\n  auto gy = ir::Var::make(\"gridY\", Int32, false, false, true);\n  auto grid = Grid(gx, gy);\n  auto dist = TensorDistribution(grid);\n\n  Tensor&lt;double&gt; a(\"a\", {dim, dim}, Format{Dense, Dense}, dist);\n  Tensor&lt;double&gt; b(\"b\", {dim, dim}, Format{Dense, Dense}, dist);\n  Tensor&lt;double&gt; c(\"c\", {dim, dim}, Format{Dense, Dense}, dist);\n\n  IndexVar i(\"i\"), j(\"j\"), io(\"in\"), jo(\"jn\"), ii(\"il\"), ji(\"jl\"), k(\"k\"), ki(\"ki\"), ko(\"ko\");\n  IndexVar iln(\"iln\"), ill(\"ill\");\n  a(i, j) = b(i, k) * c(k, j);\n\n  auto placeALowered = lowerLegionSeparatePartitionCompute(a.getPlacementStatement(), \"placeLegionA\");\n  auto placeBLowered = lowerLegionSeparatePartitionCompute(b.getPlacementStatement(), \"placeLegionB\");\n  auto placeCLowered = lowerLegionSeparatePartitionCompute(c.getPlacementStatement(), \"placeLegionC\");\n采用distal提供的调度接口：\n  auto stmt = a.getAssignment().concretize() \n    .distribute({i, j}, {io, jo}, {ii, ji}, grid);\n    .divide(k, ko, ki, gx);\n    .reorder({ko, ii, ji, ki});\n    .communicate(a(i, j), jo);\n    .communicate(b(i, k), ko);\n    .communicate(c(i, j), ko);\n这里每一步的调度返回值都可以打印为文本形式：\n\n\n\n\n\n\n\nschedule\nstmt\n\n\n\n\na(i, j) = b(i, k) * c(k, j);\nforall(i, forall(j, forall(k, a(i,j) += b(i,k) * c(k,j))))\n\n\ndistribute({i, j}, {io, jo}, {ii, ji}, grid)\nsuchthat(forall(distFused, forall(il, forall(jl, forall(k, a(i,j) += b(i,k) * c(k,j)))), Distributed, NoRaces), divide(i, in, il, gridX) and divide(j, jn, jl, gridY) and multiFuse({in, jn}, reorder(in, jn)))\n\n\ndivide(k, ko, ki, gx)\nsuchthat(forall(distFused, forall(il, forall(jl, forall(ko, forall(ki, a(i,j) += b(i,k) * c(k,j))))), Distributed, NoRaces), divide(i, in, il, gridX) and divide(j, jn, jl, gridY) and multiFuse({in, jn}, reorder(in, jn)) and divide(k, ko, ki, gridX))\n\n\nreorder({ko, ii, ji, ki})\nsuchthat(forall(distFused, forall(ko, forall(il, forall(jl, forall(ki, a(i,j) += b(i,k) * c(k,j))))), Distributed, NoRaces), divide(i, in, il, gridX) and divide(j, jn, jl, gridY) and multiFuse({in, jn}, reorder(in, jn)) and divide(k, ko, ki, gridX))\n\n\ncommunicate(a(i, j), jo)\nsuchthat(forall(distFused, forall(ko, forall(il, forall(jl, forall(ki, a(i,j) += b(i,k) * c(k,j))))), Distributed, NoRaces, transfers: transfer(a(i,j))), divide(i, in, il, gridX) and divide(j, jn, jl, gridY) and multiFuse({in, jn}, reorder(in, jn)) and divide(k, ko, ki, gridX))\n\n\ncommunicate(b(i, k), ko)\nsuchthat(forall(distFused, forall(ko, forall(il, forall(jl, forall(ki, a(i,j) += b(i,k) * c(k,j)))), transfers: transfer(b(i,k))), Distributed, NoRaces, transfers: transfer(a(i,j)), transfer(b(i,k))), divide(i, in, il, gridX) and divide(j, jn, jl, gridY) and multiFuse({in, jn}, reorder(in, jn)) and divide(k, ko, ki, gridX))\n\n\ncommunicate(c(i, j), ko)\nsuchthat(forall(distFused, forall(ko, forall(il, forall(jl, forall(ki, a(i,j) += b(i,k) * c(k,j)))), transfers: transfer(b(i,k)), transfer(c(i,j))), Distributed, NoRaces, transfers: transfer(a(i,j)), transfer(b(i,k)), transfer(c(i,j))), divide(i, in, il, gridX) and divide(j, jn, jl, gridY) and multiFuse({in, jn}, reorder(in, jn)) and divide(k, ko, ki, gridX))\n\n\n\n如果采用伪代码的形式可以更好理解：\n# naive\nfor i in range(0, M):\n  for j in range(0, N):\n    for k in range(0, K):\n      a[i,j] += b[i,k] * c[k,j]\n# distribute\nfor io in range(0, X): @ parallel\n  for jo in range(0, Y): @ parallel\n    for ii in range(0, M / X):\n      for jj in range(0, N / Y):\n        for k in range(0, K):\n          a[io*(M/X)+ii,jo*(N/Y)+ji] += b[io*(M/X)+ii,k] * c[k,jo*(N/Y)+ji]\n# divide\nfor io in range(0, X): @ parallel\n  for jo in range(0, Y): @ parallel\n    for ii in range(0, M / X):\n      for jj in range(0, N / Y):\n        for ko in range(0, X):\n          for ki in range(0, K / X):\n            a[io*(M/X)+ii,jo*(N/Y)+ji] += b[io*(M/X)+ii,ko*(K/X)+ki] * c[ko*(K/X)+ki,jo*(N/Y)+ji]\n# reorder\nfor io in range(0, X): @ parallel\n  for jo in range(0, Y): @ parallel\n    for ko in range(0, X):\n      for ii in range(0, M / X):\n        for jj in range(0, N / Y):\n          for ki in range(0, K / X):\n            a[io*(M/X)+ii,jo*(N/Y)+ji] += b[io*(M/X)+ii,ko*(K/X)+ki] * c[ko*(K/X)+ki,jo*(N/Y)+ji]\n# communicate\nfor io in range(0, X): @ parallel\n  for jo in range(0, Y): @ parallel\n    sub_a = load(a[M/X,N/Y])\n    for ko in range(0, X):\n      sub_b = load(b[M/X,K/X])\n      sub_c = load(c[K/X,N/Y])\n      for ii in range(0, M / X):\n        for jj in range(0, N / Y):\n          for ki in range(0, K / X):\n            sub_a[ii,ji] += sub_b[ii,ki] * sub_c[ki,ji]"
  },
  {
    "objectID": "posts/dbscan.html",
    "href": "posts/dbscan.html",
    "title": "DBSCAN算法原理及实现",
    "section": "",
    "text": "因为模式识别需要分组讲一个聚类算法，所以我挑选了这个算法。"
  },
  {
    "objectID": "posts/dbscan.html#思路",
    "href": "posts/dbscan.html#思路",
    "title": "DBSCAN算法原理及实现",
    "section": "思路",
    "text": "思路\n现在的思路是将所有的核心节点构造成网络图，通过DFS遍历的方式，确定所有簇"
  },
  {
    "objectID": "posts/ct-augments.html",
    "href": "posts/ct-augments.html",
    "title": "Control Theory Augment",
    "section": "",
    "text": "CT Augment是论文ReMixmatch中提出的一种不需要通过控制方法不需要使用强化学习即可调整数据增强测量的一种方法。今天仔细学习一下。\n\n\n初始化选择概率矩阵\n\n首先，CTAugment将每个变化的每个参数范围划分为数个分组，在开始训练时将每个分组的权重设置为1，比如一共9种数据增强ops，数据增强分级为10级，此时权重参数log_prob形状为[9,10]。同时设置更新速率矩阵rates为1，形状为[9,10]。\n\n均匀随机选取数据增强方式以及数据增强分级参数\ndef _sample_ops_uniformly(self) -&gt; [tf.Tensor, tf.Tensor]:\n  \"\"\"Uniformly samples sequence of augmentation ops.\"\"\"\n  op_indices = tf.random.uniform(\n      shape=[self.num_layers], maxval=len(AUG_OPS), dtype=tf.int32)\n  op_args = tf.random.uniform(shape=[self.num_layers], dtype=tf.float32)\n  return op_indices, op_args\n均匀随机选取可以更好覆盖全部情况\n根据所选取的参数实施增强得到probe_data\n通过模型对probe_data进行分类，得到probe_probs\n使用label得到对应样本的正确分类probe_probs称为proximity\n根据公式更新rate矩阵\n此处的op_idx, level_idx是之前均匀随机选取的增强操作、分级参数。decay为衰减率默认0.999。\nalpha = 1 - decay\nrate[op_idx, level_idx] += (proximity - rate[op_idx, level_idx]) * alpha\n当所得到的分类概率较高则rate会随之增加，反之则降低。\n将rate转换为选择概率probs\nprobs = tf.maximum(self.rates, self.epsilon)\nprobs = probs / tf.reduce_max(probs, axis=1, keepdims=True) # 将概率锐化，类似softmax\nprobs = tf.where(probs &lt; self.confidence_threshold, tf.zeros_like(probs),\n                probs) # 如果概率小于阈值，那么设置为0\nprobs = probs + self.epsilon  # 防止概率为0\nprobs = probs / tf.reduce_sum(probs, axis=1, keepdims=True)  # 再次锐化\n将probs更新到log_prob\n对于训练的样本则根据log_prob进行数据增强参数的选取。\ndef _sample_ops(self, local_log_prob):\n  \"\"\"Samples sequence of augmentation ops using current probabilities.\"\"\"\n  # choose operations\n  op_indices = tf.random.uniform(\n      shape=[self.num_layers], maxval=len(AUG_OPS), dtype=tf.int32)\n  # sample arguments for each selected operation\n  selected_ops_log_probs = tf.gather(local_log_prob, op_indices, axis=0)\n  op_args = tf.random.categorical(selected_ops_log_probs, num_samples=1)\n  op_args = tf.cast(tf.squeeze(op_args, axis=1), tf.float32)\n  op_args = (op_args + tf.random.uniform([self.num_layers])) / self.num_levels\n  return op_indices, op_args\n重复以上过程。\n\n\n总结\n整个更新过程就是这样。通过选取对应的数据增强种类，得到此数据增强下的分类概率，当分类概率低时，rate会降低，经过锐化后此数据增强被选中的概率也会降低。其中decay控制了更新速率。还有confidence_threshold，我觉得可能要batch越大的时候才比较有用，如果batch较小很难一次性更新rate超过confidence_threshold，如果没有超过confidence_threshold那么此数据增强被选中的概率依旧还是比较低的。\n所实话对于虽然不用强化学习的方法来更新数据增强策略了，但这两个超参数的选取还是有点头疼。并且这个控制方式缺少一定的收敛性分析。我训练半天的选取概率矩阵如下：\n[0.11852807, 0.13082333, 0.00013127, 0.12403152, 0.13140538, 0.00013127, 0.1205155 , 0.12174512, 0.12513067, 0.12755796],\n[0.20564014, 0.00020543, 0.19176407, 0.00020543, 0.2006021 , 0.00020543, 0.20226233, 0.00020543, 0.19870412, 0.00020543],\n[0.11055039, 0.11402953, 0.11110956, 0.1050452 , 0.11322882, 0.11464192, 0.11097319, 0.10542616, 0.11488046, 0.00011477],\n[0.51186407, 0.48404494, 0.00051135, 0.00051135, 0.00051135, 0.00051135, 0.00051135, 0.00051135, 0.00051135, 0.00051135],\n[0.14486092, 0.1384983 , 0.14066745, 0.13853313, 0.15168588, 0.1444478 , 0.14085191, 0.00015153, 0.00015153, 0.00015153],\n[0.34809318, 0.00034775, 0.00034775, 0.00034775, 0.3339483 , 0.00034775, 0.00034775, 0.00034775, 0.31552422, 0.00034775],\n[0.11353768, 0.11433525, 0.00011519, 0.11392737, 0.11094389, 0.10420952, 0.10411835, 0.11530466, 0.11302778, 0.11048029],\n[0.0009901 , 0.0009901 , 0.0009901 , 0.0009901 , 0.0009901 , 0.0009901 , 0.0009901 , 0.0009901 , 0.99108905, 0.0009901 ],\n[0.14962535, 0.15079339, 0.13698637, 0.14928676, 0.13616142, 0.13792172, 0.00015064, 0.00015064, 0.00015064, 0.13877314]\n可视化效果如下，这个0.99108905我感觉很有可能是恰好上个probe使用了这个增强，但是一下就把概率拉到0.99也太夸张了把，按道理应该是越弱的增强级别概率越大才对。"
  },
  {
    "objectID": "posts/csharp-trick.html",
    "href": "posts/csharp-trick.html",
    "title": "csharp 问题记录",
    "section": "",
    "text": "记录一些开发csharp时遇到的问题以及解决方案。\n\n\n注意隐式转换与构造函数的冲突问题\n因为cshapr每次都要写new,所以添加了一些隐式类型转换语法糖. 但是下面的代码就会出现问题, 就是new WildCardPattern(Name)这里其实并不是调用默认的WildCardPattern(string Name, ExprPattern? Pattern), 而是又被隐式类型转换成了WildCardPattern然后准备调用复制构造函数构造,但是隐式类型转换的时候就陷入递归了.\npublic sealed record WildCardPattern(string Name, ExprPattern? Pattern) : ExprPattern\n{\n    private static int _globalCardIndex = 0;\n\n    public WildCardPattern() : this($\"wc_{_globalCardIndex++}\", null)\n    {\n    }\n\n    public static implicit operator WildCardPattern(string Name) =&gt; new WildCardPattern(Name);\n\n    public override bool MatchLeaf(Expr expr) =&gt; (Pattern?.MatchLeaf(expr) ?? true) && MatchCheckedType(expr);\n}\n\n\nC# delegate的本质\n最近在弄一个直接jit生成代码然后用c#动态调用的东西,但是需要动态调用你必须要告诉当前的函数指针一个delegate的定义,不然c#就不知道你要输入什么返回什么. 那么既然是jit,我们的就不能提前写好这个定义, 所以需要动态构造一个delegate.\n给定一个类:\npublic class CustomType\n{\n    public delegate float declf(float x, float y);\n}\n他的delegate的修饰符并不是一种attr,而是一种表示他继承自MulticastDelegate, 所以你可以发现他的是一个NestedType,并且他的基类是MulticastDelegate.\nvar t = cls_type.GetNestedType(\"declf\");\nAssert.Equal(t.BaseType, typeof(MulticastDelegate));\n同时他还具备了4个方法,其中一个是构造方法,以及三个重写的方法. \n通过检查他的il我们可以发现declf就是个class,所以事情就简化成了构造这个类的问题.:\n.class private auto ansi '&lt;Module&gt;'\n{\n} // end of class &lt;Module&gt;\n\n.class public auto ansi beforefieldinit CustomType\n    extends [mscorlib]System.Object\n{\n    // Nested Types\n    .class nested public auto ansi sealed declf\n        extends [mscorlib]System.MulticastDelegate\n    {\n        // Methods\n        .method public hidebysig specialname rtspecialname \n            instance void .ctor (\n                object 'object',\n                native int 'method'\n            ) runtime managed \n        {\n        } // end of method declf::.ctor\n\n        .method public hidebysig newslot virtual \n            instance float32 Invoke (\n                float32 x,\n                float32 y\n            ) runtime managed \n        {\n        } // end of method declf::Invoke\n\n        .method public hidebysig newslot virtual \n            instance class [mscorlib]System.IAsyncResult BeginInvoke (\n                float32 x,\n                float32 y,\n                class [mscorlib]System.AsyncCallback callback,\n                object 'object'\n            ) runtime managed \n        {\n        } // end of method declf::BeginInvoke\n\n        .method public hidebysig newslot virtual \n            instance float32 EndInvoke (\n                class [mscorlib]System.IAsyncResult result\n            ) runtime managed \n        {\n        } // end of method declf::EndInvoke\n\n    } // end of class declf\n\n\n    // Methods\n    .method public hidebysig specialname rtspecialname \n        instance void .ctor () cil managed \n    {\n        // Method begins at RVA 0x2050\n        // Code size 7 (0x7)\n        .maxstack 8\n\n        IL_0000: ldarg.0\n        IL_0001: call instance void [mscorlib]System.Object::.ctor()\n        IL_0006: ret\n    } // end of method CustomType::.ctor\n\n} // end of class CustomType\n接下来就照葫芦画瓢把这个类的定义给生成出来,然后我们再用这个定义的类型拿去binding那个dll里面的函数,然后我们就可以动态的invoke生成的代码了~\nAssemblyName aName = new AssemblyName(\"DynamicAssemblyExample\");\nAssemblyBuilder ab = AssemblyBuilder.DefineDynamicAssembly(aName, AssemblyBuilderAccess.RunAndCollect);\nModuleBuilder mb = ab.DefineDynamicModule(aName.Name);\nTypeBuilder tb = mb.DefineType(\"MyDynamicType\", TypeAttributes.Public);\nTypeBuilder nesttb = tb.DefineNestedType(\"DynamicDelegate\", TypeAttributes.NestedPublic | TypeAttributes.Sealed, typeof(MulticastDelegate));\nvar ctor = nesttb.DefineConstructor(MethodAttributes.Public | MethodAttributes.HideBySig | MethodAttributes.SpecialName | MethodAttributes.RTSpecialName, CallingConventions.Standard | CallingConventions.HasThis, new[] { typeof(object), typeof(IntPtr) });\n\nILGenerator ctorIL = ctor.GetILGenerator();\nctorIL.Emit(OpCodes.Ldarg_0);\nctorIL.Emit(OpCodes.Ldarg_1);\nctorIL.Emit(OpCodes.Ret);\n\nvar invoke = nesttb.DefineMethod(\"Invoke\", MethodAttributes.Public | MethodAttributes.Virtual | MethodAttributes.HideBySig | MethodAttributes.NewSlot, CallingConventions.Standard | CallingConventions.HasThis, typeof(float), new[] { typeof(float), typeof(float) });\nvar beginInvoke = nesttb.DefineMethod(\"BeginInvoke\", MethodAttributes.Public | MethodAttributes.Virtual | MethodAttributes.HideBySig | MethodAttributes.NewSlot, CallingConventions.Standard | CallingConventions.HasThis, typeof(IAsyncResult), new[] { typeof(float), typeof(float), typeof(IAsyncResult), typeof(object) });\n\nvar endInvoke = nesttb.DefineMethod(\"EndInvoke\", MethodAttributes.Public | MethodAttributes.Virtual | MethodAttributes.HideBySig | MethodAttributes.NewSlot, CallingConventions.Standard | CallingConventions.HasThis, typeof(float), new[] { typeof(IAsyncResult) });\nvar created_class = tb.CreateType();\nreturn created_class;****\n\n\ndotnet test crash时的解决方案\n\n开启–blame以及–blame-crash\n或者通过环境变量开启dump。\n\nexport DOTNET_DbgEnableMiniDump=1\nexport DOTNET_DbgMiniDumpType=4\nexport DOTNET_DbgMiniDumpName=/tmp/crash_%p.dmp\n\n通过lldb+sos插件调试core dump.\n\n比如lldb exec -c xx.dmp。 这个exec可以是python或者dotnet。\n\n\n安装dotnet sos，参考\n\ndotnet tool install –global dotnet-sos\ndotnet tool install -g dotnet-symbol\ndotnet sos install\n\nsudo cp /Applications/Xcode.app/Contents/Developer/usr/bin/lldb /usr/local/bin\nsudo install_name_tool -add_rpath /Applications/Xcode.app/Contents/SharedFrameworks /usr/local/bin/lldb\nsudo codesign --force --sign - /usr/local/bin/lldb\n然后用这个/usr/local/bin/lldb进行调试, 要注意使用的这里的命令 才能看到csharp中的内容。 比如把bt替换成dumpstack\n\n\nP/invoke中使用AddressSanitizer\n我需要调试ffi中的内存引用问题: 1. 首先编译时开启-fsanitize=address -g, 注意得编译为动态库。 2. 在macos中，可以在xcode中找到asan的动态库:\n❯ find /Applications/Xcode.app -name '*libclang_rt.asan*osx_dynamic.dylib'\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/17/lib/darwin/libclang_rt.asan_osx_dynamic.dylib\n\n声明环境变量export DYLD_INSERT_LIBRARIES=$ASAN_LIB，并执行代码"
  },
  {
    "objectID": "posts/cross-entropy-calc.html",
    "href": "posts/cross-entropy-calc.html",
    "title": "Cross Entropy的数值稳定计算",
    "section": "",
    "text": "今天在看centernet的heatmap损失函数时,发现他的损失和熵差不多,但是我用tf的实现会导致loss为Nan,因此我看了下Cross Entropy的计算优化,这里记录一下."
  },
  {
    "objectID": "posts/cross-entropy-calc.html#tensorflow中的cross_entropy计算",
    "href": "posts/cross-entropy-calc.html#tensorflow中的cross_entropy计算",
    "title": "Cross Entropy的数值稳定计算",
    "section": "Tensorflow中的cross_entropy计算",
    "text": "Tensorflow中的cross_entropy计算\n令\\(x = logits\\),\\(z = labels\\): \\[\n\\begin{aligned}\n    &  z * -\\log(\\text{sigmoid}(x)) + (1 - z) * -\\log(1 - \\text{sigmoid}(x)) \\\\\n=& z * -\\log(\\frac{1}{1 + e^{-x}}) + (1 - z) * -\\log(\\frac{e^{-x}}{1 + e^{-x}}) \\\\\n=& z * \\log(1 + e^{-x}) + (1 - z) * (-\\log(e^{-x}) + \\log(1 + e^{-x})) \\\\\n=& z * \\log(1 + e^{-x}) + (1 - z) * (x + \\log(1 + e^{-x}) \\\\\n=& (1 - z) * x + \\log(1 + e^{-x}) \\\\\n=& x - x * z + \\log(1 + e^{-x}) \\\\\n=& \\log(e^x) - x * z + \\log(1 + e^{-x}) \\\\\n=& - x * z + \\log(1 + e^{x})\n\\end{aligned}\n\\]\n下面为了避免\\(e^{x}\\)数值溢出,因此优化为如下:\n\\[\n\\begin{aligned}\n  &  \\log(1 + e^{x}) \\\\\n=&  \\log(1 + e^{-|x|}) + \\max(x, 0)\n\\end{aligned}   \n\\]\nNOTE: tensorflow中有个专门的函数\\(softplus(x)=\\log(1 + e^{x})\\),其中已经包含了数值溢出的优化."
  },
  {
    "objectID": "posts/cross-entropy-calc.html#centernet中的focalloss计算",
    "href": "posts/cross-entropy-calc.html#centernet中的focalloss计算",
    "title": "Cross Entropy的数值稳定计算",
    "section": "Centernet中的FocalLoss计算",
    "text": "Centernet中的FocalLoss计算\n先给出他的FocalLoss部分代码:\ndef _neg_loss(pred, gt):\n  ''' Modified focal loss. Exactly the same as CornerNet.\n      Runs faster and costs a little bit more memory\n    Arguments:\n      pred [batch,c,h,w]\n      gt_regr [batch,c,h,w]\n  '''\n  pos_inds = gt.eq(1).float()\n  neg_inds = gt.lt(1).float()\n\n  neg_weights = torch.pow(1 - gt, 4)\n\n  loss = 0\n\n  pos_loss = torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds\n  neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * neg_weights * neg_inds\n\n  num_pos  = pos_inds.float().sum()\n  pos_loss = pos_loss.sum()\n  neg_loss = neg_loss.sum()\n\n  if num_pos == 0:\n    loss = loss - neg_loss\n  else:\n    loss = loss - (pos_loss + neg_loss) / num_pos\n  return loss\nNOTE: 注意这里的pred是经过sigmoid的.\n将上述代码转换为公式,令\\(x = logits\\),\\(z = labels\\),\\(x_s=\\text{sigmoid}(x)\\): \\[\n\\begin{aligned}\n  & -\\log(\\text{sigmoid}(x))*(1-x_s)^2-\\log(1-\\text{sigmoid}(x))* x_s^2\\\\\n= & -\\log(\\frac{1}{1+e^{-x}})*(1-x_s)^2-\\log(\\frac{e^{-x}}{1+e^{-x}})* x_s^2\\\\\n= & \\log(1+e^{-x})*(1-x_s)^2+[-\\log(e^{-x}) + \\log(1 + e^{-x})]*x_s^2] \\\\\n= & \\text{softplus}(-x)*(1-x_s)^2+[x + \\text{softplus}(-x)]*x_s^2]\n\\end{aligned}   \n\\]\n优化后对应代码为:\n  def focal_loss(self, true_hm: tf.Tensor, pred_hm: tf.Tensor) -&gt; tf.Tensor:\n      \"\"\" Modified focal loss. Exactly the same as CornerNet.\n          Runs faster and costs a little bit more memory\n\n      Parameters\n      ----------\n      true_hm : tf.Tensor\n          shape : [batch, out_h , out_w, calss_num]\n      pred_hm : tf.Tensor\n          shape : [batch, out_h , out_w, calss_num]\n\n      Returns\n      -------\n      tf.Tensor\n          heatmap loss\n          shape : [batch,]\n      \"\"\"\n      z = true_hm\n      x = pred_hm\n      x_s = tf.sigmoid(pred_hm)\n\n      pos_inds = tf.cast(tf.equal(z, 1.), tf.float32)\n      neg_inds = 1 - pos_inds\n      neg_weights = tf.pow(1 - z, 4)\n\n      # neg entropy loss =  −log(sigmoid(x)) ∗ (1−sigmoid(x))^2 − log(1−sigmoid(x)) ∗ sigmoid(x)^2\n      loss = tf.add(tf.nn.softplus(-x) * tf.pow(1 - x_s, 2) * pos_inds, (x + tf.nn.softplus(-x)) * tf.pow(x_s, 2) * neg_weights * neg_inds)\n\n      num_pos = tf.reduce_sum(pos_inds, [1, 2, 3])\n      loss = tf.reduce_sum(loss, [1, 2, 3])\n\n      return tf.div_no_nan(loss, num_pos)"
  },
  {
    "objectID": "posts/cpp-trick.html",
    "href": "posts/cpp-trick.html",
    "title": "cpp挖坑&爬坑",
    "section": "",
    "text": "记录一些遇到的cpp新特性或者实现技巧上的问题。"
  },
  {
    "objectID": "posts/cpp-trick.html#变长参数",
    "href": "posts/cpp-trick.html#变长参数",
    "title": "cpp挖坑&爬坑",
    "section": "变长参数",
    "text": "变长参数\n我记得三年前我还好好学过，现在都忘光了，重新整理一下：\n\n基本定义\n/**\n * @brief 获取不同长度的可变参数，打印参数的个数\n *\n * @tparam T\n * @param args\n */\ntemplate &lt;typename... T&gt;\nvoid func(T... args) {\n  cout &lt;&lt; sizeof...(args) &lt;&lt; endl;\n}\n\nTEST(test_varg, basic) {\n  func();\n  func(1);\n  func(1, 2);\n  func(1, 2, '3', vector&lt;int&gt;{3});\n}\n/**\n [ RUN      ] test_varg.basic\n  0\n  1\n  2\n  4\n  [       OK ] test_varg.basic (0 ms)\n */\n\n\n循环展开\n\n错误写法\n参数匹配的时候需要注意，多个参数输入，最后会匹配到单参数的输入，但是单参数的时候不会调用无参数的终止函数。\nvoid print() { cout &lt;&lt; \" ;\" &lt;&lt; endl; }\ntemplate &lt;typename T&gt;\nvoid print(T v) {\n  cout &lt;&lt; v &lt;&lt; \" \";\n}\ntemplate &lt;typename T, typename... Args&gt;\nvoid print(T head, Args... args) {\n  print(head);\n  cout &lt;&lt; \"remain size : \" &lt;&lt; sizeof...(args) &lt;&lt; endl;\n  print(args...);\n}\n\nTEST(test_varg, recursive_print_error_version) {\n  print();\n  print(1, 2);\n  print(1, 2.3, '3');\n}\n/**\n [ RUN      ] test_varg.recursive_print\n  ;\n  1 remain size : 1\n  2 1 remain size : 2\n  2.3 remain size : 1\n  3 [       OK ] test_varg.recursive_print (0 ms)\n */\n\n\n正确写法\n所以我们通常用一个无参的函数来终止，在多参数的函数中调用单个参数的处理方式。下面的例子就展示了正确的log函数以及一个从任意base累加的函数。\nvoid ic() { cout &lt;&lt; \" ; \" &lt;&lt; endl; }\ntemplate &lt;typename T, typename... Args&gt;\nvoid ic(T head, Args... args) {\n  cout &lt;&lt; head &lt;&lt; \" \";\n  ic(args...);\n}\n\ntemplate &lt;typename T&gt;\nT sums() {\n  return (T)(-2);\n}\ntemplate &lt;typename T, typename... Ts&gt;\nT sums(T v, Ts... vs) {\n  return v + sums&lt;T&gt;(vs...);\n}\n\nTEST(test_varg, recursive_print_right_version) {\n  ic(sums(1, 2));\n  ic(sums(1, 2.3, 3.5));\n  ic(sums(2.3, 1, 3.5));\n}\n/*\n[ RUN      ] test_varg.recursive_print_right_version\n1  ; \n4  ; \n4.8  ; \n[       OK ] test_varg.recursive_print_right_version (0 ms)\n*/\n\n\n\n变参模板展开\n这里利用c++17中的特性方便的展开并对参数进行处理。\n\ntemplate &lt;typename T&gt;\nint toint(T t) {\n  return (int)t;\n}\n\ntemplate &lt;typename... Args&gt;\nvector&lt;int&gt; toints(Args... args) {\n  vector&lt;int&gt; arr = {toint(args)...};\n  return arr;\n}\n// 利用if constexpr编译期间即构造出对应的输出方式。\ntemplate &lt;typename Head, typename... Args&gt;\nvoid print2(Head head, Args... args) {\n  cout &lt;&lt; head;\n  if constexpr (sizeof...(args) &gt; 0) {\n    cout &lt;&lt; \" , \";\n    print2(args...);\n  } else {\n    cout &lt;&lt; \" ; \" &lt;&lt; endl;\n  }\n}\n\nTEST(test_varg, expand_right_version) {\n  auto arr = toints(1.2, 2.4, 3.6, 4.7);\n  for (size_t i = 0; i &lt; arr.size(); i++) { ic(arr[i]); }\n  auto arr2 = countargs(1.2, 2.4, 3.6, 4.7);\n  for (size_t i = 0; i &lt; arr2.size(); i++) { ic(arr2[i]); }\n  print2(\"hello\", \"word\", \"yes\", 1, 3, 4.5);\n}\n\n我们可以利用一个函数对每个参数进行操作，并且结合逗号表达式可以做一些别的事情。\n[ RUN      ] test_varg.expand_right_version\n1  ; \n2  ; \n3  ; \n4  ; \n0  ; \n1  ; \n2  ; \n3  ; \nhello , word , yes , 1 , 3 , 4.5 ; \n[       OK ] test_varg.expand_right_version (0 ms)\n\n\n折叠表达式\n用折叠表达式可以把一些简单的递归写的更加简洁，但是他的运算方向和人通常认为的不同：\ntemplate &lt;typename... T&gt;\nauto rsub(T... t) {\n  return (t - ...);\n}\ntemplate &lt;typename... T&gt;\nauto lsub(T... t) {\n  return (... - t);\n}\n\nTEST(test_varg, fold_expressions) {\n  cout &lt;&lt; rsub(1, 2, 3, 4, 5) &lt;&lt; endl;\n  cout &lt;&lt; lsub(1, 2, 3, 4, 5) &lt;&lt; endl;\n}\n[ RUN      ] test_varg.fold_expressions\n3\n-13\n[       OK ] test_varg.fold_expressions (0 ms)\n\n\ninteger_sequence\n利用integer_sequence我们可以展开一些数组，并对数组中的每个元素做处理，下面就给出一段编译期展开循环去计算卷积的程序：\ntemplate &lt;typename T, size_t... W&gt;\nvoid conv1xM(float& sum, T r, T k, std::index_sequence&lt;W...&gt;) {\n  ((sum += r[W] * k[W]), ...);\n}\n\ntemplate &lt;size_t Filter_W, typename T, size_t Filter_H, size_t... H&gt;\nvoid convNxM(float& sum, std::array&lt;T, Filter_H&gt;& r, std::array&lt;T, Filter_H&gt;& k,\n             std::index_sequence&lt;H...&gt;) {\n  (conv1xM(sum, r[H], k[H], std::make_index_sequence&lt;Filter_W&gt;{}), ...);\n}\n\ntemplate &lt;size_t Filter_W, typename T, size_t Filter_H&gt;\nvoid convNxM(float& sum, std::array&lt;T, Filter_H&gt;& r,\n             std::array&lt;T, Filter_H&gt;& k) {\n  convNxM&lt;Filter_W&gt;(sum, r, k, std::make_index_sequence&lt;Filter_H&gt;{});\n}\n\nTEST(test_tmp, conv) {\n  constexpr size_t K_h = 3, K_w = 6;\n  size_t I_h = 32, I_w = 64;\n  float kernel[K_h * K_w];\n  std::iota(kernel, kernel + K_h * K_w, 0);\n  float image[I_h * I_w];\n  std::iota(image, image + I_h * I_w, 0);\n  IC(image[0]);\n\n  std::array&lt;float*, K_h&gt; r{image, image + I_w, image + I_w * 2};\n  std::array&lt;float*, K_h&gt; k{kernel, kernel + K_w, kernel + K_w * 2};\n  float sum = 0.;\n  convNxM&lt;K_w&gt;(sum, r, k);\n  IC(sum);\n}\n输出\n[ RUN      ] test_tmp.conv\nic| image[0]: 0\nic| sum: 14835\n[       OK ] test_tmp.conv (0 ms)\n总之我们利用模板传递静态时期的常量，然后利用index sequence对当前的数组进行索引从而获得展开循环的加速效果。"
  },
  {
    "objectID": "posts/cpp-trick.html#右值左值",
    "href": "posts/cpp-trick.html#右值左值",
    "title": "cpp挖坑&爬坑",
    "section": "右值&左值",
    "text": "右值&左值\n\n一个问题\nvoid dosomething(vector&lt;int&gt;& arr) { arr[2] = 0; }\n\nTEST(rvalue, basic) {\n  vector&lt;int&gt; arr{1, 3, 4};\n  dosomething(arr);\n  dosomething(vector&lt;int&gt;{1, 3, 4});\n}"
  },
  {
    "objectID": "posts/cpp-trick.html#编译期通过类型执行不同的代码",
    "href": "posts/cpp-trick.html#编译期通过类型执行不同的代码",
    "title": "cpp挖坑&爬坑",
    "section": "编译期通过类型执行不同的代码",
    "text": "编译期通过类型执行不同的代码\nclass Mat {\n public:\n  float* data;\n  Mat() { data = new float[100]; }\n  ~Mat(){};\n};\n\ntemplate &lt;typename T&gt;\nvoid make_mat(T& m) {\n  if constexpr (std::is_pointer&lt;T&gt;()) {\n    IC(\"is pointer\");\n    m[0] = 100;\n  } else {\n    IC(\"is Mat\");\n    m.data[0] = 100;\n  }\n}\n\nTEST(test_tmp, integral_constant) {\n  Mat m1;\n  float* m2 = new float[10];\n  make_mat(m1);\n  make_mat(m2);\n}\n输出\n[ RUN      ] test_tmp.integral_constant\nic| std::string(\"is Mat\"): \"is Mat\"\nic| std::string(\"is pointer\"): \"is pointer\"\n[       OK ] test_tmp.integral_constant (0 ms)"
  },
  {
    "objectID": "posts/cpp-trick.html#类型保存值",
    "href": "posts/cpp-trick.html#类型保存值",
    "title": "cpp挖坑&爬坑",
    "section": "类型保存值",
    "text": "类型保存值\n这个是我写了半天才发现c++模板的套路,其实就是把一个变量看成一个类型,下面就是把这个模板参数用两种方式表示(不过我暂时还不知道如何选择这两种方式):\ntemplate &lt;uint64_t mmu_item,\n          uint64_t start_bank,\n          MMU_CONF_WIDTH width,\n          uint64_t start_depth,\n          uint64_t depth&gt;\nstruct inst_mmu_conf_warper\n{\n  using type = std::index_sequence&lt;0x12, mmu_item,\n                                   start_bank,\n                                   static_cast&lt;uint64_t&gt;(width),\n                                   start_depth,\n                                   depth&gt;;\n  constexpr static auto values = std::index_sequence&lt;0x12, mmu_item,\n                                                     start_bank,\n                                                     static_cast&lt;uint64_t&gt;(width),\n                                                     start_depth,\n                                                     depth&gt;{};\n};"
  },
  {
    "objectID": "posts/cpp-trick.html#结构体模板元函数的套路",
    "href": "posts/cpp-trick.html#结构体模板元函数的套路",
    "title": "cpp挖坑&爬坑",
    "section": "结构体模板元函数的套路",
    "text": "结构体模板元函数的套路\n其实一开始写模板看不懂就是因为c++的语法太多了,不过我们还是只需要掌握一些主要的语法就可以了.\n我主要接触到的模板主要分以下几种表示方法.\n/* 模板元函数的写法 1 */\n\n// 定义元函数的入参,这里表明这个结构体接收一个类型作为参数\ntemplate &lt;typename T&gt;\nstruct method_1 {};\n\n// 我们特化上面的那个元函数,通常特化直接写值,但是由于我们当前给的参数还依赖一个未知的`Value`,因此还需要给元函数再加一个模板类型.\ntemplate &lt;size_t Value&gt;\nstruct method_1&lt;std::integral_constant&lt;size_t, Value&gt;&gt;\n{\n  // 同时对于这个模板元的返回值也有两种方法,可以是一个静态的变量,也可以是对应的类型(此时那个类型其实也保存了值)\n  constexpr static size_t one_v = Value + 1;\n  using one_t = std::integral_constant&lt;size_t, Value + 1&gt;;\n};"
  },
  {
    "objectID": "posts/cpp-trick.html#模板元函数编写与匹配的几个套路",
    "href": "posts/cpp-trick.html#模板元函数编写与匹配的几个套路",
    "title": "cpp挖坑&爬坑",
    "section": "模板元函数编写与匹配的几个套路",
    "text": "模板元函数编写与匹配的几个套路\n在c++17之前我们可以用std::enable_if来决议这个匹配是不是有效的,首先对于同一个函数,他的返回值应该是一致的(保证我们思维的一致性),所以通过enable_if决议当前的输入类型下是否可以匹配. 下面这个例子就是把整形和array类型通过决议分开匹配,从而实现不同的assgin_to_array,不然输入的array还是会被默认匹配到第一个函数:\ntemplate &lt;typename T, size_t... Is&gt;\nconstexpr std::enable_if_t&lt;std::is_integral_v&lt;T&gt;, void&gt; assgin_to_array(std::array&lt;uint8_t, sizeof(T)&gt; &dest, const T &src, std::index_sequence&lt;Is...&gt;)\n{\n  ((dest[Is] = (src &gt;&gt; (Is * 8))), ...);\n}\n\ntemplate &lt;typename T1, typename T2, size_t N, size_t... Is&gt;\nconstexpr std::enable_if_t&lt;sizeof(T1) == sizeof(T2), void&gt; assgin_to_array(std::array&lt;T1, N&gt; &dest, const std::array&lt;T2, N&gt; &src, std::index_sequence&lt;Is...&gt;)\n{\n  copy(dest.data(), src.data(), 0, std::index_sequence&lt;Is...&gt;{});\n}\n不过现在有了constexpr if,我们可以直接在同一个函数中不同的操作,这里要注意一个小坑,就是std::is_array只能检测是不是c风格的数组,但是他不能检测std::array,所以我这里重写了一个is_std_array:\ntemplate &lt;typename T, size_t... Is&gt;\nconstexpr void assgin_to_array(std::array&lt;uint8_t, sizeof(T)&gt; &dest, const T &src, std::index_sequence&lt;Is...&gt;)\n{\n  if constexpr (std::is_integral_v&lt;T&gt;)\n  {\n    ((dest[Is] = (src &gt;&gt; (Is * 8))), ...);\n  }\n  else if constexpr (is_std_array&lt;T&gt;::value)\n  {\n    copy(dest.data(), src.data(), 0, std::index_sequence&lt;Is...&gt;{});\n  }\n}"
  },
  {
    "objectID": "posts/cpp-trick.html#tuple元素进行fold-expressions中重载操作符遇到的坑",
    "href": "posts/cpp-trick.html#tuple元素进行fold-expressions中重载操作符遇到的坑",
    "title": "cpp挖坑&爬坑",
    "section": "tuple元素进行fold expressions中重载操作符遇到的坑",
    "text": "tuple元素进行fold expressions中重载操作符遇到的坑\n我想重载+然后对tuple进行操作,但是遇到找不到重载加号的问题.最后发现这个操作必须要声明到std才有效."
  },
  {
    "objectID": "posts/cpp-trick.html#变长模板匹配的模式",
    "href": "posts/cpp-trick.html#变长模板匹配的模式",
    "title": "cpp挖坑&爬坑",
    "section": "变长模板匹配的模式",
    "text": "变长模板匹配的模式\nc++变长模板他不能匹配seq&lt;Le..., Ls&gt;这种模式,只能匹配seq&lt;Ls,Le...&gt;,也就是取第一个元素是方便的(或者前n个元素都是方便的),如果你想取最后一个元素那么就需要递归一次.\ntemplate &lt;typename T&gt;\nstruct take_head\n{\n};\n\ntemplate &lt;size_t Ls, size_t... Le&gt;\nstruct take_head&lt;seq&lt;Ls, Le...&gt;&gt;\n{\n  constexpr static size_t value = Ls;\n};\n\ntemplate &lt;typename A, typename B&gt;\nstruct take_two_head\n{\n};\n\ntemplate &lt;size_t Ls, size_t... Le, size_t Rs, size_t... Re&gt;\nstruct take_two_head&lt;seq&lt;Le..., Ls&gt;, seq&lt;Re..., Rs&gt;&gt;\n{\n  constexpr static size_t value_L = Ls;\n  constexpr static size_t value_R = Rs;\n};\n\nTEST(test, take_two_head)\n{\n  take_two_head&lt;seq&lt;0, 1, 2, 3, 4&gt;, seq&lt;5, 6&gt;&gt; two{};\n  //  auto lh = {} value;\n  ic(two.value_L);\n  ic(two.value_R);\n}"
  },
  {
    "objectID": "posts/cpp-trick.html#通过隐式转换来实现零成本抽象",
    "href": "posts/cpp-trick.html#通过隐式转换来实现零成本抽象",
    "title": "cpp挖坑&爬坑",
    "section": "通过隐式转换来实现零成本抽象",
    "text": "通过隐式转换来实现零成本抽象\n通过vector包\n\n#include &lt;algorithm&gt;\n#include &lt;array&gt;\n#include &lt;cmath&gt;\n#include &lt;cstddef&gt;\n#include &lt;cstdint&gt;\n#include &lt;cstring&gt;\n#include &lt;numeric&gt;\n#include &lt;optional&gt;\n#include &lt;span&gt;\n#include &lt;utility&gt;\n#include &lt;arm_neon.h&gt;\n#include &lt;iostream&gt;\n\ntemplate &lt;class T, size_t... Lanes&gt; struct native_vector_type;\n\ntemplate &lt;&gt; struct native_vector_type&lt;float, 32&gt; {\n    using type = float32x4_t[8];\n};\n\ntemplate &lt;&gt; struct native_vector_type&lt;float, 4&gt; {\n    using type = float32x4_t;\n};\n\n\ntemplate &lt;class T, size_t... Lanes&gt; struct vector {\n    using element_type = T;\n    using value_type = typename native_vector_type&lt;T, Lanes...&gt;::type;\n\n  private:\n    alignas(sizeof(value_type)) value_type v_;\n\n  public:\n    vector() = default;\n\n    vector(const value_type &vec) : v_(vec) {}\n\n    constexpr operator value_type() const noexcept { return v_; }\n\n    constexpr operator value_type &() noexcept { return v_; }\n\n    constexpr auto buffer() noexcept {\n        return std::span(reinterpret_cast&lt;element_type *&gt;(&v_),\n                         (1 * ... * Lanes));\n    }\n\n};\n\nint main()\n{\n    vector&lt;float,4&gt; *a = new vector&lt;float,4&gt;();\n    a-&gt;buffer()[0] = 1.0f;\n    a-&gt;buffer()[1] = 2.0f;\n    vector&lt;float,4&gt; b;\n    b.buffer()[0] = 1.0f;\n    b.buffer()[1] = 2.0f;\n    // float32x4_t d;\n    vector&lt;float,4&gt; c = *a * b;\n    std::cout &lt;&lt; c.buffer()[0] &lt;&lt; std::endl;\n    std::cout &lt;&lt; c.buffer()[1] &lt;&lt; std::endl;   \n}"
  },
  {
    "objectID": "posts/cpp-trick.html#explicit",
    "href": "posts/cpp-trick.html#explicit",
    "title": "cpp挖坑&爬坑",
    "section": "explicit",
    "text": "explicit\nexplicit是用于表示当前的构造函数不能进行隐式转换，比如如下代码：\nclass StrBlob {\n public:\n  shared_ptr&lt;vector&lt;string&gt;&gt; data;\n\n  StrBlob() : data(make_shared&lt;vector&lt;string&gt;&gt;()){};\n  StrBlob(initializer_list&lt;string&gt; il)\n      : data(make_shared&lt;vector&lt;string&gt;&gt;(il)){};\n  size_t size() { return data-&gt;size(); };\n  size_t use_count() { return data.use_count(); };\n\n  const string& front() {\n    check(0, \"front\");\n    return data-&gt;front();\n  }\n  const string& back() {\n    check(0, \"back\");\n    return data-&gt;back();\n  }\n  void push_back(const string& s) { data-&gt;push_back(s); }\n  void pop_back() {\n    check(0, \"pop_back\");\n    data-&gt;pop_back();\n  }\n\n  void combine(StrBlob sb) {\n    for (auto p = sb.data-&gt;begin(); p != sb.data-&gt;end(); p++)\n      data-&gt;push_back(*p);\n  }\n\n private:\n  void check(size_t i, const string& msg) const {\n    if (i &gt;= data-&gt;size()) { throw out_of_range(msg); }\n  }\n};\n\nTEST(test, t_12_5) {\n  StrBlob b1{\"1\", \"2\", \"3\"};\n  ic(*b1.data);\n  b1.combine({\"4\", \"5\", \"6\"});\n  ic(*b1.data);\n}\n我们在调用combine函数的时候需要的是一个StrBlob对象，但是我们实际combine的时候传入的是一个初始化列表，那么这个时候就会把隐式调用对应的构造函数，完成参数传递。这个时候如果我们给对应初始化列表的构造函数进行explicit限制，那么就会出现编译错误，要求传入一个正确的StrBlob对象。"
  },
  {
    "objectID": "posts/cpp-trick.html#构造函数",
    "href": "posts/cpp-trick.html#构造函数",
    "title": "cpp挖坑&爬坑",
    "section": "构造函数",
    "text": "构造函数\n这里有点奇怪，不强行指定initializer_list&lt;int&gt;是无法通过编译的，这太蛋疼了。\nmake_shared&lt;vector&lt;int&gt;&gt;(initializer_list&lt;int&gt;{1, 2, 3})\n同时make_shared还不支持new的方式构造shared_ptr。 ## 拷贝赋值\n他的赋值是直接把被赋值对象的内容给清空了，被赋值之后，q和 p其实都是q了，最后退出scope的时候，p的use_count会因为q的释放减少。\nTEST(test, shared_ptr_copy) {\n  auto p = make_shared&lt;int&gt;(10);\n  {\n    auto q = make_shared&lt;int&gt;(11);\n    p = q;\n    ic(*q, q.use_count());\n    ic(*p, p.use_count());\n  }\n  ic(*p, p.use_count());\n}\n[ RUN      ] test.shared_ptr_copy\nic| *q: 11, q.use_count(): 2\nic| *p: 11, p.use_count(): 2\nic| *p: 11, p.use_count(): 1\n[       OK ] test.shared_ptr_copy (0 ms)"
  },
  {
    "objectID": "posts/cpp-trick.html#c-munmap_chunk-invalid-pointer",
    "href": "posts/cpp-trick.html#c-munmap_chunk-invalid-pointer",
    "title": "cpp挖坑&爬坑",
    "section": "C++: munmap_chunk(): invalid pointer",
    "text": "C++: munmap_chunk(): invalid pointer\n\n通常这个问题应该是delete的内存不是被new出来的。\n指针运行时被修改\n指针越界（数组越界赋值了，但是当时不报错）"
  },
  {
    "objectID": "posts/cpp-trick.html#taskset",
    "href": "posts/cpp-trick.html#taskset",
    "title": "cpp挖坑&爬坑",
    "section": "taskset",
    "text": "taskset\ntaskset是用于分配cpu资源, 但是在我这个物理上是128核的机器, 使用taskset是可以设定到0-255核的, 那么说明这个taskset的逻辑和物理的逻辑不一样. 但是taskset和mpi的配合好像不是很好:\n❯ taskset -c 0-9 mpiexec -n 8 --bind-to core  python summa_3d.py\n6 6\n2 2\n4 4\n0 0\n5 5\n1 1\n7 7\n3 131\n❯ taskset -c 0-4 mpiexec -n 8 --bind-to core  python summa_3d.py\n4 4\n0 0\n5 5\n1 1\n7 7\n3 131\n6 134\n2 130"
  },
  {
    "objectID": "posts/cpp-trick.html#numactl",
    "href": "posts/cpp-trick.html#numactl",
    "title": "cpp挖坑&爬坑",
    "section": "numactl",
    "text": "numactl\nnumactl 是分配内存资源的, 可以和taskset一起使用, 先使用numactl. 是用前可以通过hardware查看自身机器状态\n❯ numactl --hardware\navailable: 2 nodes (0-1)\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191\nnode 0 size: 1019903 MB\nnode 0 free: 951010 MB\nnode 1 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255\nnode 1 size: 1032158 MB\nnode 1 free: 813857 MB\nnode distances:\nnode   0   1 \n  0:  10  32 \n  1:  32  10 \n\n❯ numactl --cpunodebind=0 -l --physcpubind=0-64 xxx"
  },
  {
    "objectID": "posts/cpp-trick.html#mpi指定拓扑",
    "href": "posts/cpp-trick.html#mpi指定拓扑",
    "title": "cpp挖坑&爬坑",
    "section": "mpi指定拓扑",
    "text": "mpi指定拓扑\n这里我使用的是mpich的mpiexec, 他提供了--bind-to和--map-by两个参数, 最简单的方式是使用user的方式来设定\n❯ mpiexec -n 8 --bind-to user:0,1,2,3,3,2,1,0  python summa_3d.py\n6 1\n5 2\n1 1\n2 2\n4 3\n7 0\n0 0\n3 3"
  },
  {
    "objectID": "posts/constraints-solver-internals.html",
    "href": "posts/constraints-solver-internals.html",
    "title": "Constraints Solver Internals",
    "section": "",
    "text": "关于ortools中Constraints Solver的内部逻辑.\n\n首先是数据结构:\n@startuml\n\nclass Decision {\n  &lt;color:green&gt; 表示在搜索树中的一个选择点, Apply向右, Refute向左. &lt;/color&gt;\n  + void {abstract} Apply(Solver* s)&lt;color:green&gt; // 决策执行时调用 &lt;/color&gt;\n  + void {abstract} Refute(Solver* s) &lt;color:green&gt; // 回溯后调用 &lt;/color&gt;\n  + void {abstract} Accept(DecisionVisitor* visitor)\n}\n\nclass Solver {\n  + DefaultSolverParameters() ConstraintSolverParameters\n  + AddConstraint(Constraint*)\n  + Solve(DecisionBuilder*, vector&lt;SearchMonitor*&gt;)\n}\n\nclass DecisionBuilder {\n  &lt;color:green&gt; 构造搜索树 &lt;/color&gt;\n  + Decision* Next(Solver*)  &lt;color:green&gt; 返回下一步决策, 如果为空则结束. &lt;/color&gt;\n  + void AppendMonitors(Solver*, vector&lt;SearchMonitor*&gt;*) &lt;color:green&gt; 搜索开始前添加monitor. &lt;/color&gt;\n  + void Accept(ModelVisitor*)\n}\n\nclass RestoreAssignment {\n  &lt;color:green&gt; 只执行一次, 将Assignment全部恢复到Var &lt;/color&gt;\n}\n\nDecisionBuilder &lt;|-- RestoreAssignment\n\nclass StoreAssignment {\n  &lt;color:green&gt; 只执行一次, 将Var当前值存储到Assignment &lt;/color&gt;\n}\n\nDecisionBuilder &lt;|-- StoreAssignment\n\nDecisionBuilder *--- Decision\n\nclass Demon {\n  &lt;color:green&gt; 传播队列的基础元素, 负责在变量值域中传播变量的约束以及删除不满足约束的值 &lt;/color&gt;\n  &lt;color:green&gt; 主要思想是附加到变量上监听变量的修改. &lt;/color&gt;\n  + void {abstract} Run(Solver* s) &lt;color:green&gt; //  &lt;/color&gt;\n  + DemonPriority {abstract} priority() &lt;color:green&gt; // 执行的优先级 &lt;/color&gt;\n  + void inhibit(Solver* s) &lt;color:green&gt; // 在当前位置抑制demon &lt;/color&gt;\n  + void desinhibit(Solver* s) &lt;color:green&gt; // 解除之前的抑制 &lt;/color&gt;\n}\n\nclass PropagationBaseObject {\n  + Solver* solver() \n  + void FreezeQueue()\n  + void UnfreezeQueue()\n  + void EnqueueDelayedDemon(Demon* const d)\n  + void EnqueueVar(Demon* const d)\n  + void ExecuteAll(const SimpleRevFIFO&lt;Demon*&gt;& demons)\n  + void EnqueueAll(const SimpleRevFIFO&lt;Demon*&gt;& demons)\n}\n\nabstract class Constraint {\n  &lt;color:green&gt; 建模约束 &lt;/color&gt;\n  + void {abstract} Post() &lt;color:green&gt; // 在solve过程中将demon附加到var上. &lt;/color&gt;\n  + void {abstract} InitialPropagate() &lt;color:green&gt; // 初始化传播, 在Post之后调用 &lt;/color&gt;\n  void PostAndPropagate() &lt;color:green&gt; // 在root节点是同时做两件事 &lt;/color&gt;\n  + void {abstract} Accept(ModelVisitor* visitor)\n  bool IsCastConstraint() &lt;color:green&gt; // 是否是cast到integer &lt;/color&gt;\n  + IntVar*{abstract} Var(); &lt;color:green&gt; // 返回代表约束满足的bool var, 如果为Null表示不支持 &lt;/color&gt;\n} \n\nPropagationBaseObject &lt;|-- Constraint\n\nclass CastConstraint {\n  &lt;color:green&gt; 特殊的约束 &lt;/color&gt;\n  + IntVar* target_var() \n}\n\nConstraint &lt;|-- CastConstraint\n\nclass IntExpr {\n  &lt;color:green&gt; 整数表达式, 建模的基础, 可以进行如下操作 &lt;/color&gt;\n  &lt;color:green&gt;  1. 查询边界值 &lt;/color&gt;\n  &lt;color:green&gt;  2. 设定边界值 &lt;/color&gt;\n  &lt;color:green&gt;  3. 监听边界值改变 &lt;/color&gt;\n  &lt;color:green&gt;  4. 构造对应var &lt;/color&gt;\n  + int64_t  {abstract} Min();\n  + void  {abstract} SetMin(int64_t m);\n  + int64_t  {abstract} Max();\n  + void  {abstract} SetMax(int64_t m);\n  + void  {abstract} Range(int64_t* l, int64_t* u);\n  + void  {abstract} SetRange(int64_t l, int64_t u);\n  + void  {abstract} SetValue(int64_t v);\n  + bool  {abstract} Bound();\n  + bool  {abstract} IsVar();\n  + IntVar* {abstract}  Var();\n  + void  {abstract} WhenRange(Demon* d);\n}\n\nPropagationBaseObject &lt;|-- IntExpr\n\nclass IntVar {\n  &lt;color:green&gt; Var也是表达式 &lt;/color&gt;\n  + int64_t {abstract} Value()\n  + void {abstract} RemoveValue(int64_t v)\n  + void {abstract} RemoveInterval(int64_t l, int64_t u)\n  + void {abstract} RemoveValues(const std::vector&lt;int64_t&gt;& values);\n  + void {abstract} SetValues(const std::vector&lt;int64_t&gt;& values);\n  + void {abstract} WhenBound(Demon* d)\n  + void WhenBound(Solver::Closure closure)\n  + void WhenBound(Solver::Action action)\n  + void {abstract} WhenDomain(Demon* d)\n  + void WhenDomain(Solver::Closure closure)\n  + void WhenDomain(Solver::Action action)\n  + uint64_t {abstract} Size()\n  + bool {abstract} Contains(int64_t v)\n  + IntVarIterator*{abstract}  MakeHoleIterator(bool reversible)\n  + IntVarIterator*{abstract}  MakeDomainIterator(bool reversible)\n  + int64_t {abstract} OldMin()\n  + int64_t {abstract} OldMax()\n  + int {abstract} VarType() const;\n  + IntVar* {abstract} IsEqual(int64_t constant)\n  + IntVar* {abstract} IsDifferent(int64_t constant)\n  + IntVar* {abstract} IsGreaterOrEqual(int64_t constant)\n  + IntVar* {abstract} IsLessOrEqual(int64_t constant)\n  + int index()\n}\n\nIntExpr &lt;|-- IntVar\n\n\nabstract class SearchMonitor {\n  + void {abstract} EnterSearch()\n  + void {abstract} RestartSearch()\n  + void {abstract} ExitSearch()\n  + void {abstract} BeginNextDecision(DecisionBuilder* b)\n  + void {abstract} EndNextDecision(DecisionBuilder* b, Decision* d)\n  + void {abstract} ApplyDecision(Decision* d)\n  + void {abstract} RefuteDecision(Decision* d)\n  + void {abstract} AfterDecision(Decision* d, bool apply)\n  + void {abstract} BeginFail()\n  + void {abstract} EndFail()\n  + void {abstract} BeginInitialPropagation()\n  + void {abstract} EndInitialPropagation()\n  + bool {abstract} AcceptSolution()\n  + bool {abstract} AtSolution()\n  + void {abstract} NoMoreSolutions()\n  + bool {abstract} LocalOptimum()\n  + bool {abstract} AcceptDelta(Assignment* delta, Assignment* deltadelta)\n  + void {abstract} AcceptNeighbor()\n  + void {abstract} AcceptUncheckedNeighbor()\n  + bool {abstract} IsUncheckedSolutionLimitReached(\n  + void {abstract} PeriodicCheck()\n  + int {abstract} ProgressPercent()\n  + void {abstract} Accept(ModelVisitor* visitor)\n  + void {abstract} Install() &lt;b&gt;&lt;color:green&gt; // 注册自身&lt;/color&gt; \n}\n\nclass SolutionCollector {\n  &lt;color:green&gt; 收集添加过的变量 &lt;/color&gt;\n  + void Add(IntVar* var)\n  + void Add(const std::vector&lt;IntVar*&gt;& vars)\n  + void Add(IntervalVar* var)\n  + void Add(const std::vector&lt;IntervalVar*&gt;& vars)\n  + void Add(SequenceVar* var)\n  + void Add(const std::vector&lt;SequenceVar*&gt;& vars)\n  + void AddObjective(IntVar* objective)\n  + void AddObjectives(const std::vector&lt;IntVar*&gt;& objectives)\n}\n\nSearchMonitor &lt;|-- SolutionCollector\n\nclass ObjectiveMonitor {\n  &lt;color:green&gt; 优化目标监控基类 &lt;/color&gt;\n  + IntVar* ObjectiveVar(int index)\n  + IntVar* MinimizationVar(int index)\n  + int64_t Step(int index)\n  + bool Maximize(int index)\n  + int64_t BestValue(int index)\n  + int Size()\n  + void EnterSearch()\n  + bool AtSolution()\n  + bool AcceptDelta(Assignment* delta, Assignment* deltadelta)\n  + void Accept(ModelVisitor* visitor)\n}\n\nSearchMonitor &lt;|-- ObjectiveMonitor\n\nclass OptimizeVar {\n  &lt;color:green&gt; 概括目标, 指定方向, 变量, 步幅即可 &lt;/color&gt;\n  + int64_t best()\n  + IntVar* var()\n  + void BeginNextDecision(DecisionBuilder* db)\n  + void RefuteDecision(Decision* d)\n  + bool AtSolution()\n  + bool AcceptSolution()\n  + void ApplyBound()\n}\n\nObjectiveMonitor &lt;|-- OptimizeVar\n\n\nclass SearchLimit {\n  + void EnterSearch() \n  + void BeginNextDecision(DecisionBuilder* b) \n  + void PeriodicCheck() \n  + void RefuteDecision(Decision* d) \n  + void Install()\n  + bool crossed() &lt;b&gt;&lt;color:green&gt; // 检查limit是否已经失败 &lt;/color&gt;  \n  + bool Check() &lt;b&gt;&lt;color:green&gt; // 检查limit状态 &lt;/color&gt;  \n  + bool {abstract} CheckWithOffset(absl::Duration offset) &lt;b&gt;&lt;color:green&gt; // 自定义检查方法 &lt;/color&gt;  \n}\n\nSearchMonitor &lt;|-- SearchLimit\n\nclass RegularLimit {\n  &lt;color:green&gt; 基于搜索时间/探索分支数/失败错误来限制搜索 &lt;/color&gt;\n  + bool CheckWithOffset(absl::Duration offset)\n} \n\nclass ImprovementSearchLimit {\n  &lt;color:green&gt;基于目标变量的改善率或者词典序进行限制.&lt;/color&gt;\n  + int64_t wall_time()\n  + int64_t branches()\n  + int64_t failures()\n  + int64_t solutions()\n  + bool CheckWithOffset(absl::Duration offset)\n} \n\nSearchLimit &lt;|-- RegularLimit\nSearchLimit &lt;|-- ImprovementSearchLimit\n\n\nclass IntervalVar {\n  &lt;color:green&gt;包含duration的var.&lt;/color&gt;\n}\n\nPropagationBaseObject &lt;|-- IntervalVar\n\nclass SequenceVar {\n  &lt;color:green&gt;包含多个IntervalVar.&lt;/color&gt;\n  + IntervalVar* Interval(int index)\n}\n\nPropagationBaseObject &lt;|-- SequenceVar\n\nclass AssignmentElement {\n\n}\nclass IntVarElement {\n\n}\n\nAssignmentElement &lt;|-- IntVarElement\n\nclass IntervalVarElement {\n\n}\n\nAssignmentElement &lt;|-- IntervalVarElement\n\nclass SequenceVarElement {\n\n}\n\nAssignmentElement &lt;|-- SequenceVarElement\n\nclass Assignment {\n  &lt;color:green&gt; 包含domain到var的映射, 用于展示solution.&lt;/color&gt;\n  + void Clear();\n  + bool Empty()\n  + int Size()\n  + int NumIntVars()\n  + int NumIntervalVars()\n  + int NumSequenceVars()\n  + void Store()\n  + void Restore()\n  + bool Load(const std::string& filename)\n}\n\nPropagationBaseObject &lt;|-- Assignment\n\nclass Pack {\n  &lt;color:green&gt; 多个var映射到bin的约束. &lt;/color&gt;\n  + void AddWeightedSumLessOrEqualConstantDimension(const std::vector&lt;int64_t&gt;& weights, const std::vector&lt;int64_t&gt;& bounds);\n  + void AddWeightedSumLessOrEqualConstantDimension(Solver::IndexEvaluator1 weights, const std::vector&lt;int64_t&gt;& bounds);\n  + void AddWeightedSumLessOrEqualConstantDimension(Solver::IndexEvaluator2 weights, const std::vector&lt;int64_t&gt;& bounds);\n  + void AddWeightedSumEqualVarDimension(const std::vector&lt;int64_t&gt;& weights,const std::vector&lt;IntVar*&gt;& loads);\n  + void AddWeightedSumEqualVarDimension(Solver::IndexEvaluator2 weights,const std::vector&lt;IntVar*&gt;& loads);\n  + void AddSumVariableWeightsLessOrEqualConstantDimension(const std::vector&lt;IntVar*&gt;& usage, const std::vector&lt;int64_t&gt;& capacity);\n  + void AddWeightedSumOfAssignedDimension(const std::vector&lt;int64_t&gt;& weights,IntVar* cost_var);\n  + void AddCountUsedBinDimension(IntVar* count_var);\n  + void AddCountAssignedItemsDimension(IntVar* count_var);\n}\n\nConstraint &lt;|-- Pack\n\n\nConstraint &lt;|-- DisjunctiveConstraint\n\n\n@enduml\n其中整个优化模型由IntExpr和Constraints构成. 构建好约束后通过 DecisionBuilder来生成Decision, 每个Decision会给变量进行分配值, 分配好值\n@startuml\nNextSolution -&gt; BeginInitialPropagation\nNextSolution &lt;- BeginInitialPropagation\nNextSolution -&gt; Next : DecisionBuilder.Next() \nNext -&gt; Apply : Decision.Apply() \nApply -&gt; SetValue  : Decision.Apply() \nSetValue -&gt; EnqueueVar : EnqueueVar the Demon Handler\nEnqueueVar -&gt; Queue.Process : when freeze_level_ == 0\nQueue.Process -&gt; Demon.Run : pop the enqueued demons and run it\nDemon.Run -&gt; Var.ExecuteAll : exec the the constraints\n@enduml"
  },
  {
    "objectID": "posts/cmakestm32.html",
    "href": "posts/cmakestm32.html",
    "title": "cmake构建stm32工程",
    "section": "",
    "text": "用习惯了cmake，cmake的编译输出比makefile好看许多。对于stm32cubemx生成的makefile工程，我是否可以转换成cmake的工程呢？\n\n\n生成makefile\n我首先使用stm32cubemx生成了一个点亮led的工程。该工程的makefile如下\n##########################################################################################################################\n# File automatically-generated by tool: [projectgenerator] version: [2.29.2] date: [Mon Aug 13 11:26:30 CST 2018]\n##########################################################################################################################\n\n# ------------------------------------------------\n# Generic Makefile (based on gcc)\n#\n# ChangeLog :\n#   2017-02-10 - Several enhancements + project update mode\n#   2015-07-22 - first version\n# ------------------------------------------------\n\n######################################\n# target\n######################################\nTARGET = 431test\n\n\n######################################\n# building variables\n######################################\n# debug build?\nDEBUG = 1\n# optimization\nOPT = -Og\n\n\n#######################################\n# paths\n#######################################\n# Build path\nBUILD_DIR = build\n\n######################################\n# source\n######################################\n# C sources\nC_SOURCES =  \\\nSrc/main.c \\\nSrc/gpio.c \\\nSrc/stm32l4xx_it.c \\\nSrc/stm32l4xx_hal_msp.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_tim.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_tim_ex.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_i2c.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_i2c_ex.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_rcc.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_rcc_ex.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash_ex.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash_ramfunc.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_gpio.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_dma.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_dma_ex.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_pwr.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_pwr_ex.c \\\nDrivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c \\\nSrc/system_stm32l4xx.c  \n\n# ASM sources\nASM_SOURCES =  \\\nstartup_stm32l431xx.s\n\n\n#######################################\n# binaries\n#######################################\nPREFIX = arm-none-eabi-\n# The gcc compiler bin path can be either defined in make command via GCC_PATH variable (&gt; make GCC_PATH=xxx)\n# either it can be added to the PATH environment variable.\nifdef GCC_PATH\nCC = $(GCC_PATH)/$(PREFIX)gcc\nAS = $(GCC_PATH)/$(PREFIX)gcc -x assembler-with-cpp\nCP = $(GCC_PATH)/$(PREFIX)objcopy\nSZ = $(GCC_PATH)/$(PREFIX)size\nelse\nCC = $(PREFIX)gcc\nAS = $(PREFIX)gcc -x assembler-with-cpp\nCP = $(PREFIX)objcopy\nSZ = $(PREFIX)size\nendif\nHEX = $(CP) -O ihex\nBIN = $(CP) -O binary -S\n \n#######################################\n# CFLAGS\n#######################################\n# cpu\nCPU = -mcpu=cortex-m4\n\n# fpu\nFPU = -mfpu=fpv4-sp-d16\n\n# float-abi\nFLOAT-ABI = -mfloat-abi=hard\n\n# mcu\nMCU = $(CPU) -mthumb $(FPU) $(FLOAT-ABI)\n\n# macros for gcc\n# AS defines\nAS_DEFS = \n\n# C defines\nC_DEFS =  \\\n-DUSE_HAL_DRIVER \\\n-DSTM32L431xx\n\n\n# AS includes\nAS_INCLUDES = \n\n# C includes\nC_INCLUDES =  \\\n-IInc \\\n-IDrivers/STM32L4xx_HAL_Driver/Inc \\\n-IDrivers/STM32L4xx_HAL_Driver/Inc/Legacy \\\n-IDrivers/CMSIS/Device/ST/STM32L4xx/Include \\\n-IDrivers/CMSIS/Include\n\n\n# compile gcc flags\nASFLAGS = $(MCU) $(AS_DEFS) $(AS_INCLUDES) $(OPT) -Wall -fdata-sections -ffunction-sections\n\nCFLAGS = $(MCU) $(C_DEFS) $(C_INCLUDES) $(OPT) -Wall -fdata-sections -ffunction-sections\n\nifeq ($(DEBUG), 1)\nCFLAGS += -g -gdwarf-2\nendif\n\n\n# Generate dependency information\nCFLAGS += -MMD -MP -MF\"$(@:%.o=%.d)\"\n\n\n#######################################\n# LDFLAGS\n#######################################\n# link script\nLDSCRIPT = STM32L431RBTx_FLASH.ld\n\n# libraries\nLIBS = -lc -lm -lnosys \nLIBDIR = \nLDFLAGS = $(MCU) -specs=nano.specs -T$(LDSCRIPT) $(LIBDIR) $(LIBS) -Wl,-Map=$(BUILD_DIR)/$(TARGET).map,--cref -Wl,--gc-sections\n\n# default action: build all\nall: $(BUILD_DIR)/$(TARGET).elf $(BUILD_DIR)/$(TARGET).hex $(BUILD_DIR)/$(TARGET).bin\n\n\n#######################################\n# build the application\n#######################################\n# list of objects\nOBJECTS = $(addprefix $(BUILD_DIR)/,$(notdir $(C_SOURCES:.c=.o)))\nvpath %.c $(sort $(dir $(C_SOURCES)))\n# list of ASM program objects\nOBJECTS += $(addprefix $(BUILD_DIR)/,$(notdir $(ASM_SOURCES:.s=.o)))\nvpath %.s $(sort $(dir $(ASM_SOURCES)))\n\n$(BUILD_DIR)/%.o: %.c Makefile | $(BUILD_DIR) \n    $(CC) -c $(CFLAGS) -Wa,-a,-ad,-alms=$(BUILD_DIR)/$(notdir $(&lt;:.c=.lst)) $&lt; -o $@\n\n$(BUILD_DIR)/%.o: %.s Makefile | $(BUILD_DIR)\n    $(AS) -c $(CFLAGS) $&lt; -o $@\n\n$(BUILD_DIR)/$(TARGET).elf: $(OBJECTS) Makefile\n    $(CC) $(OBJECTS) $(LDFLAGS) -o $@\n    $(SZ) $@\n\n$(BUILD_DIR)/%.hex: $(BUILD_DIR)/%.elf | $(BUILD_DIR)\n    $(HEX) $&lt; $@\n    \n$(BUILD_DIR)/%.bin: $(BUILD_DIR)/%.elf | $(BUILD_DIR)\n    $(BIN) $&lt; $@    \n    \n$(BUILD_DIR):\n    mkdir $@        \n\n#######################################\n# clean up\n#######################################\nclean:\n    -rm -fR $(BUILD_DIR)\n  \n#######################################\n# dependencies\n#######################################\n-include $(wildcard $(BUILD_DIR)/*.d)\n\n# *** EOF ***\n\n\n移植cmake\n我首先安装他的makefile来移植出来的是这样：\n# CMAKE最小版本\ncmake_minimum_required (VERSION 2.6)\n# 设置工程名称\nproject (431test)\n# 设置可执行文件名称\nset(MY_TARGET 431test)\n\n\n######################################\n# 构建变量\n######################################\n# debug build?\nset(DEBUG  \"1\") \n# optimization\nset(OPT  \"-Og\")\nset(CMAKE_VERBOSE_MAKEFILE ON)\n\n\n\n#######################################\n# 设置编译器\n#######################################\n# 开启汇编\nENABLE_LANGUAGE(ASM C CXX)\n\n# 设置目标平台系统\nset(CMAKE_SYSTEM_NAME Generic)\nset(CMAKE_SYSTEM_PROCESSOR arm)\n\n\n# 设置交叉编译器\nset(CMAKE_C_COMPILER \"/opt/gccStm32/bin/arm-none-eabi-gcc\")\nset(CMAKE_ASM_COMPILER \"/opt/gccStm32/bin/arm-none-eabi-gcc\")\nset(CMAKE_CXX_COMPILER \"/opt/gccStm32/bin/arm-none-eabi-g++\")\nSET(CMAKE_FIND_ROOT_PATH \"/opt/gccStm32/\")\nSET(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)\nSET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)\nSET(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)\n\n\n\n\n\n\n\n\n#######################################\n# 设置文件目录\n#######################################\n\n\n#设置执行文件输出目录\nset(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/bin)\n#设置库输出路径\nset(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/lib)\n\nmessage(\"++++++++++++++Start Build+++++++++++++++++\")\n\n# 添加头文件目录\ninclude_directories(${PROJECT_SOURCE_DIR}/Inc)\ninclude_directories(${PROJECT_SOURCE_DIR}/Drivers/STM32L4xx_HAL_Driver/Inc)\ninclude_directories(${PROJECT_SOURCE_DIR}/Drivers/STM32L4xx_HAL_Driver/Inc/Legacy)\ninclude_directories(${PROJECT_SOURCE_DIR}/Drivers/CMSIS/Device/ST/STM32L4xx/Include)\ninclude_directories(${PROJECT_SOURCE_DIR}/Drivers/CMSIS/Include)\n\n\n\n# 添加源文件目录\naux_source_directory(${PROJECT_SOURCE_DIR}/Src USRSRC)\naux_source_directory(${PROJECT_SOURCE_DIR}/Drivers/STM32L4xx_HAL_Driver/Src HALSRC)\naux_source_directory(${PROJECT_SOURCE_DIR} ASMSRC)\n\n\n\n\n\n#######################################\n# 设置CFLAGS\n#######################################\n# cpu\nset(CPU  \"-mcpu=cortex-m4\")\n# fpu\nset(FPU  \"-mfpu=fpv4-sp-d16\")\n# float-abi\nset(FLOAT-ABI  \"-mfloat-abi=hard\")\n# mcu\nset(MCU  \"$(CPU) -mthumb  $(FPU) $(FLOAT-ABI)\")\n\nADD_DEFINITIONS(-DUSE_HAL_DRIVER -DSTM32L431xx)\nset(CMAKE_C_EXTENSIONS \"$(MCU) -fno-builtin $(OPT)  -Wall -std=gnu99 -fdata-sections -ffunction-sections\")\nset(CMAKE_CXX_EXTENSIONS \"$(MCU) -fno-builtin $(OPT) -Wall -std=c++11 -fdata-sections -ffunction-sections\")\nset(CMAKE_ASM_FLAGS \"$(MCU) -x assembler-with-cpp\")\n\n\n\n\n#######################################\n# 设置编译链接\n#######################################\n\n# 添加动态库\nlink_directories(\"/opt/gccStm32/\")\nset(LIBS \"-lc -lm -lnosys\" )\n\n# 添加link script\nset(LDSCRIPT \"STM32L431RBTx_FLASH.ld\")\n# 这是LDFLAGS\nset(CMAKE_EXE_LINKER_FLAGS \"$(MCU) -specs=nano.specs -T$(LDSCRIPT) -Wl,--gc-sections\")\nset(CMAKE_MODULE_LINKER_FLAGS \"$(MCU) \" )\nset(CMAKE_SHARED_LINKER_FLAGS \"$(MCU) \" )\n\n\n\n# 添加可执行文件（可执行文件名 [配置] 源文件）\nadd_executable(${MY_TARGET} ${USRSRC} ${HALSRC} ${ASMSRC} )\n\n\n# 执行文件链接属性\nset_target_properties(${MY_TARGET} PROPERTIES LINK_DEPENDS ${LDSCRIPT})\ntarget_link_libraries(${MY_TARGET} ${LIBS})\n\n\n问题 1\n我第一次尝试构建：\n➜  431test cd build\n➜  build cmake ..\n-- The C compiler identification is GNU 7.2.0\n-- The CXX compiler identification is GNU 7.2.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- The ASM compiler identification is GNU\n-- Found assembler: /opt/gccStm32/bin/arm-none-eabi-gcc\n++++++++++++++Start Build+++++++++++++++++\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/zqh/Program/Stm32/431test/build\n➜  build make\nScanning dependencies of target 431test\n[  4%] Building C object CMakeFiles/431test.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c.o\n/tmp/ccqCkW1b.s: Assembler messages:\n/tmp/ccqCkW1b.s:482: Error: selected processor does not support `dsb 0xF' in ARM mode\n/tmp/ccqCkW1b.s:495: Error: selected processor does not support `dsb 0xF' in ARM mode\n/tmp/ccqCkW1b.s:898: Error: selected processor does not support `dmb 0xF' in ARM mode\n/tmp/ccqCkW1b.s:944: Error: selected processor does not support `dsb 0xF' in ARM mode\n/tmp/ccqCkW1b.s:947: Error: selected processor does not support `isb 0xF' in ARM mode\nCMakeFiles/431test.dir/build.make:206: recipe for target 'CMakeFiles/431test.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c.o' failed\nmake[2]: *** [CMakeFiles/431test.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c.o] Error 1\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/431test.dir/all' failed\nmake[1]: *** [CMakeFiles/431test.dir/all] Error 2\nMakefile:83: recipe for target 'all' failed\nmake: *** [all] Error 2\n可以看到编译出错在：\nError: selected processor does not support `dsb 0xF' in ARM mode\n说明汇编器没有起作用，我看到上面cmake的配置过程中提示：\n-- The ASM compiler identification is GNU\n-- Found assembler: /opt/gccStm32/bin/arm-none-eabi-gcc\n肯定是这一行出现了问题。\n通过观察原makefile，发现其汇编器的定义是：AS = $(GCC_PATH)/$(PREFIX)gcc -x assembler-with-cpp说明这里的汇编器不是另一个可执行文件，而是arm-none-eabi-gcc添加命令得到。所以需要对CMakeLists.txt做一些修改。\n\n\n问题1解决\n发现在CMakeLists.txt第90和91行\nset(CMAKE_C_EXTENSIONS \"$(MCU) -fno-builtin $(OPT)  -Wall -std=gnu99 -fdata-sections -ffunction-sections\")\nset(CMAKE_CXX_EXTENSIONS \"$(MCU) -fno-builtin $(OPT) -Wall -std=c++11 -fdata-sections -ffunction-sections\")\n应该把CMAKE_C_EXTENSIONS改成CMAKE_C_FLAGS。。。。。这个错误让我非常蛋疼啊\n\n\n问题2\n修改后运行编译还是出现问题：\n/opt/gccStm32/bin/arm-none-eabi-gcc -DSTM32L431xx -DUSE_HAL_DRIVER -I/home/zqh/Program/Stm32/431test/Inc -I/home/zqh/Program/Stm32/431test/Drivers/STM32L4xx_HAL_Driver/Inc -I/home/zqh/Program/Stm32/431test/Drivers/STM32L4xx_HAL_Driver/Inc/Legacy -I/home/zqh/Program/Stm32/431test/Drivers/CMSIS/Device/ST/STM32L4xx/Include -I/home/zqh/Program/Stm32/431test/Drivers/CMSIS/Include   -fno-builtin   -Wall -std=gnu99 -fdata-sections -ffunction-sections   -o CMakeFiles/431test.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c.o   -c /home/zqh/Program/Stm32/431test/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c\n/tmp/ccxPxBMi.s: Assembler messages:\n/tmp/ccxPxBMi.s:494: Error: selected processor does not support `dsb 0xF' in ARM mode\n观察后发现是$(MCU)这个变量没有起作用。原来是因为cmake的变量和makefile是不同的！要使用${MCU}才可以！\n\n\n问题3\n终于编译到最后了，但是还是有一个问题\n/opt/gccStm32/bin/arm-none-eabi-gcc -mcpu=cortex-m4 -mthumb  -mfpu=fpv4-sp-d16 -mfloat-abi=hard -fno-builtin -Og  -Wall -std=gnu99 -fdata-sections -ffunction-sections  -mcpu=cortex-m4 -mthumb  -mfpu=fpv4-sp-d16 -mfloat-abi=hard -specs=nano.specs -TSTM32L431RBTx_FLASH.ld -Wl,--gc-sections -rdynamic CMakeFiles/431test.elf.dir/Src/gpio.c.o CMakeFiles/431test.elf.dir/Src/main.c.o CMakeFiles/431test.elf.dir/Src/stm32l4xx_hal_msp.c.o CMakeFiles/431test.elf.dir/Src/stm32l4xx_it.c.o CMakeFiles/431test.elf.dir/Src/system_stm32l4xx.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_dma.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_dma_ex.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash_ex.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash_ramfunc.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_gpio.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_i2c.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_i2c_ex.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_pwr.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_pwr_ex.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_rcc.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_rcc_ex.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_tim.c.o CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_tim_ex.c.o  -o ../bin/431test.elf\narm-none-eabi-gcc: error: unrecognized command line option '-rdynamic'\nCMakeFiles/431test.elf.dir/build.make:617: recipe for target '../bin/431test.elf' failed\nmake[2]: *** [../bin/431test.elf] Error 1\nmake[2]: Leaving directory '/home/zqh/Program/Stm32/431test/build'\nCMakeFiles/Makefile2:70: recipe for target 'CMakeFiles/431test.elf.dir/all' failed\nmake[1]: *** [CMakeFiles/431test.elf.dir/all] Error 2\nmake[1]: Leaving directory '/home/zqh/Program/Stm32/431test/build'\nMakefile:86: recipe for target 'all' failed\nmake: *** [all] Error 2\n他说没有这个命令-rdynamic，但是我发现我没有输入这个命令啊.经过一番寻找，终于找到解决方案： 在CMakeLists.txt之前加上：\n# 取消-rdynamic的错误\nset(CMAKE_SHARED_LIBRARY_LINK_C_FLAGS \"\")\nset(CMAKE_SHARED_LIBRARY_LINK_CXX_FLAGS \"\")\n\n\n文件格式转换\n这个时候我们就可以成功生成431test.elf文件，但是我们要烧录到板子里面还是需要bin文件或者hex文件，所以需要格式转换 在CMakeLists.txt最后添加如下\n# 将elf文件转hex和bin\nadd_custom_command(\n    TARGET  ${MY_TARGET}.elf  #当 ${MY_TARGET}.elf被重新生成是执行以下命令\n    COMMAND ${CMAKE_OBJCOPY} -O ihex ${MY_TARGET}.elf ${MY_TARGET}.hex)\nadd_custom_command(\n    TARGET  ${MY_TARGET}.elf\n    POST_BUILD COMMAND ${CMAKE_OBJCOPY} -O binary ${MY_TARGET}.elf ${MY_TARGET}.bin)\n\n# 显示代码段大小\nadd_custom_command(\n    TARGET  ${MY_TARGET}.elf  #当 ${MY_TARGET}.elf被重新生成是执行以下命令\n    POST_BUILD COMMAND ${CMAKE_SIZE}  ${MY_TARGET}.elf )\n\n\n效果\n进入我的工程中\n➜  431test cd build\n➜  build cmake ..\n-- The C compiler identification is GNU 7.2.0\n-- The CXX compiler identification is GNU 7.2.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n++++++++++++++Start Build+++++++++++++++++\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/zqh/Program/Stm32/431test/build\n➜  build make\nScanning dependencies of target 431test.elf\n[  4%] Building C object CMakeFiles/431test.elf.dir/Src/gpio.c.o\n[  8%] Building C object CMakeFiles/431test.elf.dir/Src/main.c.o\n[ 13%] Building C object CMakeFiles/431test.elf.dir/Src/stm32l4xx_hal_msp.c.o\n[ 17%] Building C object CMakeFiles/431test.elf.dir/Src/stm32l4xx_it.c.o\n[ 21%] Building C object CMakeFiles/431test.elf.dir/Src/system_stm32l4xx.c.o\n[ 26%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal.c.o\n[ 30%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_cortex.c.o\n[ 34%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_dma.c.o\n[ 39%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_dma_ex.c.o\n[ 43%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash.c.o\n[ 47%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash_ex.c.o\n[ 52%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_flash_ramfunc.c.o\n[ 56%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_gpio.c.o\n[ 60%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_i2c.c.o\n[ 65%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_i2c_ex.c.o\n[ 69%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_pwr.c.o\n[ 73%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_pwr_ex.c.o\n[ 78%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_rcc.c.o\n[ 82%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_rcc_ex.c.o\n[ 86%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_tim.c.o\n[ 91%] Building C object CMakeFiles/431test.elf.dir/Drivers/STM32L4xx_HAL_Driver/Src/stm32l4xx_hal_tim_ex.c.o\n[ 95%] Building ASM object CMakeFiles/431test.elf.dir/startup_stm32l431xx.s.o\n[100%] Linking C executable 431test.elf\n   text    data     bss     dec     hex filename\n   4584      24    1568    6176    1820 431test.elf\n[100%] Built target 431test.elf\n完全ojbk\n\n\nCMakeLists.txt\n放出完整文档：\n# CMAKE最小版本\ncmake_minimum_required (VERSION 2.6)\n# 设置工程名称\nproject (431test)\n# 设置可执行文件名称\nset(MY_TARGET 431test)\n\n\n######################################\n# 构建变量\n######################################\n# debug build?\nset(DEBUG  \"1\") \n# optimization\nset(OPT  \"-Og\")\n# 开启详细输出\n# set(CMAKE_VERBOSE_MAKEFILE ON)\n\n\n\n#######################################\n# 设置编译器\n#######################################\n# 开启汇编\nENABLE_LANGUAGE(ASM)\n\n# 设置目标平台系统\nset(CMAKE_SYSTEM Generic)\nset(CMAKE_SYSTEM_PROCESSOR arm)\n\n\n# 设置交叉编译器\n# set(CMAKE_TRY_COMPILE_TARGET_TYPE STATIC_LIBRARY)\nset(CMAKE_C_COMPILER \"/opt/gccStm32/bin/arm-none-eabi-gcc\")\nset(CMAKE_ASM_COMPILER \"/opt/gccStm32/bin/arm-none-eabi-gcc\")\nset(CMAKE_CXX_COMPILER \"/opt/gccStm32/bin/arm-none-eabi-g++\")\nset(CMAKE_OBJCOPY \"/opt/gccStm32/bin/arm-none-eabi-objcopy\")\nset(CMAKE_OBJDUMP \"/opt/gccStm32/bin/arm-none-eabi-objdump\" )\nset(CMAKE_SIZE \"/opt/gccStm32/bin/arm-none-eabi-size\" )\nset(CMAKE_DEBUGER \"/opt/gccStm32/bin/arm-none-eabi-gdb\")\nset(CMAKE_CPPFILT \"/opt/gccStm32/bin/arm-none-eabi-c++filt\" )\nset(CMAKE_FIND_ROOT_PATH \"/opt/gccStm32/\")\nset(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)\nset(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)\nset(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)\n\n\n\n\n#######################################\n# 设置文件目录\n#######################################\n\n\n# #设置执行文件输出目录\n# set(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/bin)\n# #设置库输出路径\n# set(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/lib)\n\nmessage(\"++++++++++++++Start Build+++++++++++++++++\")\n\n# 添加头文件目录\ninclude_directories(${PROJECT_SOURCE_DIR}/Inc)\ninclude_directories(${PROJECT_SOURCE_DIR}/Drivers/STM32L4xx_HAL_Driver/Inc)\ninclude_directories(${PROJECT_SOURCE_DIR}/Drivers/STM32L4xx_HAL_Driver/Inc/Legacy)\ninclude_directories(${PROJECT_SOURCE_DIR}/Drivers/CMSIS/Device/ST/STM32L4xx/Include)\ninclude_directories(${PROJECT_SOURCE_DIR}/Drivers/CMSIS/Include)\n\n\n\n# 添加源文件目录\naux_source_directory(${PROJECT_SOURCE_DIR}/Src USRSRC)\naux_source_directory(${PROJECT_SOURCE_DIR}/Drivers/STM32L4xx_HAL_Driver/Src HALSRC)\n\n\n\n\n\n#######################################\n# 设置CFLAGS\n#######################################\n# cpu\nset(CPU  \"-mcpu=cortex-m4\")\n# fpu\nset(FPU  \"-mfpu=fpv4-sp-d16\")\n# float-abi\nset(FLOAT-ABI  \"-mfloat-abi=hard\")\n# mcu\nset(MCU  \"${CPU} -mthumb ${FPU} ${FLOAT-ABI}\")\n\n\nADD_DEFINITIONS(-DUSE_HAL_DRIVER -DSTM32L431xx)\nset(CMAKE_C_FLAGS \"${MCU} -fno-builtin ${OPT}  -Wall -std=gnu99 -fdata-sections -ffunction-sections\")\nset(CMAKE_CXX_FLAGS \"${MCU} -fno-builtin ${OPT} -Wall -std=c++11 -fdata-sections -ffunction-sections\")\nset(CMAKE_ASM_FLAGS \"${MCU} -x assembler-with-cpp\")\n\n\n\n\n#######################################\n# 设置编译链接\n#######################################\n\n# 取消-rdynamic的错误\nset(CMAKE_SHARED_LIBRARY_LINK_C_FLAGS \"\")\nset(CMAKE_SHARED_LIBRARY_LINK_CXX_FLAGS \"\")\n\n# 添加动态库\nlink_directories(\"/opt/gccStm32/\")\nset(LIBS \"-lc -lm -lnosys\" )\n\n# 添加link script\nset(LDSCRIPT \"${PROJECT_SOURCE_DIR}/STM32L431RBTx_FLASH.ld\")\n# 这是LDFLAGS\nset(CMAKE_EXE_LINKER_FLAGS \"-specs=nano.specs -T${LDSCRIPT} -Wl,-Map=${PROJECT_BINARY_DIR}/${MY_TARGET}.map,--cref -Wl,--gc-sections\")\n\n\n\n# 添加可执行文件（可执行文件名 [配置] 源文件）\nadd_executable(${MY_TARGET}.elf ${USRSRC} ${HALSRC} ${PROJECT_SOURCE_DIR}/startup_stm32l431xx.s)\n\n\n# 执行文件链接属性\ntarget_link_libraries(${MY_TARGET}.elf ${LIBS})\nset_target_properties(${MY_TARGET}.elf PROPERTIES LINK_DEPENDS ${LDSCRIPT})\n\n# 将elf文件转hex和bin\nadd_custom_command(\n    TARGET  ${MY_TARGET}.elf   \n    COMMAND ${CMAKE_OBJCOPY} -O ihex ${MY_TARGET}.elf ${MY_TARGET}.hex)\nadd_custom_command(\n    TARGET  ${MY_TARGET}.elf\n    POST_BUILD COMMAND ${CMAKE_OBJCOPY} -O binary ${MY_TARGET}.elf ${MY_TARGET}.bin)\n\n# 显示代码段大小\nadd_custom_command(\n    TARGET  ${MY_TARGET}.elf\n    POST_BUILD COMMAND ${CMAKE_SIZE}  ${MY_TARGET}.elf )"
  },
  {
    "objectID": "posts/claude-code-tricks.html",
    "href": "posts/claude-code-tricks.html",
    "title": "Vibe Coding 使用经验",
    "section": "",
    "text": "Vibe Coding绝对是未来必不可少的工具。所以现在还是应该花时间去掌握工具，所以最近在使用Claude Code以及Copilot过程中，做一些总结。\n\n\nClaude Code通知提醒\n我比较喜欢在vscode中使用Claude Code，然后让多个session自己运行，我可以节省时间干别的事情，但是如何在需要交互的时候提醒我呢？这里还是需要一番配置。\n\n首先安装Terminal Notification插件，然后开启vscode的OSC通知功能，在settings.json中添加：\n\n{\n  \"terminal.integrated.enableVisualBell\": true,\n  \"terminal.integrated.focusAfterRun\": \"terminal\",\n}\n\n添加一个监听脚本到~/notify_osc_listener.sh\n\n#!/bin/bash\n# 在普通 VSCode 终端中运行此脚本，接收 Claude Code 的通知并输出 OSC 777\nPIPE=\"/tmp/claude-notify-pipe\"\n\n# 创建命名管道\nrm -f \"$PIPE\"\nmkfifo \"$PIPE\"\n\ncleanup() {\n    rm -f \"$PIPE\"\n    echo \"监听已停止\"\n    exit 0\n}\ntrap cleanup EXIT INT TERM\n\necho \"正在监听 Claude Code 通知... (Ctrl+C 停止)\"\n\nwhile true; do\n    if IFS='|' read -r event message &lt; \"$PIPE\"; then\n        printf \"\\e]777;notify;%s;%s\\a\" \"$event\" \"$message\"\n    fi\ndone\n并执行他\nbash ~/notify_osc_listener.sh\n\n添加一个hook脚本到~/.claude/hooks/notify_osc.sh\n\n#!/bin/bash\n# Claude Code hook: 将通知写入命名管道，由监听脚本输出 OSC 777\nEVENT=\"$1\"\nMESSAGE=\"$2\"\nPIPE=\"/tmp/claude-notify-pipe\"\n\nif [ -p \"$PIPE\" ]; then\n    # 用 timeout 避免没有监听者时阻塞\n    timeout 1 bash -c \"echo '${EVENT}|${MESSAGE}' &gt; '${PIPE}'\" 2&gt;/dev/null &\nfi\n\n在~/.claude/settings.json 设置hooks\n\n{\n\"hooks\": {\n    \"Stop\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"~/.claude/hooks/notify_osc.sh 'Stop' 'Claude 响应完成，等待输入'\"\n          }\n        ]\n      }\n    ],\n  }\n}\n\n再使用Claude Code，当他完成响应时，你就会收到vscode的通知了。"
  },
  {
    "objectID": "posts/circle-loss.html",
    "href": "posts/circle-loss.html",
    "title": "Circle Loss",
    "section": "",
    "text": "本文是对旷视所提出的论文：Circle Loss: A Unified Perspective of Pair Similarity Optimization的个人解读。\n\n\n摘要\nCircle Loss对于数据对学习以及分类标签学习提出了一种统一的视角即：最大化类内相似度\\(s_p\\)，最小化类间相似度\\(s_n\\)。同时发现对于大多数损失函数，实际是将相似度\\(s_p,s_n\\)进行嵌入并减小\\((s_n-s_p)\\)，而传统的损失方式对于每个相似度的惩罚都相等，实际上应该根据不相似的程度进行惩罚。因此提出了Circle Loss，他可以同时对数据对以及分类标签的样本进行学习。\n例如triplet loss，softmax loss及其变体具有相似的优化模式。他们都嵌入\\(s_n,s_p\\)到相似度对，并寻求降低\\((s_n-s_p)\\)的方法。在\\((s_n-s_p)\\)中，增加\\(s_p\\)等效于减少\\(s_n\\)。这种对称的优化方式容易出现以下两个问题。\n\n图1，传统方式优化的\\(s_n-s_p\\)与建议的\\(\\alpha_n s_n - \\alpha_p s_p\\)对比。(a)对于A、B、C三点处的梯度均相同，且对于\\(T,T'\\)的决策面也相同。(b)Circle Loss可以动态调整梯度，使得优化方向更加明确。\n\n缺乏优化的灵活性\n\n对于\\(s_n\\)和\\(s_p\\)的惩罚强度被限制为相等，具体在第二节中说明。比如在图1中(a)中的点A，对于决策面已经相当解决了但梯度还是和远离决策面的点相同。\n\n收敛状态不明确\n\n优化\\((s_n-s_p)\\)通常目标会是一个决策边界\\(s_n-s_p=m\\)，其中\\(m\\)为间距。但这个决策面可能会混淆，比如图1中(a)的\\(T={s_n:0.2,s_p=0.5}\\)与\\(T'={s'_n:0.4,s'_p=0.7}\\)，虽然他们的间距相同，但\\(s'_n\\)与\\(s_p\\)的间距为0.1，因此混淆的决策边界会影响可分离性。\n因此针对以上缺点，考虑根据相似度的大小进行惩罚，所以首先推广\\((s_n-s_p)\\)至\\(\\alpha_n s_n - \\alpha_p s_p\\)，其中\\(\\alpha_n,\\alpha_p\\)具有独立的权重参数允许\\(s_n,s_p\\)以不同的速率优化。将\\(\\alpha_n,\\alpha_p\\)实现为分别关于\\(s_n,s_p\\)的线性函数，当相似度与最佳值偏离的越远，加权参数就越大，最终可以得到决策面为\\(\\alpha_n s_n - \\alpha_p s_p=m\\)，在\\((s_n,s_p)\\)空间中为圆形区域，所以称之为Circle Loss。 NOTE: 实际上amsoftmax也是圆形的决策面，可以由Circle Loss退化得到。\n\n\n统一视角下的损失函数\n深度特征学习旨在最大化类内相似度，并最小化类间相似度。例如，在余弦相似度度量下，我们期望\\(s_p\\rightarrow1\\)和\\(s_n\\rightarrow0\\)。为了从统一的视角看待之前众多的损失函数，首先定义如下。给定单个样本\\(x\\)在特征空间中，有\\(K\\)个类内相似度分数，\\(L\\)个类间相似度分数，那么类内相似度分数定义为\\(\\{s^i_p\\}(i=1,2,\\ldots,K)\\)，类间相似度分数定义为\\(\\{s^j_n\\}(j=1,2,\\ldots,L)\\)。为了最小化\\(s^j_n\\)同时最大化\\(s^i_p\\)，统一的损失函数Unified Loss定义为如下： \\[\n\\begin{aligned}\n  \\mathcal{L}_{uni}&=\\log\\left[1+\\sum^K_{i=1}\\sum^L_{j=1}\\exp(\\gamma(s^j_n-s^i_p+m))\\right]\\\\\n  &=\\log\\left[1+\\sum^L_{j=1}\\exp(\\gamma(s^j_n+m))\\sum^K_{i=1}\\exp(\\gamma(-s^i_p))\\right]\n\\end{aligned}\\tag{1}\n\\]\n其中\\(\\gamma,m\\)分别为尺度系数与间距系数。现在我们可以尝试修改这个损失函数到之前的损失函数中：\n\n分类标签数据\n假设有共有\\(N\\)类，则嵌入的分类权重向量为\\(w_i,i\\in\\{1,2,\\ldots,N\\}\\)，在am-softmax中最后的全连接层实际上就是分别计算特征\\(x\\)与权重向量\\(w_i\\)间的余弦相似度，不太清楚的可以看我之前写的博客。因为分类标签数据所定义的类内标签只有一个，类间标签有\\(N-1\\)个，然后即可从Unified Loss推导至am-softmax：\n\\[\n\\begin{aligned}\n  \\mathcal{L}_{uni}&=\\log\\left[1+\\sum^K_{i=1}\\sum^L_{j=1}\\exp(\\gamma(s^j_n-s^i_p+m))\\right]\\\\\n  &=\\log\\left[1+\\sum^L_{j=1}\\exp(\\gamma(s^j_n))\\sum^K_{i=1}\\exp(\\gamma(m-s^i_p))\\right]\\\\\n  \\text{Let}\\ \\ \\ \\ K&=1,L=N-1\\\\\n  \\mathcal{L}_{uni}&=\\log\\left[1+\\sum^{N-1}_{j=1}\\exp(\\gamma(s^j_n))\\exp(\\gamma(m-s_p))\\right]\\\\\n  &=-\\log\\left[\\frac{1}{1+\\sum^{N-1}_{j=1}\\exp(\\gamma(s^j_n))\\exp(\\gamma(m-s_p))}\\right]\\\\\n  &=-\\log\\left[\\frac{1}{1+\\sum^{N-1}_{j=1}\\exp(\\gamma(s^j_n))\\frac{1}{\\exp(\\gamma(s_p-m))}}\\right]\\\\\n  &=-\\log\\left[\\frac{\\exp(\\gamma(s_p-m))}{\\exp(\\gamma(s_p-m))+\\sum^{N-1}_{j=1}\\exp(\\gamma(s^j_n))}\\right]\\\\\n  \\text{Let}\\ \\ \\ \\ s^j_n&=\\frac{w^T_jx}{\\parallel w_j\\parallel \\parallel x\\parallel }=\\cos\\theta_j,\\ \\ s_p=\\frac{w^T_{y_i}x}{\\parallel w_{y_i}\\parallel \\parallel x\\parallel }=\\cos\\theta_{y_i}\\\\\n  \\mathcal{L}_{uni}&=-\\log\\left[\\frac{\\exp(\\gamma(\\cos\\theta_{y_i}-m))}{\\exp(\\gamma(\\cos\\theta_{y_i}-m))+\\sum^{N-1}_{j=1}\\exp(\\gamma(\\cos\\theta_j))}\\right]\\\\\n  \\mathcal{L}_{ams} &= - \\log \\frac{e^{s\\cdot(\\cos\\theta_{y_i} -m)}}{e^{s\\cdot (\\cos\\theta_{y_i} -m)}+\\sum^c_{j=1,i\\neq t}  e^{s\\cdot\\cos\\theta_j }}\n\\end{aligned}\\tag{2}\n\\]\n当定义相似度指标为余弦距离时，可以看到由Unified Loss推出的倒数第二个公式，和我之前博客中的am-softmax公式是一样的。同时如果将间距系数\\(m\\)设置为0，比例系数\\(\\gamma\\)设置为1，那么就继续退化到普通的softmax损失了。\n\n\n配对标签数据\n对于配对的标签数据，计算一个batch中\\(x\\)与其他特征的相似度。特别的，\\(s^j_n=\\frac{x^T_j x}{\\parallel x_j \\parallel \\parallel x \\parallel}\\)，其中\\(x_j\\)为负样本集合\\(\\mathcal{N}\\)中第\\(j\\)个样本。\\(s^i_p= \\frac{x^T_i x}{\\parallel x_i \\parallel \\parallel x \\parallel}\\)，其中\\(x_i\\)为正样本集合\\(\\mathcal{P}\\)中第\\(i\\)个样本。相应地\\(K=|\\mathcal{P}|,L=|\\mathcal{N}|\\)。则Unified Loss通过难例挖掘退化到triplet loss。\n\\[\n\\begin{aligned}\n\\mathcal{L}_{t r i} &=\\lim _{\\gamma \\rightarrow+\\infty} \\frac{1}{\\gamma} \\mathcal{L}_{u n i} \\\\\n&=\\lim _{\\gamma \\rightarrow+\\infty} \\frac{1}{\\gamma} \\log \\left[1+\\sum_{i=1}^{K} \\sum_{j=1}^{L} \\exp \\left(\\gamma\\left(s_{n}^{j}-s_{p}^{i}+m\\right)\\right)\\right] \\\\\n&=\\max \\left[s_{n}^{j}-s_{p}^{i}\\right]_{+}\n\\end{aligned}\\tag{3}\n\\]\n\n\n梯度分析\n公式2与公式3展示了由Unified Loss推出的一系列变体，对这些变体进行梯度分析：\n\n图2：对于a与b，他们的\\(s_p\\)的梯度相比与\\(s_n\\)被限制为相等，且梯度会突然减小缺乏连续性。比如点\\(A\\)的类内相似度已经接近较大值，但梯度依旧较大。同时决策面是平行的，会产生分类混淆。\n\n\n\n新的损失函数\n\n自定步数权重\n考虑通过允许每个相似性分数按照自己的进度学习，而不依赖于其当前的优化状态，从而提高优化灵活性。首先忽略公式1中的间距参数\\(m\\)，然后通过以下方法将Unified Loss(公式1)推导至Circle Loss中： \\[\n\\begin{aligned}\n\\mathcal{L}_{\\text {circle}} &=\\log \\left[1+\\sum_{i=1}^{K} \\sum_{j=1}^{L} \\exp \\left(\\gamma\\left(\\alpha_{n}^{j} s_{n}^{j}-\\alpha_{p}^{i} s_{p}^{i}\\right)\\right)\\right] \\\\\n&=\\log \\left[1+\\sum_{j=1}^{L} \\exp \\left(\\gamma \\alpha_{n}^{j} s_{n}^{j}\\right) \\sum_{i=1}^{K} \\exp \\left(-\\gamma \\alpha_{p}^{i} s_{p}^{i}\\right)\\right]\n\\end{aligned}\\tag{4}\n\\]\n其中\\(\\alpha_{n}^{j},\\alpha_{p}^{i}\\)是非负的权重参数。在训练时\\(\\alpha_{n}^{j}s^j_n-\\alpha_{p}^{i}s^i_p\\)的梯度会分布乘上\\(\\alpha_{n}^{j},\\alpha_{p}^{i}\\)。假设最优的\\(s^i_p\\)为\\(O_p\\)，最优的\\(s^j_n\\)为\\(O_n\\)且\\((O_n&lt;O_p)\\)。当相似度与最优值差距较大时，应该具有较大的权重以便有效更新，权重参数定义如下：\n\\[\n\\begin{aligned}\\alpha_{p}^{i}=[O_p-s^i_p]_+ \\\\\n\\alpha_{n}^{j}=[s^j_n-O_n]_+\n\\end{aligned}\\tag{5}\n\\]\n其中\\([\\cdot]_+\\)表示从0截断运算符，保证权重参数非负。通常损失中都带有缩放因子\\(\\gamma\\)，不过Circle Loss的加权项实际代替了缩放因子的作用，不过就算加上缩放因子也没有问题，因为自适应加权项会自适应。\n传统基于softmax的损失函数通常把分类问题解释为样本属于某个类别的概率，而概率要求向量的相似度计算要在相同的缩放因子下进行。Circle Loss通过自适应加权放弃了这种观点，使用相似对优化的观点，这样可以更灵活的进行优化。\n\n\n内类间距与类间间距参数\n对于优化\\((s_n-s_p)\\)，添加间距参数\\(m\\)可以加权优化性能。因为\\(s_n\\)和\\(-s_p\\)具有零点对称性，因此对\\(s_n\\)添加正间距相当于对\\(s_p\\)添加负间距。但Circle Loss中\\(s_n\\)和\\(s_p\\)不具有零点对称性，因此需要考虑\\(s_n\\)和\\(s_p\\)各自的间距： \\[\n\\begin{aligned}\n  \\mathcal{L}_{\\text {circle}}=\\log \\left[1+\\sum_{j=1}^{L} \\exp \\left(\\gamma \\alpha_{n}^{j}\\left(s_{n}^{j}-\\Delta_{n}\\right)\\right) \\sum_{i=1}^{K} \\exp \\left(-\\gamma \\alpha_{p}^{i}\\left(s_{p}^{i}-\\Delta_{p}\\right)\\right)\\right]\n\\end{aligned}\\tag{6}\n\\]\n其中\\(\\Delta_{n},\\Delta_{p}\\)分别为类间间距参数和类内间距参数。在公式6中，Circle Loss期望\\(s_{p}^{i}&gt;\\Delta_{p}\\)，\\(s_{n}^{j}&lt;\\Delta_{n}\\)。\n通过推导决策边界进一步分析\\(\\Delta_{n}\\)与\\(\\Delta_{p}\\)的设置，简单起见，考虑二分类的情况下决策面为\\(\\alpha_{n}\\left(s_{n}-\\Delta_{n}\\right)-\\alpha_{p}\\left(s_{p}-\\Delta_{p}\\right)=0\\)。带入公式5与公式6得到决策边界为： \\[\n\\begin{aligned}\n\\left(s_{n}-\\frac{O_{n}+\\Delta_{n}}{2}\\right)^{2}+\\left(s_{p}-\\frac{O_{p}+\\Delta_{p}}{2}\\right)^{2}=C\\\\\nC=\\left(\\left(O_{n}-\\Delta_{n}\\right)^{2}+\\left(O_{p}-\\Delta_{p}\\right)^{2}\\right) / 4\n\\end{aligned} \\tag{7}\n\\]\n公式7表明了分类边界是一个圆形。圆心的坐标为\\(s_{n}=\\left(O_{n}+\\Delta_{n}\\right) / 2, s_{p}=\\left(O_{p}+\\Delta_{p}\\right) / 2\\)，半径为\\(\\sqrt{C}\\)。\n此时综合以上公式，在Circle Loss中的超参数为\\(O_p,O_n,\\gamma,\\Delta_p,\\Delta_n\\)，为了简单起见，简化\\(O_p=1+m,O_n=-m,\\Delta_p=1-m,\\Delta_n=m\\)。所以公式7简化为：\n\\[\n\\begin{aligned}\n  \\left(s_{n}-0\\right)^{2}+\\left(s_{p}-1\\right)^{2}=2 m^{2}\n\\end{aligned}\\tag{8}\n\\]\n最终决策面被定义为公式8，其中优化目标从\\(s_p\\rightarrow1,s_n\\rightarrow0\\)变化为了\\(s_p&gt;1-m,s_n&lt;m\\)。此时称\\(m\\)为松弛因子，\\(\\gamma\\)为比例因子。\n我觉得对于分类标签样本与配对标签样本还是需要不一样的形式的，这样代码实现起来比较方便。文章里面没有给出最后的损失定义，可能也是这个想法，这里我自己总结了一下，给出最终的损失定义。(当然也有可能作者实现时直接用原本的损失写法)\n\n分类样本损失\n\\[\n\\begin{aligned}\n    \\mathcal{L}_{\\text {circle}}&=\\log \\left[1+\\sum_{j=1}^{L} \\exp \\left(\\gamma \\alpha_{n}^{j}\\left(s_{n}^{j}-\\Delta_{n}\\right)\\right) \\sum_{i=1}^{K} \\exp \\left(-\\gamma \\alpha_{p}^{i}\\left(s_{p}^{i}-\\Delta_{p}\\right)\\right)\\right]\\\\\n    \\text{Let}\\ \\ \\ \\ K&=1,\\ \\ L=N-1\\\\\n    \\mathcal{L}_{\\text {circle}}&=\\log \\left[1+\\sum_{j=1}^{N-1} \\exp \\left(\\gamma \\alpha_{n}^{j}\\left(s_{n}^{j}-\\Delta_{n}\\right)\\right) \\exp \\left(-\\gamma \\alpha_{p}^{y_i}\\left(s_{p}^{y_i}-\\Delta_{p}\\right)\\right)\\right]\\\\\n    &=-\\log \\left[\\frac{1}{1+\\sum_{j=1}^{N-1} \\exp \\left(\\gamma \\alpha_{n}^{j}\\left(s_{n}^{j}-\\Delta_{n}\\right)\\right) \\exp \\left(-\\gamma \\alpha_{p}^{y_i}\\left(s_{p}^{y_i}-\\Delta_{p}\\right)\\right)}\\right]\\\\\n    &=-\\log \\left[\\frac{\\exp \\left(\\gamma \\alpha_{p}^{y_i}\\left(s_{p}^{y_i}-\\Delta_{p}\\right)\\right)}{\\exp \\left(\\gamma \\alpha_{p}^{y_i}\\left(s_{p}^{y_i}-\\Delta_{p}\\right)\\right)+\\sum_{j=1}^{N-1} \\exp \\left(\\gamma \\alpha_{n}^{j}\\left(s_{n}^{j}-\\Delta_{n}\\right)\\right)}\\right]\n\\end{aligned}\n\\]\n这里将相似度变化对比如下图所示：\n\n\n\n配对标签损失\n\\[\n\\begin{aligned}\n    \\mathcal{L}_{\\text {circle}}&=\\log \\left[1+\\sum_{j=1}^{L} \\exp \\left(\\gamma \\alpha_{n}^{j}\\left(s_{n}^{j}-\\Delta_{n}\\right)\\right) \\sum_{i=1}^{K} \\exp \\left(-\\gamma \\alpha_{p}^{i}\\left(s_{p}^{i}-\\Delta_{p}\\right)\\right)\\right]\\\\\n    \\text{Let}\\ \\ \\ \\ K&=1,\\ \\ L=1\\\\\n    \\mathcal{L}_{\\text {circle}}&=\\log \\left[1+\\exp \\left(\\gamma \\alpha_{n}\\left(s_{n}-\\Delta_{n}\\right)\\right) \\exp \\left(-\\gamma \\alpha_{p}\\left(s_{p}-\\Delta_{p}\\right)\\right)\\right]\n\\end{aligned}\n\\]\nNOTE 前面提到优化目标为\\(s_p\\rightarrow1,s_n\\rightarrow0\\)，实际上不是要求相似度求出来就在\\(0\\sim1\\)内。作者采用cos相似性。对于cos相似性不是要求neg的相似性为-1。因为当你要求A和B的相似性为-1， 同时要求A和C的相似性为-1，那么，你就在要求B和C的相似性为 1。所以说，0才是不相似，-1是另一种相似。以上函数中的\\([\\cdot]_+\\)在tensorflow中可以用relu实现。\n\n\n\n\n实验\n作者用am-softmax和Circle Loss做了下对比实验，结果如下：\n\n图3：训练收敛后数据分布可视化结果，蓝色点表示训练过程中位于决策边界线的分类相似对，绿色点是收敛后的标记相似对。(a)中am-softmax在训练结束后，相似对的分布比较分散。(b,c)中Circle Loss通过圆形的决策边界，使得相似对聚集到相对集中的区域中。\nNOTE 对于别的实验结果我不再赘述，有兴趣的可以自行查看论文。"
  },
  {
    "objectID": "posts/cdecl.html",
    "href": "posts/cdecl.html",
    "title": "C语义转换",
    "section": "",
    "text": "你有没有曾经对c语言的定义苦恼？\n\n今天发现一个有趣的工具：cdecl 点击进入即可。 这是一个可以将c语言的定义转成英语的小工具，对于一些看着头疼的定义直接可以给出解释。 例如：int (*(*foo)(void ))[3] 解释：declare foo as pointer to function (void) returning pointer to array 3 of int\n但是经过我的测试也发现了一些不足，比如我拿了一个极端点的：(*(void (*)())0)()，程序就蒙了(ಡωಡ)。\n我忽然有点想自己写个软件用它的接口，再搞个翻译，给一些人用，不是美滋滋？ 噢对了，linux上可以直接安装这个～～"
  },
  {
    "objectID": "posts/capsnet.html",
    "href": "posts/capsnet.html",
    "title": "CapsNet实现以及踩坑",
    "section": "",
    "text": "我本来打算用tensorflow 2.0去写capsule net的,结果被tf 2.0中的tensorboard坑的放弃了…\n然后我换成tf 1.13去写了,下面做一个实现过程记录."
  },
  {
    "objectID": "posts/capsnet.html#squash",
    "href": "posts/capsnet.html#squash",
    "title": "CapsNet实现以及踩坑",
    "section": "1. squash",
    "text": "1. squash\n\n问题描述\n激活函数公式为:\n\\[ \\begin{aligned}\n    v_j=\\frac{||s_j||^2}{1+||s_j||^2}\\frac{s_j}{||s_j||}\n\\end{aligned} \\]\n下面my_squash是我写的:\ndef my_squash(s):\n    s_norm = tf.norm_v2(s)\n    s_square_norm = tf.square(s_norm)\n    v = (s_square_norm * s)/((1+s_square_norm)*s_norm)\n    return v\n然后我训练的时候一直没有效果,我找了半天才明白.\n\n\n问题解决\n问题在于tf.norm_v2这里的维度控制,他这里的范数指的是每一个向量的长度范数,所以需要指定维度为s_norm = tf.norm_v2(s, axis=-1, keepdims=True),下面是正确的维度演示: \\[ \\begin{aligned}\n    令 S_j &= [batch,1152,8] \\\\\n    则 ||S_j|| &= [batch,1152,1] \\\\\n    ||S_j||^2 &= [batch,1152,1] \\\\\n    v&= [batch,1152,8]\n\\end{aligned} \\]\n并且这个函数其实可以优化为: \\[ \\begin{aligned}\n    v_j&=\\frac{||s_j||^2}{1+||s_j||^2}\\frac{s_j}{||s_j||}\\\\\n    &=\\frac{||s_j||s_j}{1+||s_j||^2}\n\\end{aligned} \\] 代码为:\nwith tf.variable_scope('squash'):\n    s_norm = tf.norm_v2(s, axis=-1, keepdims=True)\n    s_square_norm = tf.square(s_norm)\n    v = (s_norm * s)/(1+s_square_norm)\n    return v"
  },
  {
    "objectID": "posts/c-http.html",
    "href": "posts/c-http.html",
    "title": "使用c通过http发送文件",
    "section": "",
    "text": "最近要用单片机通过http发送文件到服务器，所以写了个发送文件的demo。\n\n\n讲解\nhttp协议本质上还是socket协议，我的测试demo是基于Linux的，所以我直接使用Linux的socket连接。\n当我连接到服务器之后，我就需要直接发送http格式的头，这里要设置post的位置，内容长度，以及主机域名等等：\nPOST xxxxxxxxxx HTTP/1.1\nContent-Length: xxxxxxx\nHost: xxxxxxx\nContent-Type: multipart/form-data;boundary=------FormBoundaryShouldDifferAtRuntime\n\n然后再发送http的body：\n------FormBoundaryShouldDifferAtRuntime\nContent-Disposition: form-data; name=\"deviceId\"\n\n1\n------FormBoundaryShouldDifferAtRuntime\nContent-Disposition: form-data; name=\"file\"; filename=\"debug.log\"\nContent-Type: application/octet-stream\n\n[message-part-body; type: application/octet-stream, size: 2076 bytes]\n------FormBoundaryShouldDifferAtRuntime--\n要注意的就是这里所有的换行都是\\r\\n的，因为我是linux的系统，所以之前我一直计算字符串长度与Content-Length不匹配。\n\n\n代码\n\nvoid set_header(char *pbuf, const char *host, int content_len) {\n    sprintf(pbuf,\n            \"POST /device/upload_file.do HTTP/1.1\\r\\nHost: %s\\r\\nUser-Agent: \"\n            \"curl/7.58.0\\r\\nAccept: */*\\r\\nContent-Length: %d\\r\\nContent-Type: \"\n            \"multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW; \"\n            \"boundary=------------------------7b055677edfa30ed\\r\\n\\r\\n\",\n            host, content_len);\n}\n\nvoid set_content(char *pbuf, int id, const char *content) {\n    sprintf(\n        pbuf,\n        \"--------------------------7b055677edfa30ed\\r\\nContent-Disposition: \"\n        \"form-data; \"\n        \"name=\\\"deviceId\\\"\\r\\n\\r\\n%d\\r\\n--------------------------\"\n        \"7b055677edfa30ed\\r\\nContent-Disposition: form-data; name=\\\"file\\\"; \"\n        \"filename=\\\"test.txt\\\"\\r\\nContent-Type: \"\n        \"text/plain\\r\\n\\r\\n%s\\n\\r\\n--------------------------7b055677edfa30ed--\\r\\n\",\n        id, content);\n}\n这两个简单的函数是设置http的头与内容的。但是我这里写的只是一个简单txt的文件上传。如果要上传二进制文件的话，必须要把body部分分开来发送。\n在linux下可以使用sendfile函数，但是在单片机中需要写别的函数。"
  },
  {
    "objectID": "posts/bpnnfit.html",
    "href": "posts/bpnnfit.html",
    "title": "BP神经网络回归",
    "section": "",
    "text": "利用BP神经网络实现非线性回归,我用了两个方式实现,发现用库的方式没有我从吴恩达老师作业里面改过去的好用.先看题目.\n已知函数\\(f(x)=e^{-x},1\\leq x\\leq 10\\)利用BP神经网络以及sigmod函数对上面的函数完成以下工作."
  },
  {
    "objectID": "posts/bpnnfit.html#效果图",
    "href": "posts/bpnnfit.html#效果图",
    "title": "BP神经网络回归",
    "section": "效果图",
    "text": "效果图"
  },
  {
    "objectID": "posts/bpnnfit.html#执行效果",
    "href": "posts/bpnnfit.html#执行效果",
    "title": "BP神经网络回归",
    "section": "执行效果",
    "text": "执行效果"
  },
  {
    "objectID": "posts/bin-search-template.html",
    "href": "posts/bin-search-template.html",
    "title": "二分查找-统一框架",
    "section": "",
    "text": "前天面试的时候又考到二分查找了，但是没有写出来，之前看了labuladong的鬼模板，以为自己懂了，发现其实并不懂，这几天重新学习之后，写下了这篇文章。"
  },
  {
    "objectID": "posts/bin-search-template.html#x-的平方根",
    "href": "posts/bin-search-template.html#x-的平方根",
    "title": "二分查找-统一框架",
    "section": "x 的平方根",
    "text": "x 的平方根\n\n分析\n返回类型是只保留整数部分，那么不就是要我们找到\\(mid&lt;=\\sqrt{x}\\)的上界吗？那么我们模板二走起～\n\n\n代码\nclass Solution {\n public:\n  int mySqrt(int x) {\n    long l = 0, r = x, mid;  //避免溢出\n    while (l &lt; r) {\n      mid = (l + r + 1) &gt;&gt; 1;\n      if (mid &lt;= (x / mid)) l = mid;\n      else r = mid - 1;\n    }\n    return l;\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#猜数字大小",
    "href": "posts/bin-search-template.html#猜数字大小",
    "title": "二分查找-统一框架",
    "section": "猜数字大小",
    "text": "猜数字大小\n\n分析\n调用guess函数得到不同的情况，明显的二段性：\n0---------------pick--------------N\n      mid       mid       mid\n       1         0        -1\n这题可以找上界，也可以找下界，这里我们试试找下界，模板一走起～\n\n\n代码\nclass Solution {\n public:\n  int guessNumber(int n) {\n    long l = 0, r = n, mid;\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (guess(mid) &lt;= 0) r = mid;\n      else l = mid + 1;\n    }\n    return l;\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#第一个错误的版本",
    "href": "posts/bin-search-template.html#第一个错误的版本",
    "title": "二分查找-统一框架",
    "section": "第一个错误的版本",
    "text": "第一个错误的版本\n\n分析\n典型的二段性：\n0---------------first--------------N\n      mid       mid       mid\n     false      true      true\n也就是右区域找下界，模板一走起～\n\n\n代码\nclass Solution {\n public:\n  int firstBadVersion(int n) {\n    long l = 0, r = n, mid;\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (isBadVersion(mid)) r = mid;\n      else l = mid + 1;\n    }\n    return l;\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#在排序数组中查找元素的第一个和最后一个位置",
    "href": "posts/bin-search-template.html#在排序数组中查找元素的第一个和最后一个位置",
    "title": "二分查找-统一框架",
    "section": "在排序数组中查找元素的第一个和最后一个位置",
    "text": "在排序数组中查找元素的第一个和最后一个位置\n\n分析\n这题比较好，可以让我们使用两个模板，首先我们可以找到右区域的下界（模板一），然后再找到左区域的上界（模板二），让我们开始吧！\n\n\n代码\nclass Solution {\n public:\n  vector&lt;int&gt; searchRange(vector&lt;int&gt;& nums, int target) {\n    if (nums.empty()) return {-1, -1};\n    /* 我们首先找到大于等于target的下界（模板1） */\n    int l = 0, r = nums.size() - 1, mid;\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (nums[mid] &gt;= target) r = mid;\n      else l = mid + 1;\n    }\n    if (nums[l] != target) return {-1, -1}; // 如果没有直接退出\n    /*  在新的区域内找到小于等于target的上界（模板二） */\n    int start = l;\n    r = nums.size() - 1;\n    while (l &lt; r) {\n      mid = l + r + 1 &gt;&gt; 1;\n      if (nums[mid] &lt;= target) l = mid;\n      else r = mid - 1;\n    }\n    return {start, l};\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#寻找旋转排序数组中的最小值",
    "href": "posts/bin-search-template.html#寻找旋转排序数组中的最小值",
    "title": "二分查找-统一框架",
    "section": "寻找旋转排序数组中的最小值",
    "text": "寻找旋转排序数组中的最小值\n\n分析\n\n上图看出来我们需要找下界，接下来我们只需要对比最右侧的元素确定我们当前位于右区域还是左区域即可，然后模板一走起～\n\n\n代码\nclass Solution {\n public:\n  int findMin(vector&lt;int&gt;& nums) {\n    int l = 0, r = nums.size() - 1, mid;\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (nums[mid] &lt;= nums[r]) r = mid;\n      else l = mid + 1;\n    }\n    return nums[l];\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#寻找旋转排序数组中的最小值-ii",
    "href": "posts/bin-search-template.html#寻找旋转排序数组中的最小值-ii",
    "title": "二分查找-统一框架",
    "section": "寻找旋转排序数组中的最小值 II",
    "text": "寻找旋转排序数组中的最小值 II\n\n分析\n这题多了一个点，就是数据会出现重复，也就是可能会出现如下情况：\n        /--\n       /\nL ----/          --- R\n                /\n               /\n            --/\n但是其实我们贯彻一个思路，就是找小于等于nums[r]区域的下界，那么就是如果nums[l]==nums[r]的时候，我们都向上收缩就好了，那么接下来的事情就交给模板一去做就完事了！\n\n\n代码\nclass Solution {\n public:\n  int findMin(vector&lt;int&gt;& nums) {\n    int l = 0, r = nums.size() - 1, mid;\n    while (l &lt; r) {\n      if (nums[l] == nums[r]) { // 跳过相同元素\n        l++;\n        continue;\n      }\n      mid = l + r &gt;&gt; 1;\n      if (nums[mid] &lt;= nums[r]) r = mid;\n      else l = mid + 1;\n    }\n    return nums[l];\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#搜索旋转排序数组",
    "href": "posts/bin-search-template.html#搜索旋转排序数组",
    "title": "二分查找-统一框架",
    "section": "搜索旋转排序数组",
    "text": "搜索旋转排序数组\n\n分析\n首先找到旋转排序数组中的最小值（模板一），然后根据情况重新分配区域，最后再搜索一次搜索排序数组（模板一），就搞定收工。\n\n\n代码\nclass Solution {\n public:\n  int search(vector&lt;int&gt;& nums, int target) {\n    int n = nums.size(), l = 0, r = n - 1, mid;\n    /* 找到旋转排序数组中的最小值（模板一） */\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (nums[mid] &lt;= nums[r]) r = mid;\n      else l = mid + 1;\n    }\n    /* 根据情况重新分配区域 */\n    if (target &lt;= nums[n - 1]) r = n - 1;\n    else r = l - 1, l = 0;\n    /* 搜索排序数组（模板一） */\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (nums[mid] &gt;= target) r = mid;\n      else l = mid + 1;\n    }\n    return nums[l] == target ? l : -1;\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#寻找峰值",
    "href": "posts/bin-search-template.html#寻找峰值",
    "title": "二分查找-统一框架",
    "section": "寻找峰值",
    "text": "寻找峰值\n\n分析\n        /\\\n   /\\  /  \\\n\\ /  \\/    \\\n            \\\n这题看着复杂，其实挺简单，他的两段性在数据中，我们可以找： 1. 上升区域的上界，上升区域必然有nums[mid]&gt;nums[mid-1] 2. 下降区域的下界，下降区域必然有nums[mid]&gt;nums[mid+1]\n当然还需要考虑mid在两侧的情况，这里我们选第一种做法吧，模板二走起～\n\n\n代码\nclass Solution {\n public:\n  int findPeakElement(vector&lt;int&gt;& nums) {\n    int n = nums.size(), l = 0, r = n - 1, mid;\n    while (l &lt; r) {\n      mid = (l + r + 1) &gt;&gt; 1;\n      if ((mid &gt; 0 and nums[mid] &gt; nums[mid - 1]) or\n          (mid == 0 and nums[mid] &gt; nums[mid + 1]))\n        l = mid;\n      else\n        r = mid - 1;\n    }\n    return l;\n  };\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#找到-k-个最接近的元素",
    "href": "posts/bin-search-template.html#找到-k-个最接近的元素",
    "title": "二分查找-统一框架",
    "section": "找到 K 个最接近的元素",
    "text": "找到 K 个最接近的元素\n\n分析\n这题其实就是找到距离target最接近的元素，找大于等于target的下界与找上界差别不大，然后需要需要注意当前点和另外一个点到target的距离，来确定起始点，最后双指针。\n\n\n代码\nclass Solution {\n public:\n  vector&lt;int&gt; findClosestElements(vector&lt;int&gt;& arr, int k, int x) {\n    // 找到小于等于x的上界，用模板二\n    int l = 0, r = arr.size() - 1, mid;\n    while (l &lt; r) {\n      mid = (l + r + 1) &gt;&gt; 1;\n      if (arr[mid] &lt;= x) l = mid;\n      else r = mid - 1;\n    }\n    // 找到分界点之后要判断一下左边还是右边\n    if ((l &lt; arr.size() - 1) and (abs(arr[l] - x) &gt; abs(arr[l + 1] - x))) {\n      l = r = (l + 1);\n    }\n    // 双指针收尾\n    k--;\n    while (k--) {\n      if (l == 0) {\n        r++;\n      } else if (r == arr.size() - 1) {\n        l--;\n      } else {\n        int ldiff = abs(arr[l - 1] - x), rdiff = abs(arr[r + 1] - x);\n        if (ldiff &lt;= rdiff) {\n          l--;\n        } else if (ldiff &gt; rdiff) {\n          r++;\n        }\n      }\n    }\n    return vector&lt;int&gt;(arr.begin() + l, arr.begin() + r + 1);\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#搜索长度未知的有序数组",
    "href": "posts/bin-search-template.html#搜索长度未知的有序数组",
    "title": "二分查找-统一框架",
    "section": "搜索长度未知的有序数组",
    "text": "搜索长度未知的有序数组\n\n分析\n这题和x 的平方根类似，那题我们用模板二找的上界，这题我们用模板一找下界。\n\n\n代码\nclass Solution {\n public:\n  bool isPerfectSquare(int num) {\n    long l = 1, r = num, mid;\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (mid &gt;= num / mid) r = mid;\n      else l = mid + 1;\n    }\n    return (l * l == num) ? true : false;\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#寻找比目标字母大的最小字母",
    "href": "posts/bin-search-template.html#寻找比目标字母大的最小字母",
    "title": "二分查找-统一框架",
    "section": "寻找比目标字母大的最小字母",
    "text": "寻找比目标字母大的最小字母\n\n分析\n典型的找下界题目，并且这里的范围是必须比target大，模板一用起来！\n\n\n代码\nclass Solution {\n public:\n  char nextGreatestLetter(vector&lt;char&gt;& letters, char target) {\n    int l = 0, r = letters.size() - 1, mid;\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (letters[mid] &gt; target) r = mid;\n      else l = mid + 1;\n    }\n    return letters[l] &gt; target ? letters[l] : letters[(l + 1) % letters.size()];\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#两数之和-ii---输入有序数组",
    "href": "posts/bin-search-template.html#两数之和-ii---输入有序数组",
    "title": "二分查找-统一框架",
    "section": "两数之和 II - 输入有序数组",
    "text": "两数之和 II - 输入有序数组\n\n分析\n写多了，都麻了，这题简单循环加二分找下界即可。\n\n\n代码\nclass Solution {\n public:\n  vector&lt;int&gt; twoSum(vector&lt;int&gt;& numbers, int target) {\n    int l = 0, r = numbers.size() - 1, mid;\n    int start, end;\n    for (int i = 0; i &lt; numbers.size() - 1; i++) {\n      start = i, l = i + 1, r = numbers.size() - 1;\n      while (l &lt; r) {\n        mid = l + r &gt;&gt; 1;\n        if (numbers[mid] &gt;= (target - numbers[i])) r = mid;\n        else l = mid + 1;\n      }\n      end = l;\n      if (numbers[end] == (target - numbers[start])) { break; }\n    }\n    return {start + 1, end + 1};\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#找出第-k-小的距离对",
    "href": "posts/bin-search-template.html#找出第-k-小的距离对",
    "title": "二分查找-统一框架",
    "section": "找出第 k 小的距离对",
    "text": "找出第 k 小的距离对\n\n分析\n第k的最小距离，讲道理得用堆做，不过这题用二分也是可以做的，其实第k小的距离对，表明了当前数组中比如存在着\\(n-k\\geq 0\\)个距离对，他们的距离大于我们需要找的那一对。那么这题我们就是在\\(n-k\\geq 0\\)这个区间中找下界了，用模板一。\n当然难的二分题难点都不是二分，这题的困难在于如何统计有多少对距离对大于mid，并且我们找下界的方式可以提前退出。\n\n\n代码\nclass Solution {\n public:\n  int smallestDistancePair(vector&lt;int&gt;& nums, int k) {\n    sort(nums.begin(), nums.end());\n    int l = 0, r = ((*nums.rbegin()) - (*nums.begin())), mid;\n    while (l &lt; r) {\n      mid = l + r &gt;&gt; 1;\n      if (has_K_pair(nums, mid, k)) r = mid;\n      else l = mid + 1;\n    }\n    return l;\n  }\n  bool has_K_pair(vector&lt;int&gt;& nums, int mid, int k) {\n    int cnt = 0, l = 0;\n    // 用双指针的方式检测当前有多少个pair的差值大于mid\n    for (int r = 0; r &lt; nums.size(); r++) {\n      while (nums[r] - nums[l] &gt; mid) { l++; }\n      cnt += (r - l);\n      if (cnt &gt;= k) return true; \n    }\n    return false;\n  }\n};"
  },
  {
    "objectID": "posts/bin-search-template.html#总结",
    "href": "posts/bin-search-template.html#总结",
    "title": "二分查找-统一框架",
    "section": "总结",
    "text": "总结\n\n感谢y总的视频,以及宫水三叶小姐姐的一些题解～\n本文中的题目都选自二分查找的leetbook。\n大部分简单的二分题用模板就能轻松解决。\n困难的二分题大多都很难看出二段性，需要加强锻炼。"
  },
  {
    "objectID": "posts/benchmark-notes.html",
    "href": "posts/benchmark-notes.html",
    "title": "benchmark的经验与技巧",
    "section": "",
    "text": "为了公平对比性能都不是一件容易的事情. 各个框架的runtime都可能存在一些不同配置, 需要把他们安排到统一基准线去对比才有意义."
  },
  {
    "objectID": "posts/benchmark-notes.html#meta-schedule-禁用多线程",
    "href": "posts/benchmark-notes.html#meta-schedule-禁用多线程",
    "title": "benchmark的经验与技巧",
    "section": "meta schedule 禁用多线程",
    "text": "meta schedule 禁用多线程\nfrom tvm import meta_schedule as ms\n\n# disable parallel when num cores = 1.\nrules = ms.ScheduleRule.create('llvm')\nnewrules = []\nfor rule in rules:\n  if isinstance(rule, ms.schedule_rule.ParallelizeVectorizeUnroll):\n    newrules.append(ms.schedule_rule.ParallelizeVectorizeUnroll(-1, 64, [0, 16, 64, 512], True))\n  else:\n    newrules.append(rule.clone())\nmutators = ms.Mutator.create('llvm')\nnewmutators = []\nfor m in mutators:\n  if isinstance(m, ms.mutator.MutateParallel):\n    newmutators.append(ms.mutator.MutateParallel(-1))\n  else:\n    newmutators.append(m.clone())\nsg = ms.space_generator.PostOrderApply(sch_rules=newrules)\n\ndatabase = ms.tune_tir(\n    mod=prim_func,\n    target=\"llvm --num-cores=1\",\n    max_trials_global=64,\n    num_trials_per_iter=64,\n    work_dir=\"./tune_tmp\",\n    num_tuning_cores=4,\n    space=sg,\n)\n\nsch = ms.tir_integration.compile_tir(database, prim_func, \"llvm --num-cores=1\")"
  },
  {
    "objectID": "posts/benchmark-notes.html#numpy-禁止多线程",
    "href": "posts/benchmark-notes.html#numpy-禁止多线程",
    "title": "benchmark的经验与技巧",
    "section": "numpy 禁止多线程",
    "text": "numpy 禁止多线程"
  },
  {
    "objectID": "posts/benchmark-notes.html#iree-benchmark",
    "href": "posts/benchmark-notes.html#iree-benchmark",
    "title": "benchmark的经验与技巧",
    "section": "iree benchmark",
    "text": "iree benchmark\n基本流程\niree-compile \\\n  iree/matmul.mlir \\\n  --iree-hal-target-backends=llvm-cpu \\\n  --iree-llvmcpu-target-cpu=host \\\n  --iree-llvmcpu-target-cpu-features=host \\\n  --iree-llvmcpu-loop-interleaving \\\n  --iree-llvmcpu-slp-vectorization \\\n  --iree-llvmcpu-loop-unrolling \\\n  --iree-llvmcpu-loop-vectorization \\\n  --compile-mode=std \\\n  -o out/iree/matmul.vmbf\n\niree-benchmark-module \\\n  --module=out/iree/matmul.vmbf \\\n  --device=local-task \\\n  --task_topology_cpu_ids=0,1,2,3 \\\n  --function=abs \\\n  --input=1x1024x2048xf32=2 \\\n  --input=1x2048x512xf32=1\n–iree-hal-target-backends: - cuda - llvm-cpu, - cpu target 存在下面这些类型 - aarch64 - AArch64 (little endian) - aarch64_32 - AArch64 (little endian ILP32) - aarch64_be - AArch64 (big endian) - arm - ARM - arm64 - ARM64 (little endian) - arm64_32 - ARM64 (little endian ILP32) - armeb - ARM (big endian) - riscv32 - 32-bit RISC-V - riscv64 - 64-bit RISC-V - thumb - Thumb - thumbeb - Thumb (big endian) - wasm32 - WebAssembly 32-bit - wasm64 - WebAssembly 64-bit - x86 - 32-bit X86: Pentium-Pro and above - x86-64 - 64-bit X86: EM64T and AMD64 - metal-spirv - rocm - vmvx - vmvx-inline - vulkan-spirv\n–iree-benchmark-module –list_devices - local-sync:// synchronous, single-threaded driver that executes work inline - local-task:// asynchronous, multithreaded driver built on IREE’s “task” system"
  },
  {
    "objectID": "posts/benchmark-notes.html#带宽性能测试",
    "href": "posts/benchmark-notes.html#带宽性能测试",
    "title": "benchmark的经验与技巧",
    "section": "带宽性能测试",
    "text": "带宽性能测试\n暂时没有找到测试l1,l2 cache带宽的工具, 但是有lm_bench可以测试带宽的各种指标. 他提供了传统的bw mem:\nbw_mem_cp [ -P &lt;并行度&gt; ] [ -W &lt;热身次数&gt; ] [ -N &lt;重复次数&gt; ] 大小 rd|wr|rdwr|cp|fwr|frd|bzero|bcopy [对齐]\n描述\nbw_mem 分配两倍指定内存量，将其归零，然后将前半部分复制到后半部分。将每秒移动的兆字节数作为结果进行报告。 大小规范可能以“k”或“m”结尾，表示千字节 (* 1024) 或兆字节 (* 1024 * 1024)。\n输出\n输出格式为 CB”%0.2f %.2f“, 兆字节，兆字节每秒，即 8.00 25.33\nbw_mem 中有九种不同的内存基准。它们各自测量读取、写入或复制数据的方法略有不同。\n\nrd, 测量将数据读入处理器的时间。它计算整数值数组的总和。它每次访问四个字。\nwr, 测量将数据写入内存的时间。它为整数值数组的每个内存分配一个常量值。它每次访问四个字。\nrdwr 测量将数据读入内存然后将数据写入同一内​​存位置的时间。对于数组中的每个元素，它将当前值添加到运行总和中，然后再为元素分配新的（常量）值。它每次访问四个字。\ncp 测量将数据从一个位置复制到另一个位置的时间。它执行数组复制：dest[i] = source[i]。它每次访问四个字。\nfrd 测量将数据读入处理器的时间。它计算整数值数组的总和。\nfwr 测量将数据写入内存的时间。它为整数值数组的每个内存分配一个常量值。\nfcp 测量将数据从一个位置复制到另一个位置的时间。它执行数组复制：dest[i] = source[i]。\nbzero 测量系统清零内存的速度。\nbcopy 测量系统复制数据的速度。\n\n内存利用率\n此基准测试最多可将请求的内存移动三倍。Bcopy 将使用 2-3 倍的内存带宽：从源读取一次，然后写入目标。写入通常会导致缓存行读取，然后在稍后的某个时间点写回缓存行。如果处理器架构实现了“加载缓存行”和“存储缓存行”指令（以及“getcachelinesize”），内存利用率可能会减少 1/3。\n首先测到频率为\n/usr/lib/lmbench/bin/x86_64-linux-gnu/mhz\n2448 MHz, 0.4085 nanosec clock\n再测l1大小的读取\n/usr/lib/lmbench/bin/x86_64-linux-gnu/bw_mem 32K rd -N=10\n0.032000 100634.84\n然后计算byte/cycle:\n\\[\n\\begin{aligned}\n\\frac{Byte}{Hz} = \\frac{MB\\times 1024 \\times 1024}{S} \\div \\frac{MHz\\times 10^6}{S} = 43 B/Hz\n\\end{aligned}\n\\]\n计算写入带宽, 这里就勉强算一半吧.\n/usr/lib/lmbench/bin/x86_64-linux-gnu/bw_mem 32K wr -N=10\n0.032000 56970.93"
  },
  {
    "objectID": "posts/avltree.html",
    "href": "posts/avltree.html",
    "title": "平衡二叉树",
    "section": "",
    "text": "最后好久没写数据结构了，今天我把之前写的函数都写成C++的了。舒服的用一波C++中的queue和stack。 废话少说直接上代码（这次又调整了打印二叉树的程序 美滋滋）\n\n\nmain.cpp\n#include \"bantree.h\"\n#include &lt;iostream&gt;\n\nusing namespace std;\n\nvoid UseageHelp(void) {\n    cout &lt;&lt; \"\\n \\\n    AVL树的操作\\n \\\n    (1).创建新的AVL树;\\n \\\n    (2).插入元素;\\n \\\n    (3).删除元素;\\n \\\n    (4).寻找最大元素;\\n \\\n    (5).寻找最小元素;\\n \\\n    (6).绘制二叉树;\\n \\\n    (h).帮助;\\n \\\n    (q).quit;\\n \\\n    \" &lt;&lt; endl;\n}\n\nint main(int argc, char const *argv[]) {\n    ElementType i = 0;\n    AvlTree pTree = NULL;\n    AvlTree tmp = NULL;\n    UseageHelp();\n    while (1) {\n        switch (getchar()) {\n        case '1':\n            pTree = MakeEmpty(pTree);\n            cout &lt;&lt; \"创建成功 Tree:\" &lt;&lt; pTree &lt;&lt; endl;\n            break;\n        case '2':\n            cout &lt;&lt; \"请输入插入元素:\";\n            cin &gt;&gt; i;\n            pTree = Insert(i, pTree);\n            cout &lt;&lt; endl;\n            break;\n        case '3':\n            cout &lt;&lt; \"请输入删除元素:\";\n            cin &gt;&gt; i;\n            Delete(i, pTree);\n            cout &lt;&lt; endl;\n            break;\n        case '4':\n            tmp = FindMax(pTree);\n            if (tmp != nullptr) {\n                cout &lt;&lt; \"最大元素：\" &lt;&lt; tmp-&gt;Element &lt;&lt; endl;\n            }\n            cout &lt;&lt; endl;\n            break;\n        case '5':\n            tmp = FindMin(pTree);\n            if (tmp != nullptr) {\n                cout &lt;&lt; \"最小元素：\" &lt;&lt; tmp-&gt;Element &lt;&lt; endl;\n            }\n            cout &lt;&lt; endl;\n            break;\n        case '6':\n            DrawTree(pTree);\n            cout &lt;&lt; endl;\n            break;\n        case 'h':\n            UseageHelp();\n            break;\n        case 'q':\n            exit(0);\n            break;\n        default:\n            break;\n        }\n    }\n    return 0;\n}\n\n\nbantree.h\n#ifndef _BANTREE_H\n#define _BANTREE_H\n\n#define ElementType int\n\nstruct AvlNode;\ntypedef struct AvlNode *Position;\ntypedef struct AvlNode *AvlTree;\n\nAvlTree MakeEmpty(AvlTree T);\nvoid DrawTree(AvlTree BT);\nPosition Find(ElementType X, AvlTree T);\nPosition FindMin(AvlTree T);\nPosition FindMax(AvlTree T);\nAvlTree Insert(ElementType X, AvlTree T);\nAvlTree Delete(ElementType X, AvlTree T);\nElementType Retrieve(Position P);\nPosition SingleRotateWithLeft(Position K2);\nPosition SingleRotateWithRight(Position K2);\nPosition DoubleRotateWithLeft(Position K3);\nPosition DoubleRotateWithRight(Position K3);\n\n\nstruct AvlNode {\n    ElementType Element;\n    AvlTree Left;\n    AvlTree Right;\n    int Height;\n};\n\n#endif\n\n\nbantree.cpp\n#include \"bantree.h\"\n#include &lt;cmath&gt;\n#include &lt;iostream&gt;\n#include &lt;queue&gt;\n#include &lt;stack&gt;\n#include &lt;string&gt;\nusing namespace std;\n\nAvlTree MakeEmpty(AvlTree T) {\n\n    if (T != nullptr) {\n        MakeEmpty(T-&gt;Left);\n        MakeEmpty(T-&gt;Right);\n        delete T;\n    }\n    return NULL;\n}\n\nstatic int Height(Position P) {\n    if (P == nullptr) {\n        return -1;\n    } else {\n        return P-&gt;Height;\n    }\n}\n\n/**\n * description  输出二叉树的高度\n * @param[in]   BinTree_t\n * @retval      int\n **/\nstatic int FindTreeHeight(AvlTree BT) {\n    int rightlen, leftlen, maxlen;\n    if (BT) {\n        rightlen = FindTreeHeight(BT-&gt;Right);\n        leftlen = FindTreeHeight(BT-&gt;Left);\n        maxlen = leftlen &gt; rightlen ? leftlen : rightlen;\n        return maxlen + 1;\n    } else {\n        return 0;\n    }\n}\n\n/**\n * @brief  用于清空队列元素\n * @param[in]\n * @param[out]\n * @return\n **/\nstatic void ClearQueue(queue&lt;AvlTree&gt; &q) {\n    queue&lt;AvlTree&gt; empty;\n    swap(empty, q);\n}\n\n/**\n * @brief  简单的绘制二叉树图像\n * @param[in]\n * @param[out]\n * @return\n **/\nvoid DrawTree(AvlTree BT) {\n    int height = FindTreeHeight(BT);\n    int cnt = 0;\n\n    /* 开始层序遍历二叉树并且绘制图形 */\n    queue&lt;AvlTree&gt; CurLevelqueue;  /*记录当前层的元素  */\n    queue&lt;AvlTree&gt; NextLevelqueue; /*记录下一层的元素  */\n    queue&lt;AvlTree&gt; CurTempqueue;   /*当前队列副本  */\n    queue&lt;AvlTree&gt; NextTempqueue;  /*下一层队列副本  */\n    AvlTree temp, lasttemp = NULL;\n    int width = 0;\n    CurLevelqueue.push(BT); //将头节点入队\n\n    /*现在修改了入队函数,空指针也可以入队\n    所以在出队的时候就需要进行判断  */\n    for (int i = 0; i &lt; height; i++) {\n        cnt = (int)pow(2, i);           //当前行的个数21\n        width = pow(2, height - i + 1); //设置宽度为2^(height-i+1)\n        CurTempqueue = CurLevelqueue;   /* 临时记录 */\n        while (cnt--) {\n            temp = CurLevelqueue.front();\n            if (temp != NULL) {\n                printf(\"%*d%*c\", width, temp-&gt;Element, width, ' ');\n                NextLevelqueue.push(temp-&gt;Left);\n                NextLevelqueue.push(temp-&gt;Right); //将下一层子节点入队\n            } else {\n                printf(\"%*c%*c\", width, ' ', width, ' ');\n                NextLevelqueue.push(nullptr);\n                NextLevelqueue.push(nullptr); //如果此层是空，那么再入两个空指针\n            }\n            CurLevelqueue.pop(); //将上一层节点出队\n        }\n        printf(\"\\n\");\n        NextTempqueue = NextLevelqueue; /* 记录下一层元素 */\n        /* 接下来打印层间符号 */\n        if (i != height - 1) { /* 若不是最后一行 */\n            /* 先记录下一行元素宽度间隔 */\n            int nextwidth = (int)pow(2, height - i);\n            // /* 按当前层元素位置打印'+' */\n            // for (int J = 0; J &lt; (int)pow(2, i); J++) {\n            //     /* 如果不是最后一行且为null指针 不打印 */\n            //     if (CurTempqueue.front() != nullptr) {\n            //         printf(\"%*c%*c\", width, '+', width, ' ');\n            //     } else {\n            //         printf(\"%*c%*c\", width, ' ', width, ' ');\n            //     }\n            //     CurTempqueue.pop();\n            // }\n            // printf(\"\\n\");\n            /* 并且以他下一层元素的个数打印'-' */\n            for (int k = 0; k &lt; (int)pow(2, i + 1); k++) {\n                /* 每隔一位去打印width个'-' */\n                if ((k % 2) == 0) { /* 偶数位打印 空格+`-` */\n                    if (NextTempqueue.front() != nullptr) {\n                        printf(\"%*c\", nextwidth, '-');\n                        for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                            printf(\"-\");\n                        }\n                    } else {\n                        printf(\"%*c\", nextwidth, ' ');\n                        for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                            printf(\" \");\n                        }\n                    }\n                    NextTempqueue.pop();\n                } else { /* 奇数位打印 空格+`-` */\n                    if (NextTempqueue.front() != nullptr) {\n                        for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                            printf(\"-\");\n                        }\n                        printf(\"%*c\", nextwidth, ' ');\n                    } else {\n                        for (uint8_t z = 0; z &lt; nextwidth; z++) {\n                            printf(\" \");\n                        }\n                        printf(\"%*c\", nextwidth, ' ');\n                    }\n                    NextTempqueue.pop();\n                }\n            }\n            printf(\"\\n\");\n        }\n        printf(\"\\r\");\n        swap(CurLevelqueue, NextLevelqueue); /* 交换队列 */\n    }\n}\n\nPosition Find(ElementType X, AvlTree T) {\n\n    if (T == nullptr) {\n        return nullptr;\n    }\n\n    if (X &lt; T-&gt;Element) {\n        return Find(X, T-&gt;Left);\n    } else if (X &gt; T-&gt;Element) {\n        return Find(X, T-&gt;Right);\n    } else {\n        return T;\n    }\n}\nPosition FindMin(AvlTree T) {\n    if (T == nullptr) {\n        return nullptr;\n    }\n    if (T-&gt;Left != nullptr) {\n        return FindMin(T-&gt;Left);\n    } else {\n        return T;\n    }\n}\nPosition FindMax(AvlTree T) {\n    if (T == nullptr) {\n        return nullptr;\n    }\n    if (T-&gt;Right != nullptr) {\n        return FindMin(T-&gt;Right);\n    } else {\n        return T;\n    }\n}\n/**\n * @brief  平衡树的插入，需要记录每个节点的高度,并实现旋转\n * @param[in]\n * @param[out]\n * @return\n **/\nAvlTree Insert(ElementType X, AvlTree T) {\n\n    if (T == nullptr) {\n        T = new AvlNode;\n        if (T == nullptr) {\n            std::cout &lt;&lt; \"no mem!\" &lt;&lt; std::endl;\n        } else {\n            T-&gt;Element = X, T-&gt;Height = 0;\n            T-&gt;Left = T-&gt;Right = nullptr;\n        }\n    } else if (X &lt; T-&gt;Element) {\n        T-&gt;Left = Insert(X, T-&gt;Left);\n        if ((Height(T-&gt;Left) - Height(T-&gt;Right)) == 2) {\n            if (X &lt; T-&gt;Left-&gt;Element) {\n                /* new node in left's left */\n                T = SingleRotateWithLeft(T);\n            } else {\n                /* new node in left's right */\n                T = DoubleRotateWithLeft(T);\n            }\n        }\n    } else if (X &gt; T-&gt;Element) {\n        T-&gt;Right = Insert(X, T-&gt;Right);\n        if ((Height(T-&gt;Right) - Height(T-&gt;Left)) == 2) {\n            if (X &gt; T-&gt;Right-&gt;Element) {\n                /* new node in right's right */\n                T = SingleRotateWithRight(T);\n            } else {\n                /* new node in right's left */\n                T = DoubleRotateWithRight(T);\n            }\n        }\n    }\n    T-&gt;Height = max(Height(T-&gt;Left), Height(T-&gt;Right)) + 1;\n    return T;\n}\n\nAvlTree Delete(ElementType X, AvlTree T) {\n    Position tmpCell;\n    if (T == nullptr) {\n        std::cout &lt;&lt; \"error\" &lt;&lt; std::endl;\n        return nullptr;\n    }\n\n    else if (X &lt; T-&gt;Element) {\n        T-&gt;Left = Delete(X, T-&gt;Left);\n    } else if (X &gt; T-&gt;Element) {\n        T-&gt;Right = Delete(X, T-&gt;Right);\n    }\n\n    else if (T-&gt;Left && T-&gt;Right) {\n        tmpCell = FindMin(T-&gt;Right);\n        T-&gt;Element = tmpCell-&gt;Element;\n        T-&gt;Right = Delete(tmpCell-&gt;Element, T-&gt;Right);\n    }\n\n    else {\n        tmpCell = T;\n        if (T-&gt;Left == nullptr) {\n            T = T-&gt;Right;\n        } else if (T-&gt;Right == nullptr) {\n            T = T-&gt;Right;\n        }\n        delete tmpCell;\n    }\n}\nElementType Retrieve(Position P) {}\n\n/**\n* @brief  单左旋转\n    当“麻烦节点”在“被发现者”的左子树的左边时，被称为LL插入\n    需要对 '被发现者' RL旋转（左单旋）\n**/\nPosition SingleRotateWithLeft(Position K2) {\n    Position K1;\n    K1 = K2-&gt;Left;\n    K2-&gt;Left = K1-&gt;Right;\n    K1-&gt;Right = K2;\n    K2-&gt;Height = max(Height(K2-&gt;Left), Height(K2-&gt;Right)) + 1;\n    K1-&gt;Height = max(Height(K1-&gt;Left), K2-&gt;Height) + 1;\n    return K1; /* 新的根 */\n}\n/**\n* @brief  单右旋转\n    当“麻烦节点”在“被发现者”的右子树的右边时，被称为RR插入\n    需要对 '被发现者' RR旋转（右单旋）\n**/\nPosition SingleRotateWithRight(Position K2) {\n    Position K1;\n    K1 = K2-&gt;Right;\n    K2-&gt;Right = K1-&gt;Left;\n    K1-&gt;Left = K2;\n    K2-&gt;Height = max(Height(K2-&gt;Left), Height(K2-&gt;Right)) + 1;\n    K1-&gt;Height = max(Height(K1-&gt;Left), K2-&gt;Height) + 1;\n    return K1; /* 新的根 */\n}\n\nPosition DoubleRotateWithLeft(Position K3) {\n    /* first rotate the k3'left */\n    K3-&gt;Left = SingleRotateWithRight(K3-&gt;Left);\n    return SingleRotateWithLeft(K3);\n}\n\nPosition DoubleRotateWithRight(Position K3) {\n    /* first rotate the k3'right */\n    K3-&gt;Right = SingleRotateWithLeft(K3-&gt;Right);\n    return SingleRotateWithRight(K3);\n}\n\n\nMakefile\nCC = g++\nCFLAGS = #-g \nCLIBS = #-lpthread\n \nINCLUDE = $(wildcard ./*.h) # INCLUDE = a.h b.h ... can't be defined like \"INCLUDE = ./*.h\"\nSOURCES = $(wildcard ./*.cpp)\n \nTARGET = avltree\nOBJECTS = $(patsubst %.cpp,%.o,$(SOURCES))\n \n$(TARGET) : $(OBJECTS)\n    $(CC) $(CFLAGS) $^ -o $@ $(CLIBS)\n    rm -rf *.o\n\n$(OBJECTS) : %.o : %.cpp\n    $(CC) -c $(CFLAGS) $&lt; -o $@\n \n.PHONY : clean\nclean:\n    rm -rf *.o $(TARGET)  \n\n\n测试命令\n➜  balanceTree cat test.txt\n1\n2\n32\n2\n33\n2\n31\n2\n36\n2\n38\n2\n20\n2\n24\n2\n94\n2\n58\n2\n12\n6\nq\n➜  balanceTree make && cat test.txt | ./avltree\ng++ -c  main.cpp -o main.o\ng++ -c  bantree.cpp -o bantree.o\ng++  main.o bantree.o -o avltree\nrm -rf *.o\n\n     AVL树的操作\n     (1).创建新的AVL树;\n     (2).插入元素;\n     (3).删除元素;\n     (4).寻找最大元素;\n     (5).寻找最小元素;\n     (6).绘制二叉树;\n     (h).帮助;\n     (q).quit;\n\n创建成功 Tree:0\n请输入插入元素:\n请输入插入元素:\n请输入插入元素:\n请输入插入元素:\n请输入插入元素:\n请输入插入元素:\n请输入插入元素:\n请输入插入元素:\n请输入插入元素:\n请输入插入元素:\n                              32\n               ---------------------------------\n              24                              36\n       -----------------               -----------------\n      20              31              33              58\n   -----                                           ---------\n  12                                              38      94"
  },
  {
    "objectID": "posts/arraypointer.html",
    "href": "posts/arraypointer.html",
    "title": "C数组指针",
    "section": "",
    "text": "今天看c陷阱与缺陷，发现这个数组指针挺有意思的。 首先定义一个数组指针：int (*p)[4]。程序如下：\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char const *argv[])\n{\n    int calendar[3][4]={\n    {1,2,3,4,},\n    {5,6,7,8,},\n    {9,10,11,12,},};\n    int (*p)[4];\n    int i;\n    p=calendar;\n\n    return 0;\n}\n通过p=calendar，这样p就指向了calendar第一个元素，也就是calendar的3个有着4个元素的元素的第一个元素。"
  },
  {
    "objectID": "posts/arraypointer.html#定义",
    "href": "posts/arraypointer.html#定义",
    "title": "C数组指针",
    "section": "",
    "text": "今天看c陷阱与缺陷，发现这个数组指针挺有意思的。 首先定义一个数组指针：int (*p)[4]。程序如下：\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char const *argv[])\n{\n    int calendar[3][4]={\n    {1,2,3,4,},\n    {5,6,7,8,},\n    {9,10,11,12,},};\n    int (*p)[4];\n    int i;\n    p=calendar;\n\n    return 0;\n}\n通过p=calendar，这样p就指向了calendar第一个元素，也就是calendar的3个有着4个元素的元素的第一个元素。"
  },
  {
    "objectID": "posts/arraypointer.html#例子",
    "href": "posts/arraypointer.html#例子",
    "title": "C数组指针",
    "section": "例子",
    "text": "例子\n这里通过一系列的小例子去实验说明p的用法。\n\n例子1\nprintf(\"%d\\n\",calendar[2][2]);\nprintf(\"%d\\n\",*(*(calendar+2)+2));\nprintf(\"%d\\n\",*(*(p+2)+2));\n输出\n11\n11\n11\n这个例子还是挺好理解的，p=calendar那么他们的地址相同，用法相同即可寻找到对应的元素。 当然要注意一点*(calendar+2)这样才能指向二维数组的第3行。\n例子2\nprintf(\"%d\\n\",calendar[2][2]);\nprintf(\"%d\\n\",*(*calendar+10));\nprintf(\"%d\\n\",*(*p+10));\n输出\n11\n11\n11\nprintf(\"%p\\n\",p);\nprintf(\"%p\\n\",calendar);\nprintf(\"%p\\n\",*p);\nprintf(\"%p\\n\",*calendar);\n输出\n0x7ffdcddf8480\n0x7ffdcddf8480\n0x7ffdcddf8480\n0x7ffdcddf8480\n这里要注意虽然他们的指向的地址是一样的，但是只能使用*(*p+10)这样的形式，*(p+10)这个形式是不被允许的。 这个应该是由于c语言中对于&calendar==calendar的定义，虽然编译器会提示warning，但是其返回值是成立的。\n例子3\nprintf(\"%d\\n\",(**p+10));\nprintf(\"%d\\n\",**p);\nprintf(\"%d\\n\",calendar[0][0]);\n输出\n11\n1\n1\n这个**p指向的就是二维数组第一个元素的值。"
  },
  {
    "objectID": "posts/animegan.html",
    "href": "posts/animegan.html",
    "title": "Anime GAN",
    "section": "",
    "text": "最近偶然看到一篇AnimeGAN的推送，他的官方实现在这里。我很感兴趣，尝试学习并复现，下面是我的一些记录。\nNOTE：虽然很有趣，但是没算力真的让我想放弃。"
  },
  {
    "objectID": "posts/animegan.html#生成器",
    "href": "posts/animegan.html#生成器",
    "title": "Anime GAN",
    "section": "生成器",
    "text": "生成器\n作者这里应该也是用他参考的项目里面模型。反正就是加残差的编码解码模型。这里用了谱归一化，我提供了一个实现在这里。"
  },
  {
    "objectID": "posts/animegan.html#判别器",
    "href": "posts/animegan.html#判别器",
    "title": "Anime GAN",
    "section": "判别器",
    "text": "判别器\n判别器也挺普通，但是因为我生成图像太真实，发现最后判别器的判别效果太好，我在想要不要加点Dropout，不过算力不够暂时还没有实验过。"
  },
  {
    "objectID": "posts/animegan.html#初始化损失",
    "href": "posts/animegan.html#初始化损失",
    "title": "Anime GAN",
    "section": "初始化损失",
    "text": "初始化损失\n由于GAN比较不稳定，首先先训练一个epoch的一致性损失：\nNOTE： 这里我用了MobileNetV2，原作者使用VGG19，实际训练并无多大差别，还有就是weights的使用，实际上在我做互信息的时候，仅初始化过得模型对于输入输出的映射是更加单一的，所以尝试了不用预训练模型，实际训练也没有多大差别。\n  def local_variables_init(self):\n    \"\"\" 定义一个辅助模型，是的模型生成图像更加符合原本图像\"\"\"\n    inputs = tf.keras.Input([256, 256, 3])\n    model = tf.keras.applications.MobileNetV2(\n        include_top=False,\n        alpha=1.3,\n        weights=None,  #'imagenet',\n        input_tensor=inputs,\n        pooling=None,\n        classes=1000)\n    self.p_model: tf.keras.Model = tf.keras.Model(\n        inputs,\n        model.get_layer('block_6_expand').output)\n    # model: tf.keras.Model = tf.keras.applications.VGG19(\n    #     include_top=False,\n    #     weights='imagenet',\n    #     input_tensor=inputs,\n    #     pooling=None,\n    #     classes=1000)\n    # self.p_model = tf.keras.Model(\n    #     inputs,\n    #     tf.keras.layers.Activation('linear', dtype=tf.float32)(\n    #         model.get_layer('block4_conv4').output))\n    self.p_model.trainable = False\n一致性损失代码如下。\n  @staticmethod\n  def con_loss(pre_train_model, real_data, fake_data):\n    real_fmap = pre_train_model(real_data, training=False)\n    fake_fmap = pre_train_model(fake_data, training=False)\n    con_loss = AnimeGanInitLoop.l1_loss(real_fmap, fake_fmap)\n    return con_loss"
  },
  {
    "objectID": "posts/animegan.html#生成器损失",
    "href": "posts/animegan.html#生成器损失",
    "title": "Anime GAN",
    "section": "生成器损失",
    "text": "生成器损失\n生成器损失由以下几部分组成：\n\n一致性损失，风格损失\n\n生成图像与现实图像的一致性损失，生成图像与动漫图像的风格损失。\n  @staticmethod\n  def style_loss(style, fake):\n    return AnimeGanLoop.l1_loss(AnimeGanLoop.gram(style), AnimeGanLoop.gram(fake))\n\n  @staticmethod\n  def con_sty_loss(pre_train_model, real, anime, fake):\n    real_feature_map = pre_train_model(real, training=False)\n\n    fake_feature_map = pre_train_model(fake, training=False)\n\n    anime_feature_map = pre_train_model(anime, training=False)\n\n    c_loss = AnimeGanLoop.l1_loss(real_feature_map, fake_feature_map)\n    s_loss = AnimeGanLoop.style_loss(anime_feature_map, fake_feature_map)\n\n    return c_loss, s_loss\n\n色彩损失\n\n生成图像与动漫图像的色彩损失。\n  @staticmethod\n  def color_loss(con, fake):\n    con = AnimeGanLoop.rgb2yuv(con)\n    fake = AnimeGanLoop.rgb2yuv(fake)\n\n    return AnimeGanLoop.l1_loss(con[..., 0], fake[..., 0]) + huber_loss(\n        con[..., 1], fake[..., 1]) + huber_loss(con[..., 2], fake[..., 2])\n\n生成损失\n\n我尝试之后发现还是lsgan最稳定，这里用的就是lsgan。\n  @staticmethod\n  def generator_loss(loss_type, fake_logit):\n\n    if loss_type == 'wgan-gp' or loss_type == 'wgan-lp':\n      fake_loss = -tf.reduce_mean(fake_logit)\n\n    if loss_type == 'lsgan':\n      fake_loss = tf.reduce_mean(tf.square(fake_logit - 1.0))\n\n    if loss_type == 'gan' or loss_type == 'dragan':\n      fake_loss = tf.reduce_mean(\n          tf.nn.sigmoid_cross_entropy_with_logits(\n              labels=tf.ones_like(fake_logit), logits=fake_logit))\n\n    if loss_type == 'hinge':\n      fake_loss = -tf.reduce_mean(fake_logit)\n\n    return fake_loss"
  },
  {
    "objectID": "posts/animegan.html#判别器损失",
    "href": "posts/animegan.html#判别器损失",
    "title": "Anime GAN",
    "section": "判别器损失",
    "text": "判别器损失\n分别对动画图像、生成图像、灰度动画图像、模糊动画图像进行判别。\nNOTE：我之前发现生成的图像太真实，给作者提了个issue，他说让我给fake_loss增强权重，然后我发现生成的图像更加真实了。。具体我在下面实验部分说明。\n  @staticmethod\n  def discriminator_loss(loss_type, real, gray, fake, real_blur):\n    real_loss = 0\n    gray_loss = 0\n    fake_loss = 0\n    real_blur_loss = 0\n\n    if loss_type == 'wgan-gp' or loss_type == 'wgan-lp':\n      real_loss = -tf.reduce_mean(real)\n      gray_loss = tf.reduce_mean(gray)\n      fake_loss = tf.reduce_mean(fake)\n      real_blur_loss = tf.reduce_mean(real_blur)\n\n    if loss_type == 'lsgan':\n\n      real_loss = tf.reduce_mean(tf.square(real - 1.0))\n      gray_loss = tf.reduce_mean(tf.square(gray))\n      fake_loss = tf.reduce_mean(tf.square(fake))\n      real_blur_loss = tf.reduce_mean(tf.square(real_blur))\n\n    if loss_type == 'gan' or loss_type == 'dragan':\n      real_loss = tf.reduce_mean(\n          tf.nn.sigmoid_cross_entropy_with_logits(\n              labels=tf.ones_like(real), logits=real))\n      gray_loss = tf.reduce_mean(\n          tf.nn.sigmoid_cross_entropy_with_logits(\n              labels=tf.zeros_like(gray), logits=gray))\n      fake_loss = tf.reduce_mean(\n          tf.nn.sigmoid_cross_entropy_with_logits(\n              labels=tf.zeros_like(fake), logits=fake))\n      real_blur_loss = tf.reduce_mean(\n          tf.nn.sigmoid_cross_entropy_with_logits(\n              labels=tf.zeros_like(real_blur), logits=real_blur))\n\n    if loss_type == 'hinge':\n      real_loss = tf.reduce_mean(tf.nn.relu(1.0 - real))\n      gray_loss = tf.reduce_mean(tf.nn.relu(1.0 + gray))\n      fake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake))\n      real_blur_loss = tf.reduce_mean(tf.nn.relu(1.0 + real_blur))\n\n    return real_loss, gray_loss, fake_loss, real_blur_loss"
  },
  {
    "objectID": "posts/akg-micro20.html",
    "href": "posts/akg-micro20.html",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "",
    "text": "这篇文章是赵捷老师在mircro 2020上发表的论文,我觉得这篇文章对理解akg系列的工作比较重要,所以仔细阅读了一遍."
  },
  {
    "objectID": "posts/akg-micro20.html#a.tiling-and-fusion-in-the-polyhedral-model",
    "href": "posts/akg-micro20.html#a.tiling-and-fusion-in-the-polyhedral-model",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "A.Tiling and Fusion in the Polyhedral Model",
    "text": "A.Tiling and Fusion in the Polyhedral Model\n多面体模型是对于自动并行化和局部性优化的抽象.它使用迭代域/访问关系/依赖与调度来表示程序.典型的,多面体模型的schedule同时表示原始的程序计算顺序,和经过调度算法优化后的结果.schedule是一个跨多个statement实例的一个affine function,(比如迭代域).一个调度算法比如考虑所有statement 实例之间的计算访问依赖关系.访问关系是statement 实例与内存位置的map.\n\nFig.1(a): The loop nests\n\n\n\nFig.1(a): The loop nests\n\n\n如图1(a)考虑一个在输入图像A和kernel B的上做2D卷积,输出buffer为C.其中 statement \\(S_1\\)表示初始化矩阵.\\(S_2\\) 执行reduction. \\(S_3\\)执行 RELU 操作.使用多面体模型,最初的schedule可以被表示为多维仿射schedule如: \\[\n\\left[S_0\\left(h,w\\right)\\rightarrow \\left(0,h,w\\right);  \\\\\nS_1\\left(h,w\\right) \\rightarrow \\left(1,h,w,0\\right); \\\\\nS_2\\left(h,w,kh,kw\\right) \\rightarrow \\left(1,h,w,1,kh,kw\\right); \\\\\nS_3\\left(h,w\\right) \\rightarrow \\left(2,h,w\\right)\\right]\n\\]\n\n\nFig.1(b): The code and GPU mapping of a conservative fusion heuristic\n\n\n\nFig.1(b): The code and GPU mapping of a conservative fusion heuristic\n\n\n多面体模型可以通过融合不同的启发式fusion来计算出一个新的/tiling友好的schedule结果.通过保守的启发式fusion,得到新的schedule被表示为: \\[\n\\left[ S_0\\left(h,w\\right) \\rightarrow \\left(0,h,w\\right); \\\\\nS_1\\left(h,w\\right) \\rightarrow \\left(1,h,w,0,0,0\\right); \\\\\nS_2\\left(h,w,kh,kw\\right) \\rightarrow\\left(1,h,w,kh,kw,1\\right); \\\\\nS_3\\left(h,w\\right) \\rightarrow\\left(1,h,w,KH − 1,KW − 1,2\\right)\\right]\n\\]\n然后使用\\((\\{S_0\\},\\{S_1,S_2,S_3\\})\\)表示整个fusion结果.现在就可以应用一个矩形tiling在fusion结果上,比如\\(\\{S_0\\}\\)上使用\\(T_0 \\times T_1\\)的tlie size,在\\(\\{S_1,S_2,S_3\\}\\)使用\\(T_2\\times T_3\\)的tile size.按照图1(b),将输入图像A矩阵使用\\(T_0\\times T_1\\)切开后,每个维度上tile的迭代变量使用\\(ht,wt\\)来表示,\\(hp,wp\\)表示指针循环.(按照Halide中的说法,可以理解成外循环和内循环,外循环指向每个tile,内循环指向的是tile内部).⚠️tile size必须是固定的整型常量,但是为了在本文中为了解释方便,这里使用符号化的\\(T\\)来表示.\n\n\nFig.1(c): The code of an aggressive fusion heuristic\n\n\n\nFig.1(c): The code of an aggressive fusion heuristic\n\n\n另一方面,如图1(c)所示,激进的启发式fusion将通过合并所有的statemens来最大化数据的局部性.虽然这个策略最大化了producer-consumer局部性,但是也降低了可tile的维度以及最外层的并行度.多面体模型的内部修改transform也会导致另外的问题:在神经网络的域特的情况下由于维度的permutation引起的fused loop的维度与用户指定的tile size不匹配.\n接下来本文将讨论tiling后fusion的策略在现存的多面体优化中是无法充分利用内存层级,因此需要一个其他的方式通过改变tiling和fusion的顺序来避免做可切分/可并行以及局部性之间的trade-off.为了实现reorder功能,本文使用了schedule trees32的方法.\n32 Polyhedral ast generation is more than scanning polyhedra"
  },
  {
    "objectID": "posts/akg-micro20.html#b.schedule-trees",
    "href": "posts/akg-micro20.html#b.schedule-trees",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "B.Schedule Trees",
    "text": "B.Schedule Trees\n多面体模型使用多维的仿射schedule来表示整个代码的计算顺序,但是这种表示方法无法简单的扩展到GPU上的自动内存管理(比如自动插入thread-level的同步指令).affine schedule可以被显式的编码到树结构33,这样可以简化多面体编译器中的自动内存管理建模.\n33 Polyhedral ast generation is more than scanning polyhedra34 Code generation in the polyhedral model is easier than you think35 Polyhedral ast generation is more than scanning polyhedra36 Polyhedral code generation in the real world根据多维仿射schedule为程序建立初始调度树,一个调度树开始于一个被称为domain 的节点,这个节点存储了所有的statement 实例(比如Presburger表示的迭代域),引入一个sequence的节点来显式的表示被使用在多维仿射schedule中的标量维度,为他的子节点定义了特别的顺序.每个sequence的子节点都必须是filter节点,他只能包含其父节点的statement与当前节点引入的statement的一个子集.band节点是用于将常量或变量的维度编码,来以分段线性的形式表示在迭代域上的多维仿射程序.许多多面体代码生成器34,35,36输入迭代域和从多面体模型产生的新调度树,用于生成命令式代码.将迭代域和schedule放在一起,仅扫描整个树,就可以生成代码.\n\nFig.2(a): The initial schedule tree\n\n\n\nFig.2(a): The initial schedule tree\n\n\n\n\nFig.2(b): The schedule tree after fusion\n\n\n\nFig.2(b): The schedule tree after fusion\n\n\n比如将图1(a)转化为图2(a)的调度树的形式,其中domain节点可以被表示为Presburger set: \\[\n\\{ S_0(h,w) : 0 \\leqslant h \\lt H \\land 0 \\leqslant w \\lt W ;\\\\\n  S_1(h,w) : 0 \\leqslant h \\leqslant H − KH \\land 0 \\leqslant w \\leqslant W −KW ;  \\\\\n  S_2(h,w,kh,kw) : 0 \\leqslant h \\leqslant H − KH \\land 0 \\leqslant w \\leqslant W − KW \\land 0 \\leqslant kh \\lt KH \\land 0 \\leqslant kw \\lt KW ; \\\\\n  S_3(h,w) : 0 \\leqslant h \\leqslant H − KH \\land 0 \\leqslant w \\leqslant W − KW  \\}\n\\] 初始后的调度树可以通过附加到band节点上的并行/tile信息来自动的transform到新的调度树.\nband节点用于表示嵌套循环,并添加上可以指导编译器进行transformation的信息.\\(band_0\\)是一个仿射函数\\([\\{S_0(h,w)\\rightarrow (h,w)\\}]\\)被添加上了两个属性, 一个是bool的permutable,用于表示当前的嵌套循环是否可以tile. 另一个是vector的coincident,每一个元素都用来表示当前单个的循环的并行度(1是可并行,0是不可并行).在图2(a)中,permutable和coincident分别是1和[1,1],这表示这两个循环的嵌套可以被tiling也可以进行2维的并行.\n这里只展示了本文工作所需要的节点类型,更加详细的内容需要参考Grosser等人的工作37中关于整个调度树的描述.特别的,本文引入了一种extension的节点,定义为仿射函数在其外部调度维度到statement实例或数组/标量元素.extension节点可用于通过将自身保持在调度树中的合适位置来自动化内存管理.后面本文将利用extension节点来实现tiling后fusion算法.\n37 Polyhedral ast generation is more than scanning polyhedra"
  },
  {
    "objectID": "posts/akg-micro20.html#a.extracting-upwards-exposed-data",
    "href": "posts/akg-micro20.html#a.extracting-upwards-exposed-data",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "A.Extracting Upwards Exposed Data",
    "text": "A.Extracting Upwards Exposed Data\nreduction space中数据访问可以分为读访问和写访问.本文用upwards exposed data表示reduction space读取的数据(这个数据在quantization space中被定义).可以通过组建reduction space的依赖关系和读访问关系来轻松的构建计算tile与upwards exposed data之间的关系.tensor A的data space在图4的中描述,每个数据图用相同的颜色表示,并在reduction space中表示与其相应的tile.\n\nFig.4: Constructing tile shapes via upwards exposed data\n\n\n\nFig.4: Constructing tile shapes via upwards exposed data\n\n\n现在开始解释这个例子,以简单起见,本文仅讨论\\(S_2\\)在reduction space中读取tensor A为例.应用在reduction space上的一段transform可以被表示为如下: \\[\n\\left[\\left\\{S_{2}(h,w,k h,k w) \\rightarrow\\left(o_{0}=h / T_{2},o_{1}=w / T_{3}\\right)\\right\\}\\right] \\tag{2}\n\\] 其中\\(o_0\\)和\\(o_1\\)表示tile的维度.\\(S_2\\)上与upwards exposed data的访问关系表示为如下: \\[\n\\begin{array}{r}\n\\left\\{S_{2}(h,w,k h,k w) \\rightarrow A(h+k h,w+k w): 0 \\leq h \\leq H-K H \\right.\\\\\n\\wedge 0 \\leq w \\leq W - K W \\wedge 0 \\leq k h \\lt K H \\wedge 0 \\leq k w \\lt K W\\}\n\\end{array} \\tag{3}\n\\]\ntile维度\\((o_0,o_1)\\)和upwards exposed data间的关系可以通过isl的交集+反转得到:\n\\[\n\\begin{array}{r}\n\\left\\{\\left(o_{0},o_{1}\\right) \\rightarrow A\\left(h^{\\prime},w^{\\prime}\\right): 0 \\leq o_{0} \\lt \\left\\lceil(H-K H+1) / T_{2}\\right\\rceil\\right.\\\\\n\\wedge 0 \\leq o_{1} \\lt \\left\\lceil(W-K W+1) / T_{3}\\right\\rceil \\wedge T_{2} \\cdot o_{0} \\leq h^{\\prime} \\lt T_{2} \\cdot o_{0}+ \\\\\n\\left.K H+T_{2}-1 \\wedge T_{3} \\cdot o_{1} \\leq w^{\\prime} \\lt T_{3} \\cdot o_{1}+K W+T_{3}-1\\right\\}\n\\end{array} \\tag{4}\n\\]\n如图4中reduction space与data space之间的虚线.它允许在两个连续计算tile之间计算重叠的内存足迹.\n继续关注reduction space中的蓝色tile和红色tile,为了通用性,假设\\(T_2=T_3=2\\),蓝色tile可以被表示为坐标\\((o_0 =1,o_1 = 0)\\),红色tile可以被表示为坐标\\((o_0 =1,o_1 = 1)\\).可以应用上述公式获得他们的内存足迹,分别表示为 ${A(h’,w’): 2h’ w’ } \\(和\\){A(h’,w’):2h’ w’ }$. 换句话说,两个tile都可以访问它们的交集,即data space中蓝色和红色tile之间的交错区域"
  },
  {
    "objectID": "posts/akg-micro20.html#b.tiling-intermediate-computation-spaces",
    "href": "posts/akg-micro20.html#b.tiling-intermediate-computation-spaces",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "B.Tiling Intermediate Computation Spaces",
    "text": "B.Tiling Intermediate Computation Spaces\n通过公式4获得的内存足迹可用于构建quantization space的tile shape,既写入tensor A所需要的内存.使用多面体模型,在affine map使用基本的operation就可以得到quantization space的tile shape.\n多面体模型可以提供quantization space在tensor A上的访问关系. 可以通过反转写访问的relation 来获得tensor A 到\\(S_0\\)的affine map. \\[\n\\left\\{A(h,w) \\rightarrow S_{0}(h,w): 0 \\leq h \\lt H \\wedge 0 \\leq w&lt;W\\right\\} \\tag{5}\n\\]\ndata space与quantization space的访问关系如图4中点线所示,结合公式4和5得到另一个affine relation: \\[\n\\begin{array}{r}\n\\left\\{\\left(o_{0},o_{1}\\right) \\rightarrow S_{0}(h,w): 0 \\leq o_{0}\\lt \\left \\lceil (H-K H+1) / T_{2} \\right \\rceil \\right.\\\\\n\\wedge 0 \\leq o_{1} \\lt \\left\\lceil(W-K W+1) / T_{3}\\right\\rceil \\wedge T_{2} \\cdot o_{0} \\leq h \\lt T_{2} \\cdot o_{0} \\\\\n\\left.+K H+T_{2}-1 \\wedge T_{3} \\cdot o_{1} \\leq w \\lt T_{3} \\cdot o_{1}+K W+T_{3}-1\\right\\}\n\\end{array} \\tag{6}\n\\] 表示了tile \\((o_0,o_1)\\)从reduction space到\\(S_0\\)的affine function.换句话说就是 reduction space的tile 将\\(S_0\\)分割成了多个子集,这个tile使用的并不是他本身tiling schedule.本文使用extension schedule 表示这种通过data space所推导出来的schedule,后续将在章节4中使用这种extension节点.\n结合公式4和5可以看作图4中的点线和虚线的结合.所有quantization space中的蓝色tile的statement可以表示为\\(\\{S_0(h,w):2 \\leq h \\leq 5 \\land 0 \\leq w \\leq 3\\}\\),红色tile可以被表示为\\(\\{S_0(h,w):2 \\leq h \\leq 5 \\land 2 \\leq w \\leq 5\\}\\).这可以使用extension schedule算出的quantization space中的tile相互之间有overlap,不需要建模非affine的表达式,也不需要完善调度算法.目前使用extension schedule来实施loop tiling在现有的框架中是不支持的(38,39,40,41,42).\n38 Tiramisu: A polyhedral compiler for expressing fast and portable code39 A practical automatic polyhedral parallelizer and locality optimizer40 Polly–performing polyhedral optimizations on a low-level intermediate representation41 The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated gpu kernels, automatically42 Polyhedral parallel code generation for cuda"
  },
  {
    "objectID": "posts/akg-micro20.html#c.the-tiling-algorithm",
    "href": "posts/akg-micro20.html#c.the-tiling-algorithm",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "C.The Tiling Algorithm",
    "text": "C.The Tiling Algorithm\n为了总结本文构建任意tile形状的方法,假设在启动fusion后只有一个live-out computation space和多个intermediate computation spaces.算法1描述了如何构造任意的tile shape:\n\nAlg.1: Construct arbitrary tile shapes\n\n\n\nAlg.1: Construct arbitrary tile shapes\n\n\n该算法采用一组仿射集Spaces,这些空间是由保守启发式fusion算法的计算得到,然后获取live-out computation space称为liveout.再将Spaces减去liveout,后者成为intermediate computation spaces.当liveout可以tile时,首先为他生成一系列间的矩形/并行四边形tiling的tiling schedule和extension schedule的集合,称为Mixed Schedules.可以使用Pluto调度器43或其他变体来构造这些矩形/并行四边形tiling shape.然后5-6行计算live到upwards exposed data的关系.\n43 A practical automatic polyhedral parallelizer and locality optimizer第7和17行之间实现了每个intermediate computation space的tile shape的构建.该算法比较了S的可并行循环n的数量与liveout可并行的维度\\(m\\).如果\\(m\\)&gt;\\(n\\)时,表示live-out computation space比S具有更大的并行度,那么将\\(S\\)加入到Untiled集合中.如果没有这个限制,那么将会得到错误的tile shape.(这里的意思是不是指如果从live-out出发得到的并行度大于他本身能具备的并行度,那tiling出来可能也无法支持那种并行度).\n第9-16行考虑每个S与liveout融合,或添加到Untiled集合.\\(S\\)和liveout的集合也被认为是live-out computation space,这保证了Spaces中仿射集的访问顺序不会影响tiling后fusion的正确性.\n\\(m\\)和\\(n\\)之间的比较用于保证算法的正确性和有效性,以及章节4-B中的tiling后fusion.\\(m\\)大于\\(n\\)保证了具有更少的并行循环的intermediate computation space不会和具有更多并行循环的live-out computation space进行fusion.本文之前提到过permutable和coincident属性,表示是否可tiling和可并行.一个现代的多面体调度器通常倾向于外层的并行度,所以得到的n个可并行循环总是出现在最外层的循环.实际上live-out computation space的可并行层数可能大于m,比如live-out computation space有一个3维并行的属性.可以强行设置\\(m\\)=1,表示只有最外层的循环可以额并行,因为当生成CPU代码时只能1维并行,当生成GPU代码时可以生成2D并行,这样可以允许更加激进的fusion策略利用硬件的两级并行.\n比较\\(m\\)和\\(n\\)保留了liveout computation space的并行性,但它可能会失去fused intermediate computation space的并行性.最坏情况时\\(n\\)&gt;\\(m\\)=0,intermediate computation space的并行性将完全丢失.这种情况下,本文只假设live-out computation space在\\(m\\)大于0时可以对CPU进行tile,\\(m\\)大于1时可以对GPU进行tile.\n10-15行计算如公式6描述的extension schedule.12-16行对schedule使用基本变换.\\(h\\)是一组extension schedule,用于建模具有多个statement的live-out computation space的tile.每个\\(S\\)可以被\\(h\\) tile,然后加入到Mixed Schedule中.整个算法递归的应用于Untiled集合,直到它非空为止,或者liveout已经无法被tile.\n本文的tiling算法可以计算tile shape的ouverlap,而无需完善调度算法(例如Polymage框架), 通过upwards exposed data来计算intermediate computation space上的overlapped tile shape,避免引入文献44中对的仿射关系的复杂限制.更重要的是,和现在构造复杂tile shape的方法不同(45,46,47,48,49),本文算法提供了构造任意tile形状的能力,并且由于考虑了内存中数据的transformation,因此对更多应用域的有通用性.tile形状由upwards exposed data的访问方式确定.例如,可以通过微调\\(kh,kw\\)循环和相应的下标,将图1(a)中例子转换为矩阵乘法代码.读者会发现,本文的fusion技术仍然可以通过构造矩形的tile形状来适用于它.\n44 Flextended tiles: A flexible extension of overlapped tiles for polyhedral compilation45 Diamond tiling: Tiling techniques to maximize parallelism for stencil computations46 Hybrid hexagonal/classical tiling for gpus47 Effective automatic parallelization of stencil computations48 Polymage: Automatic optimization for image processing pipelines49 Flextended tiles: A flexible extension of overlapped tiles for polyhedral compilation"
  },
  {
    "objectID": "posts/akg-micro20.html#a.facilitating-fusion-using-schedule-trees",
    "href": "posts/akg-micro20.html#a.facilitating-fusion-using-schedule-trees",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "A.Facilitating Fusion using Schedule Trees",
    "text": "A.Facilitating Fusion using Schedule Trees\n图2(b)中所示的调度树是保守启发式fusion的结果,其策略\\((\\{s_0\\},\\{s_1,s_2,s_3\\})\\)使用顶部sequence节点的子节点表示.\n根据算法1,我们首先使用tiling schedule(1)的第二部分在reduction space上应用矩形tiling,该空间表示为顶部sequence节点的第二个子节点.图2(b)中\\(band_1\\)节点的原始多维affine schedule应替换为该tiling schedule,该schedule又分为两部分,其中一个表示为:\\([\\{S_1(h,w) \\rightarrow (h/T_2,w/T_3); S_2(h,w,kh,kw) \\rightarrow(h/T_2,w/T_3); S_3(h,w) \\rightarrow (h/T_2,w/T_3)\\}]\\),另一个为:\\([\\{S_1(h,w) \\rightarrow(h,w); S_2(h,w,kh,kw) \\rightarrow (h,w,kh,kw); S_3(h,w) → (h,w)\\}]\\).这种分离的操作隔离了tile shape,因此可以在computation space之间实现tile-wise的fusion.\n\nFig.5: The schedule tree of post-tiling fusion\n\n\n\nFig.5: The schedule tree of post-tiling fusion\n\n\n本文使用tile_band和point_band来表示从原始的\\(band_1\\)中分离出来的affine relation,如图5所示.tile_band表示每个tile间的迭代维度,point_band表示每个tile内部的迭代维度.接下来将解释引入节点的意义,注意到目前还没有对quantization space进行tiling,\\(band_0\\)的第一个filter node\\(\\{S_0{h,w}\\}\\)还没有改变.\n\\(S_0\\)可以看作是在filter节点 \\(\\{S_1(h,w);S_2(h,w,kh,kw);S_3(h,w)\\}\\) 的外部子树的根节点.之前在章节2-B小节中提到,本文将使用extension节点来实现tiling后fusion.extension节点最初是为了添加被affine relation使用了但没有被调度树的domain节点包含的.这本文的场景下,扩展expansion节点的表达能力来为filter节点引入额外的statement.\n如图5的左侧所示,extension节点在tile_band节点下方插入,其affine relation为公式6.在调度树上的这种简单操作实现了\\(S_0\\)的tiling overlap计算和两个原始computation的 space的tile-wise fusion.注意,这种对调度树表示的扩展使得在多面体模型中实现tiling后fusion的融合是可能的,但这在现有的多面体编译框架50,51中是不可能的,尽管他们也使用调度树.\n50 The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated gpu kernels, automatically51 Polyhedral parallel code generation for cuda52 Modeling the conflicting demands of parallelism and temporal/spatial locality in affine scheduling现在,可以调度在extension节点下的\\(S_0\\).为了实现期望实现tile-wise的fusion,必须在extension节点下方引入sequence节点.第一个子节点应该是原始quantization space的filter节点,即\\(\\{S_0(h,w)\\}\\),而第二个应为原始reduction space的子节点.在新添加的filter节点\\(\\{S_1(h,w); S_2(h,w,kh,kw); S_3(h,w)\\}\\)下面再附带一个point_band节点.同时在extended filter节点\\(\\{S_0(h,w)\\}\\)下也引入\\(band_0\\)节点,用于指导如何调度上面那个节点.重复此类子树保证将同时调度由extend filter节点包裹的多个statement.在tile band下引入sequence节点也有利于intra-tile distribution transformation52,该变换用于提升小的scratchpad上的空间局部性.\n当使用extension节点fusion了\\(S_0\\)时,他的原始调度(即要忽略顶部sequence的左子树),如图5所示,可以通过在调度树中引入mark节点来实现这一点.mark节点用于将信息添加到调度树上,从而为代码生成的多面体模型提供了更多的灵活性.本方法将字符串skipped添加到mark节点,提示codegen绕过下面的子树.\ncodegen按如图5所示的方式扫描这个调度树来生成对应的代码.红色和蓝色箭头代表band节点与它们代表的循环之间的关系.与图1(b)所示的代码不同,现在代码将所有三个循环融合到一个组中,从而使tensor A分配到小的spad上.此外,tiling后fusion策略不会失去fused维度上的并行性,并且在target为CPU时,可以在最外层的循环之前添加OpenMP Pragma.在为GPU生成CUDA代码时,可以通过启动单个内核来执行整个嵌套循环,\\(ht,wt\\)映射到线程块,每对\\(hp,wp\\)映射到线程,此时A可以被声明在shared memory上."
  },
  {
    "objectID": "posts/akg-micro20.html#b.the-fusion-algorithm",
    "href": "posts/akg-micro20.html#b.the-fusion-algorithm",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "B.The Fusion Algorithm",
    "text": "B.The Fusion Algorithm\n算法2描述了tiling后fusion的策略.它需要两个输入：一个是根据多面体调度程序获得的multi-dimensional affine schedule构建的调度树,另一个是算法1的输出.\n\nAlg.2: The post-tiling fusion algorithm\n\n\n\nAlg.2: The post-tiling fusion algorithm\n\n\nMixed Schedules中的tiling schedule数量正好是算法1建议的fusion组数量. 对于每个组,首先使用Tiling来替换原始的band节点(第1行),然后将其分为两个部分(tile_band,point_band),如第4-A节(第3行)所述.第5-10行在intermediate computation spaces上迭代,该space与当前live-out computation space融合在一起.当\\(m &gt; n\\)(第7行)时,不应融合当前space的extension schedule,其中m和n分别代表live-out computation space和I的可并行循环数量.比较\\(m\\)和\\(n\\)的目的已在第3-C节中解释了.第8-10行在调度树上执行各种操作.\n算法2返回了\\(\\{S_0,S_1,S_2,S_3\\}\\)的fusion策略展示在图5中.本文的tiling后fusion算法没有使用现有优化器53,54,55,56使用的繁琐的激进启发式fusion方法,以最大程度地提高数据局部性.更重要的是,与图1(c)所示的代码不同.Tiling后fusion算法不会丧失整个代码的并行性,保证了生成的代码可以再多个架构上根据内存层级适配从而得到好的性能.\n53 A practical automatic polyhedral parallelizer and locality optimizer54 Polly–performing polyhedral optimizations on a low-level intermediate representation55 The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated gpu kernels, automatically56 Polyhedral parallel code generation for cuda"
  },
  {
    "objectID": "posts/akg-micro20.html#c.generalization",
    "href": "posts/akg-micro20.html#c.generalization",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "C.Generalization",
    "text": "C.Generalization\n目前为止,假设只存在一个live-out computation space,现在开始讨论多个live-out computation space的情况.为了不失通用性,假设存在两个live-out computation space,\\(liveout_0\\)和\\(liveout_1\\).intermediate computation space可以分为三类：第一个是\\(liveout_0\\)的所有intermediate computation space,第二个是\\(liveout_1\\)使用的集合,第三个由两者共同使用的所有intermediate computation space组成.难点是如何处理第三类的intermediate computation space(可能存在左右两侧tile shape不匹配的问题.)\n考虑如图6(a)所示的场景,当\\(op_0\\)定义值后被\\(op_1\\)和\\(op_2\\)使用.使用\\(op’_0\\)代表\\(op_0\\)被\\(op_1\\)使用的子集,以及\\(op’’_0\\)表示\\(op_0\\)写入\\(op_2\\)读取的值的子集.现有的启发式方法57,58,59会因为可能的冗余计算而不fusion第三种情况.本文观察到在这种情况下有时仍然可以进行fusion.使用tiling后fusion的策略,当\\(op’_0\\)和\\(op’’_0\\)不存在相交的情况时(如图6(b)所示),还是可以进行fusion,因为此时不会产生冗余计算. 当\\(op’_0\\)和\\(op’’_0\\)交集非空时,本文方法不允许fusion,因为这将产生冗余计算.总之,这种融合策略永远不会引入代码的冗余.\n57 A model for fusion and code motion in an automatic parallelizing compiler58 An effective fusion and tile size model for optimizing image processing pipelines59 Revisiting loop fusion in the polyhedral framework\nFig.6(a): One definition,multiple uses\n\n\n\nFig.6(a): One definition,multiple uses\n\n\n\n\nFig.6(b): Fusion result\n\n\n\nFig.6(b): Fusion result\n\n\n算法3描述了合并tiling和fuison的方法.该使用多面体模型作为输入获得的多维仿射调度构建的调度树,并生成tiled和fused的调度树作为输出.它分为三个步骤:\n1.每个live-out computation及其intermediate computation spaces都是从输入调度树的迭代域提取并保存在\\(Spaces\\)中的,然后将算法1应用于它上面(第3行).这样可以防止live-out computation space之间的fusion,因为live-out value不一定需要在spad上分配.\\(GroupsSet\\)是\\(Group\\)的集合,即每个\\(Spaces\\)应用算法1的输出. 2.处理被多个liveout computation space所使用的intermediate computation space \\(SharedSpace\\).第四行是计算所有的extension schedule对于\\(SharedSpace\\)的交集,如果交集非空,使用tiling schedule替换extension节点,表示不对它进行fusion. 3.将算法2应用于\\(Groups\\)(第7行),用于在tiling和fusion后构建调度树.如果\\(SharedSpace\\)不能与任何一个使用了它的fuse在一起,则算法将删除与调度树中与引入的相关与\\(SharedSpace\\)的节点,从而防止可能的fusion,避免冗余计算.\n注意,在某些极端情况下,算法3也可以执行DCE.假设使用公式(6)计算的tile是\\(S_0\\)的迭代域的严格子集,同时这些tile仍然可以与reduction space的子树fusion在一起,那么代表\\(S_0\\)的原始子树被跳过.在这种情况下,DCE消除了\\(S_0\\)的死代码,同时保持了程序的语义.现有的多面体优化器60,61,62还没有考虑到这种细粒度的优化.\n60 A practical automatic polyhedral parallelizer and locality optimizer61 Polly–performing polyhedral optimizations on a low-level intermediate representation62 Isl: An Integer set library for the polyhedral model\n\nAlg.3: Reorder the sequence of tiling and fusion\n\n\n\nAlg.3: Reorder the sequence of tiling and fusion"
  },
  {
    "objectID": "posts/akg-micro20.html#d.general-applicability",
    "href": "posts/akg-micro20.html#d.general-applicability",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "D.General Applicability",
    "text": "D.General Applicability\n现在开始讨论本文方法的通用性.首先,算法1构建的tile形状可以是矩形/平行四边形,也可以是overlepped的形式.在后一种情况下,本文算法通过最大程度地减少overlepped tile所需的冗余计算来实现tile-wise的并发启动63.当不存在extension schedule时,算法1fallback为经典的矩形/平行四边形tiling算法.\n63 Effective automatic parallelization of stencil computations第二,使用extension schedule结合tiling和fusion需要提取出跨loop的producer-consumer关系,这使得本文方法无法为单个stencil模式的嵌套循环构建复杂的tile形状.但是,本文的方法非常适合具有多个嵌套循环的应用,例如神经网络,图像处理管道,有限元方法和线性代数.一是可以展开stencil kernel的时间维度,以将其转换为多个循环,由于扩展循环引入了producer-consumer关系,本文方法即可以应用于这种情况.\n最后,本文方法对于最大化fusion且不失去intermediate computation spaces的并行性的情况非常有用,但是对于连续的reduce的程序,就不是最优选择,因为intermediate reductions的并行性无法保留."
  },
  {
    "objectID": "posts/akg-micro20.html#a.domain-specific-code-generation",
    "href": "posts/akg-micro20.html#a.domain-specific-code-generation",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "A.Domain-Specific Code Generation",
    "text": "A.Domain-Specific Code Generation\n此方法已经集成到AKG编译器中,AKG可以为其用户提供 DSL 来表达算法,而无需考虑底层架构的细节,然后将DSL转换为Halide IR,然后可以操作IR进行自动或手动的调度.比如使用\\(tile\\)源语进行tiling,使用\\(fuse\\)源语进行循环合并.\n本文方法其中一个编译Target是专门的加速器Ascend 910芯片,图7描述了Davinci架构66.Cube单元是使用L0A和L0B中的数据输入来执行张量/矩阵操作的专业执行单元,其中输出存储在L0C中.L0A/L0B可以从L1 cache获取数据.L0C中的数据也可以传输到向量单元.向量/标量单元设计用于执行向量/标量操作,他们可以对unified buffer进行双向的读取/写数据,同时可以与L0C交换数据.L1 cache和unified buffer作为片上下层缓存,用于将数据与外部内存交换,这在图中未显示.其中L1 cache和unified buffer之间也可以进行数据交换.我理解整个架构的运作机制可能如下,从l1 buffer中流水的加载数据到L0A/L0B,然后Cube单元进行内积操作,输出到L0C上,此时能从Unified buffer中取激活参数等数据送到Vector单元计算,最后写回L1 buffer.\n66 Davinci: A scalable architecture for neural network computing\nFig.7: Overview of the DaVinci architecture\n\n\n\nFig.7: Overview of the DaVinci architecture\n\n\n加速器的编程模型是通过完全考虑应用程序和基础体系结构的领域属性而设计的.比如,可以通过使用编程模型发射单个向量指令来实现卷积操作.生成的CCE代码将与芯片上的native编译器一起编译,并为实验中使用的所有版本设置相同的编译选项.\n本文工作的目的不是完善多面体调度算法来建模更多的tile形状和/或激进的融合策略,而是允许更多以前错过的fusion和tiling组合.tiling后fusion在优化神经网络应用时非常有用.AKG 项目通过充分考虑DaVinci架构实现了更准确的cost model,在评估中试验不同版本时以进行公平比较.此外,还在 AKG 项目中实现了一种处理参数化的tile size的技术,但该特性在实验中被禁用.\n另外,本文没有使用手动调度方法,而是在AKG项目中引入了另一个Pass将Halide IR lower到调度树,这个pass将使用本文方法进行优化.输出调度树将被转换回Halide IR,以生成后续代码.然后,从自动优化的Halide IR生成AI加速器的目标代码(CCE代码)."
  },
  {
    "objectID": "posts/akg-micro20.html#b.aggressive-memory-optimizations",
    "href": "posts/akg-micro20.html#b.aggressive-memory-optimizations",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "B.Aggressive Memory Optimizations",
    "text": "B.Aggressive Memory Optimizations\n本文方法最大化数据局部性而不影响可切分性或并行性,但是由于应用程序的流式传输性质,因此如果不减少层次结构存储开销,则编译优化可能不会很有效.\n其中intermediate computation space产生的数据仅在tile内部使用,因此在计算完tile后可以丢弃67,68,69.生成OpenMP代码时,本文会自动在Spad上分配此类值.使用算法1生成的affine relation确定索引表达式.PPCG的CUDA后端提供了一个软件控制的方案,可以有效地使用GPU上的shared/private内存.PPCG对于用于复杂的tile shape计算一个过估计的矩形框,该框架访问非矩形数据块,因此可以在shared/private内存上分配中间值.现有的编译技术也使用了此策略70,71.\n67 An effective fusion and tile size model for optimizing image processing pipelines68 Polymage: Automatic optimization for image processing pipelines69 Flextended tiles: A flexible extension of overlapped tiles for polyhedral compilation70 Hybrid hexagonal/classical tiling for gpus71 Flextended tiles: A flexible extension of overlapped tiles for polyhedral compilation72 The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated gpu kernels, automatically本文也使用调度将内存自动化分配到Davinci架构的higher-level cache上.本文利用mark节点,用于管理不同计算单元和extension节点之间的数据流以进行内存优化和分配,例如TensorComprechensions框架72.将isl集成到TVM中,仅通过操纵调度树来实现Ascend 910上神经网络的训练/部署."
  },
  {
    "objectID": "posts/akg-micro20.html#a.performance-on-cpu",
    "href": "posts/akg-micro20.html#a.performance-on-cpu",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "A.Performance on CPU",
    "text": "A.Performance on CPU\nImage Processing Pipelines 图像处理pipeline在输入图像上执行给定的任务,并使用了大量复杂的的stencil和reduction操作.本文使用从PolyMage基准中提取的六个图像处理管道,在结构和复杂性方面差异很大.表1列出了PolyMage基准76.将性能与特定领域的编译器,Polymage 77和Halide的78手动调度进行了比较.Polymage编译器通过将DSL作为输入来生成naive和优化后的OpenMP代码,原始版本的代码用作基线,也输入到本文方法改进过的 PPCG ,因为naive版本由 PolyMage 生成,没有包含tiling或fusion.此外,optimized版本则包含充分的fusion和tiling.\n76 Polymage benchmarks77 Polymage: Automatic optimization for image processing pipelines78 Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines将PolyMage和Halide中关于tile size,vector length以及unroll factor等参数调整到时候本文的平台.本文使用相同的auto-tuned tile size,并保证代码生成参数与PolyMage相同,以进行公平的比较.这分离了fusion策略和tile shape的效果.还记录了不同版本的tile大小与执行时间,来获得PolyMage和Halide的不同的加速,如图8所示,本文方法可平均提供20％和33％的提升.\n本文方法为 Bilateral Grid 79,80,Multiscale Interpolation, Local Laplacian Filter 81,82,Unsharp Mask 提供了比PolyMage 和 Halide更加激进的fusion策略.最终发现的激进的fusion策略意味着,与overlap的tiling结合时,可以在spad上分配更多的中间值,从而获得更好的性能.PolyMage实施tiling后fusion策略； Halide只为compute space提供调度,没有考虑data space上的转换.他们俩都无法构建像(6)这样的extension schedule,因此未能找到与本文的方法相同的融合结果.\n79 Real-time edge-aware image processing with the bilateral grid80 Bilateral Filtering81 Fast local laplacian filters: Theory and applications82 Local laplacian filters: Edge-aware image processing with a laplacian pyramid83 A combined corner and edge detectorPolyMage和本文的工作也可以自动应用Harris Corner Detection的内连转换83,这是Halide的手动调度所欠缺的.本文方法生成了与PolyMage相同的代码,因此获得了相同的结果,优于Halide的手动调度2倍.\nCamera Pipeline,本文方法产生的融合结果与PolyMage相同,但是本文可以构建由内存足迹确定的更紧密的overlapped tile shape.相反,Polymage仅通过转换computation space来应用overlap的tiling,从而过多的重计算和性能退化.\nFinite Element Method,Equake84是从SPEC CPU2000提取的基准测试.它使用3D稀疏矩阵矢量(SPMV)计算执行有限元方法.3D SPMV计算使用还原阵列更新非结构化的网格,其次是一组仿射循环,在网格上执行基本操作.3D SPMV计算的不完美循环由三个组件组成,第一个组件初始化了还原阵列,第二个使用while loop进行的reduction,第三部分收集还原变量以更新全局网格.reduction操作涉及沿第二维的动态条件,但是这是由于使用了while loop,会被由PPCG作为black box处理.\n84 Large-scale simulation of elastic wave propagation in heterogeneous media on parallel computers是个方法是可以手动将while loop permute到最内的维度,从而为PPCG创造fusion的机会.默认的策略被称为SmartFuse试图最大程度地fusion,而不会降低并行性或易用性.更保守的策略Minfuse不会融合任何嵌套循环.本文在PPCG中的实现来自Minfuse.\nSmartFuse将3D SPMV计算的三个组件fusion在一起.相反,MaxFuse将收集组件与后续仿射循环融合在一起.本文方法发现的融合策略与MaxFuse相同.在不同融合策略的基线版本上的加速比如图9所示,X轴代表问题大小.由于只有最外面的循环是可tile的,因此所有版本均未应用tiling.算法1返回带有空域的extension schedule,允许无tiling的fusion.这也验证了本文的方法的适用而无tiling的fusion.\n如果没有while loop的手动permutation,PPCG将由于while循环引入的动态条件而无法重复进行loop fusion.但是,这种permute转换对数据局部性有害,这使得PPCG方法的性能落后于本文的方法.本文方法不需要这样的手动排列.\nLinear Algebra and Data Mining,从PolyBench中提取出来的结果汇总在表2中.PolyBench是一组线性代数,stencil计算和物理模拟的算子集合.从一共30的测试中排除20个,因为这些case已经具备了完美的嵌套循环,无需进一步的loop fusion.本文的方法在10个case中有3个生成了和smartfuse相同的fusion结果. 这证明了本文的算法在最坏的情况下可以fallback到SmartFuse.由于篇幅限制,仅选择3个与SmartFuse产生不同的fustion结果的代表性kernel展示.其他的表现与此处显示的表现类似.\n其中\\(2mm\\)是执行两个矩阵乘(\\(D=A\\times B; E=C \\times D\\))当使用不同的PPCG中的方法或本文的方法时,并没有观察到其执行时间存在显着差异,因为每个启发式fusion都可以保留并行性/可切分性.HybridFuse可以达到最佳性能,因为最大程度在最内层fusion可以受益于icc编译器的自动向量化. 未来和HybridFuse整合可能是后续的研究方向.\nGemver是向量乘和矩阵加操作,它由4个嵌套循环组成.covariance用于计算数据挖掘中不同人群的数据样本的协方差.可以观察到,由于这两个case失去了并行性,使用Maxfuse方法性能会剧烈下降.本文方法可为这些基准测试实现矩形/平行四边形tiling shape.本文的策略比SmartFuse更激进性,但并没有失去并行性或可切分性.HybridFuse对于covariance会生成出segmenttation fault的错误代码.\n本文没有为这些micro kernel应用激进的存储优化.这表明本文方法利用tiling和fusion的组成也可以通过最大化数据局部性来提高程序的性能.\n\nTab.2: CPU execution time of the PolyBench benchmarks\n\n\n\nTab.2: CPU execution time of the PolyBench benchmarks"
  },
  {
    "objectID": "posts/akg-micro20.html#b.performance-on-gpu",
    "href": "posts/akg-micro20.html#b.performance-on-gpu",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "B.Performance on GPU",
    "text": "B.Performance on GPU\n现在开始评估GPU上的性能,从PolyBench基准中提取的那些case的性能遵循与CPU案例相同的趋势.因此,在这里不重复讨论.\nImage Processing Pipelines 由于PolyMage并没有对GPU的生成,因此只将性能与Halide的手动调度结果进行比较.baseline是由PPCG生成的,auto-tuned的tile size与表1相同,第4列中列出了自动调整的GPU grid参数.\n结果如图10所示,某些测试case缺少 smartfuse 和 maxfuse 的结果,因为它们无法在合理的时间内终止.后续将在第6节D中解释时间复杂度问题.\nMinFuse不能fuse Unsharp Mask的四个阶段中的任何一个,因此没有从shared/private内存中受益.SmartFuse通过将4个阶段融合为2组来获得3D并行,MaxFuse将所有阶段组合在一起,但使用128×3作为GPU grid参数来进行2D并行.Maxfuse的并行度低因此性能也下降.PPCG的方法都无法对Harris Corner Detection进行fusion,这是因为被的内存足迹的overlap限制了.而本文的方法获得了卓越的性能,因为生成的CUDA代码由于激进的fusion结果和重叠的tile形状而最大程度地利用了shared/private内存的利用,还不会失去并行度.\nHalide在Bilateral Grid 和 Unsharp Mask方面的表现略有优于本文方法,因为他可以手动的将tiling后的结果在channel维度进行unroll.这可以在指令基本进行流水并行而受益,这是将来需要研究的有趣方向.本文方法比halide平均性能提高了17％.\n\nFig.10: Performance of PolyMage benchmarks on GPU\n\n\n\nFig.10: Performance of PolyMage benchmarks on GPU\n\n\nFinite Element Method : PPCG由于存在while循环而无法生成有效的CUDA代码.它的一个改进85是使用预处理步骤将while循环转换为所谓的动态计数循环,从而允许在多面体模型中探索loop tiling和fusion.然后,代码生成算法引入了一个goto语句,消除预处理引起的过估计的迭代次数.这可以比PPCG的默认设置上快2.3×.但本文的方法与手动方法取得了相同的结果,还可以自动化的进行tiling和fusion的组合,从而通过利用GPU上的更快内存层级来提升性能.\n85 A polyhedral compilation framework for loops with dynamic data-dependent bounds"
  },
  {
    "objectID": "posts/akg-micro20.html#c.performance-on-ai-accelerator",
    "href": "posts/akg-micro20.html#c.performance-on-ai-accelerator",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "C.Performance on AI Accelerator",
    "text": "C.Performance on AI Accelerator\n本文使用Resnet-50,以对AI加速器上的执行效率进行实验.实验中使用的Resnet-50模型由各种运算符组成,包括前向/后向卷积,批归一化和Relu等.isl的SmartFuse发式未能融合卷积和批归一化.本文的方法将SmartFuse作为初始化方法,可以额得到overlapped tiling并允许每个正向卷积与批归一化fusion.MinFuse会将卷积的初始化和reduction部分分开,因此会让CCE代码无法向量化,因此本文不与之进行比较.表3显示了AI加速器上的执行时间.其中Maxfuse的编译时间太长因此不记录进去.\n\nTab.3: Results of the ResNet-50 model\n\n其中tile尺寸由DSL的专家指定,本文没有使用框架的auto-tuner.该网络接受了不少于76％的验证准确性的训练,并报告了单个训练epoch的执行时间.首先比较所有正向卷积和批处理规范化操作的执行时,因为这和本文方法的效果中的其他优化无关.本文方法可以比SmartFuse获得72％的性能提高.这是因为片外存储延迟在Ascend 910芯片比较大,而本方法避免了此类数据传输.Resnet-50的整个执行时间提升了16％.未来可进行的工作是优化反向卷积与其他操作符的融合."
  },
  {
    "objectID": "posts/akg-micro20.html#d.comparison-of-time-complexity",
    "href": "posts/akg-micro20.html#d.comparison-of-time-complexity",
    "title": "Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data",
    "section": "D.Comparison of Time Complexity",
    "text": "D.Comparison of Time Complexity\n本文方法还可以提升多面体模型的编译时间.比如图像处理和Resnet-50模型,这些模型挑战了激进启发式fusion方法的可扩展性.汇总结果生成OpenMP代码的编译时间； 生成CUDA代码的编译开销遵循与CPU的类似趋势,还有在Ascend 910芯片上生成CCE代码的数据,如表1和表3所示.\nMaxFuse在大多数图像处理的case中都无法在一天之内完成,包括Bilateral Grid,Camera Pipeline,Multiscale Interpolation and Local Laplacian Filter.SmartFuse在其中两个的例子也同样.本文的方法可以在8分钟内终止(除了Camera Pipeline)\n在Harris Corner Detection中本文的方法比启发式方法更长.这是因为在测试中显示的非常复杂的访问模式将导致对isl对upwards exposed data进行非常多的计算.\n本文算法编译到Ascend 910利用了一种激进的fusion策略,不是SmartFuse,会产生更少的computation space,从而减少了Resnet-50的代码生成时间."
  },
  {
    "objectID": "posts/ai-matting.html",
    "href": "posts/ai-matting.html",
    "title": "基于Qt的抠图工具",
    "section": "",
    "text": "写这个工具是因为在网上找了一圈没找到免费的批量自动抠图工具。因此参考稿定抠图的样式写了一个小工具，稿定抠图其实还挺好用的，就是只能免费3张图像。remove.bg我看了下也挺贵的，而且他是基于语义分割的，对于人像的分割比较好，对于一般的物品就不行了，我比较需要的是基于显著性目标检测的抠图。\nPS. 当我写到一半的时候才发现其实是有真正免费的抠图工具的，就是淘宝出品的顽兔抠图，但是我之前不论在百度搜索还是谷歌搜索都没有看到关于顽兔抠图的介绍，特别是百度出来清一色是广告。因此我用PyQt写了这个小工具。\n\n\n体会\n\n刚开始用PyQt的时候，感觉可以用qt designer来拖拖控件应该会比较方便。但是这个实际上还是有些问题，因为他会帮你按照模板生成代码，但是你没法在那个模板上改，因为重新生成又会覆盖掉，只能用比较丑陋的写法从外部继承。\n控件的改进基本时间全花在查API和看官方的demo上面了，Qt就是太大了，复杂度相当高，而且经常得不到我想要的效果，需要重复尝试。\n主要得自我反省一下，感觉自己代码写的稀碎，对于函数，还是得多写多练。\n里面的模型用的是PoolNet，本来的想法是用tensorflow复现一下，但是看到他的最终模型参数有260mb想想就算了。。卡不行暂时还是别搞了。"
  },
  {
    "objectID": "posts/affine-fusion.html",
    "href": "posts/affine-fusion.html",
    "title": "Affine Fusion Pass浅析",
    "section": "",
    "text": "学习mlir中Affine Fusion Pass, 主要关注依赖分析部分."
  },
  {
    "objectID": "posts/affine-fusion.html#computesliceunion",
    "href": "posts/affine-fusion.html#computesliceunion",
    "title": "Affine Fusion Pass浅析",
    "section": "3.1 computeSliceUnion",
    "text": "3.1 computeSliceUnion\n计算opsA和OpsB在指定循环层级位置计算得到的slice bounds是否满足他们之间的依赖. 首先对于producer来说只有写出是重要的, 因此这里的opsA为affine.store %4, %arg2[%arg5, %arg6, %arg7] : memref&lt;8x128x512xf32&gt;. 对于consumer来说, 读写同样重要, 因此此时的opsB为\n%0 = affine.load %arg2[%arg5, %arg6, %arg8] : memref&lt;8x128x512xf32&gt;\n%1 = affine.load %arg3[%arg5, %arg8, %arg7] : memref&lt;8x512x64xf32&gt;\n%2 = affine.load %arg4[%arg5, %arg6, %arg7] : memref&lt;8x128x64xf32&gt;\naffine.store %4, %arg4[%arg5, %arg6, %arg7] : memref&lt;8x128x64xf32&gt;\n因为我们需要测试前一个执行节点的内存对后面所有的执行的内存的依赖关系, 所以这里是一个全排列组合的测试:\n  for (auto *i : opsA) {\n    MemRefAccess srcAccess(i);\n    for (auto *j : opsB) {\n      MemRefAccess dstAccess(j);\n      if (srcAccess.memref != dstAccess.memref)\n        continue;\n    }\n  }\n如果他们读写的是同一块memref, 那么也就是存在着依赖, 那么就可能存在着潜在的依赖, 需要进行进一步的依赖测试:\nbool readReadAccesses = isa&lt;AffineReadOpInterface&gt;(srcAccess.opInst) &&\n                        isa&lt;AffineReadOpInterface&gt;(dstAccess.opInst);\nFlatAffineValueConstraints dependenceConstraints;\n// Check dependence between 'srcAccess' and 'dstAccess'.\nDependenceResult result = checkMemrefAccessDependence( /* 如果操作的是同一个buffer, 那么需要检查依赖 */\n    srcAccess, dstAccess, /*loopDepth=*/numCommonLoops + 1,\n    &dependenceConstraints, /*dependenceComponents=*/nullptr,\n    /*allowRAR=*/readReadAccesses)"
  },
  {
    "objectID": "posts/affine-fusion.html#checkmemrefaccessdependence",
    "href": "posts/affine-fusion.html#checkmemrefaccessdependence",
    "title": "Affine Fusion Pass浅析",
    "section": "3.2 checkMemrefAccessDependence",
    "text": "3.2 checkMemrefAccessDependence\n此时我们的src/dst分别为:\nChecking for dependence at depth: 1 between:\nmlir-asm-printer: Verifying operation: func.func\naffine.store %4, %arg2[%arg5, %arg6, %arg7] : memref&lt;8x128x512xf32&gt;\nmlir-asm-printer: Verifying operation: func.func\n%0 = affine.load %arg2[%arg5, %arg6, %arg8] : memref&lt;8x128x512xf32&gt;\n接下来从access中获得对应的access relation:\n// Create access relation from each MemRefAccess.\nFlatAffineRelation srcRel, dstRel;\nif (failed(srcAccess.getAccessRelation(srcRel)))\n  return DependenceResult::Failure;\nif (failed(dstAccess.getAccessRelation(dstRel)))\n  return DependenceResult::Failure;\n首先展示srcRel和dstRel的FlatAffineRelation:\nsrcRel:\nDomain: 0, Range: 7, Symbols: 0, Locals: 0\n11 constraints\n(Value  Value   Value   Value   None    None    None    const)\n 1      0       0       0       -1      0       0       0       = 0\n 0      1       0       0       0       -1      0       0       = 0\n 0      0       1       0       0       0       -1      0       = 0\n 1      0       0       0       0       0       0       0       &gt;= 0\n -1     0       0       0       0       0       0       7       &gt;= 0\n 0      1       0       0       0       0       0       0       &gt;= 0\n 0      -1      0       0       0       0       0       127     &gt;= 0\n 0      0       1       0       0       0       0       0       &gt;= 0\n 0      0       -1      0       0       0       0       511     &gt;= 0\n 0      0       0       1       0       0       0       0       &gt;= 0\n 0      0       0       -1      0       0       0       383     &gt;= 0\ndstRel:\nDomain: 0, Range: 7, Symbols: 0, Locals: 0\n11 constraints\n(Value  Value   Value   Value   None    None    None    const)\n 1      0       0       0       -1      0       0       0       = 0\n 0      1       0       0       0       -1      0       0       = 0\n 0      0       0       1       0       0       -1      0       = 0\n 1      0       0       0       0       0       0       0       &gt;= 0\n -1     0       0       0       0       0       0       7       &gt;= 0\n 0      1       0       0       0       0       0       0       &gt;= 0\n 0      -1      0       0       0       0       0       127     &gt;= 0\n 0      0       1       0       0       0       0       0       &gt;= 0\n 0      0       -1      0       0       0       0       63      &gt;= 0\n 0      0       0       1       0       0       0       0       &gt;= 0\n 0      0       0       -1      0       0       0       511     &gt;= 0\n这里需要先讲解一下mlir中的PresburgerSpace的变量类型enum class VarKind { Symbol, Local, Domain, Range, SetDim = Range };:\n\nSymbol, 表示一个固定但是展示未知的值.\nLocal, 表示的是存在量化变量(existentially quantified variables), 我理解就是farkas引理中的lambda系数, 可以通过约束求解来消除. 考虑这个一个space为(x,exists q), 约束为1 &lt;= x &lt;= 7, x = 2q, 此时x为维度变量,q为存在量化变量, 即(x) : (exists q : q &lt;= x &lt;= 7, x = 2q). 此时带入一些值进去, 可以得到满足约束的结果集{(2,1),(4,2),(6,3)}\nDimension变量被进一步分为Domain 和 Range变量.\n\n在mlir中多面体是和ir深度结合的,比如这里的FlatAffineValueConstraints中是包含了PresburgerSpace以及AffineValue的, 上面输出依赖多面体中的Value列实际上就是一个affine ir的ssa value, 这个例子中其实就是四个迭代变量%arg5,%arg6,%arg7,%arg8. 并且access relation中的numDomainDims和numRangeDims与presburger space中的numDomainVars和numRangeVars并不是一致的. 上面两个约束他们的domainDims和RangeDims分别都是4和3, 但是这些dim对应的变量类型都是SetDim = Range, 所以上面两个relation的Ranges变量个数为4+3=7\n将两个relation写为isl的形式如下:\nsrcRel = [i0,i1,i2,i3] -&gt; [l0,l1,l2] : \n i0 == l0 and\n i1 == l1 and \n i2 == l2 and\n 0 &lt;= i0 &lt; 8 and\n 0 &lt;= i1 &lt; 128 and\n 0 &lt;= i2 &lt; 512 and\n 0 &lt;= i3 &lt; 384\ndstRel = [j0,j1,j2,j3] -&gt; [l0,l1,l2] : \n j0 == l0 and\n j1 == l1 and\n j3 == l2 and\n 0 &lt;= j0 &lt; 8 and\n 0 &lt;= j1 &lt; 128 and\n 0 &lt;= j2 &lt; 64 and\n 0 &lt;= j3 &lt; 512\n这里获得对应的他们对应的domain:\nFlatAffineValueConstraints srcDomain = srcRel.getDomainSet();\nFlatAffineValueConstraints dstDomain = dstRel.getDomainSet();\n此时srcDomain和dstDomain的约束多面体分别如下:\nsrcDomain:\nDomain: 0, Range: 4, Symbols: 0, Locals: 3\n11 constraints\n(Value  Value   Value   Value   Local   Local   Local   const)\n 1      0       0       0       -1      0       0       0       = 0\n 0      1       0       0       0       -1      0       0       = 0\n 0      0       1       0       0       0       -1      0       = 0\n 1      0       0       0       0       0       0       0       &gt;= 0\n -1     0       0       0       0       0       0       7       &gt;= 0\n 0      1       0       0       0       0       0       0       &gt;= 0\n 0      -1      0       0       0       0       0       127     &gt;= 0\n 0      0       1       0       0       0       0       0       &gt;= 0\n 0      0       -1      0       0       0       0       511     &gt;= 0\n 0      0       0       1       0       0       0       0       &gt;= 0\n 0      0       0       -1      0       0       0       383     &gt;= 0\n\ndstDomain:\nDomain: 0, Range: 4, Symbols: 0, Locals: 3\n11 constraints\n(Value  Value   Value   Value   Local   Local   Local   const)\n 1      0       0       0       -1      0       0       0       = 0\n 0      1       0       0       0       -1      0       0       = 0\n 0      0       0       1       0       0       -1      0       = 0\n 1      0       0       0       0       0       0       0       &gt;= 0\n -1     0       0       0       0       0       0       7       &gt;= 0\n 0      1       0       0       0       0       0       0       &gt;= 0\n 0      -1      0       0       0       0       0       127     &gt;= 0\n 0      0       1       0       0       0       0       0       &gt;= 0\n 0      0       -1      0       0       0       0       63      &gt;= 0\n 0      0       0       1       0       0       0       0       &gt;= 0\n 0      0       0       -1      0       0       0       511     &gt;= 0\n实际domain的约束多面体和access relation的多面体并无大的区别, 将一些变量的类型进行了转换, 同时作为一个set他是不存在domain dims和range dims的.\n然后组合两个relation, 这里的compose实际上等价srcRel.apply_range(dstRel)\n  dstRel.inverse();\n  dstRel.compose(srcRel); // src.domain -&gt; [src.range == dst.domain] -&gt; dst.range\ncompose后此时dstRel为:\nDomain: 0, Range: 8, Symbols: 0, Locals: 0\n19 constraints\n(Value  Value   Value   Value   Value   Value   Value   Value   const)\n -1     0       0       0       1       0       0       0       0       = 0\n 0      -1      0       0       0       1       0       0       0       = 0\n 0      0       -1      0       0       0       0       1       0       = 0\n 1      0       0       0       0       0       0       0       0       &gt;= 0\n -1     0       0       0       0       0       0       0       7       &gt;= 0\n 0      1       0       0       0       0       0       0       0       &gt;= 0\n 0      -1      0       0       0       0       0       0       127     &gt;= 0\n 0      0       1       0       0       0       0       0       0       &gt;= 0\n 0      0       -1      0       0       0       0       0       511     &gt;= 0\n 0      0       0       1       0       0       0       0       0       &gt;= 0\n 0      0       0       -1      0       0       0       0       383     &gt;= 0\n 0      0       0       0       1       0       0       0       0       &gt;= 0\n 0      0       0       0       -1      0       0       0       7       &gt;= 0\n 0      0       0       0       0       1       0       0       0       &gt;= 0\n 0      0       0       0       0       -1      0       0       127     &gt;= 0\n 0      0       0       0       0       0       1       0       0       &gt;= 0\n 0      0       0       0       0       0       -1      0       63      &gt;= 0\n 0      0       0       0       0       0       0       1       0       &gt;= 0\n 0      0       0       0       0       0       0       -1      511     &gt;= 0\n这里的Range为8是因为只存在上下两个循环迭代变量的range变量, 此时的domain dims和range dims均为4, 用isl形式表示应该是:\n{ [i0, i1, i2, i3] -&gt; [j0 = i0, j1 = i1, j2, j3 = i2] : 0 &lt;= i0 &lt; 8 and 0 &lt;= i1 &lt; 128 and 0 &lt;= i2 &lt; 512 and 0 &lt;= i3 &lt; 384 and 0 &lt;= j0 &lt; 8 and 0 &lt;= j1 &lt; 128 and 0 &lt;= j2 &lt; 64 and 0 &lt;= j3 &lt; 512 }\n得到新的dstRel后, 添加顺序约束, 也就是当他们的外侧还存在有共享循环时, 需要添加顺序约束, 目前这个例子中没有共享循环, 所以也不做什么.\n// Add 'src' happens before 'dst' ordering constraints.\naddOrderingConstraints(srcDomain, dstDomain, loopDepth, &dstRel);\n最终就是检查约束dstRel.isEmpty(), 这里isEmpty检查的是否存在整数解, 也就是在当前order下上面的map约束是否能满足."
  },
  {
    "objectID": "posts/affine-fusion.html#getcomputationslicestate",
    "href": "posts/affine-fusion.html#getcomputationslicestate",
    "title": "Affine Fusion Pass浅析",
    "section": "3.3 getComputationSliceState",
    "text": "3.3 getComputationSliceState\n上面这个case检测到存在依赖, 接下来计算依赖的slice大小:\nmlir::affine::getComputationSliceState(\n    Operation *depSourceOp, Operation *depSinkOp,\n    FlatAffineValueConstraints *dependenceConstraints, unsigned loopDepth,\n    bool isBackwardSlice, ComputationSliceState *sliceState)\n首先这个case传入的参数depSourceOp为前一个块的store, depSinkOp为后一个块的load,dependenceConstraints为上一步计算得到的dst-&gt;src的map, loopDepth为需要合并到的循环深度, 当前为1. isBackwardSlice为true, 因为source op是在sink op前执行的.\n因为我们要计算的是插入到loopDepth时的slice大小, 那么第一步则是要删除所有高于loopDepth的维度. 因为是反向依赖, 所以dst loop的var在后面, 因此pos为src loop nums + loopDepth = 5, 然后num为dst loop nums - loopDepth = 3 .\n  // Project out dimensions other than those up to 'loopDepth'.\n  unsigned pos = isBackwardSlice ? numSrcLoopIVs + loopDepth : loopDepth;\n  unsigned num =\n      isBackwardSlice ? numDstLoopIVs - loopDepth : numSrcLoopIVs - loopDepth;\n  dependenceConstraints-&gt;projectOut(pos, num);\n消除不需要的变量后, dependenceConstraints为如下:\nDomain: 0, Range: 5, Symbols: 0, Locals: 0\n11 constraints\n(Value  Value   Value   Value   Value   const)\n -1     0       0       0       1       0       = 0\n 1      0       0       0       0       0       &gt;= 0\n -1     0       0       0       0       7       &gt;= 0\n 0      1       0       0       0       0       &gt;= 0\n 0      -1      0       0       0       127     &gt;= 0\n 0      0       1       0       0       0       &gt;= 0\n 0      0       -1      0       0       511     &gt;= 0\n 0      0       0       1       0       0       &gt;= 0\n 0      0       0       -1      0       383     &gt;= 0\n 0      0       0       0       1       0       &gt;= 0\n 0      0       0       0       -1      7       &gt;= 0\n等价于:\n{ [i0, i1, i2, i3] -&gt; [j0 = i0] : 0 &lt;= i0 &lt;= 7 and 0 &lt;= i1 &lt;= 127 and 0 &lt;= i2 &lt;= 511 and 0 &lt;= i3 &lt;= 383 }\n获得循环迭代的SSAValue, 这里因为是backward,因此src变量的起点为0, 总个数在这个例子中为4.\n  // Add slice loop IV values to 'sliceState'.\nunsigned offset = isBackwardSlice ? 0 : loopDepth;\nunsigned numSliceLoopIVs = isBackwardSlice ? numSrcLoopIVs : numDstLoopIVs;\ndependenceConstraints-&gt;getValues(offset, offset + numSliceLoopIVs,\n                                  &sliceState-&gt;ivs);\n\n// Set up lower/upper bound affine maps for the slice.\nsliceState-&gt;lbs.resize(numSliceLoopIVs, AffineMap());\nsliceState-&gt;ubs.resize(numSliceLoopIVs, AffineMap());\n\n// Get bounds for slice IVs in terms of other IVs, symbols, and constants.\ndependenceConstraints-&gt;getSliceBounds(offset, numSliceLoopIVs,\n                                      depSourceOp-&gt;getContext(),\n                                      &sliceState-&gt;lbs, &sliceState-&gt;ubs);\n更新后sliceState-&gt;ivs中存在了i0,i1,i2,i3四个变量. 同时为slice state的lower bounds 和 upper bounds分配好四个affine map, 并通过ir的连接关系得到这些affine map. getSliceBounds是将从offset开始的前num个维度变量上下界作为剩余变量的map, 也就是说要基于上一步的依赖约束得到基于dst为domain所对应src domain的上下界, 由于上一步中project掉了三个dst的循环变量, 因此bounds map的domain维度为1, 同时因为i0=j0, 因此得到的lower bounds为[(d0) -&gt; (d0), (d0) -&gt; (0), (d0) -&gt; (0), (d0) -&gt; (0)], upper bounds为[(d0) -&gt; (d0 + 1), (d0) -&gt; (128), (d0) -&gt; (512), (d0) -&gt; (384)].\n接下来获取dst循环的iter var value, 因为这里project out之后所以numDimsAndSymbols, 然后又跳过了offset + numSliceLoopIVs, 因此这里sliceBoundOperands只保留了一个j0. 然后将这个vector再分配给lbOperands, ubOperands. 最好这里的insertPoint就是dst loop在loop depth的位置.\n  SmallVector&lt;Value, 4&gt; sliceBoundOperands;\n  unsigned numDimsAndSymbols = dependenceConstraints-&gt;getNumDimAndSymbolVars();\n  for (unsigned i = 0; i &lt; numDimsAndSymbols; ++i) {\n    if (i &lt; offset || i &gt;= offset + numSliceLoopIVs) {\n      sliceBoundOperands.push_back(dependenceConstraints-&gt;getValue(i));\n    }\n  }\n\n  // Give each bound its own copy of 'sliceBoundOperands' for subsequent\n  // canonicalization.\n  sliceState-&gt;lbOperands.resize(numSliceLoopIVs, sliceBoundOperands);\n  sliceState-&gt;ubOperands.resize(numSliceLoopIVs, sliceBoundOperands);\n\n  // Set destination loop nest insertion point to block start at 'dstLoopDepth'.\n  sliceState-&gt;insertPoint =\n      isBackwardSlice ? dstLoopIVs[loopDepth - 1].getBody()-&gt;begin()\n                      : std::prev(srcLoopIVs[loopDepth - 1].getBody()-&gt;end());\n此时如果不考虑复杂的情况, sliceState就算是更新完毕了. 这里直接回到了canFuseLoops之后."
  },
  {
    "objectID": "posts/addfoldpy.html",
    "href": "posts/addfoldpy.html",
    "title": "代码块自动添加折叠",
    "section": "",
    "text": "今天放代码的时候,突然觉得代码太烂太长,放在那边就扫了大家浏览的兴致,所以准备给所有的代码段加个折叠块,但是加折叠块必须要每个文件修改,很蛋疼,所以就写了个小工具去自动添加\n我这个只能添加一次,运行两次就炸了~\n\n\nfold.py\nimport os\nif __name__ == \"__main__\":\n    path = '/media/zqh/文档/Blog/gitio/source/_posts'  # 文件夹目录\n    files = os.listdir(path)  # type:list\n    files = [it for it in files if '.md' in it]  # 排除别的类型文件\n    startflag = True\n    cnt = 1\n    for i in range(len(files)):\n        with open(path+'/'+files[i], 'r+') as f:\n            lines = f.readlines()\n            for i in range(len(lines)):\n                if startflag:\n                    if ('```c' in lines[i]) or ('```cpp' in lines[i]) or\\\n                            ('```python' in lines[i]):\n                        pos = lines[i].find('`')\n                        strl = list(lines[i])\n                        strl.insert(pos, '{% fold 点击显示内容 %}\\n'+pos*' ')\n                        lines[i] = ''.join(strl)\n                        # print(lines[i])\n                        startflag = False\n                else:\n                    if '```' in lines[i]:\n                        pos = lines[i].find('`')\n                        strl = list(lines[i])\n                        strl.insert(-1, '\\n'+pos*' '+'{% endfold %}\\n')\n                        lines[i] = ''.join(strl)\n                        # print(lines[i])\n                        print('修改了{}处'.format(cnt))\n                        cnt += 1\n                        startflag = True\n            f.seek(0)  # 回到文件头\n            f.truncate()  # 清空文件\n            # 重新写入文件\n            text = ''.join(lines)\n            f.write(text)"
  },
  {
    "objectID": "posts/U-GAT-IT.html",
    "href": "posts/U-GAT-IT.html",
    "title": "U-GAT-IT论文解读",
    "section": "",
    "text": "这个论文是基于CycleGan的一个改进，论文中的典型应用是将自拍转换为卡通风格。我参考了小视科技开源的代码，这里对这个论文做一个详细描述。\n我尝试用tensorflow 2.x复现这个项目，这个项目成功地把我从tensorflow劝退。。先吐槽一下，因为这个项目需要同时训练7个模型，我开启jit时用tf2.X的tf.function，autograph转换一下居然需要20分钟以上！每次启动训练得等20分钟，实在是受不了，不过转换之后的速度比pytorch实现快50%让我还可以接受。第二点是最不能忍受的，在pytorch中用8G显存即可训练，tensorflow中我降低了判别器的的层数还需要12G显存才能训练，直接劝退。\nPS. 再提一句，他的生成器中间特征处理有两种做法，轻量级的方法是GMP再生成，否则直接生成。直接生成的话模型参数200MB往上了..我看了下他们po的图训练时显存需要22G..☹"
  },
  {
    "objectID": "posts/U-GAT-IT.html#class-activation-mapcam",
    "href": "posts/U-GAT-IT.html#class-activation-mapcam",
    "title": "U-GAT-IT论文解读",
    "section": "Class Activation Map(CAM)",
    "text": "Class Activation Map(CAM)\n这个技巧是老方法了，特征经过一个全局池化层降低维度，通过全连接层后得到单个的logit，此时全连接层的权重体现了特征通道的对于输出logit的贡献大小，因此将权重乘上特征得到自注意后的新特征。这样我们的新特征就加强了部分通道特征的权重。"
  },
  {
    "objectID": "posts/U-GAT-IT.html#decode",
    "href": "posts/U-GAT-IT.html#decode",
    "title": "U-GAT-IT论文解读",
    "section": "Decode",
    "text": "Decode\n原本的论文使用的是Adaptive Layer-Instance Normalization(AdaLIN)，他是没有考虑到编码器生成content feature和style feature的融合。\n原始的AdaLIN： \\[\n\\begin{aligned}\n  AdaLIN(\\alpha,\\gamma,\\beta)&=\\rho\\cdot(\\rho\\cdot\\hat{\\alpha}_I+(1-\\rho)\\cdot\\hat{\\alpha}_L)+\\beta\\\\\n  \\hat{\\alpha}_I&=\\frac{\\alpha-\\mu_I}{\\sqrt{\\sigma^2_I+\\epsilon}},\\hat{\\alpha}_L=\\frac{\\alpha-\\mu_L}{\\sqrt{\\sigma^2_L+\\epsilon}}\\\\\n  \\rho&\\leftarrow \\text{clip}_{[0,1]}(\\rho-\\tau\\Delta\\rho)\n\\end{aligned}\n\\]\n其实就是分别对特征进行通道级的归一化和层级的归一化，使用参数\\(\\rho\\)控制各部分的权重，最后利用\\(\\gamma,\\beta\\)进行激活。NOTE： 他的\\gamma,\\beta是将style feature通过两个独立的全连接层生成的。\n小视科技提出的SoftAdaLIN如下：\n\n对于每一个解码器，引入了编码器生成的content feature，同时利用类似的方法控制content feature和style feature的权重，再生成soft gamma和soft beta，进行AdaLIN计算。\n这些结构没有数学上的证明，主要是通过大量的消融测试证明其有效性。"
  },
  {
    "objectID": "posts/U-GAT-IT.html#对抗损失",
    "href": "posts/U-GAT-IT.html#对抗损失",
    "title": "U-GAT-IT论文解读",
    "section": "对抗损失",
    "text": "对抗损失\n其中对抗损失为最小二乘GAN： \\[\n\\begin{aligned}\n  L_{l s g a n}^{s \\rightarrow t}=\\left(\\mathbb{E}_{x \\sim X_{t}}\\left[\\left(D_{t}(x)\\right)^{2}\\right]+\\mathbb{E}_{x \\sim X_{s}}\\left[\\left(1-D_{t}\\left(G_{s \\rightarrow t}(x)\\right)\\right)^{2}\\right]\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/U-GAT-IT.html#循环一致性损失",
    "href": "posts/U-GAT-IT.html#循环一致性损失",
    "title": "U-GAT-IT论文解读",
    "section": "循环一致性损失",
    "text": "循环一致性损失\n这个是cycleGAN中的标配：\n\\[\n\\begin{aligned}\n  \\left.L_{c y c l e}^{s \\rightarrow t}=\\left.\\mathbb{E}_{x \\sim X_{s}}\\left[| x-G_{t \\rightarrow s}\\left(G_{s \\rightarrow t}(x)\\right)\\right)\\right|_{1}\\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/U-GAT-IT.html#图像一致性损失",
    "href": "posts/U-GAT-IT.html#图像一致性损失",
    "title": "U-GAT-IT论文解读",
    "section": "图像一致性损失",
    "text": "图像一致性损失\n这个也是cycleGAN中的标配： \\[\n\\begin{aligned}\n  L_{i d e n t i t y}^{s \\rightarrow t}=\\mathbb{E}_{x \\sim X_{t}}\\left[\\left|x-G_{s \\rightarrow t}(x)\\right|_{1}\\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/U-GAT-IT.html#cam损失",
    "href": "posts/U-GAT-IT.html#cam损失",
    "title": "U-GAT-IT论文解读",
    "section": "CAM损失",
    "text": "CAM损失\n这个就是对之前模型生成CAM logit进行判别（这里我有个地方前面忘记说明了，他的判别器中也使用的CAM的方法），对于判别器中的生成的CAM logit判别比较简单，即正样本的logit接近于1，负样本接近0。\n生成器中的CAM logit使用交叉熵的使跨域的转换logit接近于1，同域的转换接近0。\n\\[\n\\begin{aligned}\n  \\begin{array}{l}\nL_{c a m}^{s \\rightarrow t}=-\\left(\\mathbb{E}_{x \\sim X_{s}}\\left[\\log \\left(\\eta_{s}(x)\\right)\\right]+\\mathbb{E}_{x \\sim X_{t}}\\left[\\log \\left(1-\\eta_{s}(x)\\right)\\right]\\right) \\\\\nL_{c a m}^{D_{t}}=\\mathbb{E}_{x \\sim X_{t}}\\left[\\left(\\eta_{D_{t}}(x)\\right)^{2}\\right]+\\mathbb{E}_{x \\sim X_{s}}\\left[\\left(1-\\eta_{D_{t}}\\left(G_{s \\rightarrow t}(x)\\right)^{2}\\right]\\right.\n\\end{array}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/U-GAT-IT.html#身份id损失",
    "href": "posts/U-GAT-IT.html#身份id损失",
    "title": "U-GAT-IT论文解读",
    "section": "身份ID损失",
    "text": "身份ID损失\n这是小视科技自己提的，因为要做接近于真人的效果转换，加上人脸识别损失比较好。"
  },
  {
    "objectID": "posts/U-GAT-IT.html#section",
    "href": "posts/U-GAT-IT.html#section",
    "title": "U-GAT-IT论文解读",
    "section": "6.8",
    "text": "6.8\n前面已经做了很多次实验了，动画图像我也截取了不少了。昨晚我想训练网络适合亚洲人脸，网上一下找不到好的亚洲人脸数据，然后我就找了个stylegan生成的亚洲人脸，训练了一晚发现很奇怪的问题，只要是用生成的数据做转换，很大概率会得到有问题的图像,如下所示。\n\n我发现jojo的风格还是适合模特，用模特的照片转换之后相当有味： \n不说了，我重新去找合适的数据了。明天看效果。"
  },
  {
    "objectID": "posts/Polyadd.html",
    "href": "posts/Polyadd.html",
    "title": "多项式相加",
    "section": "",
    "text": "废话不想多说，直接上程序 o(╥﹏╥)o最近腰疼的难受。\n\n\n程序\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;time.h&gt;\n\ntypedef struct PolyNode {\n    int coef; // coeffciant\n    int expon;\n    struct PolyNode *next;\n} * Polynomial;\n\nvoid CreateP(Polynomial P1, Polynomial P2) {\n    // P1=3*x^5+4*x^4-1*x^3+2*x^1-1\n    Polynomial temp = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp-&gt;coef = 3;\n    temp-&gt;expon = 5;\n    P1-&gt;next = temp;\n    temp-&gt;next = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp = temp-&gt;next;\n    temp-&gt;coef = 4;\n    temp-&gt;expon = 4;\n    temp-&gt;next = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp = temp-&gt;next;\n    temp-&gt;coef = -1;\n    temp-&gt;expon = 3;\n    temp-&gt;next = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp = temp-&gt;next;\n    temp-&gt;coef = 2;\n    temp-&gt;expon = 1;\n    temp-&gt;next = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp = temp-&gt;next;\n    temp-&gt;coef = -1;\n    temp-&gt;expon = 0;\n    temp-&gt;next = NULL;\n    // P2=2*x^4+1*x^3-7*x^2+1*x^1\n    temp = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp-&gt;coef = 2;\n    temp-&gt;expon = 4;\n    P2-&gt;next = temp;\n    temp-&gt;next = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp = temp-&gt;next;\n    temp-&gt;coef = 1;\n    temp-&gt;expon = 3;\n    temp-&gt;next = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp = temp-&gt;next;\n    temp-&gt;coef = -7;\n    temp-&gt;expon = 2;\n    temp-&gt;next = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp = temp-&gt;next;\n    temp-&gt;coef = 1;\n    temp-&gt;expon = 1;\n    temp-&gt;next = NULL;\n}\n\nint Compare(Polynomial P1, Polynomial P2) {\n\n    if (P1-&gt;expon &gt; P2-&gt;expon) {\n        return 1;\n    }\n\n    if (P1-&gt;expon &lt; P2-&gt;expon) {\n        return -1;\n    }\n\n    if (P1-&gt;expon == P2-&gt;expon) {\n        return 0;\n    }\n}\n\nvoid Attach(int Coef, int Expon, Polynomial tail) {\n    Polynomial temp = (Polynomial)malloc(sizeof(struct PolyNode));\n    temp-&gt;coef = Coef;\n    temp-&gt;expon = Expon;\n    temp-&gt;next = NULL;\n    tail-&gt;next = temp;\n    tail = temp;\n}\n\nPolynomial PolyAdd(Polynomial P1, Polynomial P2) {\n    Polynomial front, rear, temp;\n    int sum;\n    rear = (Polynomial)malloc(sizeof(struct PolyNode));\n    front = rear;\n    while (P1-&gt;next && P2-&gt;next) {\n        switch (Compare(P1-&gt;next, P2-&gt;next)) {\n        case 1:\n            Attach(P1-&gt;coef, P1-&gt;expon, rear);\n            P1 = P1-&gt;next;\n            break;\n        case -1:\n            Attach(P2-&gt;coef, P2-&gt;expon, rear);\n            P2 = P2-&gt;next;\n            break;\n        case 0:\n            sum = P1-&gt;coef + P2-&gt;coef;\n            if (sum) {\n                Attach(sum, P1-&gt;expon, rear);\n            }\n            P1 = P1-&gt;next;\n            P2 = P2-&gt;next;\n            break;\n        }\n    }\n\n    for (; P1; P1 = P1-&gt;next) {\n        Attach(P1-&gt;coef, P1-&gt;expon, rear);\n    }\n    for (; P2; P2 = P2-&gt;next) {\n        Attach(P2-&gt;coef, P2-&gt;expon, rear);\n    }\n    rear-&gt;next=NULL;\n    temp=front;\n    front=front-&gt;next;\n    free(temp);\n    return front;\n}\n\nint main(int argc, char const *argv[]) {\n    /* create the polynomial */\n    Polynomial MP1 = NULL, MP2 = NULL,MPA=NULL;\n    MP1 = (Polynomial)malloc(sizeof(struct PolyNode));\n    MP2 = (Polynomial)malloc(sizeof(struct PolyNode));\n    MPA = (Polynomial)malloc(sizeof(struct PolyNode));\n    CreateP(MP1, MP2);\n    MPA=PolyAdd(MP1,MP2);\n    return 0;\n}"
  },
  {
    "objectID": "posts/Matlab使用ThunderSVM.html",
    "href": "posts/Matlab使用ThunderSVM.html",
    "title": "Matlab使用ThunderSVM",
    "section": "",
    "text": "我决定用这个SVM库加速我的svr模型训练。我的系统是Windows，Matlab版本是2017b。我需要自己编译安装入坑一波。官方手册\n\n\n准备\n\ncmake 在cmake官网下载Windows win64-x64 Installer并安装。\nVisual C++ 这个我的电脑已经安装了Visual Studio 2015。我可以在matlab中查看我的c++编译器版本\n&gt;&gt; mex -setup c++\nMEX 配置为使用 'Microsoft Visual C++ 2015 Professional' 以进行 C++ 语言编译。\n警告: MATLAB C 和 Fortran API 已更改，现可支持\n    包含 2^32-1 个以上元素的 MATLAB 变量。您需要\n    更新代码以利用新的 API。\n    您可以在以下网址找到更多的相关信息:\n    http://www.mathworks.com/help/matlab/matlab_external/upgrading-mex-files-to-use-64-bit-api.html。\n\n要选择不同的 C++ 编译器，请从以下选项中选择一种命令:\nMicrosoft Visual C++ 2015  mex -setup:H:\\MATLAB\\R2017b\\bin\\win64\\mexopts\\msvcpp2015.xml C++\nMicrosoft Visual C++ 2015 Professional  mex -setup:C:\\Users\\59732\\AppData\\Roaming\\MathWorks\\MATLAB\\R2017b\\mex_C++_win64.xml C++\nCUDA 如果需要使用gpu加速，需要安装CUDA 7.5以上版本。(必须先安装Visual Studio)\n\n\n\nWindows版安装\n\n下载工程。 手动下载zip的话，文件夹名不一样，需要修改。\n\ngit clone https://github.com/zeyiwen/thundersvm.git\n\n构建Visual Studio工程 进入文件夹中。\n\ncd thundersvm\nmkdir build\ncd build\ncmake .. -DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=TRUE -DBUILD_SHARED_LIBS=TRUE -G \"Visual Studio 14 2015 Win64\"\n这个Visual Studio 14 2015 Win64是因为我安装了Visual Studio 15，对应的cmake选择的版本是这个，成功后生成如下文件。 \n\n编译 打开thundersvm.sln。选择生成-&gt;生成ALL_BUILD。成功后如下：\n\n5&gt;------ 已启动生成: 项目: ALL_BUILD, 配置: Debug x64 ------\n5&gt;  Building Custom Rule F:/thundersvm/CMakeLists.txt\n5&gt;  CMake does not need to re-run because F:/thundersvm/build/CMakeFiles/generate.stamp is up-to-date.\n========== 生成: 成功 5 个，失败 0 个，最新 0 个，跳过 0 个 ==========\n\n\n测试\n我这里直接测试Matlab的使用。\n\n添加路径 将他的matlab函数添加到默认路径。 \n程序 因为我是Windows，所以官方的例子的斜杠需要更换\n\nn = [\"-c\", \"10\", \"-g\", \"0.125\", \"..\\dataset\\test_dataset.txt\", \"test_dataset.model\"];\nm = cellstr(n);\nsvm_train_matlab(m);\nn = [\"..\\dataset\\test_dataset.txt\", \"test_dataset.model\", \"test_datset.out\"];\nm = cellstr(n);\nsvm_predict_matlab(m);\na=load('F:\\thundersvm\\dataset\\test_datset.out')\nsvm分类结果输出如下："
  },
  {
    "objectID": "posts/Matlab-GA函数.html",
    "href": "posts/Matlab-GA函数.html",
    "title": "Matlab GA函数",
    "section": "",
    "text": "我使用的matlab是2017b，但是我在官方查找文档，其中的例子这个版本没有。所以我摸索了一番。使用这个函数的动机是我需要是用libsvm做回归模型的训练，其中训练的优化函数是遗传算法（谢菲尔德工具箱），老的工具箱不支持并行计算以及GPU加速，所以我使用Matlab自带的遗传算法函数进行参数的优化。"
  },
  {
    "objectID": "posts/Matlab-GA函数.html#程序",
    "href": "posts/Matlab-GA函数.html#程序",
    "title": "Matlab GA函数",
    "section": "程序",
    "text": "程序\nxi = linspace(-6,2,300);\nyi = linspace(-4,4,300);\n[X,Y] = meshgrid(xi,yi);\nZ = ps_example([X(:),Y(:)]);% 构造函数\nZ = reshape(Z,size(X));\n\nsurf(X,Y,Z,'MeshStyle','none')% 绘图\ncolormap 'jet'\nview(-26,43)\nxlabel('x(1)')\nylabel('x(2)')\ntitle('ps\\_example(x)')\n\nrng default % 保证再生性\nx = ga(@ps_example,2)\n结果：\nOptimization terminated: average change in the fitness value less than options.FunctionTolerance.\n\nx =\n\n   -4.6793   -0.0860\n函数图像："
  },
  {
    "objectID": "posts/Matlab-GA函数.html#分析",
    "href": "posts/Matlab-GA函数.html#分析",
    "title": "Matlab GA函数",
    "section": "分析",
    "text": "分析\nGA函数最基本的用法就是构造一个输入为行向量，输出为数值的函数例如Z = ps_example([X(:),Y(:)])。 接下来使用GA函数的时候，第一个参数是函数句柄，第二个参数是函数参数个数。 返回值则是输出参数函数句柄所使用的参数行向量矩阵。"
  },
  {
    "objectID": "posts/Matlab-GA函数.html#程序-1",
    "href": "posts/Matlab-GA函数.html#程序-1",
    "title": "Matlab GA函数",
    "section": "程序",
    "text": "程序\n待寻优函数的输入参数是一个行向量，因此传入x(1)代表x，x(2)代表y。 如果需要设置区域范围x + y &gt;= 1和y &lt;= 5 + x的ps_example函数最小化。 首先, 将两个不等式约束转换为矩阵形式A*x &lt;= b。换言之, 获取不等式左侧的x变量, 并使两个不等式小于等于:  那么将矩阵乘法的系数作为约束条件：令A = [-1,-1;-1,1];  b = [-1;5];。全程序如下：\n    xi = linspace(-6,2,300);\n    yi = linspace(-4,4,300);\n    [X,Y] = meshgrid(xi,yi);\n    Z = ps_example([X(:),Y(:)]);% 构造函数\n    Z = reshape(Z,size(X));\n+   A = [-1,-1;% 添加约束\n+       -1,1];\n+   b = [-1;5];\n\n    surf(X,Y,Z,'MeshStyle','none')% 绘图\n    colormap 'jet'\n    view(-26,43)\n    xlabel('x(1)')\n    ylabel('x(2)')\n    title('ps\\_example(x)')\n\n    rng default; % 再生性\n+   x = ga(@ps_example,2,A,b)\n结果如下：\nOptimization terminated: average change in the fitness value less than options.FunctionTolerance.\n\nx =\n\n    0.9991   -0.0000"
  },
  {
    "objectID": "posts/Matlab-GA函数.html#程序-2",
    "href": "posts/Matlab-GA函数.html#程序-2",
    "title": "Matlab GA函数",
    "section": "程序",
    "text": "程序\n如果添加的约束条件不是小于等于，而是一个小于等于，一个等于。  那么设置约束条件时就需要分开设置(A对应b,Aeq对应beq):\nA = [-1 -1];\nb = -1;\nAeq = [-1 1];\nbeq = 5;\n修改ga函数输入为：\nx = ga(@ps_example,2,A,b,Aeq,beq)\n结果：\nOptimization terminated: average change in the fitness value less than options.FunctionTolerance.\nx = 1×2\n\n   -2.0000    2.9990"
  },
  {
    "objectID": "posts/Matlab-GA函数.html#结论",
    "href": "posts/Matlab-GA函数.html#结论",
    "title": "Matlab GA函数",
    "section": "结论",
    "text": "结论\n小于等于的约束条件和等于的约束条件要分开来写。 # 线性约束和边界优化 ## 程序 保持上一例不变，现在若要约束x,y的取值范围为1 &lt;= x &lt;= 6和-3 &lt;= y &lt;= 8。 那么设置界限lb和ub。\nlb = [1 -3];\nub = [6 8];\n修改ga函数为：\nx = ga(@ps_example,2,A,b,Aeq,beq,lb,ub)\n结果：\nOptimization terminated: average change in the fitness value less than options.FunctionTolerance.\nx = 1×2\n\n    1.0001    5.9992"
  },
  {
    "objectID": "posts/Matlab-GA函数.html#结论-2",
    "href": "posts/Matlab-GA函数.html#结论-2",
    "title": "Matlab GA函数",
    "section": "结论",
    "text": "结论\n前面四个空矩阵都是ps_example的约束条件，约束条件函数ellipsecons的约束条件是后面两个空矩阵。 # 使用非默认选项最小化 ## 程序 为了获得更精确的解决方案, 可以将约束公差设置为1e-6。并监视规划求解进度, 设置一个绘图函数。 总程序如下：\nclose all\nclear\nxi = linspace(-6,2,300);\nyi = linspace(-4,4,300);\n[X,Y] = meshgrid(xi,yi);\nZ = ps_example([X(:),Y(:)]);% 构造函数\nZ = reshape(Z,size(X));\n\nA = [-1 -1];%添加约束\nb = -1;\nAeq = [-1 1];\nbeq = 5;\n\nsurf(X,Y,Z,'MeshStyle','none')% 绘图\ncolormap 'jet'\nview(-26,43)\nxlabel('x(1)')\nylabel('x(2)')\ntitle('ps\\_example(x)')\n\n\noptions = optimoptions('ga','ConstraintTolerance',1e-6,'PlotFcn', @gaplotbestf);%设置选项\nrng default; % 再生性\nx = ga(@ps_example,2,A,b,Aeq,beq,[],[],[],options)\n结果：\nOptimization terminated: average change in the fitness value less than options.FunctionTolerance.\n\nx =\n\n   -2.0000    3.0000\n # 实例：优化libsvm训练模型 很难受，我尝试改写了程序，但是并不能使用并行加速，还是很慢。。接下来我要找别的方式去加速。"
  },
  {
    "objectID": "posts/Linuxblue2.html",
    "href": "posts/Linuxblue2.html",
    "title": "OrangePI蓝牙：搜索设备",
    "section": "",
    "text": "上一篇文章讲述了我在OrangePi中开启蓝牙的过程，这一章来讲述我如何对蓝牙进行编程操作。"
  },
  {
    "objectID": "posts/Linuxblue2.html#开发板中使用bluez",
    "href": "posts/Linuxblue2.html#开发板中使用bluez",
    "title": "OrangePI蓝牙：搜索设备",
    "section": "开发板中使用bluez",
    "text": "开发板中使用bluez\n\n安装bluez 我的OrangePi的板子上是bluez5.43版本,为了开发bluez需要安装一些必要的头文件.\n    sudo apt-get install libbluetooth-dev\n    ls /usr/include/bluetooth/\n    bluetooth.h  bnep.h  cmtp.h  hci.h  hci_lib.h  hidp.h  l2cap.h  rfcomm.h  sco.h  sdp.h  sdp_lib.h\n    ```\n安装后,我们就可以在通过添加一些头文件去调用bluez的api做一些事情了.\n\n```c\n#include &lt;bluetooth/bluetooth.h&gt;\n#include&lt;Bluetooth/hci.h&gt;\n#include&lt;Bluetooth/hci_lib.h&gt;\n配置编译 首先我使用外国老哥曾经写的文档中的代码做测试[1](这个文档非常nice),这个一个扫描周围蓝牙设备的小程序.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/socket.h&gt;\n#include &lt;bluetooth/bluetooth.h&gt;\n#include &lt;bluetooth/hci.h&gt;\n#include &lt;bluetooth/hci_lib.h&gt;\n\nint main(int argc, char **argv)\n{\ninquiry_info *ii = NULL;\nint max_rsp, num_rsp;\nint dev_id, sock, len, flags;\nint i;\nchar addr[19] = { 0 };\nchar name[248] = { 0 };\n\ndev_id = hci_get_route(NULL);\nsock = hci_open_dev( dev_id );\nif (dev_id &lt; 0 || sock &lt; 0) {\n    perror(\"opening socket\");\n    exit(1);\n}\n\nlen  = 8;\nmax_rsp = 255;\nflags = IREQ_CACHE_FLUSH;\nii = (inquiry_info*)malloc(max_rsp * sizeof(inquiry_info));\n\nnum_rsp = hci_inquiry(dev_id, len, max_rsp, NULL, &ii, flags);\nif( num_rsp &lt; 0 ) perror(\"hci_inquiry\");\n\nfor (i = 0; i &lt; num_rsp; i++) {\n    ba2str(&(ii+i)-&gt;bdaddr, addr);\n    memset(name, 0, sizeof(name));\n    if (hci_read_remote_name(sock, &(ii+i)-&gt;bdaddr, sizeof(name), \n        name, 0) &lt; 0)\n    strcpy(name, \"[unknown]\");\n    printf(\"%s  %s\\n\", addr, name);\n}\n\nfree( ii );\nclose( sock );\nreturn 0;\n}\n使用:\nroot@H5:~# vi test.c\nroot@H5:~# gcc -o simplescan test.c -lbluetooth\nroot@H5:~# ./simplescan\nCC:29:F5:79:14:21  iPhone\n交叉编译bluez\n下载bluez\n首先去官方网址下载bluez,这里我还是选择使用5.43版本.\n解压配置 这里参考网络文章[2]. 将bluez5.43解压出来.并新建一个文件夹blue,用于存放安装的文件. sh     mkdir blue     cd bluez-5.43     CC=/home/zqh/GccOrangPi/bin/aarch64-linux-gnu-gcc #指定交叉编译器     ./configure --host=aarch64-linux-gnu --prefix=/home/zqh/Program/orangepi/blue  --disable-obex --enable-library 这里如果配置检查出现错误那么参考这个文章[3]. 这里有一个比较恶心的错误: sh     configure: error: readline header files are required 我安装了readline-dev还是会出现这个错误,这里就需要把这个头文件复制到交叉编译器的头文件目录下. sh     sudo cp -r /usr/include/readline /home/zqh/GccOrangPi/aarch64-linux-gnu/libc/usr/include\n编译安装 配置完成后开始编译以及安装 sh     make     sudo make install     cd /home/zqh/Program/orangepi/blue 之后就可以把blue目录下的文件移植到开发板上. 将/bin下所有文件，放到开发板/usr/bin include所有文件，放到开发板/usr/include lib所有文件，放到开发板/usr/lib sbin所有文件，放到开发板/usr/sbin 当然bluez安装还是会把许多的文件安装在默认目录,比如/etc下,我先尝试看看会不会影响开发. ## 偷懒方法 其实orangepi可以自己apt-get到libbluetooth-dev,那么我们可以直接将板子上的动态链接库拉过来使用即可~(注意编译动态链接库的编译器版本不同可能会出现错误,那么还是要全部手动编译一波,再安装进去) 不过要是大家有什么更简单的方法,可以方便的交叉编译,可以告诉我~"
  },
  {
    "objectID": "posts/Linuxblue2.html#使用bluez",
    "href": "posts/Linuxblue2.html#使用bluez",
    "title": "OrangePI蓝牙：搜索设备",
    "section": "使用bluez",
    "text": "使用bluez\n\n配置路径\n我这里使用的是cmake配置工程:\n# 设置工程名称\nproject (TEST)\n# 设置可执行文件名称\nset(MY_TARGET t1)\n# 需要链接的动态链接库\nset(EXTRA_LIBS libpthread.so libbluetooth.so)\n# CMAKE最小版本\ncmake_minimum_required (VERSION 2.6)\n\n# 设置目标平台系统\nset(CMAKE_SYSTEM_NAME Linux)\n\n# 设置交叉编译库路径\nset(CMAKE_FIND_ROOT_PATH /home/zqh/GccOrangPi/)\n# set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)#只在交叉编译库路径中寻找\n# set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)\n\n# 设置交叉编译器\nset(CMAKE_C_COMPILER /home/zqh/GccOrangPi/bin/aarch64-linux-gnu-gcc)\nset(CMAKE_CXX_COMPILER /home/zqh/GccOrangPi/bin/aarch64-linux-gnu-g++)\nset(CMAKE_C_EXTENSIONS \"-lbluetooth -pipe -g -Wall -W -fPIE\")\nset(CMAKE_CXX_EXTENSIONS \"-lbluetooth -pipe -g -Wall -W -fPIE\")\n#设置执行文件输出目录\nset(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/bin)\n#设置库输出路径\nset(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/lib)\n\nmessage(\"++++++++++++++Start Build+++++++++++++++++\")\n\n# 添加头文件目录\ninclude_directories(${PROJECT_SOURCE_DIR}/usr/inc)\ninclude_directories(\"/home/zqh/Program/orangepi/blue/include\")\n\n# 添加源文件目录\naux_source_directory(${PROJECT_SOURCE_DIR}/usr/src USRSRC)\n\n\n# 添加子目录 子目录里面放一些别的编译好的模块\n#ADD_SUBDIRECTORY(src)\n\n# 链接库搜索路径\nlink_directories(\"/home/zqh/GccOrangPi/\" \"/home/zqh/Program/orangepi/blue/lib\")\n\n# 添加动态库\nlink_libraries(${EXTRA_LIBS})\n\n# 添加可执行文件（可执行文件名 [配置] 源文件）\nadd_executable(${MY_TARGET} ${USRSRC})\n\n# 执行文件链接属性\nTARGET_LINK_LIBRARIES(${MY_TARGET} ${EXTRA_LIBS})\n编译运行 这里使用之前的代码.很简单: sh     cd build     cmake ..     make 然后发送可执行文件到开发板上,运行: sh     root@H5:~# ./t1     CC:29:F5:79:14:21  iPhone"
  },
  {
    "objectID": "posts/Linuxblue2.html#参考资料",
    "href": "posts/Linuxblue2.html#参考资料",
    "title": "OrangePI蓝牙：搜索设备",
    "section": "参考资料",
    "text": "参考资料\n[1].http://people.csail.mit.edu/albert/bluez-intro/c404.html [2].http://www.forwhat.cn/post-436.html [3].https://blog.csdn.net/twy76/article/details/23851587"
  },
  {
    "objectID": "posts/LeNet-5.html",
    "href": "posts/LeNet-5.html",
    "title": "LeNet-5",
    "section": "",
    "text": "过几天要考试了/(ㄒoㄒ)/~~\n还要复习真的不爽.本来两周就可以把Ng的DL课程撸完,想到明天要交矩阵论作业,今天就去写矩阵作业去了,写了两个小时就不想写了(都不会2333\n然后回来撸波代码,用keras写了个LeNet-5网络.\n\n\n代码\n我是TensorFlow 1.12,只要装上Keras和Tensorflow就可以运行我这个代码.\n我这里其他的应该都和原版一样,就是最后的全连接输出层,原版用的是高斯核也就是径向基核,但是我一时没有找到,就用了softmax输出.并且这个网络多训练几次,是可以达到99%正确率的,但是我懒得跑了,我写这个主要以熟悉API为目的 2333\nimport keras as k\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.callbacks import TensorBoard\n\n\ndef expand(x: np.ndarray):\n    '''扩展矩阵 4个像素 都为0'''\n    return np.pad(x, ((0, 0), (2, 2), (2, 2)), mode='constant')\n\n\nif __name__ == \"__main__\":\n    # 读取数据\n    (x_train, y_train), (x_test, y_test) = k.datasets.mnist.load_data()\n\n    # 扩充矩阵\n    x_train = expand(x_train)\n    x_test = expand(x_test)  # [28,28] -&gt; [32,32]\n\n    # 增加通道数 为了卷积匹配\n    x_train = np.reshape(x_train, np.append(np.array(x_train.shape), 1))\n    x_test = np.reshape(x_test, np.append(np.array(x_test.shape), 1))\n\n    # one hot 编码\n    y_train = k.utils.to_categorical(y_train, num_classes=10)\n    y_test = k.utils.to_categorical(y_test, num_classes=10)\n\n    # 归一化\n    x_train = x_train / 255.0\n    x_test = x_test / 255.0\n\n    # 绘图\n    # plt.imshow(x_train[0])\n    # plt.show()\n\n    # 创建模型\n    mod = k.models.Sequential(name='leNet-5')\n\n    '''C1层是一个卷积层输入图片：32*32\n    卷积核大小：5*5\n    卷积核种类：6\n    输出featuremap大小：28*28 （32-5+1）\n    神经元数量：28*28*6\n    连接数：（5*5+1）*6*28*28'''\n    mod.add(k.layers.Convolution2D(filters=6, kernel_size=(5, 5), kernel_initializer=k.initializers.he_normal(1)))\n\n    '''\n    S2层是一个下采样层\n    输入：28*28\n    采样区域：2*2\n    采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid\n    采样种类：6\n    输出featureMap大小：14*14（28/2）\n    神经元数量：14*14*6\n    '''\n    mod.add(k.layers.AveragePooling2D(pool_size=(2, 2)))\n    mod.add(k.layers.Activation('sigmoid'))\n\n    '''\n    C3层也是一个卷积层\n    输入：S2中所有6个或者几个特征map组合\n    卷积核大小：5*5\n    卷积核种类：16\n    输出featureMap大小：10*10 (14-5+1)\n    '''\n    mod.add(k.layers.Conv2D(filters=16, kernel_size=(5, 5), kernel_initializer=k.initializers.he_normal(2)))\n\n    '''\n    S4层是一个下采样层\n    输入：10*10\n    采样区域：2*2\n    采样种类：16\n    输出featureMap大小：5*5（10/2）\n    神经元数量：5*5*16=400\n    '''\n    mod.add(k.layers.AveragePooling2D(pool_size=(2, 2)))\n\n    '''\n    C5层是一个卷积层\n    输入：S4层的全部16个单元特征map（与s4全相连）\n    卷积核大小：5*5\n    卷积核种类：120\n    输出featureMap大小：1*1（5-5+1）\n    '''\n    mod.add(k.layers.Conv2D(filters=120, kernel_size=(5, 5), kernel_initializer=k.initializers.he_normal(3)))\n\n    '''\n    F6层全连接层\n    输入：c5 120维向量\n    激活函数:sigmoid\n    '''\n    mod.add(k.layers.Flatten())\n    mod.add(k.layers.Dense(120))\n    mod.add(k.layers.Dense(10))\n    mod.add(k.layers.Activation('softmax'))\n\n    mod.compile(optimizer=k.optimizers.adam(),\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n    # 训练模型\n    his = mod.fit(x=x_train, y=y_train,\n                  batch_size=32,\n                  epochs=4,\n                  validation_split=0.1,\n                  callbacks=[TensorBoard(log_dir='./log_dir')])\n\n    '''可视化训练结果  (为什么验证集准确率比训练集还高?'''\n    # 绘制验证集,测试集准确率曲线\n    plt.plot(his.history['acc'])\n    plt.plot(his.history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # 绘制验证集,训练集 误差曲线\n    plt.plot(his.history['loss'])\n    plt.plot(his.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    '''评估测试集'''\n    evahis = mod.evaluate(x=x_test, y=y_test)\n    print('测试集准确率为{:.3f}%'.format(evahis[1] * 100))\n\n    '''显示模型'''\n    k.utils.plot_model(mod, show_shapes=True)\n\n\n运行结果\n\n输出\n\n   32/10000 [..............................] - ETA: 0s\n 1952/10000 [====&gt;.........................] - ETA: 0s\n 3776/10000 [==========&gt;...................] - ETA: 0s\n 5760/10000 [================&gt;.............] - ETA: 0s\n 7872/10000 [======================&gt;.......] - ETA: 0s\n 9984/10000 [============================&gt;.] - ETA: 0s\n10000/10000 [==============================] - 0s 26us/step\n测试集准确率为97.450%\n\n\n图像\n\n\n\n准确率曲线\n\n\n\n\n\n损失曲线\n\n\n\n\n\n网络结构"
  },
  {
    "objectID": "posts/BinaryTree.html",
    "href": "posts/BinaryTree.html",
    "title": "二叉树",
    "section": "",
    "text": "今天写的一波二叉树的操作。。感觉自己还得多多练习啊！ 依旧直接上代码了，8点了，得回去休息了T_T。\n\n\n程序\n/*\n * @Author: Zheng Qihang\n * @Date: 2018-07-10 09:38:39\n * @Last Modified by: Zheng Qihang\n * @Last Modified time: 2018-11-08 16:36:09\n */\n#include &lt;limits.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;time.h&gt;\n\n/*\n        data\n       /    \\\n      V      V\n    Left    Right\n*/\ntypedef int ElementType;\ntypedef struct TreeNode {\n    ElementType Data;\n    struct TreeNode *Left;\n    struct TreeNode *Right;\n} * BinTree;\n\n/************************* Queue define! **************************/\n#define ElementQueueType BinTree\ntypedef struct QueueNode {\n    ElementQueueType data;\n    struct QueueNode *next;\n} * QNode;\n\ntypedef struct QueueHeader {\n    QNode front;\n    QNode rear;\n} * Queue;\n\n/**\n * description  Create the Queue!\n * @param[in]   void\n * @retval      no\n **/\nQueue CreateQueue(void) {\n    Queue Q = (Queue)malloc(sizeof(struct QueueHeader));\n    Q-&gt;front = NULL;\n    Q-&gt;rear = NULL;\n    return Q;\n}\n\n/**\n * description  Add Queue Node !\n * @param[in]   Queue header and a item\n * @retval      void\n **/\nvoid AddQ(Queue Q, ElementQueueType item) {\n    if (!Q) {\n        return;\n    }\n    if (!item) {\n        return;\n    }\n    QNode temp = (QNode)malloc(sizeof(struct QueueNode));\n    temp-&gt;data = item;\n    temp-&gt;next = NULL;\n    // when the Queue is null\n    if (!Q-&gt;front && !Q-&gt;rear) {\n        Q-&gt;front = temp;\n        Q-&gt;rear = temp;\n    } else {\n        Q-&gt;rear-&gt;next = temp;\n        Q-&gt;rear = temp;\n    }\n}\n\n/**\n * description  check the queue is empty\n * @param[in]   queue\n * @retval      int\n **/\nint IsEmptyQ(Queue Q) { return !Q-&gt;front; }\n\n/**\n * description  Delete the Queue Node\n * @param[in]   Queue header\n * @retval      data\n **/\nElementQueueType DeleteQ(Queue Q) {\n    if (!Q) {\n        return NULL;\n    }\n    if (IsEmptyQ(Q)) {\n        return NULL;\n    }\n    QNode temp = Q-&gt;front;\n    ElementQueueType tempdata;\n    if (Q-&gt;front == Q-&gt;rear) {\n        Q-&gt;front = NULL;\n        Q-&gt;rear = NULL;\n    } else {\n        Q-&gt;front = Q-&gt;front-&gt;next;\n    }\n    tempdata = temp-&gt;data;\n    free(temp);\n    return tempdata;\n}\n\nvoid PrintQueue(Queue Q) {\n\n    if (IsEmptyQ(Q)) {\n        return;\n    }\n    printf(\"打印队列数据元素：\\n\");\n    QNode temp = Q-&gt;front;\n\n    for (; temp; temp = temp-&gt;next) {\n        printf(\"0x%3lX   \", (unsigned long int)temp-&gt;data & 0xFFF);\n    }\n    printf(\"\\n\");\n}\n\n/************************* Queue define! **************************/\n\n/************************* Stack define! **************************/\n\n#define ElementStackType BinTree\ntypedef struct StackNode // node defination\n{\n    ElementStackType data;\n    struct StackNode *next;\n} * Stack; // Node is a pointer to _Node\n\nint IsEmpty(Stack S) { return !S-&gt;next; }\n\nStack CreateStack(void) {\n    Stack S = (Stack)malloc(sizeof(struct StackNode));\n    S-&gt;next = NULL;\n    return S;\n}\n\nElementStackType Pop(Stack S) {\n    ElementStackType temp;\n    Stack tmepNode = S-&gt;next;\n    if (tmepNode != NULL) {\n        temp = tmepNode-&gt;data;\n        S-&gt;next = tmepNode-&gt;next;\n        free(tmepNode);\n        return temp;\n    } else {\n        return NULL;\n    }\n}\n\nvoid MakeEmpty(Stack S) {\n    while (S-&gt;next != NULL) {\n        Pop(S);\n    }\n}\nvoid DisposeStack(Stack S) {\n    MakeEmpty(S);\n    free(S);\n}\n\nvoid Push(Stack S, ElementStackType X) {\n    Stack tempNode = S-&gt;next;\n    Stack newNode = (Stack)malloc(sizeof(struct StackNode));\n    newNode-&gt;data = X;\n    // printf(\"S-&gt;next %p\\n\",S-&gt;next);\n    if (tempNode != NULL) {\n        newNode-&gt;next = tempNode;\n        // printf(\"%p = %p\\n\",newNode-&gt;next,tempNode);\n    } else {\n        newNode-&gt;next = NULL;\n    }\n    S-&gt;next = newNode;\n    // printf(\"S-&gt;next %p\\n\",S-&gt;next);\n}\n\nElementStackType Top(Stack S) {\n    if (S-&gt;next != NULL) {\n        return S-&gt;next-&gt;data;\n    } else {\n        return NULL;\n    }\n}\n\nvoid PrintStack(Stack S) {\n    Stack tep = S-&gt;next;\n    while (tep != NULL) {\n        printf(\"addr=0x%3lX  dataadd=0x%3lX  nextaddr=0x%3lX\\n\",\n               (unsigned long int)tep & 0xFFF,\n               (unsigned long int)tep-&gt;data & 0xFFF,\n               (unsigned long int)tep-&gt;next & 0xFFF);\n        tep = tep-&gt;next;\n    }\n}\n\n/************************* Stack define! **************************/\n\n/**\n* description  first root then left\n                child tree then right child tree\n* @param[in]   BinTree\n* @retval      void\n**/\nvoid PreOrderTraversal(BinTree BT) {\n    if (BT) {\n        printf(\"%d   \", BT-&gt;Data);\n        PreOrderTraversal(BT-&gt;Left); // Recursion traversal\n        PreOrderTraversal(BT-&gt;Right);\n    }\n}\n/**\n * description  traversal the leaves\n * @param[in]   BinTree\n * @retval      void\n **/\nvoid PreOrderPrintLeaves(BinTree BT) {\n    if (BT) {\n\n        if (!BT-&gt;Left && !BT-&gt;Left) {\n            printf(\"%d   \", BT-&gt;Data);\n        }\n        PreOrderPrintLeaves(BT-&gt;Left); // Recursion traversal\n        PreOrderPrintLeaves(BT-&gt;Right);\n    }\n}\n\n/**\n * description  find the Binary tree alone length\n *              Tag==0 find the right\n *              Tag!=0 find the left\n * @param[in]   bintree\n * @retval      height\n **/\nint FindTreeLength(BinTree BT, int tag) {\n    if (BT) {\n\n        if (tag) {\n            return 1 + FindTreeLength(BT-&gt;Left, tag);\n        } else {\n            return 1 + FindTreeLength(BT-&gt;Right, tag);\n        }\n    } else {\n        return 0;\n    }\n}\n/*\n\n// /**\n//  * description  find the Binary tree height\n//  *              Tag==0 find the right\n//  *              Tag!=0 find the left\n//  * @param[in]   bintree\n//  * @retval      height\n//  **/\n// int FindTreeHeight(BinTree BT) {\n//     int rightlen = FindTreeLength(BT, 0);\n//     int leftlen = FindTreeLength(BT, 1);\n//     return rightlen &gt; leftlen ? rightlen : leftlen;\n// }\n/* the better implement */\nint FindTreeHeight(BinTree BT) {\n    int rightlen, leftlen, maxlen;\n    if (BT) {\n        rightlen = FindTreeHeight(BT-&gt;Right);\n        leftlen = FindTreeHeight(BT-&gt;Left);\n        maxlen = leftlen &gt; rightlen ? leftlen : rightlen;\n        return maxlen + 1;\n    } else {\n        return 0;\n    }\n}\n\n/**\n* description  first left child tree then\n            root then  then right child tree\n* @param[in]   BinTree\n* @retval      void\n**/\nvoid InOrderTraversal(BinTree BT) {\n    if (BT) {\n        PreOrderTraversal(BT-&gt;Left); // Recursion traversal\n        printf(\"%d   \", BT-&gt;Data);\n        PreOrderTraversal(BT-&gt;Right);\n    }\n}\n/**\n* description  first left child tree then\n             right child tree then root\n* @param[in]   BinTree\n* @retval      void\n**/\nvoid PostOrderTraversal(BinTree BT) {\n    if (BT) {\n        PreOrderTraversal(BT-&gt;Left); // Recursion traversal\n        PreOrderTraversal(BT-&gt;Right);\n        printf(\"%d   \", BT-&gt;Data);\n    }\n}\n\n/*\n* description first root then left\n                child tree then right child tree\n* @param[in]   BinTree\n* @retval      void\n**/\nvoid PreStackTraversal(BinTree BT) {\n    BinTree T = BT;\n    Stack S = (Stack)malloc(sizeof(struct StackNode));\n    // T!=null or stack is not empty\n    while (T || !IsEmpty(S)) {\n        while (T) { // push the left child tree in stack\n            Push(S, T);\n            printf(\"%d   \", T-&gt;Data);\n            T = T-&gt;Left;\n        }\n        if (!IsEmpty(S)) {\n            T = Pop(S);\n            T = T-&gt;Right; // to the\n        }\n    }\n    DisposeStack(S);\n}\n/*\n* description  first left child tree then\n            root then  then right child tree\n* @param[in]   BinTree\n* @retval      void\n**/\nvoid InStackTraversal(BinTree BT) {\n    BinTree T = BT;\n    Stack S = (Stack)malloc(sizeof(struct StackNode));\n    // T!=null or stack is not empty\n    while (T || !IsEmpty(S)) {\n        while (T) { // push the left child tree in stack\n            Push(S, T);\n            T = T-&gt;Left;\n        }\n        if (!IsEmpty(S)) {\n            T = Pop(S);\n            printf(\"%d   \", T-&gt;Data);\n            T = T-&gt;Right;\n        }\n    }\n    DisposeStack(S);\n}\n/*\n* description  first left child tree then\n             right child tree then  root\n* @param[in]   BinTree\n* @retval      void\n**/\nvoid PostStackTraversal(BinTree BT) {\n    BinTree T = BT;\n    Stack S = (Stack)malloc(sizeof(struct StackNode));\n    // T!=null or stack is not empty\n    while (T || !IsEmpty(S)) {\n        while (T) { // push the left child tree in S\n            Push(S, T);\n            T = T-&gt;Left;\n        }\n        if (!IsEmpty(S)) {\n            T = Pop(S);\n            if (T != BT) { // root don't print\n                printf(\"%d   \", T-&gt;Data);\n            }\n            T = T-&gt;Right;\n        }\n    }\n    printf(\"%d   \", BT-&gt;Data); // print root\n    DisposeStack(S);\n}\n\n/* sequence traversal\n    Use queue\n    ----------------\n            A           =&gt;\n    ----------------\n    ----------------\n        B       C       =&gt;\n    ----------------\n    ----------------\n    D     F   G    I    =&gt;\n    ----------------\n    ----------------\n        E       H       =&gt;\n    ----------------\n*/\nvoid LevelOrderTraversal(BinTree BT) {\n    Queue myqueue = CreateQueue();\n    BinTree T = BT;\n    BinTree temp = NULL;\n    AddQ(myqueue, T); // header add in queue\n    while (!IsEmptyQ(myqueue)) {\n        temp = DeleteQ(myqueue);\n        printf(\"%d   \", temp-&gt;Data);\n        AddQ(myqueue, temp-&gt;Left);\n        AddQ(myqueue, temp-&gt;Right);\n    }\n}\n\n/**\n * description  create the binary tree node\n * @param[in]   data\n * @retval      bintree\n **/\nBinTree CreateTreeNode(int dat) {\n    BinTree new = (BinTree)malloc(sizeof(struct TreeNode));\n    new-&gt;Data = dat;\n    new-&gt;Left = NULL;\n    new-&gt;Right = NULL;\n    return new;\n}\nvoid ConnectNode(BinTree root, BinTree left, BinTree right) {\n\n    if (!root) {\n        printf(\"error in null\\n\");\n        return;\n    }\n    root-&gt;Left = left;\n    root-&gt;Right = right;\n}\n\n/* create bin tree like this:\n                  1                           A\n            /          \\                /          \\\n           2            3              B            C\n         /   \\        /   \\          /   \\        /   \\\n       4      5      6     7       D      E      F     G\n      /        \\    / \\           /        \\    / \\\n     8          9  10  11        H          I  J   K\n */\nBinTree CreateBinTree(void) {\n    printf(\"Create Binary Tree like this:\\n \\\n                 1                           A\\n \\\n            /          \\\\                /          \\\\ \\n \\\n           2            3              B            C \\n  \\\n        /   \\\\        /   \\\\          /   \\\\        /   \\\\ \\n \\\n       4      5      6     7       D      E      F     G \\n \\\n      /        \\\\    / \\\\           /        \\\\    / \\\\ \\n \\\n     8          9  10  11        H          I  J   K \\n \\\n    \");\n    BinTree A = CreateTreeNode(1);\n    BinTree B = CreateTreeNode(2);\n    BinTree C = CreateTreeNode(3);\n    BinTree D = CreateTreeNode(4);\n    BinTree E = CreateTreeNode(5);\n    BinTree F = CreateTreeNode(6);\n    BinTree G = CreateTreeNode(7);\n    BinTree H = CreateTreeNode(8);\n    BinTree I = CreateTreeNode(9);\n    BinTree J = CreateTreeNode(10);\n    BinTree K = CreateTreeNode(11);\n    ConnectNode(A, B, C);\n    ConnectNode(B, D, E);\n    ConnectNode(D, H, NULL);\n    ConnectNode(E, NULL, I);\n    ConnectNode(C, F, G);\n    ConnectNode(F, J, K);\n    return A;\n}\n\nBinTree BinaryTree = NULL;\nint main(int argc, char const *argv[]) {\n    printf(\"\\n \\\n    Binary Tree Options\\n \\\n    (1).make new linked Binary Tree;\\n \\\n    (2).递归先序遍历;\\n \\\n    (3).递归中序遍历;\\n \\\n    (4).递归后序遍历;\\n \\\n    (5).循环先序遍历;\\n \\\n    (6).循环中序遍历;\\n \\\n    (7).循环后序遍历;\\n \\\n    (8).打印叶节点;\\n \\\n    (9).输出树高度;\\n \\\n    (a).层序遍历;\\n \\\n    (q).quit;\\n \\\n    \");\n    while (1) {\n        switch (getchar()) {\n        case '1':\n            BinaryTree = CreateBinTree();\n            printf(\"create success!\\n\");\n            break;\n        case '2':\n            printf(\"递归先序遍历:\");\n            PreOrderTraversal(BinaryTree);\n            printf(\"\\n\");\n            break;\n        case '3':\n            printf(\"递归中序遍历:\");\n            InOrderTraversal(BinaryTree);\n            printf(\"\\n\");\n            break;\n        case '4':\n            printf(\"递归后序遍历:\");\n            PostOrderTraversal(BinaryTree);\n            printf(\"\\n\");\n            break;\n        case '5':\n            printf(\"循环先序遍历:\");\n            PreStackTraversal(BinaryTree);\n            printf(\"\\n\\n\");\n            break;\n        case '6':\n            printf(\"循环中序遍历:\");\n            InStackTraversal(BinaryTree);\n            printf(\"\\n\\n\");\n            break;\n        case '7':\n            printf(\"循环后序遍历:\");\n            PostStackTraversal(BinaryTree);\n            printf(\"\\n\\n\");\n            break;\n        case '8':\n            printf(\"打印叶节点:\");\n            PreOrderPrintLeaves(BinaryTree);\n            printf(\"\\n\\n\");\n            break;\n        case '9':\n            printf(\"打印height:%d\", FindTreeHeight(BinaryTree));\n            printf(\"\\n\\n\");\n            break;\n        case 'a':\n            printf(\"层序遍历:\");\n            LevelOrderTraversal(BinaryTree);\n            printf(\"\\n\");\n            break;\n        case 'q':\n            exit(0);\n            break;\n        default:\n            break;\n        }\n    }\n    return 0;\n}\n\n\n执行结果\n☁  binarytree  cat 1.txt | ./a.out\n\n     Binary Tree Options\n     (1).make new linked Binary Tree;\n     (2).递归先序遍历;\n     (3).递归中序遍历;\n     (4).递归后序遍历;\n     (5).循环先序遍历;\n     (6).循环中序遍历;\n     (7).循环后序遍历;\n     (8).打印叶节点;\n     (9).输出树高度;\n     (a).层序遍历;\n     (q).quit;\n     Create Binary Tree like this:\n                  1                           A\n             /          \\                /          \\\n            2            3              B            C\n          /   \\        /   \\          /   \\        /   \\\n        4      5      6     7       D      E      F     G\n       /        \\    / \\           /        \\    / \\\n      8          9  10  11        H          I  J   K\n     create success!\n递归先序遍历:1   2   4   8   5   9   3   6   10   11   7\n循环先序遍历:1   2   4   8   5   9   3   6   10   11   7\n\n递归中序遍历:2   4   8   5   9   1   3   6   10   11   7\n循环中序遍历:8   4   2   5   9   1   10   6   11   3   7\n\n递归后序遍历:2   4   8   5   9   3   6   10   11   7   1\n循环后序遍历:8   4   2   5   9   10   6   11   3   7   1\n\n打印叶节点:8   5   9   10   11   7\n\n打印height:4\n\n层序遍历:1   2   3   4   5   6   7   8   9   10   11"
  },
  {
    "objectID": "posts/5-3.html",
    "href": "posts/5-3.html",
    "title": "理解原子操作",
    "section": "",
    "text": "编写一个程序，最多可接收3个命令行参数：\n\n$ ./5-3 filename num-bytes [-x]\n该程序打开指定的文件，然后每次写入一个字节的方式，向尾部追加num-bytes字节。缺省情况下，打开文件的标志应有O_APPEND，但若存在第三个命令行参数，那么就使用lseek到文件末尾再进行写入。最后运行这两个命令查看结果：\n./5-3 f1 1000000  & ./5-3 f1 1000000 \n和\n./5-3 f1 1000000 -x & ./5-3 f1 1000000 -x\n\n\n#include &lt;ctype.h&gt;\n#include \"tlpi_hdr.h\"\n#include &lt;stdbool.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\nint main(int argc, char *argv[])\n{\n    char ch;\n    char byte='a';\n    bool x = FALSE;\n    int fd=-1,cnt;\n    /* 读取选项 */\n    if (argc &lt; 3)\n    {\n        usageErr(\"filename num-bytes [-x]\\n\");\n        exit(EXIT_SUCCESS);\n    }\n    while ((ch = getopt(argc, argv, \"x\")) != -1)\n    {\n        switch (ch)\n        {\n        case 'x':\n            printf(\"have x\\n\");\n            x = TRUE;\n            break;\n        default:\n            usageErr(\"filename num-bytes [-x]\\n\");\n            break;\n        }\n    }\n    \n    if (x)//有x的情况\n    {\n        //当判断到了-x后，参数会被重新排列\n        cnt=atoi(argv[3]);\n        byte='x';\n        //改成lseek来调整\n        fd=open(argv[2],O_RDWR | O_CREAT , S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);\n        if(fd==-1)\n        {\n            errExit(\"open\");\n        }\n        while(cnt)\n        {\n            lseek(fd,0,SEEK_END);\n            write(fd,&byte,1);\n            cnt--;\n        }   \n    }else\n    {   \n        cnt=atoi(argv[2]);\n        //只写 追加 证明了O_APPEND是原子操作，使用时不会受多线程的影响\n        fd=open(argv[1],O_RDWR | O_CREAT | O_APPEND, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);\n        if(fd==-1)\n        {\n            errExit(\"open\");\n        }\n        while(cnt)\n        {\n             write(fd,&byte,1);\n            cnt--;\n        }\n    }\n\n    exit(EXIT_SUCCESS);\n}\n\n\n\n\n不带-x参数 sh     $ ./5-3 f1 1000000 & ./5-3 f1 1000000      $ ls -lh f1     -rw-rw-r-- 1 zqh zqh 2.0M 5月   9 10:38 f1\n带-x参数\n$ ./5-3 f1 1000000 -x & ./5-3 f1 1000000 -x\n$ ls -lh f1\n-rw-rw-r-- 1 zqh zqh 1010K 5月   9 10:46 f1\n总结\n 使用O_APPEND参数打开的文件属于原子操作，两个不同的进程对一个文件做输入，是不可分割的操作  使用lseek操作，不属于原子操作。\n\n\n\n\n 我尝试修改了这个程序名为5-3.1，并且查看结果 | | 原程序 | 新程序 | |– |———-|———-| |无 -x |写入a |写入b | |有 -x |写入x |写入y |\n\n不带-x参数"
  },
  {
    "objectID": "posts/5-3.html#题目",
    "href": "posts/5-3.html#题目",
    "title": "理解原子操作",
    "section": "",
    "text": "编写一个程序，最多可接收3个命令行参数：\n\n$ ./5-3 filename num-bytes [-x]\n该程序打开指定的文件，然后每次写入一个字节的方式，向尾部追加num-bytes字节。缺省情况下，打开文件的标志应有O_APPEND，但若存在第三个命令行参数，那么就使用lseek到文件末尾再进行写入。最后运行这两个命令查看结果：\n./5-3 f1 1000000  & ./5-3 f1 1000000 \n和\n./5-3 f1 1000000 -x & ./5-3 f1 1000000 -x\n\n\n#include &lt;ctype.h&gt;\n#include \"tlpi_hdr.h\"\n#include &lt;stdbool.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\nint main(int argc, char *argv[])\n{\n    char ch;\n    char byte='a';\n    bool x = FALSE;\n    int fd=-1,cnt;\n    /* 读取选项 */\n    if (argc &lt; 3)\n    {\n        usageErr(\"filename num-bytes [-x]\\n\");\n        exit(EXIT_SUCCESS);\n    }\n    while ((ch = getopt(argc, argv, \"x\")) != -1)\n    {\n        switch (ch)\n        {\n        case 'x':\n            printf(\"have x\\n\");\n            x = TRUE;\n            break;\n        default:\n            usageErr(\"filename num-bytes [-x]\\n\");\n            break;\n        }\n    }\n    \n    if (x)//有x的情况\n    {\n        //当判断到了-x后，参数会被重新排列\n        cnt=atoi(argv[3]);\n        byte='x';\n        //改成lseek来调整\n        fd=open(argv[2],O_RDWR | O_CREAT , S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);\n        if(fd==-1)\n        {\n            errExit(\"open\");\n        }\n        while(cnt)\n        {\n            lseek(fd,0,SEEK_END);\n            write(fd,&byte,1);\n            cnt--;\n        }   \n    }else\n    {   \n        cnt=atoi(argv[2]);\n        //只写 追加 证明了O_APPEND是原子操作，使用时不会受多线程的影响\n        fd=open(argv[1],O_RDWR | O_CREAT | O_APPEND, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);\n        if(fd==-1)\n        {\n            errExit(\"open\");\n        }\n        while(cnt)\n        {\n             write(fd,&byte,1);\n            cnt--;\n        }\n    }\n\n    exit(EXIT_SUCCESS);\n}\n\n\n\n\n不带-x参数 sh     $ ./5-3 f1 1000000 & ./5-3 f1 1000000      $ ls -lh f1     -rw-rw-r-- 1 zqh zqh 2.0M 5月   9 10:38 f1\n带-x参数\n$ ./5-3 f1 1000000 -x & ./5-3 f1 1000000 -x\n$ ls -lh f1\n-rw-rw-r-- 1 zqh zqh 1010K 5月   9 10:46 f1\n总结\n 使用O_APPEND参数打开的文件属于原子操作，两个不同的进程对一个文件做输入，是不可分割的操作  使用lseek操作，不属于原子操作。\n\n\n\n\n 我尝试修改了这个程序名为5-3.1，并且查看结果 | | 原程序 | 新程序 | |– |———-|———-| |无 -x |写入a |写入b | |有 -x |写入x |写入y |\n\n不带-x参数"
  },
  {
    "objectID": "posts/4-1.html",
    "href": "posts/4-1.html",
    "title": "自己实现的tee命令",
    "section": "",
    "text": "tee命令是从标准输入中读取数据，直至文件结尾，随后将数据写入标准输出和命令行参数所指定的文件。请使用I/O系统调用实现tee命令。并实现-a选项。"
  },
  {
    "objectID": "posts/4-1.html#题目",
    "href": "posts/4-1.html#题目",
    "title": "自己实现的tee命令",
    "section": "",
    "text": "tee命令是从标准输入中读取数据，直至文件结尾，随后将数据写入标准输出和命令行参数所指定的文件。请使用I/O系统调用实现tee命令。并实现-a选项。"
  },
  {
    "objectID": "posts/4-1.html#代码",
    "href": "posts/4-1.html#代码",
    "title": "自己实现的tee命令",
    "section": "代码",
    "text": "代码\n\n#include &lt;ctype.h&gt;\n#include \"tlpi_hdr.h\"\n#include &lt;stdbool.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\nint main(int argc, char *argv[])\n{\n    char ch;\n    bool append = FALSE;\n    char buf[1024] = {0};\n    char end = EOF;\n    int outfilefd, ret;\n    ssize_t len;\n    /* 读取选项 */\n    while ((ch = getopt(argc, argv, \"a\")) != -1)\n    {\n        switch (ch)\n        {\n        case 'a':\n            // printf(\"option append\\n\");\n            append = TRUE; //在文件后面追加数据\n            break;\n        default:\n            usageErr(\" [-a] filename\\n\");\n            break;\n        }\n    }\n    if (append && argc == 3) //追加模式且有操作文件\n    {\n        /* !!!!!使用了O_APPEND和O_TRUNC时，默认还是从头开始，并且使用lseek也没有用处 */\n        outfilefd = open(argv[2], O_RDWR | O_CREAT | O_APPEND , S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH); //读写输出文件\n        if (outfilefd == -1)\n            errExit(\"open faild\\n\");\n        while (1)\n        {\n            len = read(STDIN_FILENO, buf, 1024);\n            if (len != -1)\n            {              \n                if (len == 0)\n                {\n                    close(outfilefd);\n                    exit(EXIT_SUCCESS);\n                }\n                write(STDOUT_FILENO, buf,len);\n                write(outfilefd, buf, len);\n \n                memset(buf, 0, 1024);\n            }\n        }\n    }\n    if (append && argc == 2) //追加模式但无操作文件\n    {\n        while (1)\n        {\n            len = read(STDIN_FILENO, buf, 1024);\n            if (len != -1)\n            {                \n                if (len == 0)\n                {\n                    exit(EXIT_SUCCESS);\n                }\n                write(STDOUT_FILENO, buf, len);\n                memset(buf, 0, 1024);\n            }\n        }\n    }\n    if (!append && argc == 2) //非追加模式且有操作文件\n    {\n        outfilefd = open(argv[1], O_WRONLY | O_CREAT | O_TRUNC, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH); //读写输出文件\n        if (outfilefd == -1)\n            errExit(\"open file\\n\");\n        while (1)\n        {\n            len = read(STDIN_FILENO, buf, 1024);\n            if (len != -1)\n            {\n                if (len == 0)\n                {\n                    close(outfilefd);\n                    exit(EXIT_SUCCESS);\n                }\n                write(STDOUT_FILENO, buf, len);\n                write(outfilefd, buf, len);\n                memset(buf, 0, 1024);\n            }\n        }\n    }\n    if (!append && argc == 1) //非追加模式 无操作文件\n    {\n        while (1)\n        {\n            len = read(STDIN_FILENO, buf, 1024);\n            if (len != -1)\n            {\n                if (len == 0)\n                {\n                    exit(EXIT_SUCCESS);\n                }\n                write(STDOUT_FILENO, buf, len);\n                memset(buf, 0, 1024);\n            }\n        }\n    }\n    exit(EXIT_SUCCESS);\n}"
  },
  {
    "objectID": "posts/4-1.html#总结",
    "href": "posts/4-1.html#总结",
    "title": "自己实现的tee命令",
    "section": "总结",
    "text": "总结\n\n\n打开文件的句柄不能弄错。  我之前的出错代码如下：\nret = open(argv[1], O_WRONLY | O_CREAT | O_TRUNC, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH); //读写输出文件\nif (ret == -1)\n    errExit(\"open file\\n\");\n.\n.\n.\n.\n.\nwrite(outfilefd, buf, len);\n习惯性的用ret来表示返回值，但是这里的返回值是文件句柄。\nO_APPEND和O_TRUNC不能一起使用  至少在当你想在追加的时候不能使用，当使用这截断打开，此时不仅追加没用用，lseek(fd,0,SEEK_END)也会没有用。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zheng's Notes",
    "section": "",
    "text": "Vibe Coding 使用经验\n\n\n\nVibe Coding\n\n\n\n\n\n\n\n\n\nFeb 25, 2026\n\n\nClaude Code,Copilot,踩坑经验\n\n\n\n\n\n\n\nAxe: A Simple Unified Layout Abstraction for Machine Learning Compilers\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nFeb 8, 2026\n\n\nLayout\n\n\n\n\n\n\n\nCute概念速通\n\n\n\n推理框架\n\n\n\n\n\n\n\n\n\nFeb 3, 2026\n\n\nLayout\n\n\n\n\n\n\n\n探究jax reshard优化\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nOct 15, 2025\n\n\n分布式\n\n\n\n\n\n\n\nFlash Attention记录\n\n\n\n推理框架\n\n\n\n\n\n\n\n\n\nSep 26, 2025\n\n\n算子\n\n\n\n\n\n\n\nChimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nAug 28, 2025\n\n\n后端优化,性能建模\n\n\n\n\n\n\n\n推理框架调研\n\n\n\n推理框架\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nvllm\n\n\n\n\n\n\n\nDISTAL: The Distributed Tensor Algebra Compiler\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\n多面体模型,DSL\n\n\n\n\n\n\n\ntriton-cpu初体验\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nDec 4, 2024\n\n\nTriton\n\n\n\n\n\n\n\n分布式存储架构下的矩阵乘与编译器\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nNov 7, 2024\n\n\n分布式\n\n\n\n\n\n\n\n机器学习编译概念科普\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\ntvm\n\n\n\n\n\n\n\nbenchmark的经验与技巧\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\n踩坑经验\n\n\n\n\n\n\n\nAmpl学习\n\n\n\n运筹学\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\nAmpl\n\n\n\n\n\n\n\nConstraints Solver Internals\n\n\n\n运筹学\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\nOrTools\n\n\n\n\n\n\n\nModel Driven Optimization\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\n后端优化,性能建模\n\n\n\n\n\n\n\n探索AMX: 解锁Apple Silicon隐藏性能\n\n\n\n体系结构\n\n\n\n\n\n\n\n\n\nApr 23, 2024\n\n\n指令集,Apple\n\n\n\n\n\n\n\nExplore AMX instructions: Unlock the performance of Apple Silicon\n\n\n\n体系结构\n\n\n\n\n\n\n\n\n\nApr 23, 2024\n\n\n指令集,Apple\n\n\n\n\n\n\n\nmacos中bundle的使用\n\n\n\n操作系统\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\ncmake,CPP\n\n\n\n\n\n\n\nAffine Fusion Pass浅析\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nmlir,多面体模型\n\n\n\n\n\n\n\nTileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\n多面体模型,性能建模\n\n\n\n\n\n\n\nhugging face llama使用\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nDec 26, 2023\n\n\n大语言模型,llama,踩坑经验\n\n\n\n\n\n\n\nTensor DSL总结\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nDSL,Jittor,Halide,Tiramisu\n\n\n\n\n\n\n\nMLIRSharp\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nmlir\n\n\n\n\n\n\n\ntvm dynamic shape 学习\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nTVM,动态shape\n\n\n\n\n\n\n\nmlc-llm 浅析\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nLLM,tvm\n\n\n\n\n\n\n\nAlibaba EasyDist 浅析\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\n分布式,Pytorch,中端优化\n\n\n\n\n\n\n\nTiramisu Compiler Internals\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\n多面体模型,DSL,Tiramisu\n\n\n\n\n\n\n\n基于DL的CostModel\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nCostModel,后端优化\n\n\n\n\n\n\n\nroofline Model\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\n性能建模\n\n\n\n\n\n\n\nppcg 学习\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\n多面体模型\n\n\n\n\n\n\n\nhalide metal 初体验\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nApr 9, 2023\n\n\nHalide,DSL\n\n\n\n\n\n\n\n带宽受限下的DSA后端Compute Schedule\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\n后端优化\n\n\n\n\n\n\n\nEquality Saturation优化在AI编译器中遇到的挑战\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n中端优化,Equality Saturation\n\n\n\n\n\n\n\n带宽受限下的DSA后端优化\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\n后端优化\n\n\n\n\n\n\n\nAKG 学习\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nOct 17, 2022\n\n\n多面体模型,后端优化\n\n\n\n\n\n\n\nPolyhedral Tutorials\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\n多面体模型\n\n\n\n\n\n\n\nzhihu markdown导入(2022年6月)\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nJun 30, 2022\n\n\nZhihu\n\n\n\n\n\n\n\nOptimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\n多面体模型,后端优化\n\n\n\n\n\n\n\n标量指令集编译器简易实现\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nApr 23, 2022\n\n\n指令集\n\n\n\n\n\n\n\nHalide 进阶\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nMar 19, 2022\n\n\nHalide,DSL\n\n\n\n\n\n\n\negg 浅析\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nFeb 27, 2022\n\n\n中端优化,Equality Saturation\n\n\n\n\n\n\n\nC# P/Invoke 总结\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\nCSharp,C\n\n\n\n\n\n\n\nPure Tensor Program Rewriting via Access Patterns\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n中端优化,Equality Saturation\n\n\n\n\n\n\n\nTVM TensorIR\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nDec 4, 2021\n\n\nTVM\n\n\n\n\n\n\n\n关于如何在M1上使用TorchSharp\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nNov 18, 2021\n\n\nPytorch,CSharp\n\n\n\n\n\n\n\nAutomatically Scheduling Halide Image Processing Pipelines\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nNov 14, 2021\n\n\nHalide\n\n\n\n\n\n\n\nPythonnet踩坑\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nNov 12, 2021\n\n\nPython,CSharp,踩坑经验\n\n\n\n\n\n\n\ncsharp 问题记录\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nOct 26, 2021\n\n\nCSharp\n\n\n\n\n\n\n\nC# Source Generator使用\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nOct 21, 2021\n\n\nCSharp\n\n\n\n\n\n\n\npolyhedral入门学习\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nSep 5, 2021\n\n\n多面体模型\n\n\n\n\n\n\n\n一些python中的小坑\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nAug 24, 2021\n\n\n踩坑经验,Python\n\n\n\n\n\n\n\n模板元编程实战(第一章)\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n模板元编程,CPP\n\n\n\n\n\n\n\npiecewise regression\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nJul 24, 2021\n\n\n神经网络量化\n\n\n\n\n\n\n\nnumpy中继承ndarray\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nJul 17, 2021\n\n\nPython,Numpy\n\n\n\n\n\n\n\nk210-tool-chains mac m1编译\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJul 16, 2021\n\n\nK210\n\n\n\n\n\n\n\nx86指令集使用汇总\n\n\n\n体系结构\n\n\n\n\n\n\n\n\n\nJul 5, 2021\n\n\n指令集\n\n\n\n\n\n\n\nConan使用汇总\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nJun 22, 2021\n\n\nCPP,Conan,踩坑经验\n\n\n\n\n\n\n\nHalide 入门\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\nHalide,DSL\n\n\n\n\n\n\n\nNand2Tetris week4\n\n\n\n体系结构\n\n\n\n\n\n\n\n\n\nMay 24, 2021\n\n\nNand2Tetris\n\n\n\n\n\n\n\nvscode连接远程服务器中docker\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nLinux,Vscode\n\n\n\n\n\n\n\nNand2Tetris week3\n\n\n\n体系结构\n\n\n\n\n\n\n\n\n\nMay 11, 2021\n\n\nNand2Tetris\n\n\n\n\n\n\n\nNand2Tetris week2\n\n\n\n体系结构\n\n\n\n\n\n\n\n\n\nMay 6, 2021\n\n\nNand2Tetris\n\n\n\n\n\n\n\nNand2Tetris week1\n\n\n\n体系结构\n\n\n\n\n\n\n\n\n\nMay 4, 2021\n\n\nNand2Tetris\n\n\n\n\n\n\n\nncnn学习\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\nncnn\n\n\n\n\n\n\n\ncpp挖坑&爬坑\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nApr 25, 2021\n\n\nCPP,踩坑经验\n\n\n\n\n\n\n\ncmake踩坑&爬坑\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nApr 25, 2021\n\n\ncmake,CPP,踩坑经验\n\n\n\n\n\n\n\n二分查找-统一框架\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\n二分法\n\n\n\n\n\n\n\n神经网络量化-基本原理\n\n\n\n编译器\n\n\n\n\n\n\n\n\n\nMar 27, 2021\n\n\n神经网络量化\n\n\n\n\n\n\n\nleetcode刷题总结\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJan 21, 2021\n\n\nLeetCode\n\n\n\n\n\n\n\nParameter-Free Style Projection for Arbitrary Style Transfer\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\nGAN\n\n\n\n\n\n\n\npytorch从任意层截断并提取数据\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nNov 14, 2020\n\n\nPytorch\n\n\n\n\n\n\n\nPytorch Webdataset初体验\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nNov 12, 2020\n\n\nPytorch\n\n\n\n\n\n\n\nDesign GAN\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nNov 9, 2020\n\n\nGAN\n\n\n\n\n\n\n\npytorch-lighting隐藏的坑\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nOct 16, 2020\n\n\nPytorch,踩坑经验\n\n\n\n\n\n\n\nWhite-box GAN\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nOct 9, 2020\n\n\nGAN\n\n\n\n\n\n\n\nModel-Agnostic Meta-Learning\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nOct 3, 2020\n\n\n元学习\n\n\n\n\n\n\n\nmindspore vs tensorflow\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nSep 21, 2020\n\n\nTensorflow,mindspore\n\n\n\n\n\n\n\n关于mindspore\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nSep 19, 2020\n\n\nmindspore\n\n\n\n\n\n\n\nUncertainty Weight Loss\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\nTensorflow,概率论\n\n\n\n\n\n\n\n基于Qt的抠图工具\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\nPython,Qt\n\n\n\n\n\n\n\n8月总结&淋巴结发炎预防与治疗\n\n\n\n生活\n\n\n\n\n\n\n\n\n\nAug 29, 2020\n\n\n\n\n\n\n\n统计学习方法:PageRank\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nAug 10, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:潜在狄利克雷分配模型\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nAug 3, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:马尔科夫链蒙特卡洛法\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJul 28, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:概率潜在语义模型\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJul 21, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\ntensorflow与pytorch代码差异\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJul 5, 2020\n\n\nTensorflow,Pytorch\n\n\n\n\n\n\n\n半监督学习：RealMix与EnAET\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJun 19, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\nProxy Anchor Loss for Deep Metric Learning论文解读\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJun 17, 2020\n\n\nTensorflow\n\n\n\n\n\n\n\n统计学习方法:聚类方法\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJun 16, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:条件随机场\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJun 13, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:隐马尔可夫模型\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJun 10, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\nU-GAT-IT论文解读\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJun 7, 2020\n\n\nGAN\n\n\n\n\n\n\n\n统计学习方法:EM算法\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJun 6, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:提升方法\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJun 5, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:支持向量机\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJun 1, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:逻辑回归\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMay 29, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:决策树\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMay 27, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:朴素贝叶斯\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMay 23, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:KNN\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMay 23, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\n统计学习方法:感知机\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMay 19, 2020\n\n\n统计学习方法,概率论\n\n\n\n\n\n\n\nOpenPose人体姿态估计\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMay 16, 2020\n\n\n姿态估计,Tensorflow\n\n\n\n\n\n\n\ntf.keras.Model.outputs隐藏问题\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMay 15, 2020\n\n\nTensorflow,Keras\n\n\n\n\n\n\n\nvscode Python Docstring Generator\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\nVscode,Python\n\n\n\n\n\n\n\nPython格式化配置\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nApr 27, 2020\n\n\nPython\n\n\n\n\n\n\n\nUbuntu配置clash\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nApr 23, 2020\n\n\nLinux,科学上网\n\n\n\n\n\n\n\nAnime GAN\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\nTensorflow,GAN\n\n\n\n\n\n\n\nCircle Loss\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nApr 6, 2020\n\n\nTensorflow,损失函数\n\n\n\n\n\n\n\ntf.keras实现Spectral Normalization\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nApr 6, 2020\n\n\nTensorflow,Keras,归一化\n\n\n\n\n\n\n\ninfomax中一些错误总结\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMar 30, 2020\n\n\n踩坑经验,半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：SimCLR\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMar 28, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\nH5模型转pb模型\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMar 17, 2020\n\n\nTensorflow,Keras\n\n\n\n\n\n\n\nControl Theory Augment\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMar 7, 2020\n\n\nTensorflow,数据增强\n\n\n\n\n\n\n\nAugmentation For Mel Spectrogram\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 29, 2020\n\n\nTensorflow,声音信号处理\n\n\n\n\n\n\n\nmxnet模型转tflite\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 24, 2020\n\n\nTensorflow,mxnet\n\n\n\n\n\n\n\nbce focal loss\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 16, 2020\n\n\nTensorflow\n\n\n\n\n\n\n\ntf2.0 全局添加regularizers\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 12, 2020\n\n\nTensorflow\n\n\n\n\n\n\n\n半监督学习：FixMatch\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 6, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：ReMixMatch\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 5, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：Unsupervised Data Augmentation\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：MixMatch\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 3, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：Interpolation Consistency Training\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 2, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：mixup\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 1, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：Virtual Adversarial Training\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 31, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：mean teacher\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 30, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：Π model\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 29, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\n半监督学习：pseudo label\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 29, 2020\n\n\n半监督学习,Tensorflow\n\n\n\n\n\n\n\ntensorflow实现各种iou\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 25, 2020\n\n\nTensorflow,目标检测,Yolo\n\n\n\n\n\n\n\ntensorflow人脸识别\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 14, 2020\n\n\nTensorflow,人脸识别\n\n\n\n\n\n\n\ntf2.0 自定义Model高级用法\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 7, 2020\n\n\nTensorflow\n\n\n\n\n\n\n\nPFLD总结\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nDec 21, 2019\n\n\nTensorflow,人脸检测\n\n\n\n\n\n\n\nretinaface总结\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nDec 19, 2019\n\n\nRetinaface,Tensorflow,目标检测\n\n\n\n\n\n\n\ntf2.0得到子boolmask在boolmask中的索引\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nDec 13, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\ntf2.0数组索引与赋值\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nDec 3, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\ntf.keras实现动态多尺度训练\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nNov 13, 2019\n\n\nTensorflow,Yolo,Keras\n\n\n\n\n\n\n\n测试tf.keras中callback的运行状态\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nNov 13, 2019\n\n\nTensorflow,Yolo,Keras\n\n\n\n\n\n\n\nskimage中resize内存泄漏\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nNov 1, 2019\n\n\n踩坑经验\n\n\n\n\n\n\n\nTensorflow 1.15中TensorBoard错误\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nOct 21, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\nLookahead优化器的tf.Keras实现\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nOct 20, 2019\n\n\nTensorflow,优化器\n\n\n\n\n\n\n\nCross Entropy的数值稳定计算\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nSep 22, 2019\n\n\nTensorflow,损失函数\n\n\n\n\n\n\n\ntf.keras损失函数聚合测试\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nSep 22, 2019\n\n\nTensorflow,Keras\n\n\n\n\n\n\n\n配置CenterNet环境\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nAug 29, 2019\n\n\nPytorch,目标检测\n\n\n\n\n\n\n\ntf.keras中优化metric计算(提取loss至metric)\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nAug 28, 2019\n\n\nTensorflow,Keras\n\n\n\n\n\n\n\nUbuntu多cuda版本控制\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nAug 28, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\n条件VAE\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nAug 18, 2019\n\n\nTensorflow,VAE\n\n\n\n\n\n\n\nTensorflow中的错误记录\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nAug 12, 2019\n\n\nTensorflow,Keras,踩坑经验\n\n\n\n\n\n\n\n人脸识别方法总结\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nAug 1, 2019\n\n\n人脸识别\n\n\n\n\n\n\n\n互信息：无监督提取特征\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJul 31, 2019\n\n\n无监督学习\n\n\n\n\n\n\n\n树莓派修改配置使能串口登陆\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJul 30, 2019\n\n\nLinux,树莓派\n\n\n\n\n\n\n\n变分自编码器(VAE) 直观推导\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJul 29, 2019\n\n\nVAE,概率论\n\n\n\n\n\n\n\n概率模型第四章 ： 大数定理\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJul 28, 2019\n\n\n概率论,Tensorflow\n\n\n\n\n\n\n\n概率模型第三章 ： MCMC\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJul 27, 2019\n\n\n概率论,Tensorflow\n\n\n\n\n\n\n\n概率模型第二章 ： A little more on TFP\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJul 26, 2019\n\n\n概率论,Tensorflow\n\n\n\n\n\n\n\n设置路由器使用LAN口\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nJul 25, 2019\n\n\n树莓派,Vscode\n\n\n\n\n\n\n\nEM算法与EM路由\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJul 11, 2019\n\n\n聚类方法\n\n\n\n\n\n\n\n实现yolo时踩过的坑！\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJul 10, 2019\n\n\nTensorflow,Yolo,目标检测\n\n\n\n\n\n\n\n比较kmdoel和tflite推理输出\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJul 6, 2019\n\n\nTensorflow,K210\n\n\n\n\n\n\n\ntf.keras自定义loss报错shape mismatch\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nTensorflow,踩坑经验\n\n\n\n\n\n\n\ntf.dataset无法推断shape导致错误\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJun 10, 2019\n\n\nTensorflow,踩坑经验,Keras\n\n\n\n\n\n\n\ntf.keras多输出模型自定义loss\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJun 9, 2019\n\n\nTensorflow,Keras\n\n\n\n\n\n\n\nnumpy中动态范围切片\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nJun 8, 2019\n\n\nPython,Numpy\n\n\n\n\n\n\n\nL softmx -&gt; A softmx -&gt; AM softmax\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJun 3, 2019\n\n\n损失函数,Tensorflow\n\n\n\n\n\n\n\ntf.keras中分析性能\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMay 23, 2019\n\n\nTensorflow,Keras\n\n\n\n\n\n\n\ntf.Keras完美使用tf.data API\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMay 21, 2019\n\n\nTensorflow,Keras\n\n\n\n\n\n\n\n变分自编码器(VAE)学习\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMay 13, 2019\n\n\nVAE,概率论\n\n\n\n\n\n\n\npython返回值进行unpack\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nMay 9, 2019\n\n\nPython\n\n\n\n\n\n\n\nTensorflow 2.0中使用global steps\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMay 6, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\npython实现stft\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nApr 24, 2019\n\n\n声音信号处理,Python\n\n\n\n\n\n\n\nk210中使用core1\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nApr 24, 2019\n\n\nK210,踩坑经验\n\n\n\n\n\n\n\nCapsNet实现以及踩坑\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nApr 22, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\n对比tensordot、matmul、einsum速度\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nApr 20, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\n图像缩放-双线性插值\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nApr 17, 2019\n\n\n图像处理,K210\n\n\n\n\n\n\n\n声音信号处理-STFT变化\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nApr 12, 2019\n\n\n声音信号处理\n\n\n\n\n\n\n\nMakefile中使用shell赋值变量\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nApr 6, 2019\n\n\n踩坑经验,Makefile\n\n\n\n\n\n\n\ncpp模板编译踩坑记\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nApr 1, 2019\n\n\n踩坑经验,CPP\n\n\n\n\n\n\n\n声音信号处理-FFT性质\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMar 27, 2019\n\n\n声音信号处理\n\n\n\n\n\n\n\n声音信号处理-DTF与DTFT\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMar 25, 2019\n\n\n声音信号处理\n\n\n\n\n\n\n\n安装conda之后pip执行全局的问题\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nMar 21, 2019\n\n\n踩坑经验,Python\n\n\n\n\n\n\n\n解决Ubuntu下nvidia驱动导致画面撕裂\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nMar 20, 2019\n\n\n踩坑经验\n\n\n\n\n\n\n\n重新编译Tensorflow\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMar 14, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\nyolo中anchor值的解释\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMar 12, 2019\n\n\nTensorflow,Yolo,目标检测\n\n\n\n\n\n\n\nYolo中loss函数分析\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMar 8, 2019\n\n\nYolo,Tensorflow,目标检测\n\n\n\n\n\n\n\n使用c通过http发送文件\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nMar 8, 2019\n\n\nC,Linux\n\n\n\n\n\n\n\nRaspi蓝牙:编程实现录音\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nMar 4, 2019\n\n\n蓝牙,Linux,树莓派\n\n\n\n\n\n\n\nyolo中precision降低的原因分析\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nMar 3, 2019\n\n\n踩坑经验,Tensorflow,Yolo\n\n\n\n\n\n\n\npulseaudio交叉编译\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nMar 1, 2019\n\n\n蓝牙,树莓派\n\n\n\n\n\n\n\nRaspi蓝牙:播放与录音\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nFeb 27, 2019\n\n\n蓝牙,树莓派,Linux\n\n\n\n\n\n\n\n使用argparse解析Bool型的坑\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nFeb 24, 2019\n\n\nPython,踩坑经验\n\n\n\n\n\n\n\n解决目标检测任务测试集recall率低\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 23, 2019\n\n\nTensorflow,踩坑经验,目标检测\n\n\n\n\n\n\n\nk210图片测试\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nFeb 14, 2019\n\n\nK210\n\n\n\n\n\n\n\nTensorflow加载pb推理输出不正确\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nFeb 7, 2019\n\n\nTensorflow,踩坑经验\n\n\n\n\n\n\n\n使k210支持Tensorflow卷积\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJan 30, 2019\n\n\nK210,Tensorflow\n\n\n\n\n\n\n\nTensorflow加载pb文件继续训练\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 28, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\nTensorflow中动态学习率无效\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 27, 2019\n\n\n踩坑经验,Tensorflow\n\n\n\n\n\n\n\ntqdm中后缀的添加\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 24, 2019\n\n\nTensorflow,Python\n\n\n\n\n\n\n\nmobilenet测试\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nJan 20, 2019\n\n\nTensorflow\n\n\n\n\n\n\n\npython 多维数组指定区域中寻找元素索引\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nJan 6, 2019\n\n\nPython\n\n\n\n\n\n\n\nLeNet-5\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nDec 24, 2018\n\n\nTensorflow\n\n\n\n\n\n\n\n机器学习作业第八周\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nDec 17, 2018\n\n\n吴恩达课程\n\n\n\n\n\n\n\n机器学习作业第七周\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nDec 15, 2018\n\n\n吴恩达课程\n\n\n\n\n\n\n\n机器学习作业第六周\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nDec 13, 2018\n\n\n吴恩达课程\n\n\n\n\n\n\n\n代码块自动添加折叠\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nDec 11, 2018\n\n\nPython\n\n\n\n\n\n\n\nBP神经网络回归\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nDec 11, 2018\n\n\nPython\n\n\n\n\n\n\n\n机器学习作业第五周\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nDec 9, 2018\n\n\n吴恩达课程\n\n\n\n\n\n\n\n机器学习作业第四周\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nDec 8, 2018\n\n\n吴恩达课程\n\n\n\n\n\n\n\n机器学习作业第三周\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nDec 4, 2018\n\n\n吴恩达课程\n\n\n\n\n\n\n\nnumpy切片中的坑\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nDec 2, 2018\n\n\nPython,踩坑经验,Numpy\n\n\n\n\n\n\n\nntfs文件损坏\n\n\n\n操作系统\n\n\n\n\n\n\n\n\n\nDec 2, 2018\n\n\nLinux,踩坑经验\n\n\n\n\n\n\n\nlibmpfr错误\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\nK210,踩坑经验\n\n\n\n\n\n\n\n机器学习作业第二周\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nNov 29, 2018\n\n\n吴恩达课程\n\n\n\n\n\n\n\nfcitx配置\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nNov 26, 2018\n\n\n踩坑经验\n\n\n\n\n\n\n\n机器学习作业第一周\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n吴恩达课程\n\n\n\n\n\n\n\n最大最小聚类\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nNov 24, 2018\n\n\n聚类方法,Python\n\n\n\n\n\n\n\n常大宿舍移动网有线上网\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nNov 24, 2018\n\n\nLinux,科学上网\n\n\n\n\n\n\n\nkmeans 实现\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nNov 22, 2018\n\n\n聚类方法\n\n\n\n\n\n\n\nPython神经网络编程\n\n\n\n深度学习\n\n\n\n\n\n\n\n\n\nNov 21, 2018\n\n\nPython\n\n\n\n\n\n\n\nWordCloud库使用\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nNov 20, 2018\n\n\nPython,WordCloud\n\n\n\n\n\n\n\n自己实现的进度条库\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nNov 16, 2018\n\n\nC,Linux\n\n\n\n\n\n\n\n蒙特卡洛法\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n概率论\n\n\n\n\n\n\n\nLinux下实现\n\n\n\n操作系统\n\n\n\n\n\n\n\n\n\nNov 13, 2018\n\n\nLinux,树莓派\n\n\n\n\n\n\n\nk210与cutecom冲突解决\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nNov 12, 2018\n\n\nK210,踩坑经验\n\n\n\n\n\n\n\nk210_双核测试\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nNov 11, 2018\n\n\nK210\n\n\n\n\n\n\n\nsom算法\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nNov 7, 2018\n\n\n聚类方法\n\n\n\n\n\n\n\n树莓派NB-IOT使用\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nNov 4, 2018\n\n\n树莓派,NB-IOT\n\n\n\n\n\n\n\nk210 高速gpio与中断\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nNov 2, 2018\n\n\nK210\n\n\n\n\n\n\n\nk210_GPIO使用\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nNov 2, 2018\n\n\nK210\n\n\n\n\n\n\n\nk210环境搭建_Windows\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nNov 1, 2018\n\n\nK210\n\n\n\n\n\n\n\n回溯法\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nNov 1, 2018\n\n\nLeetCode\n\n\n\n\n\n\n\nk210环境搭建_Linux\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nOct 30, 2018\n\n\nK210,Linux\n\n\n\n\n\n\n\nDBSCAN算法原理及实现\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nOct 30, 2018\n\n\n聚类方法\n\n\n\n\n\n\n\naggressicecows\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nSep 27, 2018\n\n\nLeetCode\n\n\n\n\n\n\n\nBoolean Expressions\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nSep 25, 2018\n\n\nLeetCode\n\n\n\n\n\n\n\nc++成员函数中static变量\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nSep 17, 2018\n\n\nCPP,踩坑经验\n\n\n\n\n\n\n\n字典序\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nSep 10, 2018\n\n\nC\n\n\n\n\n\n\n\nC语义转换\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nSep 3, 2018\n\n\nC\n\n\n\n\n\n\n\n排序算法小集\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nSep 1, 2018\n\n\n排序\n\n\n\n\n\n\n\n哈夫曼树\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nAug 23, 2018\n\n\n树\n\n\n\n\n\n\n\nstm32使用静态库\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nAug 16, 2018\n\n\nstm32\n\n\n\n\n\n\n\n堆\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nAug 14, 2018\n\n\n堆栈\n\n\n\n\n\n\n\ncmake构建stm32工程\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nAug 14, 2018\n\n\nLinux,stm32\n\n\n\n\n\n\n\nlinux stm32 开发\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nAug 13, 2018\n\n\nLinux,stm32\n\n\n\n\n\n\n\nc与c++字符串赋值\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nAug 6, 2018\n\n\nC,CPP,踩坑经验\n\n\n\n\n\n\n\nOrangpi使用ads1118\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nAug 4, 2018\n\n\nLinux,香橙派,踩坑经验\n\n\n\n\n\n\n\nOrangPi开启spi-dev\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJul 31, 2018\n\n\nLinux,香橙派,踩坑经验\n\n\n\n\n\n\n\n平衡二叉树\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJul 31, 2018\n\n\n树\n\n\n\n\n\n\n\ni2c之总结\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJul 25, 2018\n\n\nstm32\n\n\n\n\n\n\n\nOLED错误修复\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJul 18, 2018\n\n\nPCB,踩坑经验\n\n\n\n\n\n\n\n二叉搜索树\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJul 14, 2018\n\n\n树\n\n\n\n\n\n\n\n二叉树\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJul 10, 2018\n\n\n树\n\n\n\n\n\n\n\n多项式相加\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJul 9, 2018\n\n\n链表\n\n\n\n\n\n\n\n中缀表达式转后缀表达式\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJul 6, 2018\n\n\n堆栈\n\n\n\n\n\n\n\n栈\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJul 5, 2018\n\n\n堆栈\n\n\n\n\n\n\n\n循环链表\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJul 4, 2018\n\n\n链表\n\n\n\n\n\n\n\n单链表\n\n\n\n数据结构\n\n\n\n\n\n\n\n\n\nJul 2, 2018\n\n\n链表\n\n\n\n\n\n\n\nC数组指针\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nJun 28, 2018\n\n\nC\n\n\n\n\n\n\n\nC函数指针的使用\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nJun 27, 2018\n\n\nC\n\n\n\n\n\n\n\nOrangePI蓝牙：蓝牙耳机\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJun 22, 2018\n\n\nLinux,树莓派,蓝牙\n\n\n\n\n\n\n\nubuntu18.04安装owncloud\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nJun 20, 2018\n\n\nLinux\n\n\n\n\n\n\n\nOrangePI蓝牙：串口通信\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJun 19, 2018\n\n\nLinux,树莓派,蓝牙\n\n\n\n\n\n\n\nOrangePI蓝牙：搜索设备\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nJun 19, 2018\n\n\nLinux,树莓派,蓝牙\n\n\n\n\n\n\n\nMatlab使用ThunderSVM\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJun 8, 2018\n\n\nMatlab,SVM\n\n\n\n\n\n\n\nMatlab GA函数\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nJun 8, 2018\n\n\nMatlab,遗传算法\n\n\n\n\n\n\n\n使用git下载项目数据\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nJun 4, 2018\n\n\ngit\n\n\n\n\n\n\n\nMatlab svm使用\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMay 30, 2018\n\n\nMatlab,SVM\n\n\n\n\n\n\n\nOrangePI蓝牙：开启蓝牙\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nMay 29, 2018\n\n\nLinux,树莓派,蓝牙\n\n\n\n\n\n\n\nlinux wpa wifi自动连接\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nMay 29, 2018\n\n\nLinux,香橙派\n\n\n\n\n\n\n\n实现一个setenv函数\n\n\n\n操作系统\n\n\n\n\n\n\n\n\n\nMay 23, 2018\n\n\nLinux\n\n\n\n\n\n\n\n理解原子操作\n\n\n\n操作系统\n\n\n\n\n\n\n\n\n\nMay 19, 2018\n\n\nLinux\n\n\n\n\n\n\n\nMatlab Classification Learner\n\n\n\n机器学习\n\n\n\n\n\n\n\n\n\nMay 16, 2018\n\n\nMatlab\n\n\n\n\n\n\n\n自己实现tail命令\n\n\n\n操作系统\n\n\n\n\n\n\n\n\n\nMay 9, 2018\n\n\nLinux\n\n\n\n\n\n\n\n自己实现的tee命令\n\n\n\n操作系统\n\n\n\n\n\n\n\n\n\nMay 8, 2018\n\n\nLinux\n\n\n\n\n\n\n\n自己实现cp的程序\n\n\n\n操作系统\n\n\n\n\n\n\n\n\n\nMay 8, 2018\n\n\nLinux\n\n\n\n\n\n\n\nCMake编译静态库\n\n\n\n编程语言\n\n\n\n\n\n\n\n\n\nMay 8, 2018\n\n\ncmake,CPP,Linux\n\n\n\n\n\n\n\n资源整理\n\n\n\n工具使用\n\n\n\n\n\n\n\n\n\nMay 8, 2018\n\n\n资源汇总\n\n\n\n\n\n\n\n四轴飞行器姿态解算介绍\n\n\n\n边缘计算\n\n\n\n\n\n\n\n\n\nMay 7, 2018\n\n\n四轴飞行器\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/13-5.html",
    "href": "posts/13-5.html",
    "title": "自己实现tail命令",
    "section": "",
    "text": "这是linux系统编程手册中的题目。\n\n\n代码\n废话不多说直接上代码，虽然感觉还有个小bug。。\n#include \"tlpi_hdr.h\"\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;ctype.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;getopt.h&gt;\n\n#define BUFFSIZE 4096\nchar buf[BUFFSIZE] = {0};\nint cntline = 0;   //用于计算已经读取的行数\nint showline = 10; //默认是10\nbool over = false;\n\nint serchline(int fd, int pos)\n{\n    while (pos &gt; BUFFSIZE)\n    {\n        pos = lseek(fd, -BUFFSIZE, SEEK_END); //如果文件长度大于4096,那么移动-4096字节开始读\n        read(fd, buf, BUFFSIZE);\n        for (int i = (BUFFSIZE - 1); i &gt; 0; i--) //倒序访问\n        {\n            if (buf[i] == '\\n')\n            {\n                cntline++;\n            }\n            if (cntline &gt; showline) //如果读取到了10行\n            {\n                pos += i; //记录位置\n                over = true;\n                return pos;\n            }\n        }\n        memset(buf,0,BUFFSIZE);//要清空一下数组\n    }\n    //执行到这里说明找了4096字节都没找到showline行,或者是文件本身就没有这么长\n    lseek(fd, 0, SEEK_SET);       //那么从头开始读\n    read(fd, buf, pos);           //读postion个字节\n    for (int i = pos; i &gt; 0; i--) //倒序访问\n    {\n        if (buf[i] == '\\n')\n        {\n            cntline++;\n            // printf(\"cntline : %d postion : %d \\n\", cntline, pos);  \n        }\n        if (cntline &gt; showline) //如果读取到了showline行\n        {\n            pos = i; //记录位置\n            over = true;\n            return pos;\n        }\n    }\n    memset(buf,0,BUFFSIZE);//要清空一下数组\n    //执行到这里说明寻找结束也没有showline行\n    return -1;\n}\n\nint main(int argc, char *argv[])\n{\n    int fd = -1;\n    char ch;\n    int postion = -1;\n    int readsize=0;\n    if (argc == 1)\n    {\n        usageErr(\"[ -n num ] filename \\n to printf last num line of file\\n\");\n    }\n    while ((ch = getopt(argc, argv, \"n:num\")) != -1)\n    {\n        switch (ch)\n        {\n        case 'n':\n            showline = atoi(optarg);\n            break;\n        default:\n            usageErr(\"[ -n num ] filename \\n to printf last num line of file\\n\");\n            break;\n        }\n    }\n    if (showline &gt; 0)\n    { //如果要显示大于0的行数\n        if ((fd = open(argv[optind], O_RDONLY)) == -1)\n        {\n            errExit(\"open\");\n        }\n\n        if ((postion = lseek(fd, 0, SEEK_END)) == -1) //先得出文件总长度\n        {\n            errExit(\"lseek\");\n        }\n        // printf(\"file all length: %d\\n\", postion);\n        postion = serchline(fd, postion);\n        if (postion != -1)\n            lseek(fd, postion, SEEK_SET); //说明找到了最后showline行\n        do\n        {\n            readsize=read(fd, buf, BUFFSIZE);\n            printf(\"%s\",buf);\n            memset(buf,0,BUFFSIZE);//输出完了就清空\n        } while(readsize &gt; BUFFSIZE);//输出所有\n    }\n    else\n    {\n        usageErr(\"[ -n num ] filename \\n to printf last num line of file\\n\");\n    }\n    return 0;\n}\n\n\n总结\n通过这个题目，学会了如何在vscode里面使用gdb： vscode自带的gdb调试不好使，还是要下载插件Native Debug才好使。我使用自带的gdb调试，第一句话就报错退出。但是我用上插件就没有问题了。"
  },
  {
    "objectID": "posts/4-2.html",
    "href": "posts/4-2.html",
    "title": "自己实现cp的程序",
    "section": "",
    "text": "编写一个类似cp命令的程序，当使用该程序复制一个包含空洞的普通文件时，要求目标文件的空洞与源文件一致。\n\n\n准备工作\n\n创建一个空洞文件  首先先编写一个小程序去创建空洞文件。 代码如下：\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(void)\n{\n    int fd;\n    int ret;\n    fd=open(\"hole.txt\",O_CREAT|O_RDWR,0644);\n    if(fd==-1)\n        exit(EXIT_FAILURE);\n    write(fd,\"hello\",5);\n    ret=lseek(fd,100000,SEEK_CUR);\n    if(ret==-1)\n        exit(EXIT_FAILURE);\n    write(fd,\"world\",5);\n    close(fd);\n    exit(EXIT_SUCCESS);\n}\n编译运行 sh     $ gcc 4-2.c -o hole     $ ./hole     $ cat hole.txt     helloworld     $ du -h hole.txt     8.0K    hole.txt     $ od -c hole.txt     0000000   h   e   l   l   o  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0     0000020  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0     *     0303240  \\0  \\0  \\0  \\0  \\0   w   o   r   l   d     0303252\n程序代码\n#include &lt;ctype.h&gt;\n#include \"tlpi_hdr.h\"\n#include &lt;stdbool.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\nint main(int argc, char *argv[])\n{\nchar buf[1024];\nint input,output, ret;\nssize_t len;\n/* 读取选项 */\n\nif(argc!=3||strcmp(argv[1],\"--help\")==0)\n{\n printf(\"usage:%s cp file1 to file2 \\n\",argv[0]);\n exit(EXIT_SUCCESS);\n}\ninput=open(argv[1], O_RDONLY , S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);\nif(input==-1)\n    errExit(\"open input\");\noutput=open(argv[2], O_WRONLY | O_CREAT, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);\nif(input==-1)\n    errExit(\"open output\");\nwhile((ret=read(input,buf,1024))&gt;0)\n{\n    write(output,buf,ret);\n}\n\nexit(EXIT_SUCCESS);\n}  \n编译运行\nmake && ./4-2 hole.txt test.txt\n检查结果\n$ od -c test.txt\n0000000   h   e   l   l   o  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0\n0000020  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0\n*\n0303240  \\0  \\0  \\0  \\0  \\0   w   o   r   l   d\n0303252"
  },
  {
    "objectID": "posts/6-3.html",
    "href": "posts/6-3.html",
    "title": "实现一个setenv函数",
    "section": "",
    "text": "我发现我直接使用如下程序，可以直接设置环境变量：\n\nconst char name1[6]=\"SHELL\";\nconst char name2[8]=\"USENAME\";\nconst char vaule[4]=\"zqh\";\nputenv(\"SHELL=zqh\");\nputenv(\"USENAME=zqh\");\nprintf(\"%s\\n\", getenv(name1));\nprintf(\"%s\\n\", getenv(name2));\n结果如下：\nzqh\nzqh"
  },
  {
    "objectID": "posts/6-3.html#问题",
    "href": "posts/6-3.html#问题",
    "title": "实现一个setenv函数",
    "section": "",
    "text": "我发现我直接使用如下程序，可以直接设置环境变量：\n\nconst char name1[6]=\"SHELL\";\nconst char name2[8]=\"USENAME\";\nconst char vaule[4]=\"zqh\";\nputenv(\"SHELL=zqh\");\nputenv(\"USENAME=zqh\");\nprintf(\"%s\\n\", getenv(name1));\nprintf(\"%s\\n\", getenv(name2));\n结果如下：\nzqh\nzqh"
  },
  {
    "objectID": "posts/6-3.html#但是",
    "href": "posts/6-3.html#但是",
    "title": "实现一个setenv函数",
    "section": "但是",
    "text": "但是\n当我使用函数的时候，自己将字符串赋值进去的时候，却无法修改环境变量：\nint my_setenv(const char *name,const char *var)\n{\n    char *pbuf = NULL;\n    if (getenv(name) == NULL)\n    {\n         printf(\"now add %s\\n\",name);\n    }\n    else\n    {\n         printf(\"now modfiy %s\\n\",name);\n    }\n    pbuf = (char *)malloc(strlen(name)+1 + strlen(var)+1 + 1);\n    if (NULL == pbuf)\n    {\n        return -1;\n    }\n    sprintf(pbuf, \"%s=%s\", name, var);\n    printf(\"pbuf %s\\n\",pbuf);\n    printf(\"putenv %d\\n\",putenv(pbuf));\n    free(pbuf);\n    return 0;\n}\n\n\nint main(int argc, char const *argv[])\n{\n    const char name1[6]=\"SHELL\";\n    const char name2[8]=\"USENAME\";\n    const char vaule[4]=\"zqh\";\n    my_setenv(name1,vaule);\n    my_setenv(name2,vaule);\n    printf(\"%s\\n\", getenv(name1));\n    printf(\"%s\\n\", getenv(name2));\n    return 0;\n}\n结果如下：\nnow modfiy SHELL\npbuf SHELL=zqh\nputenv 0\nnow add USENAME\npbuf USENAME=zqh\nputenv 0\n[1]    7326 segmentation fault (core dumped)  ./6-3\n可以看出，设置变量貌似是成功了，但是当我想获取我设置的变量值时，就会出现segmentation fault的错误。这是由于我想printf一个NULL指针引发的。\n这个就非常令人难受了，我在函数内打印pbuf的值，发现他的值的确是我要输入的值，但是却还是不能正确的putenv。"
  },
  {
    "objectID": "posts/6-3.html#修复过程",
    "href": "posts/6-3.html#修复过程",
    "title": "实现一个setenv函数",
    "section": "修复过程",
    "text": "修复过程\n我现在尝试把东西放在主函数中运行，程序如下：\nint main(int argc, char const *argv[])\n{\n    const char name1[6]=\"SHELL\";\n    const char name2[8]=\"USENAME\";\n    const char vaule[4]=\"zqh\";\n    char *pbuf=NULL;\n    pbuf=(char*)malloc(strlen(name1)+strlen(vaule)+1);\n    sprintf(pbuf,\"%s=%s\",name1,vaule);\n    printf(\"%s\\n\", getenv(name1));\n    putenv(pbuf);\n    printf(\"%s\\n\", getenv(name1));\n    return 0;\n}\n结果：\n/bin/zsh\nzqh\n现在非常正常，但是为什么我放在函数中运行，就会失败呢？接下来我尝试继续使用函数。\n\n使用函数：\n现在我把他放到函数中：\nint my_setenv(const char *pName,const char *pVar)\n{\n    char *pbuf=NULL;\n    pbuf=(char*)malloc(strlen(pName)+strlen(pVar)+1);\n    sprintf(pbuf,\"%s=%s\",pName,pVar);\n    printf(\"%s\\n\", getenv(pName));\n    putenv(pbuf);\n    printf(\"%s\\n\", getenv(pName));\n}\n\nint main(int argc, char const *argv[])\n{\n    const char name1[6]=\"SHELL\";\n    const char name2[8]=\"USENAME\";\n    const char vaule[4]=\"zqh\";\n    my_setenv(name1,vaule);\n    return 0;\n}\n运行结果：\n/bin/zsh\nzqh"
  },
  {
    "objectID": "posts/6-3.html#解决问题",
    "href": "posts/6-3.html#解决问题",
    "title": "实现一个setenv函数",
    "section": "解决问题",
    "text": "解决问题\n很无语，现在写的这个函数，与原来没有什么区别，但是就是没有问题。所以这个问题我解决了，但是就好像没有解决一般。令人难受。"
  },
  {
    "objectID": "posts/Cfuncpointer.html",
    "href": "posts/Cfuncpointer.html",
    "title": "C函数指针的使用",
    "section": "",
    "text": "今天发现有本C陷阱与缺陷，就打开看看，发现其中有个非常有意思的东西： (*(void(*)())0()).作者介绍到，这一语句的作用是调用首地址为0的地址的程序。我对他进行了一些学习。"
  },
  {
    "objectID": "posts/Cfuncpointer.html#函数指针",
    "href": "posts/Cfuncpointer.html#函数指针",
    "title": "C函数指针的使用",
    "section": "函数指针",
    "text": "函数指针\n首先来一个简单的函数指针使用。\nvoid prt(void)\n{\n    printf(\"hello\\n\");\n}\n\nint main(int argc, char const *argv[])\n{\n    void (*pf)(void);\n    pf=prt;\n    pf();\n    return 0;\n}\n我们将这个函数指针的定义做一些小小的改动，可以获得一个类型转换符。"
  },
  {
    "objectID": "posts/Cfuncpointer.html#类型转换符",
    "href": "posts/Cfuncpointer.html#类型转换符",
    "title": "C函数指针的使用",
    "section": "类型转换符",
    "text": "类型转换符\n改动定义如下。\nvoid (*pf)(void) ---&gt; (void (*)()) \n从前面我们可以发现函数指针的赋值，其实也是地址的交互。那么我可以将一个地址强制转换为函数指针并使用吗？\nvoid prt(void)\n{\n    printf(\"hello\\n\");\n}\n\nint main(int argc, char const *argv[])\n{\n    void (*pf)(void);\n    pf=prt;\n    pf();\n    \n    int64_t addr=(int64_t)prt;\n    pf=(void (*)())addr;\n    pf();\n\n    return 0;\n}\n输出如下：\n☁  Desktop  gcc 1.c && ./a.out\nhello\nhello\n显而易见这个强制转换是起作用的。"
  },
  {
    "objectID": "posts/Cfuncpointer.html#任意地址运行程序",
    "href": "posts/Cfuncpointer.html#任意地址运行程序",
    "title": "C函数指针的使用",
    "section": "任意地址运行程序",
    "text": "任意地址运行程序\n那么现在可以做最后一步：\nvoid prt(void)\n{\n    printf(\"hello\\n\");\n}\n\nint main(int argc, char const *argv[])\n{\n    int64_t addr=(int64_t)prt;\n    (*(void (*)())addr)();\n    return 0;\n}\n输出：\n☁  Desktop  gcc 1.c && ./a.out\nhello\n现在我们就可以运行任意地址的程序了~"
  },
  {
    "objectID": "posts/Linuxblue1.html",
    "href": "posts/Linuxblue1.html",
    "title": "OrangePI蓝牙：开启蓝牙",
    "section": "",
    "text": "我使用的板子是OrangePI zero Plus2，基于全志H5。我在上面安装好了armbian系统。我现在要使用蓝牙功能，对接我的蓝牙耳机。"
  },
  {
    "objectID": "posts/Linuxblue1.html#安装bluez",
    "href": "posts/Linuxblue1.html#安装bluez",
    "title": "OrangePI蓝牙：开启蓝牙",
    "section": "安装Bluez",
    "text": "安装Bluez\nlinux上的蓝牙官方协议即为Bluez。所以需要先安装Bluez：\nroot@H5:~# apt-get install bluez bluez-tools\n安装完成后可以观察到系统上多出了几个工具：\nroot@H5:~# hci\nhciattach  hciconfig  hcitool \n使用工具：\nroot@H5:~# hciconfig \nroot@H5:~# \n发现并没有蓝牙设备。"
  },
  {
    "objectID": "posts/Linuxblue1.html#开启蓝牙设备",
    "href": "posts/Linuxblue1.html#开启蓝牙设备",
    "title": "OrangePI蓝牙：开启蓝牙",
    "section": "开启蓝牙设备",
    "text": "开启蓝牙设备\n接下来我在网络上寻找解决方法。找到armbian论坛中的这个帖子，其中thc013网友描述到：\nenable overlay uart 1 and add param_uart1_rtscts=1 to armbianenv.txt and adjust /etc/init.d/ap6212-bluetooth so it looks like this\n# Start patching\n        rfkill unblock all  \n        echo \"0\" &gt; /sys/class/rfkill/rfkill0/state\n        echo \"1\" &gt; /sys/class/rfkill/rfkill0/state\n        echo \" \" &gt; /dev/$PORT\n        devmem2 0x1f00060 b 1\n        echo 10 &gt; /sys/class/gpio/export\n        echo out &gt; /sys/class/gpio/gpio10/direction\n        echo 0 &gt; /sys/class/gpio/gpio10/value\n        echo 1 &gt; /sys/class/gpio/gpio10/value\n        sleep 0.1\n        hciattach /dev/$PORT bcm43xx 115200 flow bdaddr \n        $MAC_OPTIONS\n        hciconfig hci0 up\nand reboot oh you might need to install devmem2\n当然我要尝试使用这个方法了。\n\n首先使能覆盖串口1 根据描述这个应该是在armbianEnv.txt中，我当即去寻找这个文件,这文件应该是在启动分区中的，所以我想将启动分区mount后去修改。\nroot@H5:~# mount /dev/mmcblk2boot1 /mnt\nmount: /dev/mmcblk2boot1 is write-protected, mounting read-only\nmount: wrong fs type, bad option, bad superblock on /dev/mmcblk2boot1,\n   missing codepage or helper program, or other error\n\n   In some cases useful info is found in syslog - try\n   dmesg | tail or so.\n然后我发现这个armbian系统和一般的系统不太一样。boot相关文件不需要挂载直接可以查看：\nroot@H5:~# cd /boot/\nroot@H5:/boot# ls\narmbianEnv.txt                  dtb-4.16.0-sunxi64\narmbian_first_run.txt.template  Image\nboot.bmp                        initrd.img-4.16.0-sunxi64\nboot.cmd                        System.map-4.16.0-sunxi64\nboot-desktop.png                uInitrd\nboot.scr                        uInitrd-4.16.0-sunxi64\nconfig-4.16.0-sunxi64           vmlinuz-4.16.0-sunxi64\ndtb\nroot@H5:/boot# vi armbianEnv.txt\n在overlays后添加上了uart1，以及又添加了一行param_uart1_rtscts=1\n调整/etc/init.d/ap6212-bluetooth 编辑这个文件 sh     root@H5:~# vi /etc/init.d/ap6212-bluetooth 根据上面的帖子去修改这个文件。\n安装devmem2 这是一个内核调试实用的工具，需要安装的话使用这个链接 将软件包传入板子内安装： sh     root@H5:~# dpkg -i devmem2_0.0-0ubuntu1_arm64.deb     Selecting previously unselected package devmem2.     (Reading database ... 31834 files and directories currently installed.)     Preparing to unpack devmem2_0.0-0ubuntu1_arm64.deb ...     Unpacking devmem2 (0.0-0ubuntu1) ...     Setting up devmem2 (0.0-0ubuntu1) ...     Processing triggers for man-db (2.7.6.1-2) ... 安装完成重启测试蓝牙功能。\n失败 我发现这个方法并不可行，我现在对这个方法有所了解了。他其实是使用这个方式开启蓝牙：\nroot@H5:~# /etc/init.d/ap6212-bluetooth start\n[....] Starting ap6212-bluetooth (via systemctl): ap6212-bluetooth.serviceWarning: ap6212-bluetooth.service changed on disk. Run 'systemctl daemon-reload' to reload units.\n\nJob for ap6212-bluetooth.service failed because the control process exited with error code.\nSee \"systemctl status ap6212-bluetooth.service\" and \"journalctl -xe\" for details.\nfailed!\n\nroot@H5:~# systemctl status ap6212-bluetooth.service\n● ap6212-bluetooth.service - LSB: Patch firmware for ap6212 adapter\nLoaded: loaded (/etc/init.d/ap6212-bluetooth; generated; vendor preset: enabled)\nActive: failed (Result: exit-code) since Sun 2018-05-27 06:20:39 UTC; 2min 16s ago\n    Docs: man:systemd-sysv-generator(8)\nProcess: 2164 ExecStart=/etc/init.d/ap6212-bluetooth start (code=exited, status=1/FAILURE)\n\nMay 27 06:20:29 H5 ap6212-bluetooth[2164]: Value at address 0x1F00060 (0xffff97239060): 0x1\nMay 27 06:20:29 H5 ap6212-bluetooth[2164]: Written 0x1; readback 0x1\nMay 27 06:20:29 H5 ap6212-bluetooth[2164]: sh: echo: I/O error\nMay 27 06:20:39 H5 ap6212-bluetooth[2164]: Initialization timed out.\nMay 27 06:20:39 H5 ap6212-bluetooth[2164]: bcm43xx_init\nMay 27 06:20:39 H5 ap6212-bluetooth[2164]: Can't get device info: No such device\nMay 27 06:20:39 H5 systemd[1]: ap6212-bluetooth.service: Control process exited, code=exited status=1\nMay 27 06:20:39 H5 systemd[1]: Failed to start LSB: Patch firmware for ap6212 adapter.\nMay 27 06:20:39 H5 systemd[1]: ap6212-bluetooth.service: Unit entered failed state.\nMay 27 06:20:39 H5 systemd[1]: ap6212-bluetooth.service: Failed with result 'exit-code'.\nWarning: ap6212-bluetooth.service changed on disk. Run 'systemctl daemon-reload' to reload units."
  },
  {
    "objectID": "posts/Linuxblue3.html",
    "href": "posts/Linuxblue3.html",
    "title": "OrangePI蓝牙：串口通信",
    "section": "",
    "text": "上一篇文章描述了如何使用编程的方式去搜索附近蓝牙，这一次记录我如何编程与HC-06串口蓝牙模块通信。"
  },
  {
    "objectID": "posts/Linuxblue3.html#搜索以及配对",
    "href": "posts/Linuxblue3.html#搜索以及配对",
    "title": "OrangePI蓝牙：串口通信",
    "section": "搜索以及配对",
    "text": "搜索以及配对\n如果自己写程序进行搜索与配对，会十分麻烦，这里我就偷懒，使用bluez自带的方式进行蓝牙的配对。 使用bluetoothctl配对蓝牙设备，操作如下：\nbluetoothctl\nscan on #开启搜索\ndevices #查看附近设备\nscan off #关闭搜索\nagent on #开启代理\ndefault-agent\npair 98:D3:37:91:00:DD #配对蓝牙，会提示输入ping码\nquit #退出"
  },
  {
    "objectID": "posts/Linuxblue3.html#rfcomm通信",
    "href": "posts/Linuxblue3.html#rfcomm通信",
    "title": "OrangePI蓝牙：串口通信",
    "section": "rfcomm通信",
    "text": "rfcomm通信\nrfcomm是模拟串口的一种通信方式，与串口蓝牙也是以这个方式通信的。方式有两种。\n\n命令行操作 sh     rfcomm connect hci0 98:D3:37:91:00:DD & #放入后台执行 出现一下内容代表建立rfcomm成功。 sh     Connected /dev/rfcomm0 to 98:D3:37:91:00:DD on channel 1 现在我们可以通过对/dev/rfcomm0操作来发送数据，比如： sh     echo \"shell send hello!\" &gt; /dev/rfcomm0 可以看到蓝牙接收到这条数据了：\n\n程序操作 编写程序如下（要先配对才可以）：\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/socket.h&gt;\n#include &lt;bluetooth/bluetooth.h&gt;\n#include &lt;bluetooth/rfcomm.h&gt;\n#include \"tlpi_hdr.h\"\nint main(int argc, char **argv)\n{\n    struct sockaddr_rc addr = { 0 };\n    int s, status;\n    char dest[18] = \"98:D3:37:91:00:DD\";\n\n    // allocate a socket\n    s = socket(AF_BLUETOOTH, SOCK_STREAM, BTPROTO_RFCOMM);\n\n    // set the connection parameters (who to connect to)\n    addr.rc_family = AF_BLUETOOTH;\n    addr.rc_channel = (uint8_t) 1;\n    str2ba( dest, &addr.rc_bdaddr );\n\n    // connect to server\n    status = connect(s, (struct sockaddr *)&addr, sizeof(addr));\n\n    // send a message\n    if( status == 0 ) {\n        status = write(s, \"C send hello!\\n\", 16);\n    }\n\n    if( status &lt; 0 ) \n    {\n        errExit(\"connct error\");\n    }\n\n    close(s);\n    return 0;\n}\n编译运行：\nroot@H5:~# gcc -o rfuart_client rfuart_client.c -lbluetooth\nroot@H5:~# ./rfuart_client\n[1]+  Done                    rfcomm connect hci0 98:D3:37:91:00:DD\n结果如下："
  },
  {
    "objectID": "posts/Matlab_svm.html",
    "href": "posts/Matlab_svm.html",
    "title": "Matlab Classification Learner",
    "section": "",
    "text": "在matlab中，既可以使用函数来对数据进行分类，也使用图形化界面的工具箱来进行分类操作。接下来讲讲如何使用。这里我主要介绍受监督的训练模型分类。\n\n  使用此工具箱, 我们可以使用各种分类器来探索受监督的机器学习。同时可以浏览数据、选择功能、指定验证方案、培训模型和评估结果。可以执行自动培训以搜索最佳分类模型类型, 包括决策树、判别分析、支持向量机、逻辑回归、最近邻居和集合分类等等。\n\n什么是受监督的机器学习？\n\n  通过提供已知的一组输入数据 (观察或示例) 和已知的数据响应 (例如, 标签或类) 来执行受监督的机器学习。使用数据培训模型, 以生成对新数据的响应的预测。若要将模型与新数据一起使用, 或者要了解编程分类, 可以将模型导出到工作区或生成 MATLABR用于重新创建经过培训的模型的代码。\n\n\n\n\n点击开始自动分类器\n  我们可以使用Classification Learner对数据进行自动训练并且能选择不同的模型。   我们在APP选项卡中单击Classification Learner即可启动。\n\n\n\n新建训练模型\n  单击New Session建立新模型,出现下图对话框:\n\n\nStep1:选择数据   这里输入的数据必须要参数与标签合一的数据，比如我放入的数据是data，为套管缺陷数据。是一个\\(286*24\\)的矩阵，其\\(1\\sim23\\)列为数据参数，第\\(24\\)列为标签，有缺陷为\\(1\\),无缺陷则为\\(-1\\)。\n  其中\\(1\\sim200\\)行为无缺陷数据，所以对于的\\(24\\)列都为\\(-1\\)，\\(201\\sim286\\)行为有缺陷数据，所以对于的\\(24\\)列都为\\(1\\)。\nStep2:选择预测者与反应者\n  首先这里放入数据是，我选择了column模式，所以是以列作为读取数据顺序。\\(1\\sim23\\)列都为数据参数，所以作为Predictor导入。\\(24\\)列作为标签，所以作为Response导入。\nStep3:选择验证模式\n  这里选择交叉验证(Cross validation),目的是为了得到可靠稳定的模型。在建立PCR 或PLS 模型时，一个很重要的因素是取多少个主成分的问题。用cross validation 校验每个主成分下的PRESS值，选择PRESS值小的主成分数。或PRESS值不再变小时的主成分数。   也可以Holdout验证，随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。\nStep4:点击开始\n\n\n\n开始训练\n  新建模型完毕后，可以选择各种类型的分类器进行数据分类。\n * Step1:选择PCA参数   我们可以对数据进行PCA降维，也可以调整PCA中的某些参数来调整训练模型。 * Step2:选择分类器类型   这里可以选择SVM分类器、KNN分类器、逻辑回归分类器等等。每种分类器中都可以选择不同的核函数，比如SVM分类器，可以选择Linear核、Quadratic核、cubic核、Gaussian核等。 * Step3:选择扩展设置   在这里我们可以分类器的参数做调节，典型的参数有:惩罚因子等级、核函数缩放因子、参数是否标准化等等。 * Step4:运行   接下来直接运行即可得出训练模型。\n\n\n训练结果\n  我这里对所有的SVM模型都进行了运行，可以查看到如下界面。\n\n\n1:分类模型状态\n  可以查看到当前有100%的准确率，这个准确率是之前使用交叉验证，模型对于自己的分类结果做的验证的准确率。\n2:数据图表\n  这里可以对数据各个部分进行可视化分析。\n3:图表选项\n  因为目前是24维数据，对于2维图形是无法全部绘制的，所以可以选择对任意两个维度作图。\n4:导出模型\n  模型建立完毕后，可以选择导出模型，进行下一步的工作。\n\n\n\n预测结果\n  我将导出的模型对新输入缺陷与非缺陷数据进行识别。代码如下:\n%% 开始预测\nyfit = trainedClassifier1.predictFcn(testdata);\nn=sum(testlabel ==  yfit)/length(testlabel)*100;\ndisp('Linear SVM');\ndisp(n);\n\nyfit = trainedClassifier2.predictFcn(testdata);\nn=sum(testlabel ==  yfit)/length(testlabel)*100;\ndisp('Quadratic SVM');\ndisp(n);\n\nyfit = trainedClassifier3.predictFcn(testdata);\nn=sum(testlabel ==  yfit)/length(testlabel)*100;\ndisp('Cubic SVM');\ndisp(n);\n\nyfit = trainedClassifier4.predictFcn(testdata);\nn=sum(testlabel ==  yfit)/length(testlabel)*100;\ndisp('Fine Gaussian SVM');\ndisp(n);\n\nyfit = trainedClassifier5.predictFcn(testdata);\nn=sum(testlabel ==  yfit)/length(testlabel)*100;\ndisp('Medium Gaussian SVM');\ndisp(n);\n\nyfit = trainedClassifier6.predictFcn(testdata);\nn=sum(testlabel ==  yfit)/length(testlabel)*100;\ndisp('Coarse Gaussian SVM');\ndisp(n);\n结果如下:\nLinear SVM\n   84.2105\n\nQuadratic SVM\n   84.2105\n\nCubic SVM\n   84.2105\n\nFine Gaussian SVM\n   86.4662\n\nMedium Gaussian SVM\n   84.9624\n\nCoarse Gaussian SVM\n   84.2105"
  },
  {
    "objectID": "posts/Mullapudi2016.html",
    "href": "posts/Mullapudi2016.html",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "",
    "text": "关于halide中自动调度baseline算法的论文笔记. 二作目前还在维护halide."
  },
  {
    "objectID": "posts/Mullapudi2016.html#scheduling-for-producer-consumer-locality",
    "href": "posts/Mullapudi2016.html#scheduling-for-producer-consumer-locality",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "3.1 Scheduling for Producer-Consumer Locality",
    "text": "3.1 Scheduling for Producer-Consumer Locality\n对于上面的代码, 第一个简单的调度方式如下,当需要输出一个图像时,首先计算生成blurx所有需要的元素, 将这些数据放到一个大的buffer中,然后再使用这个buffer计算最终的结果. 这种调度将会受限于带宽.\n\n此时换一种调度方式, 将计算blurx放到不同的循环位置, 那么计算完blurx立刻被消耗, 不同的循环位置决定了不同的data reuse distance. 当前这种方式最大化了producer-consumer locality, 但是也带来了recompute的问题, 即多算了3次.\n\n为了更好的平衡了局部性和冗余计算的问题, 可以采用新的调度. 因为中间数据量比较小,因此可以开一个大一些的local buffer,这样每次的recompute只是在tile的边界区域,减少了冗余计算的次数. 接下来就需要选择合理的tling大小, 让并行的计算提升超过冗余计算带来的开销.\n\n即使在这个简单的例子中,全局循环reorder也对程序的性能产生了大量影响. 真实世界图像处理管道可能包含数百个function,为了有效地调度它们,我们要调整多维tiling的大小/生产者和消费者在循环中的位置, 从而在局部性/并行性/冗余计算之间的trade-off."
  },
  {
    "objectID": "posts/Mullapudi2016.html#scheduling-for-input-reuse",
    "href": "posts/Mullapudi2016.html#scheduling-for-input-reuse",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "3.2 Scheduling for Input Reuse",
    "text": "3.2 Scheduling for Input Reuse\n除了调整producer的位置放到consumer里距离更近的地方来增加局部性. 当data reuse比producer-consumer locality重要时, 建议通过调整consumer的位置增加读取相同数据局部性.\n比如矩阵乘的例子, 当在尺寸\\(N\\times N\\)的输入A和B上执行该程序时,计算C将每次n次访问A和B的每个元素. 如下图所示,在访问到相同的输入值之间访问输入函数的整个行或列(重用距离与输入矩阵的大小成比例). 对于大输入矩阵,输入值将不再留在在处理器的缓存中,导致低性能.\n\n此时可以利用tiling将问题减少到一个较小的矩阵乘法序列,这样可以把输入保存在高速cache中.\n\n综上, 利用循环tiling来最大化输入局部性是计算过程中的关键优化."
  },
  {
    "objectID": "posts/Mullapudi2016.html#function-bounds-analysis",
    "href": "posts/Mullapudi2016.html#function-bounds-analysis",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "3.3 Function Bounds Analysis",
    "text": "3.3 Function Bounds Analysis\nHalide的一些调度原语 (compute_at, reorder, tile) 可以帮助我们做到上面那些事情,但是为了量化如何做, 编译器必须能够确定适当的loop bound和中间buffer的大小.\n比如上面在计算blur的out时, 必须得先计算出blurx的(x, y-1..y+1)三个值并保存. halide通过对基于表达式的区间分析来推导出每个符号函数的边界.\n比如计算blur的function中, out是的在\\((x_{min}..x_{max}, y_{min}..y_{max})\\)的domain上定义的. 然后halide反推出symbolic bounds expression:\n\\[\n\\begin{aligned}\n&blurx: (x_{min}..x_{max}, &y_{min}-1..y_{max}+1) \\\\\n&in: (x_{min}-1..x_{max}+1, &y_{min}-1..y_{max}+1)\n\\end{aligned}\n\\]\n如果我们设置固定的输出信息, 比如设置out为\\((5..10,10..20)\\) 那么将得到 \\[\n\\begin{aligned}\n&blurx: (5..10,&9..21)\\\\\n&in: (4..11,&9..21)\n\\end{aligned}\n\\]\n从输出函数开始,边界推理依据函数依赖关系链式传播, 然后将边界信息更新到整个DAG.如果halide无法推理整个函数的边界信息时(比如隐式依赖导致无法建立传播链), 我们也可以直接显式的为函数设置边界信息, 因为自动化的schedule是离不开边界分析所提供做决策的信息的,所以无论如何必须要可以获得它."
  },
  {
    "objectID": "posts/Mullapudi2016.html#function-grouping-and-tiling",
    "href": "posts/Mullapudi2016.html#function-grouping-and-tiling",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "4.2 Function Grouping and Tiling",
    "text": "4.2 Function Grouping and Tiling\n经过预处理之后, 开始进行函数分组, 分组则是为了找到程序中有益于重组计算顺序来提升 producer-consumer locality的一个点. 这就需要确定如何tiling consumer的循环,以及在那个循环级别放置producer.\n本文算法使用迭代的贪心方法将halide program进行分组.分组后, 每个组执行独立的reordering/tiling schedule. 每个组中包含多个halide的funcion, 但是每个组也只能有一个output funcion(分离出所有来自于同一函数的依赖边), 所有组中的其他函数都将在output function的循环中进行计算(也就是producer放到consumer的那层循环).\n首先将每个函数分为一个group, 对于每个单例组寻找到可以最大化input data reuse的tiling size(如果可以reuse).然后每个迭代中尝试将合并两个组来检查是否能增加producer- consumer locality. 当调度器无法再进行merge group的操作时,停止迭代. 每个function 分组迭代包含了以下几个步骤:\n\n枚举所有剩余的group合并机会\n对于每个可merge的机会,评估收益(merge之后tiling size是否需要修改?和前面group合并还是后面的group合并?)\n选择产生最大的提升的group进行合并\n\n\n4.2.1 Initialization: Tiling for Input Data Reuse\n调度器尝试查找通过每个non-pure的函数来改进输入数据访问的循环tiling。(pure-function 即不包含任何halide更新定义或reduce domain的函数,例如不用tiling直接就具备良好的input locality), 然后采用了一个单级存储层级的简单模型进行分析,其执行的伪代码如下:\n\n主要就是检查一系列的tiling size 是不是可以满足l1 cache size等. 然后通过计算load ops检查数据重用的机会,找到满足条件的最优tiling.\n这个方法假设所有的非纯函数都还没有被tiling过,所有的输入数据都是cache misses(通过total_load_op计算得到). 然后tiling所有输出需要的输入, 让这些输入可以放到cache中, 最后算法找到计算输出所需要的最少load次数时的tile size.\n为了加速tile size的搜索, 本文将tile size向着可以得到更大input data reuse的方向上增加tile size.同时自动调度器还考虑到tile size下tile的个数能不能满足最小的并行化限度.(这里默认能做parallel肯定会更快)\n\n\n4.2.2 Enumerating Merging Opportunities\nGrouping是为了增加组中函数的producer-comsumer locality. 因此, 如果两个组g1和g2, g1将被g2中的某个函数所消耗,那么将他们作为一个candidate.\n由于在两个不同的loop nests中取计算同一个function将会造成重复执行, 因此grouping只会尝试合并唯一消耗g1的输出的那个组. 比如上图所示, 函数对A-B,C-E,D-E都是candidate. 但是B-C和B-D不是,因为B的输出同时被C和D所消耗了.\n\n\n4.2.3 Evaluating Potential Merges\n评估整体代码的performce收益来确定是否进行mege. 通过函数evaluate_group_merge来确定合并当前两个group的最佳schedule, 然后返回估计的benefit.\n\n\nGenerating tile sizes.\n计算tile size对于性能的影响很大,但是对于一个高维的nest loop,遍历去evaluate所有的tile size来计算cost是也是不现实的. 因此本文按章节4.1中所描述的, 按照group 输出function的data reuse方向取选择潜在的tile size. 具体地,本文将tile size约束为N-D的hypercubes,并在最大的输出函数data reuse的方向上拉伸。 当tile变大时可以有效减少因为tile所引入的冗余计算。 进一步我们要求tile 在最内的循环上扩展最小的维度,同时把最小值设置为目标机器向量宽度的小倍数,保证不会超出cache size同时可以轻易的vectorize。 后面再由评估函数group_tile_footprint(行28)要求编译器通过根据候选区块大小调度group的循环顺序来确定所需临时buffer分配的大小.\n\n\nComparing costs.\n对于每个候选的tilie size,需要估计并比较执行grouping的成本,对于不合并的cost由生产者和消费者组的算术成本(Arith_cost,加上每个组的内存载入cost(Group_loads)组成。 总的load次数(整个程序的执行)由其输出函数(11-12行)的具体边界和group当前设置的tile size给出。 其中LOAD_COST则是从mian memory中加载数据cost的系数.\n假设所有的被merge的group所产生的intermediate buffer都存储在cache上, 那么merged output的一个tile的cost则是由算数操作加上所需要的input的load(gropu_tile_loads)所组成. 总的merged cost则是每个tile cost的乘积.\n最后evaluate_group_merge 返回找到的最佳tile下的performance."
  },
  {
    "objectID": "posts/Mullapudi2016.html#function-inlining",
    "href": "posts/Mullapudi2016.html#function-inlining",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "4.3 Function Inlining",
    "text": "4.3 Function Inlining\n此外将producer函数内联到consumer中可能是有利的(如果是手动编写的Halide schedule是需要显式的使用inline指令).\n虽然inline的概念类似于在grouping的过程中选择tile size, 但是inline后所产生的code 会更加高效(中间计算结果不通过load store, 直接存储在寄存器中).\n本文在这里描述function inling,但是其实际执行的位置是在recompute之后, merge group之前.\n内联也是通过类似的贪心迭代方法得到的, 但是有如下的区别:\n\n为了简单起见, 调度器是综合每个函数的基础上确定是否inling. 一个函数要么inline到他所有的consumer上(最终从dag上移除), 要么不做任何inline(后续在group merge中进行优化).\n如果当前一个函数可以被inline到consumer中, 计算cost的时候会计算把当前函数inline到每个consumer后的cost. 而不是像group merge一样只考虑当前两个group merge后的cost.\n自动调度器不是用边界分析来估计inline后的arithmetic cost. 相反, 他是将producer的表达式直接替换成consumer, 然后估计consumer的cost, 这样可以得到更加精确的预测结果."
  },
  {
    "objectID": "posts/Mullapudi2016.html#final-schedule-generation",
    "href": "posts/Mullapudi2016.html#final-schedule-generation",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "4.4. Final Schedule Generation",
    "text": "4.4. Final Schedule Generation\n当inline和merge group结束之后, 调度器返回了一堆group,每个group固定了一个loop tiling. 调度的最后一步就是为每个group生成完整的优化调度.\n首先是reorder每个group的output function, 达到最高的input locality. 为了保证数据访问的空间局部性和vectorize的能力, loop reorder是不会将最内侧的循环从其起始位置进行移动.\n接下来auto-scheduler unroll这些一些小循环, 最后再添加一些外部循环上的并行.\n在这些操作之后,每个group都有完整的schedule。后续就是codegen了."
  },
  {
    "objectID": "posts/Mullapudi2016.html#find_transitive_calls",
    "href": "posts/Mullapudi2016.html#find_transitive_calls",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "0. find_transitive_calls",
    "text": "0. find_transitive_calls\n遍历最后输出的outputs func, 每个funcion accept 一个FindCall visitor. 这个FindCall visitor就是把所有call {Type=Halide}的节点全部取出来.\n这里我觉得accept是个比较好的设计, 因为expr visitor只能通过visitor自己去遍历expr,但是如果你定义了一堆非expr的数据结构,然后你的visitor还得扩展visit函数,非常麻烦. 那么应该是每个扩展的数据结构中自定义accept函数,主动调用visitor进行遍历.这样就可以支持各种形式的遍历了."
  },
  {
    "objectID": "posts/Mullapudi2016.html#topological_order",
    "href": "posts/Mullapudi2016.html#topological_order",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "1. topological_order",
    "text": "1. topological_order\n得到调用顺序"
  },
  {
    "objectID": "posts/Mullapudi2016.html#validating-no-partial-schedules",
    "href": "posts/Mullapudi2016.html#validating-no-partial-schedules",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "2. Validating no partial schedules",
    "text": "2. Validating no partial schedules\n当前的auto schedule无法对一些已经schedule的program进行进一步调度. 需要先排除."
  },
  {
    "objectID": "posts/Mullapudi2016.html#checking-estimates-on-outputs",
    "href": "posts/Mullapudi2016.html#checking-estimates-on-outputs",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "3. Checking estimates on outputs",
    "text": "3. Checking estimates on outputs\n遍历每个output的index var, 然后检查bounds的min max是否被定义, 只是单纯检查有没有定义,还没有进行边界计算"
  },
  {
    "objectID": "posts/Mullapudi2016.html#inlining-all-trivial-functions",
    "href": "posts/Mullapudi2016.html#inlining-all-trivial-functions",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "4. Inlining all trivial functions",
    "text": "4. Inlining all trivial functions\n把可以inline的函数都进行内敛."
  },
  {
    "objectID": "posts/Mullapudi2016.html#realization_order",
    "href": "posts/Mullapudi2016.html#realization_order",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "5. realization_order",
    "text": "5. realization_order\n暂时不是很理解, 可能是消除了一些输入节点的函数实例化的顺序"
  },
  {
    "objectID": "posts/Mullapudi2016.html#inlining-all-element-wise-functions",
    "href": "posts/Mullapudi2016.html#inlining-all-element-wise-functions",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "6. Inlining all element-wise functions",
    "text": "6. Inlining all element-wise functions\n这里是要对一些局部定义的function进行inline. 比如:\nf1(x) = x\nf2(x) = f1(x) + 2\nf3(x) = f1(x) * 2\nf4(x) = f2(x) + f3(x)\nf5(x) = f4(x) + 3\n第一次inline把f3、和f2集成到f4中了,然后现在f1也可以被inline到f4中了."
  },
  {
    "objectID": "posts/Mullapudi2016.html#computing-function-value-bounds",
    "href": "posts/Mullapudi2016.html#computing-function-value-bounds",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "7. Computing function value bounds",
    "text": "7. Computing function value bounds\n这里是正经的计算所有function 值的bounds.做法就是把所有的bound的expr推导出来,然后做一些常量消除. NOTE 这里是推断的具体的数值区间,而不是shape."
  },
  {
    "objectID": "posts/Mullapudi2016.html#initializing-region-costs",
    "href": "posts/Mullapudi2016.html#initializing-region-costs",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "8. Initializing region costs",
    "text": "8. Initializing region costs\ncost中包含了数学运算以及memory的信息, 到时候利用memory去计算load store的cost. 也是利用vistor的方式收集信息,不过他这里和论文有所出入, 实际上所有的binary的cost都是1. 对于除法并没有给定额外的cost值.\n对于halide的call, cost =( arith+1, memory+ types.bytes). 然后将当前call的数据bytes存储下来,给后面需要用这个函数的地方使用.\nif (call-&gt;call_type == Call::Halide || call-&gt;call_type == Call::Image) {\n    // Each call also counts as an op since it results in a load instruction.\n    arith += 1;\n    memory += call-&gt;type.bytes();\n    detailed_byte_loads[call-&gt;name] += (int64_t)call-&gt;type.bytes();\n下面是一个function的cost计算结果,后面的((int64)9, (int64)16)分别表示数学cost和 memory cost.\n(((float32)repeat_edge(x, y, 2)*0.114000f) + (((float32)repeat_edge(x, y, 0)*0.299000f) + ((float32)repeat_edge(x, y, 1)*0.587000f)))\n(f0, 0) -&gt; ((int64)9, (int64)16)\n(((float32)f0(x + 1, y + 1) + ((((float32)f0(x, y + 1)*2.000000f) + (((float32)f0(x, y + -1)*-2.000000f) + ((float32)f0(x + -1, y + 1) - (float32)f0(x + -1, y + -1)))) - (float32)f0(x + 1, y + -1)))*0.083333f)\n(f1, 0) -&gt; ((int64)25, (int64)28)\n(let t3 = (((float32)f3(x + 1, y + 1) + ((float32)f3(x + 1, y) + ((float32)f3(x + 1, y + -1) + ((float32)f3(x, y + 1) + ((float32)f3(x, y) + ((float32)f3(x, y + -1) + ((float32)f3(x + -1, y + 1) + ((float32)f3(x + -1, y + -1) + (float32)f3(x + -1, y))))))))) + ((float32)f4(x + 1, y + 1) + ((float32)f4(x + 1, y) + ((float32)f4(x + 1, y + -1) + ((float32)f4(x, y + 1) + ((float32)f4(x, y) + ((float32)f4(x, y + -1) + ((float32)f4(x + -1, y + 1) + ((float32)f4(x + -1, y + -1) + (float32)f4(x + -1, y)))))))))) in ((let f9.t2 = ((float32)f5(x + 1, y + 1) + ((float32)f5(x + 1, y) + ((float32)f5(x + 1, y + -1) + ((float32)f5(x, y + 1) + ((float32)f5(x, y) + ((float32)f5(x, y + -1) + ((float32)f5(x + -1, y + 1) + ((float32)f5(x + -1, y + -1) + (float32)f5(x + -1, y))))))))) in ((((float32)f3(x + 1, y + 1) + ((float32)f3(x + 1, y) + ((float32)f3(x + 1, y + -1) + ((float32)f3(x, y + 1) + ((float32)f3(x, y) + ((float32)f3(x, y + -1) + ((float32)f3(x + -1, y + 1) + ((float32)f3(x + -1, y + -1) + (float32)f3(x + -1, y)))))))))*((float32)f4(x + 1, y + 1) + ((float32)f4(x + 1, y) + ((float32)f4(x + 1, y + -1) + ((float32)f4(x, y + 1) + ((float32)f4(x, y) + ((float32)f4(x, y + -1) + ((float32)f4(x + -1, y + 1) + ((float32)f4(x + -1, y + -1) + (float32)f4(x + -1, y)))))))))) - (f9.t2*f9.t2))) - ((t3*t3)*0.040000f)))\n(f11, 0) -&gt; ((int64)153, (int64)184)\n(((float32)f0(x + 1, y + 1) + ((((float32)f0(x + 1, y)*2.000000f) + (((float32)f0(x + -1, y)*-2.000000f) + ((float32)f0(x + 1, y + -1) - (float32)f0(x + -1, y + -1)))) - (float32)f0(x + -1, y + 1)))*0.083333f)\n(f2, 0) -&gt; ((int64)25, (int64)28)\n(let t0 = (float32)f2(x, y) in (t0*t0))\n(f3, 0) -&gt; ((int64)3, (int64)8)\n(let t1 = (float32)f1(x, y) in (t1*t1))\n(f4, 0) -&gt; ((int64)3, (int64)8)\n((float32)f2(x, y)*(float32)f1(x, y))\n(f5, 0) -&gt; ((int64)4, (int64)12)\n(float32)f11(x + 2, y + 2)\n(output1, 0) -&gt; ((int64)4, (int64)8)\n((float32)f11(x + 2, y + 2)*(float32)factor)\n(output2, 0) -&gt; ((int64)5, (int64)8)\n(let lambda_0._2 = max(min(likely(_2), (input.extent.2 + input.min.2) + -1), input.min.2) in (let lambda_0._1 = max(min(likely(_1), (input.extent.1 + input.min.1) + -1), input.min.1) in (let lambda_0._0 = max(min(likely(_0), (input.extent.0 + input.min.0) + -1), input.min.0) in (float32)input(lambda_0._0, lambda_0._1, lambda_0._2))))\n(repeat_edge, 0) -&gt; ((int64)14, (int64)8)"
  },
  {
    "objectID": "posts/Mullapudi2016.html#dependence-analysis-and-computing-pipeline-bounds",
    "href": "posts/Mullapudi2016.html#dependence-analysis-and-computing-pipeline-bounds",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "9. dependence analysis and Computing pipeline bounds",
    "text": "9. dependence analysis and Computing pipeline bounds\n通过依赖分析以及给定参数的estimates推理整个pipeline的bounds. halide的var其实表示的就是index的, 这里他把输出看做一个box,我觉得可以叫tensor,每个维度上有自己的min max. 得到的结果如下:\n\n================\nPipeline graph:\n================\nf0: {f1, f2}\nf1: {f4, f5}\nf11: {output1, output2}\nf2: {f3, f5}\nf3: {f11}\nf4: {f11}\nf5: {f11}\nrepeat_edge: {f0}\n================\n\n================\nPipeline bounds:\n================\nf0 -&gt; {[0, 1027], [0, 1027]}\nf1 -&gt; {[1, 1026], [1, 1026]}\nf11 -&gt; {[2, 1025], [2, 1025]}\nf2 -&gt; {[1, 1026], [1, 1026]}\nf3 -&gt; {[1, 1026], [1, 1026]}\nf4 -&gt; {[1, 1026], [1, 1026]}\nf5 -&gt; {[1, 1026], [1, 1026]}\ninput -&gt; {[0, 1023], [0, 1023], [0, 2]}\noutput1 -&gt; {[0, 1023], [0, 1023]}\noutput2 -&gt; {[0, 1023], [0, 1023]}\nrepeat_edge -&gt; {[0, 1027], [0, 1027], [0, 2]}\n==============="
  },
  {
    "objectID": "posts/Mullapudi2016.html#initializing-partitioner",
    "href": "posts/Mullapudi2016.html#initializing-partitioner",
    "title": "Automatically Scheduling Halide Image Processing Pipelines",
    "section": "10. Initializing partitioner",
    "text": "10. Initializing partitioner"
  },
  {
    "objectID": "posts/RealMix-EnAET.html",
    "href": "posts/RealMix-EnAET.html",
    "title": "半监督学习：RealMix与EnAET",
    "section": "",
    "text": "今天看两篇论文，因为只是学习一下他们的思路，所以不进行细写。\n\nRealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms\nEnAET: Self-Trained Ensemble AutoEncoding Transformations forSemi-Supervised Learning\n\n\n\nEnAET\nEnAET表示的是Ensemble AutoEncoding Transformations。\n他的初步想法是对于数据增强的样本，我们可以直接对特征进行解码从而获得对应的数据增强变化。\n\n如上图所示，解码器实际上可以作为无监督训练的附加项附带在传统的半监督学习方法中，因此他的损失如下： \\[\n\\begin{aligned}\n  \\mathcal{L}&=\\min_\\Theta \\mathcal{L}_\\text{SSL}+\\sum_{k=1}^{N}\\lambda_k\\mathcal{L}_{\\text{AET}_k}\\\\\n  &=\\min_\\Theta \\mathcal{L}_\\text{SSL}+\\sum_{k=1}^{N}\\lambda_k \\mathbb{E}_{x,t_k}\\parallel D[E(x),E(t_k(x))]-t_k \\parallel^2\n\\end{aligned}\n\\]\n其中\\(\\mathcal{L}_\\text{SSL}\\)他使用的是mixmatch的损失。我看代码中一共五种变换，他相应地构建了五个解码器模型，然后训练解码器可以解码出对应的变换，使得编码器能更好的提取特征，从而提升效果。\n总体来说这个套路可以套用到各种地方，额外加个解码器模型做个信息瓶颈约束或者用别的方法做个xx相似度约束，理论上都是能稳定提升的。不过这里额外加了5个解码器着实有些太麻烦了。\n\n\nRealMix\n这篇文章是2019年末的，是在fixmatch出来之前的，他的效果没有fixmatch那么好。他的思路是加入了训练退火以及根据置信度mask掉部分的无监督损失。\n\n\n\n总结\n哎😑，想不出好想法，也找不到能提供思路的文献～"
  },
  {
    "objectID": "posts/Uncertainty-Weigh-Loss.html",
    "href": "posts/Uncertainty-Weigh-Loss.html",
    "title": "Uncertainty Weight Loss",
    "section": "",
    "text": "这是对论文Multi Task Learning with Homoscedastic Uncertainty的一个学习。\n\n\n似然定义\n设置\\(\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\)表示为输入\\(x\\)通过权重\\(\\boldsymbol{W}\\)的输出特征向量。本文将回归问题和分类问题分开考虑他们的概率表示：\n\n回归问题：\\(p(\\mathbf{y}|\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}))=\\mathcal{N}(\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}),\\sigma^2)\\),即另模型输出作为均值，额外用一个参数表示方差，建模为高斯分布。\n分类问题：\\(p(\\mathbf{y}|\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}))=\\text{Softmax}(\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}))\\),分类时经过\\(\\text{Softmax}\\)即为概率分布。\n\n接下来对于多任务结合的概率表示，简单讲就是概率乘积： \\[\n\\begin{aligned}  \np\\left(\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{K} \\mid \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right)=p\\left(\\mathbf{y}_{1} \\mid \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right) \\ldots p\\left(\\mathbf{y}_{K} \\mid \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right)\n\\end{aligned}\n\\]\n\n\n最大似然\n为了最大化似然，先构造对数似然：\n\n回归问题： \\(\\log p\\left(\\mathbf{y} \\mid \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right) \\propto-\\frac{1}{2 \\sigma^{2}}\\left\\|\\mathbf{y}-\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right\\|^{2}-\\log \\sigma\\)，这里\\(\\mathbf{y}\\)就是标准数据，\\(\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\)为均值，后面的\\(-\\log \\sigma\\)其实是他在对数化的时候把系数给省略了。\n分类问题：\\(p\\left(\\mathbf{y} \\mid \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}), \\sigma\\right)=\\operatorname{Softmax}\\left(\\frac{1}{\\sigma^{2}} \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right)\\)，这里为了添加一个可学习的系数，他构造\\(sigma\\)作为输出特征向量的温度系数，用来调整\\(\\text{Softmax}\\)后的熵，比如特征向量被缩放的越厉害，那么输出分布会变得均匀，那么不确定性程度就加深了。NOTE： 因为我们的分类标签是带类别的，因此对应类别的对数似然公式如下(其中\\(c^{\\prime}\\)表示不为\\(c\\)的其他类别)：\n\n\\[\n\\begin{aligned}\n\\log p\\left(\\mathbf{y}=c \\mid \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}), \\sigma\\right) &=\\frac{1}{\\sigma^{2}} f_{c}^{\\mathbf{W}}(\\mathbf{x}) -\\log \\sum_{c^{\\prime}} \\exp \\left(\\frac{1}{\\sigma^{2}} f_{c^{\\prime}}^{\\mathbf{W}}(\\mathbf{x})\\right)\n\\end{aligned}\n\\]\n接下来就是多个任务联合的对数似然并进行最大化后验概率，我们最小化负的对数似然：\n\\[\n\\begin{array}{l}\n=-\\log p\\left(\\mathbf{y}_{1}, \\mathbf{y}_{2}=c \\mid \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right) \\\\\n=-\\log \\mathcal{N}\\left(\\mathbf{y}_{1} ; \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}), \\sigma_{1}^{2}\\right) \\cdot \\operatorname{Softmax}\\left(\\mathbf{y}_{2}=c ; \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}), \\sigma_{2}\\right) \\\\\n=\\frac{1}{2 \\sigma_{1}^{2}}\\left\\|\\mathbf{y}_{1}-\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right\\|^{2}+\\log \\sigma_{1}-\\log p\\left(\\mathbf{y}_{2}=c \\mid \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x}), \\sigma_{2}\\right)\\\\\n=\\frac{1}{2 \\sigma_{1}^{2}} \\mathcal{L}_{1}(\\mathbf{W})+\\frac{1}{\\sigma_{2}^{2}} \\mathcal{L}_{2}(\\mathbf{W})+\\log \\sigma_{1} +\\log \\frac{\\sum_{c^{\\prime}} \\exp \\left(\\frac{1}{\\sigma_{2}^{2}} f_{c^{\\prime}}^{\\mathrm{W}}(\\mathrm{x})\\right)}{\\left(\\sum_{c^{\\prime}} \\exp \\left(f_{c^{\\prime}}^{\\mathrm{W}}(\\mathbf{x})\\right)\\right)^{\\frac{1}{\\sigma_{2}^{2}}}}\\\\\n\\approx \\frac{1}{2 \\sigma_{1}^{2}}\\mathcal{L}_{1}(\\mathbf{W})+\\frac{1}{\\sigma_{2}^{2}} \\mathcal{L}_{2}(\\mathbf{W})+\\log \\sigma_{1}+\\log \\sigma_{2}\n\\end{array}\n\\]\n以上的\\(\\mathcal{L}_{1}(\\mathbf{W})=\\left\\|\\mathbf{y}_{1}-\\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right\\|^{2}\\)即为回归损失，\\(\\mathcal{L}_{2}(\\mathbf{W})=-\\log \\operatorname{Softmax}\\left(\\mathbf{y}_{2}, \\mathbf{f}^{\\mathbf{W}}(\\mathbf{x})\\right)\\)为分类误差。\n上面公式转换的时候我觉得他为了简化而简化一下，他假设\\(\\sigma_2\\rightarrow1\\),那么转换公式中\\(\\log \\frac{\\sum_{c^{\\prime}} \\exp \\left(\\frac{1}{\\sigma_{2}^{2}} f_{c^{\\prime}}^{\\mathrm{W}}(\\mathrm{x})\\right)}{\\left(\\sum_{c^{\\prime}} \\exp \\left(f_{c^{\\prime}}^{\\mathrm{W}}(\\mathbf{x})\\right)\\right)^{\\frac{1}{\\sigma_{2}^{2}}}}\\)上下两项将消除，\\(\\log1\\)就消除了。但是消除之后不能保证\\(\\sigma_2\\rightarrow1\\)了，他又添加了\\(\\log \\sigma_{2}\\)作为约束，保证他将趋向于1.\n\n\n实施\n那么根据作者的写法，不论回归还是分类损失其实都是用相同的计算公式，然后最后组合即可： \\[\nloss_{w}=\\frac{1}{2\\sigma^2}loss+\\log \\sigma\n\\]\n当然为了避免出现\\(\\log \\sigma\\)数值溢出的问题，采用\\(s:=\\log \\sigma^2\\)的方法来代替：\n\\[\nloss_{w}=\\frac{1}{2}\\cdot \\exp(-s) \\cdot loss+\\frac{1}{2}s\n\\]\n\n\n代码与实验结果\n代码如下，tf2.x运行：\nimport tensorflow as tf\nimport tensorflow.keras as k\nimport tensorflow.keras.layers as kl\nimport tensorflow.keras.losses as kls\nfrom typing import List\nimport numpy as np\nfrom datetime import datetime\nimport sys\n\n\ndef classifier(num_class: int):\n  return k.Sequential([kl.Flatten(),\n                       kl.Dense(32),\n                       kl.ReLU(),\n                       kl.Dense(num_class),\n                       kl.Softmax()])\n\n\ndef build_model():\n  inputs = k.Input((28, 28, 1))\n  encoder = k.Sequential([kl.Conv2D(16, 3, 1, 'same'),\n                          kl.ReLU(),\n                          kl.Conv2D(16, 3, 1, 'same'),\n                          kl.ReLU(),\n                          kl.MaxPool2D(2, 2),\n                          kl.Conv2D(16, 2, 1, 'same'),\n                          kl.ReLU(),\n                          kl.Conv2D(8, 2, 1, 'same'),\n                          kl.ReLU(),\n                          kl.MaxPool2D(2, 2),\n                          kl.Conv2D(8, 2, 2, 'same'),\n                          kl.ReLU(),\n                          kl.MaxPool2D(2, 2)])\n  classifier1 = classifier(3)\n  classifier2 = classifier(10)\n  reconstructor = k.Sequential([kl.Conv2DTranspose(8, 3, 3, 'valid'),\n                                kl.ReLU(),\n                                kl.Conv2DTranspose(4, 3, 2, 'same'),\n                                kl.ReLU(),\n                                kl.Conv2DTranspose(2, 3, 1, 'valid'),\n                                kl.ReLU(),\n                                kl.Conv2DTranspose(1, 2, 2, 'valid'),\n                                kl.Activation('tanh'), ])\n  x = encoder(inputs)\n  x1 = classifier1(x)\n  x2 = classifier2(x)\n  x3 = reconstructor(x)\n  model = k.Model(inputs, [x1, x2, x3])\n  return model\n\n\ndef uncertaint_weight(loss, weight):\n  return 0.5 * tf.exp(-weight) * loss + 0.5 * weight\n  # return 1 / (2 * weight * weight) * loss + tf.math.log(weight)\n\n\nif __name__ == \"__main__\":\n  USE_LEARNABLE = True if sys.argv[1] == 'True' else False\n  USE_RECONSTRUCT = True if sys.argv[2] == 'True' else False\n\n  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n  assert len(physical_devices) &gt; 0, \"Not enough GPU hardware devices available\"\n  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n\n  model: k.Model = build_model()\n  sub_dir = datetime.strftime(datetime.now(), r'%Y%m%d-%H%M%S')\n  root_dir = 'tmp/'\n  s1 = 'use_learnable' if USE_LEARNABLE else 'use_fixed'\n  s2 = 'use_recon' if USE_RECONSTRUCT else 'no_recon'\n  sub_dir = s1 + '_' + s2 + '/' + sub_dir\n\n  writer = tf.summary.create_file_writer(root_dir + sub_dir)\n  initer = k.initializers.RandomUniform(0.2, 1.0)\n  weights: List[tf.Variable] = [tf.Variable(\n      initer([], tf.float32), trainable=USE_LEARNABLE) for i in range(3)]\n  optimizer = k.optimizers.Adam(0.0001)\n  ce_fn1 = kls.CategoricalCrossentropy()\n  ce_fn2 = kls.CategoricalCrossentropy()\n  mse_fn = kls.MeanSquaredError()\n  ceacc_fn1 = k.metrics.CategoricalAccuracy()\n  ceacc_fn2 = k.metrics.CategoricalAccuracy()\n  batch_size = 256\n  epochs = 10\n\n  @tf.function\n  def step(x, y1, y2, y3):\n    with tf.GradientTape() as tape:\n      p1, p2, p3 = model(x, training=True)\n      l1 = ce_fn1(y1, p1)\n      ceacc_fn1.update_state(y1, p1)\n      l1_w = uncertaint_weight(l1, weights[0])\n      l2 = ce_fn2(y2, p2)\n      ceacc_fn2.update_state(y2, p2)\n      l2_w = uncertaint_weight(l2, weights[1])\n      l3 = mse_fn(y3, p3)\n      l3_w = uncertaint_weight(l3, weights[2])\n      l = (l1_w + l2_w + l3_w) if USE_RECONSTRUCT else (l1_w + l2_w)\n      if not USE_LEARNABLE:\n        l = (l1 + l2 + l3) if USE_RECONSTRUCT else (l1 + l2)\n\n    grads = tape.gradient(l, model.trainable_variables + weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables + weights))\n    if USE_LEARNABLE:\n      return l, l1_w, l2_w, l3_w\n    else:\n      return l, l1, l2, l3\n\n  (x_train, y_train), (x_test, y_test) = k.datasets.mnist.load_data()\n\n  x_train = ((x_train / 255.)[..., None] - 0.5) / 0.5\n  x_test = ((x_test / 255.)[..., None] - 0.5) / 0.5\n\n  # 将3与7设为2种，其他设为1种\n  y_train_1 = np.ones_like(y_train) * 2\n  y_train_1[y_train == 2] = 0\n  y_train_1[y_train == 6] = 1\n  y_train_1 = k.utils.to_categorical(y_train_1, 3)\n  y_train_2 = k.utils.to_categorical(y_train, 10)\n  y_train_3 = x_train.copy()\n\n  y_test_1 = np.ones_like(y_test) * 2\n  y_test_1[y_test == 2] = 0\n  y_test_1[y_test == 6] = 1\n  y_test_1 = k.utils.to_categorical(y_test_1, 3)\n  y_test_2 = k.utils.to_categorical(y_test, 10)\n  y_test_3 = x_test.copy()\n\n  train_ds = (tf.data.Dataset.from_tensor_slices(\n      ((x_train), (y_train_1, y_train_2, y_train_3))).\n      shuffle(batch_size * 100).\n      batch(batch_size, drop_remainder=False).\n      prefetch(None))\n\n  test_ds = (tf.data.Dataset.from_tensor_slices(\n      ((x_test), (y_test_1, y_test_2, y_test_3))).\n      shuffle(batch_size * 100).\n      batch(batch_size, drop_remainder=False).\n      prefetch(None))\n\n  for ep in range(epochs):\n    for i, (in_x, (in_y1, in_y2, in_y3)) in enumerate(train_ds):\n      l, l1, l2, l3 = step(in_x, in_y1, in_y2, in_y3)\n      cur_step = optimizer.iterations.numpy()\n      with writer.as_default():\n        tf.summary.scalar('train/loss', l.numpy(), step=cur_step)\n        tf.summary.scalar('train/loss_classify_1', l1.numpy(), step=cur_step)\n        tf.summary.scalar('train/loss_classify_2', l2.numpy(), step=cur_step)\n        tf.summary.scalar('train/acc_classify_1', ceacc_fn1.result().numpy(), step=cur_step)\n        tf.summary.scalar('train/acc_classify_2', ceacc_fn2.result().numpy(), step=cur_step)\n        tf.summary.scalar('train/loss_mse', l3.numpy(), step=cur_step)\n        for _ in range(3):\n          tf.summary.scalar(f'train/weight_{_}', weights[_].numpy(), step=cur_step)\n\n  \"\"\" eval \"\"\"\n  ceacc_fn1 = k.metrics.CategoricalAccuracy()\n  ceacc_fn2 = k.metrics.CategoricalAccuracy()\n  for i, (in_x, (in_y1, in_y2, in_y3)) in enumerate(train_ds):\n    l, l1, l2, l3 = step(in_x, in_y1, in_y2, in_y3)\n    p1, p2, p3 = model(in_x, training=False)\n    ceacc_fn1.update_state(in_y1, p1)\n    ceacc_fn2.update_state(in_y2, p2)\n  print('learnable', USE_LEARNABLE, 'Reconstruct', USE_RECONSTRUCT)\n  print('3 classify  :', ceacc_fn1.result().numpy())\n  print('10 classify :', ceacc_fn2.result().numpy())\n\n\n# python ./source/_posts/Uncertainty-Weigh-Loss/demo.py True True\n# python ./source/_posts/Uncertainty-Weigh-Loss/demo.py False True\n# python ./source/_posts/Uncertainty-Weigh-Loss/demo.py True False\n# python ./source/_posts/Uncertainty-Weigh-Loss/demo.py False False\nlearnable True Reconstruct True\n3 classify  : 0.96688336\n10 classify : 0.8987667\n\nlearnable False Reconstruct True\n3 classify  : 0.96745\n10 classify : 0.89035\n\n\nlearnable True Reconstruct False\n3 classify  : 0.9734667\n10 classify : 0.88518333\n\nlearnable False Reconstruct False\n3 classify  : 0.96508336\n10 classify : 0.8900333"
  },
  {
    "objectID": "posts/ads1118.html",
    "href": "posts/ads1118.html",
    "title": "Orangpi使用ads1118",
    "section": "",
    "text": "我的orangepi型号是zero puls2，全志H5。我使用spidev对ads1118进行驱动，其中有不少坑，在此记录。\n\n\nspi1.1与spi1.0\n板子上的系统是armbian。其中支持开启boot参数有spidev以及spi-add-cs。一开始我当然就直接都添加了，生成的设备节点为spi1.1。但是当我使用spidev_test去测试ads1118时却永远只能读取到0x00。\n后来我尝试自己拉低cs。所以就取消了spi-add-cs参数。生成的设备节点为spi1.0。但是当我手动拉低cs再去读取数据还是读取不到。后来我尝试直接读取数据发现终于读取到了！\n\n\nspi mode\n当成功读取数据后，一般肯定是先读取设备id，检查设备id有没有读取错误。按照ads1118的手册上，使用spimode1进行读写。\n但是根据我测试发现，使用这个spidev驱动。只能设置spimode2才可以正确的写入和读出数据！\n但是！在我第三次测试的时候，又只有使用spimode1才可以正确读写。实在令人摸不着头脑。\n\n\n配置ads1118寄存器\n根据手册提供的串行接口通信时序。需要配置寄存器并且读取寄存器的参数时，必须连续发送32位数据。当数据转换开始后，发送16位数据配置寄存器是无法回读成功的。"
  },
  {
    "objectID": "posts/aggressicecows.html",
    "href": "posts/aggressicecows.html",
    "title": "aggressicecows",
    "section": "",
    "text": "Farmer John has built a new long barn, with N (2 &lt;= N &lt;= 100,000) stalls. The stalls are located along a straight line at positions x1,…,xN (0 &lt;= xi &lt;= 1,000,000,000).\nHis C (2 &lt;= C &lt;= N) cows don’t like this barn layout and become aggressive towards each other once put into a stall. To prevent the cows from hurting each other, FJ want to assign the cows to the stalls, such that the minimum distance between any two of them is as large as possible. What is the largest minimum distance?"
  },
  {
    "objectID": "posts/aggressicecows.html#输入",
    "href": "posts/aggressicecows.html#输入",
    "title": "aggressicecows",
    "section": "输入",
    "text": "输入\n\nLine 1: Two space-separated integers: N and C\nLines 2..N+1: Line i+1 contains an integer stall location, xi"
  },
  {
    "objectID": "posts/aggressicecows.html#输出",
    "href": "posts/aggressicecows.html#输出",
    "title": "aggressicecows",
    "section": "输出",
    "text": "输出\n\nLine 1: One integer: the largest minimum distance"
  },
  {
    "objectID": "posts/aggressicecows.html#样例输入",
    "href": "posts/aggressicecows.html#样例输入",
    "title": "aggressicecows",
    "section": "样例输入",
    "text": "样例输入\n5 3 1 2 8 4 9"
  },
  {
    "objectID": "posts/aggressicecows.html#样例输出",
    "href": "posts/aggressicecows.html#样例输出",
    "title": "aggressicecows",
    "section": "样例输出",
    "text": "样例输出\n3"
  },
  {
    "objectID": "posts/akg-learn.html",
    "href": "posts/akg-learn.html",
    "title": "AKG 学习",
    "section": "",
    "text": "学习AKG的算子编译流程, 主要关于后端."
  },
  {
    "objectID": "posts/akg-learn.html#buildtofunc",
    "href": "posts/akg-learn.html#buildtofunc",
    "title": "AKG 学习",
    "section": "BuildToFunc",
    "text": "BuildToFunc\n\n执行提前注册好的llvm的lowersrc/codegen/lower_llvm.cc:LLVMLowerImpl对schedule的结果进行lower, 同时每个stage的逻辑都在这个文件中被注册:\n\nREG_STAGE_LOWER(\"llvm\", StageType::Begin, \"BEGIN\", LLVMLowerBegin);\nREG_STAGE_LOWER(\"llvm\", StageType::Tuning, \"TUNING\", LLVMLowerStageTuning);\nREG_STAGE_LOWER(\"llvm\", StageType::Poly, \"POLY\", LLVMLowerPoly);\nREG_STAGE_LOWER(\"llvm\", StageType::BeforeFlattern, \"BEFORE_FLATTERN\", LLVMLowerBeforeFlattern);\nREG_STAGE_LOWER(\"llvm\", StageType::Flattern, \"FLATTERN\", LLVMLowerFlattern);\nREG_STAGE_LOWER(\"llvm\", StageType::BeforeLowerFunc, \"BeforeLowerFunc\", LLVMBeforeLowerFunc);\nREG_STAGE_LOWER(\"llvm\", StageType::End, \"END\", LLVMLowerDone);\n\n在lower中通过StageManager获取所有需要lower的stage: [Begin,Tuning,Poly,BeforeFlattern,Flattern,BeforeLowerFunc,End],然后依次执行.\n\n\nBegin Stage\n对应src/codegen/lower_llvm.cc:LLVMLowerBegin. 在LLVMLowerBegin中需要执行一些基础的pass/analysis来为后续的lower pass提供信息.\nLowerInitWithSchedule -&gt; GetBinds\nnote right\n  将所有原始schedule中的tensor构造出对应的buffer并存放到`data-&gt;arg_list_0`和`data-&gt;binds_0`中.\n      注意`LowerData`中有着如下代码, 让人无法了解具体的含义.\n\n        ```cpp\n        Array&lt;NodeRef&gt; args;\n        Array&lt;NodeRef&gt; arg_list_0;\n        Map&lt;Tensor, Buffer&gt; binds;\n        Map&lt;Tensor, Buffer&gt; binds_0;  \n        ```\nend note\nLowerInitWithSchedule -&gt; AutoInline\nLowerInitWithSchedule -&gt; AutoFuse\nnote right\n  the &lt;U+0025&gt;autonumber&lt;U+0025&gt; works everywhere.\n  Here, its value is ** %autonumber% **\nend note\nLowerInitWithSchedule -&gt; TensorAccessRewrite : \n\nLowerInitWithSchedule\n\n\nGetBinds\nAutoInline 应该是inline一些op, 具体实现没有细看, 里面有许多需要特判的case.\nAutoFuse 应该是将多层的op自动fuse起来, 后续具体的fuse逻辑比较复杂. cpp     bool NeedToFuse() {       if (HasExternOp()) { // 如果包含ExternOp那么不fuse         return false;       }       // ReduceCheck 检查内部的reduce最大有没有超过shared memory的大小. (同时目前的代码是有matmul就不fuse,可能有什么限制.)        if (ReduceCheck() || !split_config_.empty()) { // 当然如果强行指定了split config那么也能fuse         return true;       }       return false;     }\n调用tvm自带的InferBound和ScheduleOps进行操作\nTensorAccessRewrite 这个应该是消除显式的tensor load/store.\n\n\nReplaceSeparator 将所有的名字中的.替换成_, 可能是为了方便调用isl.\nRewriteMultiValueFunc\nRenameRealize\nElementwiseFlatten\nFuseAxisExternOp\nAddAttrForLayoutOp\nRewriteTensorIndex"
  },
  {
    "objectID": "posts/ampl-learn.html",
    "href": "posts/ampl-learn.html",
    "title": "Ampl学习",
    "section": "",
    "text": "熟悉一下ampl的语法."
  },
  {
    "objectID": "posts/ampl-learn.html#indexing-expressions",
    "href": "posts/ampl-learn.html#indexing-expressions",
    "title": "Ampl学习",
    "section": "Indexing expressions",
    "text": "Indexing expressions\n索引表达式用于构造一个用于索引的多维集合, 因此他是通过{}包裹, 中间使用,分割的多个set expressions.\nindexing: \n  { sexpr-list } \n  { sexpr-list : lexpr } \nsexpr-list:\n  sexpr \n  dummy-member in sexpr\n  sexpr-list, sexpr\n比如:\n{A, B}\n{1..3, B}\n{i in A, B} # i可以给后续使用.\n{i in A, j in B}"
  },
  {
    "objectID": "posts/ampl-learn.html#common-expressions",
    "href": "posts/ampl-learn.html#common-expressions",
    "title": "Ampl学习",
    "section": "Common expressions",
    "text": "Common expressions\n通用表达式主要表示数学运算等内容\nexpr: \n  number \n  variable \n  expr arith-op expr # arith-op is + - less * / mod div ˆ ** \n  unary-op expr # unary-op is `+`, `-` \n  built-in( exprlist ) # built-in is `sin` `cos` ...\n  if lexpr then expr [ else expr ] \n  reduction-op indexing expr # reduction-op is sum prod max min \n  ( expr )\n例子reduction op + indexing expr + (expr binary-op expr):\nsum {i in Prod} cost[i] * Make[i]"
  },
  {
    "objectID": "posts/ampl-learn.html#logical-expressions",
    "href": "posts/ampl-learn.html#logical-expressions",
    "title": "Ampl学习",
    "section": "Logical expressions",
    "text": "Logical expressions\n逻辑表达式是需要返回true或false的表达式\nlexpr: \n  expr \n  expr compare-op expr # compare-op is &lt; &lt;= = == != &lt;&gt; &gt; &gt;= \n  lexpr logic-op lexpr # logic-op is or || and && \n  not lexpr \n  member in sexpr \n  member not in sexpr \n  sexpr within sexpr \n  sexpr not within sexpr \n  opname indexing lexpr # opname is `exists` or `forall` \n  ( lexpr )"
  },
  {
    "objectID": "posts/ampl-learn.html#set-expressions",
    "href": "posts/ampl-learn.html#set-expressions",
    "title": "Ampl学习",
    "section": "Set expressions",
    "text": "Set expressions\nset表达式专门用于构造set\nsexpr: \n  { [ member [ , member . . . ] ] }\n  sexpr set-op sexpr # set-op is `union`, `diff`, `symdiff`, `inter`, `cross` \n  opname indexing sexpr # opname is `union` or `inter` \n  expr .. expr [ by expr ] # .. 表示线性增长, 可以添加by来指定stride\n  setof indexing member  #  member是指set元素构造, setof也就是利用indexing来构造包含有member的set.\n  if lexpr then sexpr else sexpr \n  ( sexpr ) \n  interval \n  infinite-set\n  indexing # indexing表达式也属于set expr的一种\ninterval: \n  interval [ a , b ] #  {x: a ≤ x ≤ b}\n  interval ( a , b ] #  {x: a &lt; x ≤ b}\n  interval [ a , b ) #  {x: a ≤ x &lt; b}\n  interval ( a , b ) #  {x: a &lt; x &lt; b}\n  integer [ a , b ] #  {x: a ≤ x ≤ b and x ∈ I}\n  integer ( a , b ] #  {x: a &lt; x ≤ b and x ∈ I}\n  integer [ a , b ) #  {x: a ≤ x &lt; b and x ∈ I}\n  integer ( a , b ) #  {x: a &lt; x &lt; b and x ∈ I}\n例子:\nampl: set y = setof {i in 1..5} (i,iˆ2);\nampl: display y; \nset y := (1,1) (2,4) (3,9) (4,16) (5,25);"
  },
  {
    "objectID": "posts/ampl-learn.html#set-declarations",
    "href": "posts/ampl-learn.html#set-declarations",
    "title": "Ampl学习",
    "section": "Set declarations",
    "text": "Set declarations\n\\[\n\\begin{aligned}\nset\\ name\\ alias_{opt}\\ indexing{opt}\\ attributes{opt};\n\\end{aligned}\n\\]\n这里attributes:\nattribute: \n  dimen n \n  within sexpr \n  = sexpr \n  default sexpr"
  },
  {
    "objectID": "posts/ampl-learn.html#parameter-declarations",
    "href": "posts/ampl-learn.html#parameter-declarations",
    "title": "Ampl学习",
    "section": "Parameter declarations",
    "text": "Parameter declarations\n\\[\n\\begin{aligned}\nparam\\ name\\ alias_{opt}\\ indexing{opt}\\ attributes{opt};\n\\end{aligned}\n\\]\nattribute: \n  binary \n  integer \n  symbolic \n  relop expr \n  in sexpr \n  = expr \n  default expr \nrelop: [&lt;, &lt;=, =, ==, !=, &lt;&gt;, &gt;, &gt;=]\n这里symbolic表示可能是字符串或者数值"
  },
  {
    "objectID": "posts/ampl-learn.html#variable-declarations",
    "href": "posts/ampl-learn.html#variable-declarations",
    "title": "Ampl学习",
    "section": "Variable declarations",
    "text": "Variable declarations\n\\[\n\\begin{aligned}\nvar\\ name\\ alias_{opt}\\ indexing{opt}\\ attributes{opt};\n\\end{aligned}\n\\]\nattribute: \n  binary \n  integer \n  symbolic \n  &gt;= expr\n  &lt;= expr \n  := expr \n  default expr \n  = expr \n  coeff indexing_opt constraint expr\n  cover indexing_opt constraint\n  obj indexing_opt objective expr\n  in sexpr \n  suffix sufname expr"
  },
  {
    "objectID": "posts/ampl-learn.html#set的理解",
    "href": "posts/ampl-learn.html#set的理解",
    "title": "Ampl学习",
    "section": "set的理解",
    "text": "set的理解\n如果对应python中的概念, 实际ampl中的set应该是一个展平的list, list中的元素为tuple, 这个tuple就是member, set的维度指的是member这个tuple的维度. 比如这个例子, set就是多个2维的tuple组成的list, 多维的var或者param实际上就是将set A的元素作为key来索引:\nap.eval('set A = {1..2, 2..3};')\nap.display('A')\nset A := (1,2) (1,3) (2,2) (2,3);\n\nap.eval('var B {A} binary;')\nap.display('B')\nB :=\n1 2   0\n1 3   0\n2 2   0\n2 3   0"
  },
  {
    "objectID": "posts/ampl-learn.html#下标索引",
    "href": "posts/ampl-learn.html#下标索引",
    "title": "Ampl学习",
    "section": "下标索引",
    "text": "下标索引\n只用通过set构造出来的var和param才能使用下标索引, 对于set本身, 需要进行排序后通过内置函数进行访问, 而且ordered只支持1维的set.\nap.eval('set D = {3..1 by -1} ordered;')\nset D := 3 2 1;\nap.display('first(D)')\nfirst(D) = 3"
  },
  {
    "objectID": "posts/ampl-learn.html#数据加载",
    "href": "posts/ampl-learn.html#数据加载",
    "title": "Ampl学习",
    "section": "数据加载",
    "text": "数据加载\n对于参数来说, 指定十分麻烦, 但是还好ampl的python api提供了通过list/dict/pandas的方式来辅助指定参数:\nap.eval(\"param domain { 1..3 };\")\nap.param['domain'] = [2048, 384, 8192]\nap.display('domain')\ndomain [*] :=\n1  2048\n2   384\n3  8192\n;"
  },
  {
    "objectID": "posts/ampl-learn.html#中间变量定义",
    "href": "posts/ampl-learn.html#中间变量定义",
    "title": "Ampl学习",
    "section": "中间变量定义",
    "text": "中间变量定义\n通常我们会定义一个中间变量为别的计算计算的结果, 实际上这些中间变量也都是通过约束来表示的, 在ortools里面是给我们封装好了, 但是在ampl中需要分两步定义:\nvar loadA integer;\nsubject to LoadA_c : loadA = m * k;"
  },
  {
    "objectID": "posts/argparse-err.html",
    "href": "posts/argparse-err.html",
    "title": "使用argparse解析Bool型的坑",
    "section": "",
    "text": "今天又碰到一个坑…使用argparse解析bool型参数返回值总是true.\n\n\n问题出现\n首先我使用以下进行参数解析\nparser.add_argument('--train_classifier',           type=bool,  help='wether train the classsifier',    default=False)\n在使用时指定\n--train_classifier False\n得到参数都是True\n\n\n问题分析\n这里是因为argparse库对于bool型参数是这样控制的:\n--train_classifier\n使用这个选项即打开.\n\n\n问题解决\nparser.add_argument('--train_classifier',           type=str,   help='wether train the classsifier',    choices=['True', 'False'], default='False')\n现在使用选项的方式来控制bool型"
  },
  {
    "objectID": "posts/august.html",
    "href": "posts/august.html",
    "title": "8月总结&淋巴结发炎预防与治疗",
    "section": "",
    "text": "感觉生了个病八月份就要过去了。。。难受。生病的就很想休息，一点也不想学习，然而一休息就半个月没了😔，又感觉自己浪费了宝贵的时间。这次我主要总结一下生病的情况，因为其实从过年开始就有症状了，但后面去医院医生居然看不出我是什么病，导致暑假里面吃了不该吃的东西让病情严重了。"
  },
  {
    "objectID": "posts/august.html#起因",
    "href": "posts/august.html#起因",
    "title": "8月总结&淋巴结发炎预防与治疗",
    "section": "起因",
    "text": "起因\n\n2月，今年过年的时候，左脚大拇指以及脚背处突然长了冻疮，这是最直接的原因，但是在家里我没有及时的泡脚来治疗。\n5月，冻疮看起来结束之后，但是大拇指处一直还是红肿的情况，这时候左脚又感染了湿疹，通过涂药好转了一些。\n8月，放假吃了很多龙虾、虾、牛肉等，鲜的和燥性食物进一步刺激了感染处，导致红肿部位扩展到了小腿中部。"
  },
  {
    "objectID": "posts/august.html#症状",
    "href": "posts/august.html#症状",
    "title": "8月总结&淋巴结发炎预防与治疗",
    "section": "症状",
    "text": "症状\n\n2月，冻疮红肿的程度其实还是比较严重的，属于大拇指一弯就疼的程度。\n5月，大拇指关节处与脚背处皮肤内有青豆大小的“小疙瘩”，用手指用力按压会比较疼痛。在一次聚餐后，忽然肿胀变大，因为聚餐喝酒了并且大拇指关节处肿大，校医认为我是痛风。去医院我首先挂皮肤科，医生也让我查了尿酸，没有问题。然后医生说肿胀可能在骨头处，我又去了骨科，直接拍了x光，还是没有问题。接下来又挂了外科，外科医生说你这个疙瘩很小，拍ct也不一定看得出来是什么问题，不如等再严重点再来。我内心😥，因为再查也是浪费钱，我就没有继续检查了。\n8月，第一次吃了龙虾之后，左脚右侧血管处出现很大一个疙瘩，但是我没有去看病，只是偶尔的揉揉。第二次连吃两天的虾，直接从脚踝到小腿中部的一条血管都肿起来了，腿没法伸直，站起来很痛，之后开始治疗。"
  },
  {
    "objectID": "posts/august.html#治疗",
    "href": "posts/august.html#治疗",
    "title": "8月总结&淋巴结发炎预防与治疗",
    "section": "治疗",
    "text": "治疗\n\n第一次，挂了两天消炎药，感觉略有好转，然后吃了一次牛肉直接打回原形。\n第二次，又挂了两天消炎药，扎针放血，感觉都基本快好了，并且也没有吃噪性食物，但晚饭吃了一顿排骨又把我送走了。。。\n第三次，依旧挂了两天消炎药，外加抹药、泡脚，到现在我还是不敢吃任何肉。"
  },
  {
    "objectID": "posts/backtracking.html",
    "href": "posts/backtracking.html",
    "title": "回溯法",
    "section": "",
    "text": "回溯法其实质就是一个带减枝的DFS过程。 有时会遇到这样一类题目，它的问题可以分解，但是又不能得出明确的动态规划或是递归解法，又需要对整个问题进行遍历才可以得到答案时。 回溯问题一般有如下三种："
  },
  {
    "objectID": "posts/backtracking.html#构造解空间树",
    "href": "posts/backtracking.html#构造解空间树",
    "title": "回溯法",
    "section": "1. 构造解空间树",
    "text": "1. 构造解空间树\n我们以第一种问题为例，假设构造出了如下的解空间树  我们需要对这颗二叉树进行遍历。"
  },
  {
    "objectID": "posts/backtracking.html#找到约束条件",
    "href": "posts/backtracking.html#找到约束条件",
    "title": "回溯法",
    "section": "2. 找到约束条件",
    "text": "2. 找到约束条件\n我们在遍历问题较小的上，比较容易。但是一但问题的规模大到了一种程度，就不可能完全遍历整颗二叉树。所以我们需要些约束条件去限制我们的遍历。这就是约束条件。"
  },
  {
    "objectID": "posts/backtracking.html#带入回溯框架",
    "href": "posts/backtracking.html#带入回溯框架",
    "title": "回溯法",
    "section": "3. 带入回溯框架",
    "text": "3. 带入回溯框架\nboolean solve(Node n) {\n    if n is a leaf node {\n        if the leaf is a goal node, return true  # 减枝\n        else return false\n    } else {\n        for each child c of n {\n            if solve(c) succeeds, return true  # 递归\n        }\n        return false\n    }\n}"
  },
  {
    "objectID": "posts/backtracking.html#构造解空间树-1",
    "href": "posts/backtracking.html#构造解空间树-1",
    "title": "回溯法",
    "section": "1. 构造解空间树",
    "text": "1. 构造解空间树\n首先我们可以想到，每移动一步，这个问题就是一个新的状态。但是这个问题的图像难以构造，较为复杂。"
  },
  {
    "objectID": "posts/backtracking.html#找到约束条件-1",
    "href": "posts/backtracking.html#找到约束条件-1",
    "title": "回溯法",
    "section": "2. 找到约束条件",
    "text": "2. 找到约束条件\n此问题的约束条件很明显是是否无法移动，当无法移动时此节点就为叶节点，需要返回上一层。"
  },
  {
    "objectID": "posts/backtracking.html#带入回溯框架-1",
    "href": "posts/backtracking.html#带入回溯框架-1",
    "title": "回溯法",
    "section": "3. 带入回溯框架",
    "text": "3. 带入回溯框架\n根据上面的回溯框架带入如下。\nbool solvable(int board[]) {\n    if (puzzleSolved(board)) { return true; }                   /* 是否结束 */   \n    for (int position= 0; position &lt; BOARD_SIZE; position++) {  /* DFS */\n        if (canMove(board, position)) {                         /* 剪枝 */\n            int *newBoard= makeMove(board, position);           /* 生成新状态 */\n            if (solvable(newBoard)) {                           /* 递归 */\n                printBoard(newBoard);                           /* 输出状态 */\n                return true;\n            }\n        }\n    }\n    return false;\n}"
  },
  {
    "objectID": "posts/backtracking.html#完整程序",
    "href": "posts/backtracking.html#完整程序",
    "title": "回溯法",
    "section": "4.完整程序",
    "text": "4.完整程序\n#include &lt;cstring&gt;\n#include &lt;iostream&gt;\n#include &lt;unistd.h&gt;\nusing namespace std;\n#define Dcout(x) cout &lt;&lt; #x &lt;&lt; \": \" &lt;&lt; (x) &lt;&lt; endl\n\n#define BOARD_SIZE 7\n#define EMPTY -1\n#define WHITE 2\n#define BLACK 1\n\nint FIRST_BOARD[BOARD_SIZE]= {BLACK, BLACK, BLACK, EMPTY, WHITE, WHITE, WHITE};\nint FINAL_BOARD[BOARD_SIZE]= {WHITE, WHITE, WHITE, EMPTY, BLACK, BLACK, BLACK};\n\nbool puzzleSolved(int board[]);\nbool canMove(int board[], int position);\nint *makeMove(int oldBoard[], int position);\nvoid printBoard(int board[]);\n\nbool solvable(int board[]) {\n    if (puzzleSolved(board)) { return true; }\n    for (int position= 0; position &lt; BOARD_SIZE; position++) {\n        if (canMove(board, position)) {\n            int *newBoard= makeMove(board, position);\n            if (solvable(newBoard)) {\n                printBoard(newBoard);\n                return true;\n            }\n        }\n    }\n    return false;\n}\n\nbool puzzleSolved(int board[]) {\n    for (int i= 0; i &lt; BOARD_SIZE; ++i) {\n        if (board[i] != FINAL_BOARD[i]) { return false; }\n    }\n    return true;\n}\nvoid printBoard(int board[]) {\n    for (int i= 0; i &lt; BOARD_SIZE; ++i) {\n        printf(\"%s \", [&board, &i]() -&gt; const char * {\n            switch (board[i]) {\n            case EMPTY:\n                return \"_\";\n                break;\n            case BLACK:\n                return \"●\";\n                break;\n            case WHITE:\n                return \"○\";\n                break;\n            default:\n                break;\n            }\n            return \"\";\n        }());\n    }\n    printf(\"\\n\");\n}\nint *makeMove(int oldBoard[], int position) {\n    int *newboard= new int[BOARD_SIZE];\n    memcpy(newboard, oldBoard, sizeof(int) * BOARD_SIZE);\n    switch (oldBoard[position]) {\n    case WHITE: {\n        newboard[position]= EMPTY;\n        if (newboard[position - 1] == EMPTY) {\n            newboard[position - 1]= WHITE;\n        } else {\n            newboard[position - 2]= WHITE;\n        }\n    } break;\n    case BLACK: {\n        newboard[position]= EMPTY;\n        if (newboard[position + 1] == EMPTY) {\n            newboard[position + 1]= BLACK;\n        } else {\n            newboard[position + 2]= BLACK;\n        }\n    } break;\n\n    default:\n        break;\n    }\n    // delete[] oldBoard;\n    return newboard;\n}\nbool canMove(int board[], int position) {\n    switch (board[position]) {\n    case EMPTY: {\n        return false;\n    } break;\n    case BLACK: {\n        if (board[position + 1] == EMPTY || board[position + 2] == EMPTY) {\n            return true;\n        } else {\n            return false;\n        }\n    } break;\n    case WHITE: {\n        if (board[position - 1] == EMPTY || board[position - 2] == EMPTY) {\n            return true;\n        } else {\n            return false;\n        }\n    } break;\n\n    default:\n        break;\n    }\n    return false;\n}\n\nint main(int argc, char const *argv[]) {\n    int *first_board= new int[BOARD_SIZE];\n    for (int i= 0; i &lt; BOARD_SIZE; ++i) { first_board[i]= FIRST_BOARD[i]; }\n    solvable(first_board);\n    printBoard(FIRST_BOARD);\n    return 0;\n}"
  },
  {
    "objectID": "posts/backtracking.html#程序结果",
    "href": "posts/backtracking.html#程序结果",
    "title": "回溯法",
    "section": "5.程序结果",
    "text": "5.程序结果\n➜  Backtracking ./backtrack\n○ ○ ○ _ ● ● ●\n○ ○ ○ ● _ ● ●\n○ ○ _ ● ○ ● ●\n○ _ ○ ● ○ ● ●\n○ ● ○ _ ○ ● ●\n○ ● ○ ● ○ _ ●\n○ ● ○ ● ○ ● _\n○ ● ○ ● _ ● ○\n○ ● _ ● ○ ● ○\n_ ● ○ ● ○ ● ○\n● _ ○ ● ○ ● ○\n● ● ○ _ ○ ● ○\n● ● ○ ● ○ _ ○\n● ● ○ ● _ ○ ○\n● ● _ ● ○ ○ ○\n● ● ● _ ○ ○ ○\n大家不要以为我这里出错误了。我是在程序返回的时候输出，因为这是一个DFS遍历，所以我的状态输出是先从深到浅的。\n动画演示："
  },
  {
    "objectID": "posts/bilinearinter.html",
    "href": "posts/bilinearinter.html",
    "title": "图像缩放-双线性插值",
    "section": "",
    "text": "需要在k210中实现图像的裁剪缩放,所以就写了个双线性插值的程序进行处理.写好了记录一下.\n\n\n理论\n双线性插值只涉及到邻近的4个像素点，如图:\n\n通过一个比例系数算出最终图像的(distI,distJ)在源图像中的具体位置,这个位置肯定会有在两个像素点中间,这个时候就可以通过双线性插值的方式确定像素值.\n\\[\n\\begin{aligned}\n    z_1&=(f(i,j+1)-f(i,j))*u+f(i,j) \\\\\n    z_2&=(f(i+1,j+1)-f(i+1,j))*u+f(i+1,j) \\\\\n    z  &=(z_2-z_1)*v+z_1 \\\\\n    \\\\\n    f(x,y) &: x,y对应位置的像素值 \\\\\n    u  &: 映射像素在横向两个像素之间的位置 \\\\\n    v  &: 映射像素在纵向两个像素之间的位置\n\\end{aligned}\n\\]\nNOTE: 这里其实先算\\(u\\)还是\\(v\\)都没有关系,但是要注意先算\\(v\\)的话需要纵向相减.还有放大缩小的坐标需要通过\\((i+0.5)*r-0.5\\)来计算,这样才可以保证映射到中心对齐.\n\n\n程序\n两段小程序:\n这是resize rgb888 hwc格式的图像\n/**\n * @brief resize rgb image, image : [old_h,old_h,3]\n *          NOTE : image have to ba rag888\n *\n * @param old_img\n * @param new_img\n * @param old_h\n * @param old_w\n * @param new_h\n * @param new_w\n */\nvoid resize_img(uint8_t *old_img, uint8_t *new_img, int old_h, int old_w, int new_h, int new_w) {\n    if (old_img == nullptr || new_img == nullptr) { return; }\n\n    float rx= (float)old_w / new_w, ry= (float)old_h / new_h;\n    float rgx[new_w], rgy[new_h];\n    int fx0y0, fx0y1, fx1y0, fx1y1;\n    uint x0, y0, x1, y1;\n    uint z1, z2, z, cnl= 3, old_stride= old_w * cnl, new_stride= new_w * cnl;\n    for (size_t i= 0; i &lt; new_w; i++) { rgx[i]= (i + 0.5) * rx - 0.5; }\n    for (size_t i= 0; i &lt; new_h; i++) { rgy[i]= (i + 0.5) * ry - 0.5; }\n\n    for (size_t i= 0; i &lt; new_h; i++) {\n        y0= (uint)rgy[i];\n        y1= (y0 + 1) &lt; old_h ? (y0 + 1) : old_h;\n        for (size_t j= 0; j &lt; new_w; j++) {\n            x0= (uint)rgx[j];\n            x1= (x0 + 1) &lt; old_w ? (x0 + 1) : old_w;\n\n            float u= rgx[j] - x0, v= rgy[i] - y0;\n            for (size_t k= 0; k &lt; cnl; k++) {\n                fx0y0= rgb_image[old_stride * y0 + cnl * x0 + k] &lt;&lt; 11;\n                fx0y1= rgb_image[old_stride * y1 + cnl * x0 + k] &lt;&lt; 11;\n                fx1y0= rgb_image[old_stride * y0 + cnl * x1 + k] &lt;&lt; 11;\n                fx1y1= rgb_image[old_stride * y1 + cnl * x1 + k] &lt;&lt; 11;\n                z1= (uint)((fx1y0 - fx0y0) * u + fx0y0);\n                z2= (uint)((fx1y1 - fx0y1) * u + fx0y1);\n                z= (uint)((z2 - z1) * v + z1);\n                new_img[new_stride * i + cnl * j + k]= (uint8_t)(z &gt;&gt; 11);\n            }\n        }\n    }\n}\n这是resize rgb888 CHW格式的图像\n/**\n * @brief resize k210 ai image, image have to be CHW\n * \n * @param old_img \n * @param new_img \n * @param old_h \n * @param old_w \n * @param new_h \n * @param new_w \n */\nvoid resize_ai_img(uint8_t *old_img, uint8_t *new_img, int old_h, int old_w, int new_h, int new_w) {\n    if (old_img == nullptr || new_img == nullptr) { return; }\n\n    float rx= (float)old_w / new_w, ry= (float)old_h / new_h;\n    float rgx[new_w], rgy[new_h];\n    uint x0, y0, x1, y1;\n    int fx0y0, fx0y1, fx1y0, fx1y1;\n    uint z1, z2, z, stride, new_stride, cnl= 3;\n    float u, v;\n\n    for (size_t i= 0; i &lt; new_w; i++) { rgx[i]= (i + 0.5) * rx - 0.5; }\n    for (size_t i= 0; i &lt; new_h; i++) { rgy[i]= (i + 0.5) * ry - 0.5; }\n\n    for (size_t k= 0; k &lt; cnl; k++) {\n        stride= old_w * old_h * k;\n        new_stride= new_w * new_h * k;\n        for (size_t i= 0; i &lt; new_h; i++) {\n            y0= (uint)rgy[i];\n            y1= (y0 + 1) &lt; old_h ? (y0 + 1) : old_h;\n            for (size_t j= 0; j &lt; new_w; j++) {\n                x0= (uint)rgx[j];\n                x1= (x0 + 1) &lt; old_w ? (x0 + 1) : old_w;\n\n                u= rgx[j] - x0;\n                v= rgy[i] - y0;\n\n                fx0y0= old_img[stride + old_w * y0 + x0] &lt;&lt; 11;\n                fx0y1= old_img[stride + old_w * y1 + x0] &lt;&lt; 11;\n                fx1y0= old_img[stride + old_w * y0 + x1] &lt;&lt; 11;\n                fx1y1= old_img[stride + old_w * y1 + x1] &lt;&lt; 11;\n\n                z1= (uint)((fx1y0 - fx0y0) * u + fx0y0);\n                z2= (uint)((fx1y1 - fx0y1) * u + fx0y1);\n                z= (uint)((z2 - z1) * v + z1);\n                new_img[new_stride + new_w * i + j]= (uint8_t)(z &gt;&gt; 11);\n            }\n        }\n    }\n}\n\n\n效果"
  },
  {
    "objectID": "posts/boolexp.html",
    "href": "posts/boolexp.html",
    "title": "Boolean Expressions",
    "section": "",
    "text": "The objective of the program you are going to produce is to evaluate boolean expressions as the one shown next:\n\nExpression: ( V | V ) & F & ( F | V )\n\nwhere V is for True, and F is for False. The expressions may include the following operators: ! for not , & for and, | for or , the use of parenthesis for operations grouping is also allowed.\nTo perform the evaluation of an expression, it will be considered the priority of the operators, the not having the highest, and the or the lowest. The program must yield V or F , as the result for each expression in the input file."
  },
  {
    "objectID": "posts/boolexp.html#描述",
    "href": "posts/boolexp.html#描述",
    "title": "Boolean Expressions",
    "section": "",
    "text": "The objective of the program you are going to produce is to evaluate boolean expressions as the one shown next:\n\nExpression: ( V | V ) & F & ( F | V )\n\nwhere V is for True, and F is for False. The expressions may include the following operators: ! for not , & for and, | for or , the use of parenthesis for operations grouping is also allowed.\nTo perform the evaluation of an expression, it will be considered the priority of the operators, the not having the highest, and the or the lowest. The program must yield V or F , as the result for each expression in the input file."
  },
  {
    "objectID": "posts/boolexp.html#输入",
    "href": "posts/boolexp.html#输入",
    "title": "Boolean Expressions",
    "section": "输入",
    "text": "输入\nThe expressions are of a variable length, although will never exceed 100 symbols. Symbols may be separated by any number of spaces or no spaces at all, therefore, the total length of an expression, as a number of characters, is unknown.\nThe number of expressions in the input file is variable and will never be greater than 20. Each expression is presented in a new line, as shown below."
  },
  {
    "objectID": "posts/boolexp.html#输出",
    "href": "posts/boolexp.html#输出",
    "title": "Boolean Expressions",
    "section": "输出",
    "text": "输出\nFor each test expression, print “Expression” followed by its sequence number, “:”, and the resulting value of the corresponding test expression. Separate the output for consecutive test expressions with a new line.\nUse the same format as that shown in the sample output shown below."
  },
  {
    "objectID": "posts/boolexp.html#样例输入",
    "href": "posts/boolexp.html#样例输入",
    "title": "Boolean Expressions",
    "section": "样例输入",
    "text": "样例输入\n( V | V ) & F & ( F| V)\n!V | V & V & !F & (F | V ) & (!F | F | !V & V)\n(F&F|V|!V&!F&!(F|F&V))"
  },
  {
    "objectID": "posts/boolexp.html#样例输出",
    "href": "posts/boolexp.html#样例输出",
    "title": "Boolean Expressions",
    "section": "样例输出",
    "text": "样例输出\nExpression 1: F\nExpression 2: V\nExpression 3: V"
  },
  {
    "objectID": "posts/build-tensoflow.html",
    "href": "posts/build-tensoflow.html",
    "title": "重新编译Tensorflow",
    "section": "",
    "text": "最近搬砖赚了一个GTX2060,所以这两天就在折腾安装显卡以及安装Tensorflow.下面是我的一个记录.\n\n\n卸载旧驱动与安装新驱动\n首先我要把NVIDIA-390驱动卸载掉,然后安装NVIDIA-418驱动,具体安装流程参考下面这个链接:\n注意:前面都的参考,但是我下载的驱动版本是418,cuda版本为10.0,cudnn为7.5.0.\nhttps://blog.csdn.net/qq_33200967/article/details/80689543\n\n\n编译Tensorflow\n首先参考官方教程,下载什么的我就不再写了,写几个注意点.\n\n安装bazel版本不能超过0.21.0,不然还得重新装\n配置选项尽量参照官方的例子.\n如果内存小于等于8g,那么在编译的时候要加选项限制使用资源\nUbuntu自带的gcc-7.3,所以编译选项要加--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\n编译的时候发生undefined reference to cudnnCreateLRNDescriptor@libcudnn.so.7\n参考:\nsudo touch /etc/ld.so.conf.d/cuda.conf\nsudo nano /etc/ld.so.conf.d/cuda.conf\nAfter edit the file :\n/usr/local/cuda/lib64\nSave the changes :\nsudo ldconfig\n\n\n\n最后\nNOTE: 我安装了新显卡之后,使用Tensorflow必须要添加:\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nwith tf.Session(config=config) as sess:\n然后我的软件版本为tensorflow 1.13.1,cuda 10.0,cudnn 7.5.0,我编译的安装包在这里:(https://github.com/zhen8838/tf-linux-whell)\n                          ./+o+-       zqh@pc\n                  yyyyy- -yyyyyy+      OS: Ubuntu 18.04 bionic\n               ://+//////-yyyyyyo      Kernel: x86_64 Linux 4.15.0-46-generic\n           .++ .:/++++++/-.+sss/`      Uptime: 7h 52m\n         .:++o:  /++++++++/:--:/-      Packages: 2663\n        o:+o+:++.`..```.-/oo+++++/     Shell: zsh 5.4.2\n       .:+o:+o/.          `+sssoo+/    Resolution: 3000x1920\n  .++/+:+oo+o:`             /sssooo.   DE: GNOME \n /+++//+:`oo+o               /::--:.   WM: GNOME Shell\n \\+/+o+++`o++o               ++////.   WM Theme: \n  .++.o+++oo+:`             /dddhhh.   GTK Theme: Adwaita [GTK2/3]\n       .+.o+oo:.          `oddhhhh+    Icon Theme: Adwaita\n        \\+.++o+o``-````.:ohdhhhhh+     Font: Cantarell 11\n         `:o+++ `ohhhhhhhhyo++os:      CPU: Intel Core i7-7700 @ 8x 4.2GHz [27.8°C]\n           .o:`.syhhhhhhh/.oo++o`      GPU: GeForce RTX 2060\n               /osyyyyyyo++ooo+++/     RAM: 4622MiB / 16003MiB\n                   ````` +oo+++o\\:    \n                          `oo++."
  },
  {
    "objectID": "posts/c-strip.html",
    "href": "posts/c-strip.html",
    "title": "Linux下实现",
    "section": "",
    "text": "今天在写NB-iot的程序的时候，发现一个很蛋疼的问题，程序会把接收到的数据printf出来，但是linux下\\n就会换行，所以会出现大段的空白，需要解决一下。"
  },
  {
    "objectID": "posts/c-strip.html#问题描述",
    "href": "posts/c-strip.html#问题描述",
    "title": "Linux下实现",
    "section": "问题描述",
    "text": "问题描述\n当我执行程序时出现以下现象：\n➜  build ./NB_test\nNow Config is :\n     tty_port  : /dev/ttyUSB0\n     baud_rate : 9600\nuart  init success\nPI--&gt;&gt;NB: AT+CGATT=1\nNB--&gt;&gt;PI:\n\nOK\n\nPI--&gt;&gt;NB: AT+CGATT?\nNB--&gt;&gt;PI:\n\n+CGATT:1\n\n\n\nOK\n\nPI--&gt;&gt;NB: AT+CEREG=1\nNB--&gt;&gt;PI:\n\nOK\n\nPI--&gt;&gt;NB: AT+CEREG?\nNB--&gt;&gt;PI:\n\n+CEREG:1,1\n\n\n\nOK\n\nConnected to China Mobile Network.\nq\ngood bye~\n经过寻找，发现问题出现在：\nprintf(\"PI--&gt;&gt;NB: %s\\n\", cmd);\n此处是因为是发送给NB模组的cmd是\\r\\n的，所以printf的时候就会出现问题。\nprintf(\"NB--&gt;&gt;PI: %s\\n\", UART_RX_BUF)\n此处是因为UART_RX_BUF就是有段空白的，所以需要删除空白字符。"
  },
  {
    "objectID": "posts/c-strip.html#问题解决",
    "href": "posts/c-strip.html#问题解决",
    "title": "Linux下实现",
    "section": "问题解决",
    "text": "问题解决\n为了清除多余的空格，我写了几个函数：\n/* @brief 去除字符串左边的空字符\n *\n * */\nchar *lstrip(char *str) {\n    static char out[512]= {0};\n\n    if (NULL == str) return NULL;\n\n    char *tmp= str;\n    int start= 0;\n\n    while (isspace(*tmp++)) start++;\n    int len= strlen(str) - start;\n\n    for (int i= 0; i &lt;= len; ++i) { out[i]= *(str + i + start); }\n\n    return out;\n}\n\n/* @brief 去除字符串右边的空字符\n *\n * */\nchar *rstrip(char *str) {\n    static char out[512]= {0};\n\n    if (NULL == str) return NULL;\n    char *tmp= str;\n    int len= strlen(str) - 1;\n\n    while (isspace(*(str + len))) len--;\n\n    for (int i= 0; i &lt;= len + 1; ++i) { out[i]= *(str + i); }\n    out[len + 2]= '\\0';\n\n    return out;\n}\n\n\n/* @brief 去除字符串两边的空字符\n *\n * */\nchar *strip(char *str) {\n    static char out[512]= {0};\n    \n    if (NULL == str) return NULL;\n\n    char *tmp= str;\n    int len= strlen(str) - 1;\n    int start= 0;\n\n    while (isspace(*tmp++)) start++;\n    while (isspace(*(str + len))) len--;\n\n    for (int i= 0; i &lt;= len - start + 1; ++i) { out[i]= *(str + start + i); }\n    out[len - start + 2]= '\\0';\n    \n    return out;\n}\n然后在后面的程序中进行调用：\nif (isPrintf) printf(\"PI--&gt;&gt;NB: %s\\n\", rstrip(cmd));\nif (isPrintf) printf(\"NB--&gt;&gt;PI: %s\\n\", strip(UART_RX_BUF));\n输出结果为：\n➜  build ./NB_test\nNow Config is :\n     tty_port  : /dev/ttyUSB0\n     baud_rate : 9600\n[ OK.  ] Uart Init\nPI--&gt;&gt;NB: AT+CGATT=1\nNB--&gt;&gt;PI: OK\n\nPI--&gt;&gt;NB: AT+CGATT?\nNB--&gt;&gt;PI: +CGATT:1\n\n\n\nOK\n\nPI--&gt;&gt;NB: AT+CEREG=1\nNB--&gt;&gt;PI: OK\n\nPI--&gt;&gt;NB: AT+CEREG?\nNB--&gt;&gt;PI: +CEREG:1,1\n\n\n\nOK\n\n[ OK.  ] Connected to China Mobile Network."
  },
  {
    "objectID": "posts/cczucmmcc.html",
    "href": "posts/cczucmmcc.html",
    "title": "常大宿舍移动网有线上网",
    "section": "",
    "text": "参考校园网使用指南.\n\n\n下载脚本\n首先进入此处下载.点击图标选择zip下载 \n\n\n解压运行脚本\n如果用deepin的话应该很方便,解压之后.在这个文件夹右键选择在终端打开. \n打开终端列出文件 \n运行脚本,其中生成的就是宽带账号:\n➜  cmcc-fucker ./cmcc-fucker.py 13776822953\n132c7b74dfdb46cda1ab5e25a1cc1f1491510113776822953@internet\n\n\n\n连接网络\n把账号复制下来之后,点deepin的设置 右侧会出现设置栏,选择:\n网络-&gt;DSL-&gt;创建PPPoE连接\n在账号栏输入刚刚生成的账号 密码栏输入你的宽带密码 点击保存连接即可~"
  },
  {
    "objectID": "posts/chimera.html",
    "href": "posts/chimera.html",
    "title": "Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion",
    "section": "",
    "text": "这是zheng size的一篇分析建模的文章，思路和Model Driven Optimization类似，但是细节上有一些差异，简单总结一下。\n\n\nBlock Decomposition\n首先给定一个两个矩阵乘的例子, 下图中隐含了他的reuse dim的计算方式：\n\n# 当前所有循环中最后参与访问的循环内部的循环即为reuse dims\nreuse_dims = perms[last_access_dim(buffer, perms) + 1:]\n比如上图中mnkl的顺序，其中m,k参与访问A， 因此只有l是reuse dim.\n分层是为了单独求解当前层的循环顺序，因为在Model Driven Optimization中说了，如果两个层级，那么他们的循环并不是全部组合，而是分层组合。\n然后每个层在给定的loop order中计算DV，MU，这里的Decomposition parameters S就是tile var。计算DM的逻辑比Model Driven Optimization进行了一些简化，如果是reuse dim就跳过，否则就乘上。 我理解他是认为他后面已经限制了sum(MU) &lt; capacity，所以不存在超过cachesize的问题了。\ninput : Operator chain Ops\ninput : Permutation Perm = (lp1, lp2, ..., lpI )\ninput : Decomposition parameters S = (s1, s2, ..., sI)\noutput : data movement volume DV\noutput : memory usage MU\n\nDV = 0, MU = 0\nfor op in  Ops:\n    total_DF = 0\n    for tensor T in op.allTensors():\n        DF = getFootprint(T, S)\n        total_DF += DF\n        if T in Ops.IOTensors():\n            DM = DF\n            keep_reuse = true\n            for loop lpi in reversed(Perm):\n                if lpi in op.allLoops():\n                    if lpi accesses tensor T:\n                        keep_reuse = false\n                    if not keep_reuse:\n                        DM *= ceil(Lpi / spi)\n            DV += DM\n    for loop lpi in Perm:\n        if lpi is private to op:\n            Perm.erase(lpi)\n    MU = max(MU, total_DF)\nreturn DV, MU\n通过这个算法计算之后，在mlkn的顺序下得到：\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\nDM\n\\(MK \\frac{L}{T_L}\\)\n\\(KL \\frac{M}{T_M}\\)\n0\n$ NL $\nMN \\(\\frac{L}{T_L}\\)\n\n\nDF\n\\(T_MT_K\\)\n\\(T_KT_L\\)\n\\(T_MT_L\\)\n\\(T_LT_N\\)\n\\(T_M T_N\\)\n\n\n\n所有访问到的数据都存在cache中，所以DF是通过access dim的tile来计算，DM这里是全部的access dim乘非reuse的次数。但是我还是有点疑惑，如果数据都在cache中，以MLK的顺序访问A真的会有重复load吗。\n然后在约束MU的情况下最小化DV， \\[\n\\begin{aligned}\nDV_{GEMM\\ Chain} &=D M_{A}+D M_{B}+D M_{C}+D M_{D}+D M_{E} \\\\\n& = M K\\left\\lceil\\frac{L}{T_{L}}\\right\\rceil+K L\\left\\lceil\\frac{M}{T_{M}}\\right\\rceil+N L\\left\\lceil\\frac{M}{T_{M}}\\right\\rceil+M N\\left\\lceil\\frac{L}{T_{L}}\\right\\rceil \\\\\nMU &=max \\left\\{GEMM 1_{M U}, GEMM 2_{M U}\\right\\} \\\\\nGEMM1_{M U} &=D F_{A}+D F_{B}+D F_{C}=T_{M} T_{K}+T_{K} T_{L}+T_{M} T_{L} \\\\\nGEMM 2_{M U} &=D F_{C}+D F_{D}+D F_{E}=T_{M} T_{L}+T_{L} T_{N}+T_{M} T_{N} \\\\\nmin_{\\vec{S}}\\ DV_{GEMM\\ Chain},\\ &s.t. MU \\leq MemoryCapacity (1)\n\\end{aligned}\n\\]\n\n\nOptimization for Multi-level Memory Hierarchy\n对于多内存层级，他的思路与Model Driven Optimization一致，也是假设数据移动可以并行，最小化最大的数据移动cost。\n\\[\n\\begin{gathered} min _{\\vec{S_{1}}, \\vec{S_{2}}, ..., \\vec{S_{D}}}\\left\\{max \\left\\{Cost_{1}\\left(\\vec{S_{1}}\\right), ..., Cost_{D}\\left(\\vec{S_{D}}\\right)\\right\\}\\right\\}, \\\\ s.t. MU_{1} \\leq M C_{1}, ..., MU_{D} \\leq M C_{D} \\end{gathered}\n\\]\n\n\nImplementation\n我尝试对这篇论文做了一下复现, 最后的结果如下, 但感觉搜索到的k还是比较小，循环顺序还是比较合理的，倾向于把k移动到最内层:\nbest perm:  ('m', 'n', 'k'), ('m', 'n', 'k') \nbest tile:  ( 1 ,  1,  512), ( 4,   4,   1), (m:64, k:4, n:16), 180866\nimport itertools\nfrom amplpy import AMPL\n\n\ndef solve_with_perms(all_perms: list[list[str]]):\n    ap = AMPL()\n    ap.eval('reset;')\n\n    ap.eval(\"\"\"\n  var L2_m integer;\n  var L2_k integer;\n  var L2_n integer;\n  var L1_m integer;\n  var L1_k integer;\n  var L1_n integer;\n  var L0_m integer;\n  var L0_k integer;\n  var L0_n integer;\n\n  var L2_A_DF = (L1_m * L0_m) * (L1_k * L0_k) * 4;\n  var L2_B_DF = (L1_n * L0_n) * (L1_k * L0_k) * 4 * 4 * 4;\n  var L2_C_DF = (L1_m * L0_m) * (L1_n * L0_n) * 4 * 4 * 4;\n  var L1_A_DF = (L0_m) * (L0_k) * 4;\n  var L1_B_DF = (L0_n) * (L0_k) * 4 * 4 * 4;\n  var L1_C_DF = (L0_m) * (L0_n) * 4 * 4 * 4;\n\n  \"\"\")\n\n    access = {\n        'A': ['m', 'k'],\n        'B': ['n', 'k'],\n        'C': ['m', 'n']\n    }\n    for level in [2, 1]:\n        for bf in ['A', 'B', 'C']:\n            level_perms = all_perms[len(all_perms) - level]\n\n            params = [f'L{level}_{bf}_DF']\n            axis = len(level_perms) - 1\n            while axis &gt;= 0 and level_perms[axis] not in access[bf]:\n              axis -= 1\n            while axis &gt;= 0:\n              params.append(f'L{level}_{level_perms[axis]}')\n              axis -= 1\n            dm = f'var L{level}_{bf}_DM = {\" * \".join(params)};'\n            ap.eval(dm)\n\n    ap.eval(\"\"\"\n  subject to L2_m_c: L2_m &gt;= 1;\n  subject to L2_k_c: L2_k &gt;= 1;\n  subject to L2_n_c: L2_n &gt;= 1;\n  subject to L1_m_c: L1_m &gt;= 1;\n  subject to L1_k_c: L1_k &gt;= 1;\n  subject to L1_n_c: L1_n &gt;= 1;\n  subject to L0_m_c: L0_m &gt;= 1;\n  subject to L0_k_c: L0_k &gt;= 1;\n  subject to L0_n_c: L0_n &gt;= 1;\n  subject to M_c: (L0_m*L1_m*L2_m) = 256;\n  subject to K_c: (L0_k*L1_k*L2_k) = 2048;\n  subject to N_c: (L0_n*L1_n*L2_n) = 64;\n  var l1_DF = (L1_A_DF + L1_B_DF + L1_C_DF);\n  var l2_DF = (L2_A_DF + L2_B_DF + L2_C_DF);\n  subject to l1_capacity_c: l1_DF &lt;= (512 * 1024);\n  subject to l2_capacity_c: l2_DF &lt;= (1024 * 1024);\n  \"\"\")\n    ap.eval(\"\"\"\n  var mem_cost integer;\n  subject to max_c2: ((L2_A_DM + L2_B_DM + L2_C_DM) / 64) &lt;= mem_cost;\n  subject to max_c1: ((L1_A_DM + L1_B_DM + L1_C_DM) / 16) &lt;= mem_cost;\n  minimize total_cost: mem_cost;\n  \"\"\")\n\n    ap.solve(solver='couenne') \n\n    L2_m = ap.get_value('L2_m')\n    L2_k = ap.get_value('L2_k')\n    L2_n = ap.get_value('L2_n')\n    L1_m = ap.get_value('L1_m')\n    L1_k = ap.get_value('L1_k')\n    L1_n = ap.get_value('L1_n')\n    L0_m = ap.get_value('L0_m')\n    L0_k = ap.get_value('L0_k')\n    L0_n = ap.get_value('L0_n')\n    total_cost = ap.get_value('total_cost')\n    l1_DF = ap.get_value('l1_DF')\n    l2_DF = ap.get_value('l2_DF')\n    print(\"all_perms:\", all_perms, 'L2_m', L2_m, 'L2_k', L2_k, 'L2_n', L2_n, 'L1_m', L1_m, 'L1_k',\n          L1_k, 'L1_n', L1_n, 'L0_m', L0_m, 'L0_k', L0_k, 'L0_n', L0_n, 'total_cost', total_cost, \n          'l1_DF', l1_DF,\n          'l2_DF', l2_DF)\n    return (L2_m, L2_k, L2_n, L1_m, L1_k, L1_n, L0_m, L0_k, L0_n, total_cost)\n\n\nif __name__ == \"__main__\":\n    max_cost = 9999999\n    best_res = None\n    best_perm = None\n    for l2 in itertools.permutations(['m', 'k', 'n'], 3):\n        for l1 in itertools.permutations(['m', 'k', 'n'], 3):\n            res = solve_with_perms([l2, l1])\n            if res[-1] &lt; max_cost:\n                max_cost = res[-1]\n                best_res = res\n                best_perm = (l2, l1)\n    print(\"best perm: \", best_perm, \"best tile: \", best_res)"
  },
  {
    "objectID": "posts/clash-linux.html",
    "href": "posts/clash-linux.html",
    "title": "Ubuntu配置clash",
    "section": "",
    "text": "最近我开的vultr的vps好像全挂了，ip貌似被用滥了，所以使用机场了。现在用的这个一年dlercloud的一年288的套餐还挺贵的=_=。window下配置比较简单，下载官方提供的clash-win然后直接登陆账号密码就完事了，Ubuntu下面真的难倒我了，下面说下怎么配置。\n\n\n下载Clashy\n\n因为clash配置实在麻烦，所以用这个，在releases下个deb的安装包安装即可。\n\n导入配置，打开网址复制clash的订阅连接，并黏贴到Profiles中。记得保存之后要等一会，他的反应比较慢。。\n\n\n\n设置端口\n\n我发现他的设置端口并不能修改config文件里面的内容，所以只能参考你当前使用的config文件进行设置。\n\n\n设置系统代理\n\nClashy里面设置了set as system proxy但是好像没什么用，所以我手动给系统设置了代理地址。\n\n\n选择服务节点\n\n到上一步应该就没问题了，这里还可以选择一下各种节点我给好评。\n\n\nerror: unsupported rule type RULE-SET\n手贱更新了一下订阅地址，clashy就不支持新的配置文件了，这是因为clashy使用的clash版本较低。接着我去clash的项目主页中下载了clash v1.1也不行，最后发现原来clash作者认为RULE-SET实现的不好，主版本并没有给予支持。\n\n下载clash Premium，解压\n从机场下载对应的订阅配置文件config.yaml，机场的连接一般如https://dler.cloud/subscribe/xxxxxx?clash=ss，点击连接即可下载。\n将配置文件与可执行文件放在同一个目录，终端执行./clash-linux-amd64-2020.08.16 -d .，将会自动下一个Country.mmdb文件，这个文件如果下载不下来就去网络找一个。\n软件启动后打开http://clash.razord.top/进行配置，首次进入时的外部控制设置根据我们的config.yaml中external-controller项进行设置。\n我们可以在代理中选择对应的节点，然后设置-代理模式我这里需要选择脚本才可以，以前应该不需要的。\n设置系统代理配置，对于http代理与socks代理，ip设置为127.0.0.1，端口号参考config.yaml中的port和socks-port，最后ignoreed host设置为 localhost, 127.0.0.0/8, ::1即可。\n应该可以正常访问外网了。"
  },
  {
    "objectID": "posts/cmake-error.html",
    "href": "posts/cmake-error.html",
    "title": "cmake踩坑&爬坑",
    "section": "",
    "text": "最近实习了，每天都要和cmake打交道，记录一些时常要用的东西和遇到的问题。"
  },
  {
    "objectID": "posts/cmake-error.html#cmake-最小模板",
    "href": "posts/cmake-error.html#cmake-最小模板",
    "title": "cmake踩坑&爬坑",
    "section": "cmake 最小模板",
    "text": "cmake 最小模板\ncmake_minimum_required(VERSION 3.9)\nproject(xxx)\n \n \n#设定编译参数\nset(CMAKE_CXX_STANDARD 11)\n \n#设定源码列表.cpp\naux_source_directory(&lt;dir&gt; &lt;variable&gt;)\n#比如:aux_source_directory(${CMAKE_SOURCE_DIR} DIR)  \n\n \n#设定头文件路径\ninclude_directories(../include/)\n#include_directories(\"路径1\"  “路径2”...)\n \n \n#设定链接库的路径（一般使用第三方非系统目录下的库）\nlink_directories(../build/)\n#link_directories(\"路径1\"  “路径2”...)\n \n \n#添加子目录,作用相当于进入子目录里面，展开子目录的CMakeLists.txt\n#同时执行，子目录中的CMakeLists.txt一般是编译成一个库，作为一个模块\n#在父目录中可以直接引用子目录生成的库\n#add_subdirectory(math)\n \n \n#生成动/静态库\n#add_library(动/静态链接库名称  SHARED/STATIC(可选，默认STATIC)  源码列表)\n#可以单独生成多个模块\n \n \n#生成可执行文件\nadd_executable(myLevealDB   ${SOURCE_FILES} )\n#比如：add_executable(hello_world    ${SOURCE_FILES})\n \n \ntarget_link_libraries(xxx pthred glog)#就是g++ 编译选项中-l后的内容，不要有多余空格\n \n# ADD_CUSTOM_COMMAND( #执行shell命令\n#           TARGET myLevelDB \n#           POST_BUILD #在目标文件myLevelDBbuild之后，执行下面的拷贝命令，还可以选择PRE_BUILD命令将\n# 会在其他依赖项执行前执行  PRE_LINK命令将会在其他依赖项执行完后执行  POST_BUILD命令将会在目# 标构建完后执行。\n#           COMMAND cp ./myLevelDB  ../\n# )"
  },
  {
    "objectID": "posts/cmake-error.html#add_subdirectory",
    "href": "posts/cmake-error.html#add_subdirectory",
    "title": "cmake踩坑&爬坑",
    "section": "add_subdirectory",
    "text": "add_subdirectory\n用add_subdirectory可以很方便的控制是否编译子目录，比如我们将测试代码放到test中，在根目录的cmakelist里面选择是否编译单元测试文件。"
  },
  {
    "objectID": "posts/cmake-error.html#利用通配符找到源文件",
    "href": "posts/cmake-error.html#利用通配符找到源文件",
    "title": "cmake踩坑&爬坑",
    "section": "利用通配符找到源文件",
    "text": "利用通配符找到源文件\nfile(GLOB SOURCE_FILES ${CMAKE_SOURCE_DIR}/test_*.cpp)\nadd_executable(cpp_test ${SOURCE_FILES})"
  },
  {
    "objectID": "posts/cmake-error.html#macro使用",
    "href": "posts/cmake-error.html#macro使用",
    "title": "cmake踩坑&爬坑",
    "section": "macro使用",
    "text": "macro使用\n\n\n\n变量\n说明\n\n\n\n\nARGV#\n# 是一个下标，0 指向第一个参数，累加\n\n\nARGV\n所有的定义时要求传入的参数\n\n\nARGN\n定义时要求传入的参数以外的参数列表\n\n\nARGC\n传入的实际参数的个数，也就是调用函数是传入的参数个数\n\n\n\n当要收集额外参数的时候，不能直接在宏的入口处写参数，因为那里的参数是必须传入的，只能直接传入然后用if进行解析。\nmacro(add_code name)\n    if(${ARGC} GREATER 2)\n        set(gen_name ${ARGV1})\n        set(gen_variable ${ARGV2})\n    else()\n        set(gen_name ${name})\n        set(gen_variable \"\")\n    endif()\n    add_custom_target(gen_${name} \n        COMMAND ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/gen_all -g halide_${name} -n halide_${gen_name} -o ${CMAKE_CURRENT_SOURCE_DIR}/kernels -e c_header,assembly,schedule,stmt_html target=host-no_asserts-no_bounds_query \"${gen_variable}\")\n    add_dependencies(gen_${name} gen_all)\nendmacro()"
  },
  {
    "objectID": "posts/cmake-error.html#空格被转义",
    "href": "posts/cmake-error.html#空格被转义",
    "title": "cmake踩坑&爬坑",
    "section": "空格被转义",
    "text": "空格被转义\n我想输入一个参数为\"kernel_width=3 kernel_height=3\"然后执行命令，但是我发现执行命令的时候变成了这样：\nkernel_width=3\\ kernel_height=3\n这个是因为cmake为了区别于他多个变量赋值的问题，我们可以用两个方式解决\nset(testfiles \"test1\" \"test2\") \\\\ 1. 直接多个变量赋值\nset(testfiles \"kernel_width=3;kernel_height=3\") \\\\ 2. 使用分号隔开"
  },
  {
    "objectID": "posts/cmake-error.html#检查正确性",
    "href": "posts/cmake-error.html#检查正确性",
    "title": "cmake踩坑&爬坑",
    "section": "检查正确性",
    "text": "检查正确性\n有时候我们可能需要检查cmake的内部变量是不是有错误，要加message比较麻烦，我们可以这样：\nmake VERBOSE=1"
  },
  {
    "objectID": "posts/cmake-error.html#为某个文件添加依赖自定义生成",
    "href": "posts/cmake-error.html#为某个文件添加依赖自定义生成",
    "title": "cmake踩坑&爬坑",
    "section": "为某个文件添加依赖，自定义生成",
    "text": "为某个文件添加依赖，自定义生成\n使用custom command，然后把生成的src添加到一个新的的target中，然后目标target依赖这个target。 要注意，我们生成的源文件不能直接加到目标target中，因为一开始没有生成代码，会导致cmake config出错，得找找方法去解决一下。\nadd_custom_command(\n  OUTPUT testData.cpp\n  COMMAND reswrap \n  ARGS    testData.src &gt; testData.cpp\n  DEPENDS testData.src \n)\nadd_custom_target(evaluator_k510.update SOURCES ${KERNEL_SRCS})\nadd_dependencies(evaluator_k510 evaluator_k510.update)\n如果是在没有依赖，那么可以自己加个dummy.cpp然后强制把需要生成的文件给添加上"
  },
  {
    "objectID": "posts/cmake-error.html#cmake默认target输出位置",
    "href": "posts/cmake-error.html#cmake默认target输出位置",
    "title": "cmake踩坑&爬坑",
    "section": "cmake默认target输出位置",
    "text": "cmake默认target输出位置\n在编译时通过add_executable()生成的可执行文件，以及利用add_library( SHARED )的文件，默认输出到CMAKE_RUNTIME_OUTPUT_DIRECTORY，通常是build/bin目录。"
  },
  {
    "objectID": "posts/cmake-error.html#cmake导出pseudo-target并进行调用",
    "href": "posts/cmake-error.html#cmake导出pseudo-target并进行调用",
    "title": "cmake踩坑&爬坑",
    "section": "cmake导出pseudo target，并进行调用",
    "text": "cmake导出pseudo target，并进行调用\n我首先用add_custom_command生成了一个lib，但是这个lib不是cmake构建的，所以不能直接被cmake调用，并且如果他是一个lib，必须要先写个add_library制作一个伪目标，然后再把他真实的lib添加上去，然后才能正常使用这个target。\nadd_library(hkg_${os_name}_runtime INTERFACE)\ntarget_link_libraries(hkg_${os_name}_runtime INTERFACE ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}/halide_runtime_${os_name}.${suffix} -ldl)\nadd_library(hkg::${os_name}_runtime ALIAS hkg_${os_name}_runtime)\n更加复杂的例子在这里https://cmake.org/cmake/help/latest/manual/cmake-buildsystem.7.html#interface-libraries"
  },
  {
    "objectID": "posts/cmake-error.html#cmake安装target设置其名字",
    "href": "posts/cmake-error.html#cmake安装target设置其名字",
    "title": "cmake踩坑&爬坑",
    "section": "cmake安装target设置其名字",
    "text": "cmake安装target设置其名字\n当我们设置自定义目标的时候，用别名ALIAS可以指定在整个构建过程中的target的使用名字，但是如果这个target需要导出（也就是被第三方库使用时），他的名字还得使用set_target_properties设置一下。\nadd_library(Halide::Generator ALIAS Halide_Generator)\nset_target_properties(Halide_Generator PROPERTIES EXPORT_NAME Generator)"
  },
  {
    "objectID": "posts/cmake-error.html#cmake安装目录路径",
    "href": "posts/cmake-error.html#cmake安装目录路径",
    "title": "cmake踩坑&爬坑",
    "section": "cmake安装目录路径",
    "text": "cmake安装目录路径\ncmake真的处处是坑，如果想install一个目录下的所有东西，那么必须这样写:\n    install(DIRECTORY include/\n            DESTINATION include)\n如果少了一个/安装后就会出现include/include的情况"
  },
  {
    "objectID": "posts/cmake-error.html#cmake函数返回值",
    "href": "posts/cmake-error.html#cmake函数返回值",
    "title": "cmake踩坑&爬坑",
    "section": "cmake函数返回值",
    "text": "cmake函数返回值\ncmake里面是没有函数返回值的，所以必须要在函数中设置变量的值，同时指定他的scope，同时还需要注意设置变量的写法。或者说传递变量的名字然后用${name}的方法完成了c中类似指针的功能🤣。\nFUNCTION(MY_FUNC_WITH_RET ret)\n    # The following line works by accident if the name of variable in the parent\n    # is the same as in the function\n    SET(ret \"in function\" PARENT_SCOPE)\n    # This is the correct way to get the variable name passed to the function\n    SET(${ret} \"in function\" PARENT_SCOPE)\nENDFUNCTION()\n\nSET(ret \"before function\")\nMY_FUNC_WITH_RET(ret)\nMESSAGE(\"output from function = ${ret}\")"
  },
  {
    "objectID": "posts/cmake-error.html#cmake写入多行include",
    "href": "posts/cmake-error.html#cmake写入多行include",
    "title": "cmake踩坑&爬坑",
    "section": "cmake写入多行include",
    "text": "cmake写入多行include\n还是利用configure_file，不过需要自己手动处理比较多的东西\n简单测试，在.h.in文件中添加@linux_include_list@块之后执行cmake，就可以得到正确的include了。\ncmake_minimum_required(VERSION 3.20)\nproject(fuck)\nset(base_name \"conv2d\")\nset(os_name \"linux\")\nset(header_list \"\")\nlist(APPEND feature_list \n    \"avx512\" \n    \"avx2\" \n    \"sse41\" \n    \"bare\")\nlist(APPEND full_feature_list \n    \"-sse41-avx-f16c-fma-avx2-avx512\" \n    \"-avx-avx2-f16c-fma-sse41\"\n    \"-avx-f16c-sse41\"\n    \"\")\n\nforeach(feature full_feature IN ZIP_LISTS feature_list full_feature_list)\n    message(\"${feature} ${full_feature}\")\n    list(APPEND header_list \"${base_name}-${os_name}-${feature}.h\")\nendforeach()\n\nset(linux_include_list \"\")\nforeach(header IN LISTS header_list)\n    set(linux_include_list \"${linux_include_list}\\r\\n#include \\\"${header}\\\"\")\nendforeach()\n\n\n# set(linux_include_list ${header_list})\nset(windows_include_list ${header_list})\nset(osx_include_list ${header_list})\n\n\n\nconfigure_file(src/conv2d.h.in ${CMAKE_SOURCE_DIR}/src/conv2d.h)"
  },
  {
    "objectID": "posts/cmake-error.html#cmake获取target-的library-path",
    "href": "posts/cmake-error.html#cmake获取target-的library-path",
    "title": "cmake踩坑&爬坑",
    "section": "cmake获取target 的library path",
    "text": "cmake获取target 的library path\n我遇到了一个奇怪的bug，就是使用add_library添加了一组link的对象接口之后，被链接的库没有被正确写入到target中，但是cmake又无法调试，所以只能从target中获取属性再打印出来看看，因为我的link library是interface的模式，所以要获取interface的property\nget_target_property(OUT hkg_linux_src INTERFACE_LINK_LIBRARIES)\nmessage(STATUS ${OUT})\n然后我就发现了问题，即我们在编写target_link_libraries的时候，所有的字符串都是raw模式的，也就是说我们不需要加上双引号，我原来的写法如下\ntarget_link_libraries(hkg_linux_src INTERFACE $&lt;BUILD_INTERFACE:\"${LINUX_SRCS}\"&gt;\n    $&lt;INSTALL_INTERFACE:\"${LINUX_SRCS}\"&gt;)\n则他的输出为\n$&lt;BUILD_INTERFACE:\"/home/workspace/kernels_generator/include/hkg/generated_kernels/halide_conv2d_7x7_linux_bare.o\"&gt;\n而这里面的字符串是原封不动的写入到target_link_libraries中的，此时多了的双引号就是引起错误的根源了。\n所以需要这样写，就可以避免这个问题了。\ntarget_link_libraries(hkg_linux_src INTERFACE $&lt;BUILD_INTERFACE:${LINUX_SRCS}&gt;\n    $&lt;INSTALL_INTERFACE:${LINUX_SRCS}&gt;)"
  },
  {
    "objectID": "posts/cmake-error.html#undefined-reference-to-pthread_create",
    "href": "posts/cmake-error.html#undefined-reference-to-pthread_create",
    "title": "cmake踩坑&爬坑",
    "section": "undefined reference to `pthread_create’",
    "text": "undefined reference to `pthread_create’\n这个问题也老奇怪了，明明添加了-lphread但是还是报错，发现要使用-phread才可以。"
  },
  {
    "objectID": "posts/cmake-error.html#macos中的rpath问题",
    "href": "posts/cmake-error.html#macos中的rpath问题",
    "title": "cmake踩坑&爬坑",
    "section": "macos中的rpath问题",
    "text": "macos中的rpath问题\nRPATH就是可执行文件寻找他动态链接库的路径,在linux中,大多数命令都是以以下顺序去搜索动态链接库的\n\nRPATH\nLD_LIBRARY_PATH\nRUNPATH\n/etc/ld.so.conf\nbuiltin directories\n\n通常我们设置LD_LIBRARY_PATH为给可执行程序提供库的路径,但是如果有同时存在多个版本的库都在路径中,同时我们还需要链接不同版本的库,那应该怎么办? 这时候就需要设置RPATH或者RUNPATH,提供一个指定的路径/\n但是macos的情况有些不同,他的连接器 dyld 使用每个 dylib 的完整路径来定位依赖的动态库。\n可执行文件中用完整路径记录他的依赖库:\n❯ otool -L bin/can_use_target.generator\nbin/can_use_target.generator:\n        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1292.100.5)\n        /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 905.6.0)"
  },
  {
    "objectID": "posts/cmake-error.html#conan支持riscv",
    "href": "posts/cmake-error.html#conan支持riscv",
    "title": "cmake踩坑&爬坑",
    "section": "conan支持riscv",
    "text": "conan支持riscv\n我今天才发现Conan build的时候没有按照编译器选择对应的arch,然后看了一下,他们官方居然都没有做rv的支持. 他可选的setting中没有rv这个平台. 最终的cmake中的修改如下:\n    set(TARGET_ARCH x86_64)\n    if(ENABLE_K510_RUNTIME AND BUILDING_TEST)\n        # NOTE you need add `riscv64` in `~/.conan/settings.yml` `arch, arch_build, arch_target` item.\n        # refer from https://github.com/conan-io/cmake-conan/issues/307\n        set(TARGET_ARCH \"riscv64\")\n    endif()\n    \n    conan_cmake_run(BASIC_SETUP\n                    CONANFILE conanfile-runtime.txt\n                    SETTINGS compiler.cppstd=17\n                    BUILD missing\n                    ARCH ${TARGET_ARCH}\n                    ENV CC=${CMAKE_C_COMPILER}\n                    ENV CXX=${CMAKE_CXX_COMPILER}\n                    ENV CFLAGS=${CMAKE_C_FLAGS}\n                    ENV CXXFLAGS=${CMAKE_CXX_FLAGS})\n    include(${CMAKE_BINARY_DIR}/conan_paths.cmake)"
  },
  {
    "objectID": "posts/conan-pkg.html",
    "href": "posts/conan-pkg.html",
    "title": "Conan使用汇总",
    "section": "",
    "text": "记录使用conan时遇到的各种问题."
  },
  {
    "objectID": "posts/conan-pkg.html#注意点",
    "href": "posts/conan-pkg.html#注意点",
    "title": "Conan使用汇总",
    "section": "注意点",
    "text": "注意点\n\n如果需要修改第三方库的源码，可以用patch的形式，调用conan的方法进行修改。\n\n通常都是要在第三方库中添加如下信息的，因为你所有的依赖都是交给conan了，而不是有的通过本机，有的通过Conan。conan会给所有的第三方库都生成一个findxxx.cmake，通过conan_basic_setup去执行，执行完毕后，find_package就可以找到来自与Conan的包了。同时这里的${CONAN_BIN_DIRS}也可以添加，添加之后可以调用预编译lib中的一些可执行文件。\n还有就是有的库写的find_package都是用xxx_DIR的方式去寻找xxxConfig.cmake的，这种方式和conan的行为不一致，需要修改。\n  message(STATUS \"Loading conan scripts for Clang dependencies...\")\n  include(\"${CMAKE_BINARY_DIR}/conanbuildinfo.cmake\")\n  message(STATUS \"Doing conan basic setup\")\n  conan_basic_setup()\n  list(APPEND CMAKE_PROGRAM_PATH ${CONAN_BIN_DIRS})\n  message(STATUS \"Conan setup done. CMAKE_PROGRAM_PATH: ${CMAKE_PROGRAM_PATH}\")\nlist(APPEND CMAKE_PROGRAM_PATH ${CONAN_BIN_DIRS})\n\n对于LLVM来说，最好不能使用BUILD_SHARED_LIBS，因为LLVM依赖于全局数据，这些数据最终可能会在共享库之间复制可能会导致错误。也就是默认是static的。\nconan官方的一些包的recipe里面是把他们的一些cmake文件删除了，比如他的llvm-core只有一堆静态库，这就非常蛋疼了，想依赖这个lib去构建clang是不行的。所以还得自己打包。\n默认的conan的build_folder就是同个目录，他都是打包结束后直接从当前文件夹下面选择性的去拷贝到package_folder下面。当然这个过程可以随便自定义，我觉得直接cmake install到package目录就完事了。\n报错ConanException: llvm-core/12.0.0 package_info(): Package require 'libxml2' not used in components requires\n我用官方的llvm-core就有这个问题，我一开始一直以为是我的问题，然后发现他可能打包的时候就出问题了。conan的设计思路是每个package_info里面提供了每个库的详细信息，像llvm这种库，是由多个组件构成的，为了详细起见，他得把每个component的requires写清楚，所以llvm的打包脚本里面就先用cmake生成依赖关系，然后packeage info里面解析依赖关系添加依赖，这个问题的出现就是因为，明明整个库要求了libxml2，但是里面没有一个component去依赖这个库，那不就说明不需要依赖吗，所以直接报错。\n我找了一下发现libxml2是被LLVMWindowsManifest引用的，但是输出依赖信息：windowsmanifest ['support'] ,并没有添加这个依赖。目前猜想要么是patch没有打上，要么是依赖关系没有生成正确。\n还有就是他的component.json不知道是哪里弄出来的，cmake和conan的文档里面都没有写生成这个文件的地方。\n如果直接include整个导出的llvm-core，会报错找不到一些动态链接库，然后我发现llvm官方打包的二进制里面是没有的。应该是conan生成package info的时候没有删除这些不需要的依赖。\nconan可以生成Conan find package，然后在cmake中调用find package可以找到对应的包\n用打包出来的llvm-core去链接，一直报错找不到一个函数的定义（那个函数里面有个string），各种尝试才发现是conan自己生成的conan_basic_setup里面默认会从系统的profile中读取定义compiler.libcxx=libstdc++，然后设置-D_GLIBCXX_USE_CXX11_ABI=0。但是问题在于我也是用相同的编译器选项去编译llvm的，为什么生成的llvmlib却需要GLIBCXX_USE_CXX11_ABI=1呢？\n先用命令查看一下目前的编译器abi版本，发现默认是cxx11的\ngcc -v 2&gt;&1 | sed -n 's/.*\\(--with-default-libstdcxx-abi=new\\).*/\\1/p'\n--with-default-libstdcxx-abi=new\n在cmake中可以用如下命令查看添加了说明编译定义。\nget_directory_property( DirDefs COMPILE_DEFINITIONS )\nmessage( \"COMPILE_DEFINITIONS = ${DirDefs}\" )\n最后发现是我自己忘记在编译llvm的时候添加上conan basic setup了，导致没有指定。\nlibxmls nanohttp.c:(.text+0x507): undefined reference to 'fcntl64'\n发现conan这东西出发点是好的，但是一定得需要把一个包所有的依赖全部展示清楚才好，上面这个问题就是预编译好的xml2需要的fcntl64包我的系统并没有。以后还是不要搞跨平台了，直接都用docker+linux完事了，没有环境问题。不然再怎么打包都会有奇怪的问题。。 这个问题估计是因为预编译的libxml2的版本ubuntu20的，但是我是ubuntu18的，所以重新编译安装xml2。\nLLVM ERROR: inconsistency in registered CommandLine options\n我不知道为啥编译出来的clang居然是动态链接的，而llvm是静态编译的，从而导致的问题。但是clang的cmake根本就不接受动态链接的编译配置啊，重新configure一下编译就解决了。。\n利用cmake运行Conan，因为很多开发环境是不支持自动执行conan再cmake编译的，这样就不能提供自动补全、直接debug等功能了，所以需要自动化这个流程。但是我们要对开发的库进行conan打包的时候，又需要执行conan的命令，所以conan官方的解决方案是这里https://github.com/conan-io/cmake-conan,对cmake需要做以下修改：\nif(CONAN_EXPORTED) # in conan local cache\n    # standard conan installation, deps will be defined in conanfile.py\n    # and not necessary to call conan again, conan is already running\n    include(${CMAKE_BINARY_DIR}/conanbuildinfo.cmake)\n    conan_basic_setup()\nelse() # in user space\n    include(conan.cmake)\n    # Make sure to use conanfile.py to define dependencies, to stay consistent\n    conan_cmake_configure(REQUIRES fmt/6.1.2 GENERATORS cmake_find_package)\n    conan_cmake_autodetect(settings)\n    conan_cmake_install(PATH_OR_REFERENCE . BUILD missing REMOTE conan-center SETTINGS ${settings})\nendif()\nCONAN_PKG::xxxx这个是Conan setup时候的一个选项，如果添加了TARGETS的选项，使用Conan添加的lib都用统一的接口，但是这样其实不是很好，如果当前开发环境比较混乱的话（引用的包引用了conan的包，但是你只想直接调用，就会报错找不到CONAN_PKG::xxxx），所以要么都设置成相同的依赖，要么cmake里面再多写点。\n想要正确的导出conan包，让他和原来的包一样使用，还是比较非常麻烦。比如halide，原始的halide中有个target叫Halide::Generator，他的属性中包含了GenGen.cpp这个文件。首先如果想在cmake中使用这个target，conan打包就需要把这个component明确导出，但是只要导出一个componet，你所有的requirs都得指定到对应的component。llvm中就是先分析依赖图，然后手动解析，构建出对应的component列表，然后在一个个字符串处理，找到对应的依赖、系统库依赖。上面的事情都做完了，你还是没办法使用conan导出的Halide::Generator，因为conan还不支持为component添加文件属性，真的无语了。"
  },
  {
    "objectID": "posts/conan-pkg.html#conanfile.txt-conanfile.py",
    "href": "posts/conan-pkg.html#conanfile.txt-conanfile.py",
    "title": "Conan使用汇总",
    "section": "conanfile.txt / conanfile.py",
    "text": "conanfile.txt / conanfile.py\n用conanfile.txt可以方便的添加依赖，用conanfile.py的话除了添加依赖，还可以控制更多的内容，同时打包自己的库。\n下面展示一个典型的conanfile.txt，这里generators里面需要写上需要conan生成的东西,CMakeDeps, CMakeToolchain生成出来是为了在cmake中正确find package的。 然后cmake_layout也是必不可少的，有了他才能把生成的文件放到我们需要的位置。\n[requires]\ngtest/1.17.0\nllvm-openmp/20.1.6\n\n[generators]\nCMakeDeps\nCMakeToolchain\n\n[layout]\ncmake_layout\n\n[options]"
  },
  {
    "objectID": "posts/conan-pkg.html#profile",
    "href": "posts/conan-pkg.html#profile",
    "title": "Conan使用汇总",
    "section": "profile",
    "text": "profile\n描述好了依赖，然后需要安装依赖，这个时候conan提供了profile让我们来控制依赖包的配置信息：\n这是我构建的profile.user, 默认c++20：\n[settings]\narch=armv8\nbuild_type=Release\ncompiler=clang\ncompiler.cppstd=20\ncompiler.libcxx=libc++\ncompiler.version=18\nos=Macos\n\n[conf]\ntools.cmake.cmaketoolchain:generator=Ninja\ntools.cmake.cmake_layout:build_folder=build\ntools.cmake.cmake_layout:build_folder_vars=[]\n然后安装的时候还需使用--profile:all, 因为conan他可以用于交叉编译，host/target可以配置不同的profile。但我现在只有host，所以统一使用同一个profile：\nconan install . --build=missing --profile:all=profile.user"
  },
  {
    "objectID": "posts/conan-pkg.html#cmakeuserpresets.json",
    "href": "posts/conan-pkg.html#cmakeuserpresets.json",
    "title": "Conan使用汇总",
    "section": "CMakeUserPresets.json",
    "text": "CMakeUserPresets.json\ninstall之后会自动生成build/Release/generators/CMakePresets.json的json，然后项目目录下的CMakeUserPresets.json。正常来说基于CMakePresets.json就是可以正常编译了的，但是我这里还需要添加新的选项，那么就需要改CMakeUserPresets.json。\n我这里分了一个base的配置和debug的配置，要注意最终使用的debug配置中，必须写上binaryDir,toolchainFile否则还是会无法编译。 然后还有一个隐藏坑点，conan如果按release模式安装，那么他的依赖库都需要通过release模式依赖，但是现在我需要给自己的项目编译debug版本，去依赖包的时候就会丢失各种信息，所以需要\"CMAKE_FIND_PACKAGE_PREFER_CONFIG\": true, \"CMAKE_MAP_IMPORTED_CONFIG_DEBUG\": \"Release\"把我当前的debug按release模式去依赖包：\n\"configurePresets\": [\n        {\n            \"name\": \"base\",\n            \"hidden\": true,\n            \"generator\": \"Ninja\",\n            \"installDir\": \"${sourceDir}/out/install/${presetName}\",\n            \"cacheVariables\": {\n                \"CMAKE_EXPORT_COMPILE_COMMANDS\": true\n            }\n        },\n        {\n            \"name\": \"debug\",\n            \"inherits\": [\n                \"base\",\n                \"conan-release\"\n            ],\n            \"binaryDir\": \"${sourceDir}/build/Release\",\n            \"toolchainFile\": \"generators/conan_toolchain.cmake\",\n            \"cacheVariables\": {\n                \"CMAKE_BUILD_TYPE\": \"Debug\",\n                \"CMAKE_FIND_PACKAGE_PREFER_CONFIG\": true,\n                \"CMAKE_MAP_IMPORTED_CONFIG_DEBUG\": \"Release\"\n            }\n        }\n    ]"
  },
  {
    "objectID": "posts/conan-pkg.html#x-版本",
    "href": "posts/conan-pkg.html#x-版本",
    "title": "Conan使用汇总",
    "section": "1.x 版本",
    "text": "1.x 版本\n我要安装opencv4.5.1一直网络错误，所以需要手动下载包。 首先在~/.conan/data/opencv/4.5.1/_/_/export中找到conanfile.py，然后检查他是如何获得source的。 发现他的source是来自conandata.yml中的， 然后找到：\nsources:\n  4.5.1:\n  - sha256: e27fe5b168918ab60d58d7ace2bd82dd14a4d0bd1d3ae182952c2113f5637513\n    url: https://github.com/opencv/opencv/archive/4.5.1.tar.gz\n  - sha256: 12c3b1ddd0b8c1a7da5b743590a288df0934e5cef243e036ca290c2e45e425f5\n    url: https://github.com/opencv/opencv_contrib/archive/4.5.1.tar.gz\n手动下载它们。 解压之后copy到~/.conan/data/opencv/4.5.1/_/_/source/src不保留root。 然后删除`~/.conan/data/opencv/4.5.1/_/_/source.dirty表示source已经成功了。接着build就可以直接开始编译了。"
  },
  {
    "objectID": "posts/cpp-template-log.html",
    "href": "posts/cpp-template-log.html",
    "title": "cpp模板编译踩坑记",
    "section": "",
    "text": "今天给我的东西写了个可变长模板log类,然后编译的时候踩了大坑.😤"
  },
  {
    "objectID": "posts/cpp-template-log.html#出现很多很多错误",
    "href": "posts/cpp-template-log.html#出现很多很多错误",
    "title": "cpp模板编译踩坑记",
    "section": "出现很多很多错误…",
    "text": "出现很多很多错误…\n我开始的时候想将#include放到namespace的定义中去，然后直接炸….\n出现很多类似的错误：\nIn file included from /opt/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/arm-linux-gnueabihf/include/c++/4.9.3/string:52:0,\n                 from /opt/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/arm-linux-gnueabihf/include/c++/4.9.3/bits/locale_classes.h:40,\n                 from /opt/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/arm-linux-gnueabihf/include/c++/4.9.3/bits/ios_base.h:41,\n                 from /opt/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/arm-linux-gnueabihf/include/c++/4.9.3/ios:42,\n                 from /opt/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/arm-linux-gnueabihf/include/c++/4.9.3/ostream:38,\n                 from /opt/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/arm-linux-gnueabihf/include/c++/4.9.3/iostream:39,\n                 from /home/zqh/Documents/raspi_blue/utils/inc/loger.h:13,\n                 from /home/zqh/Documents/raspi_blue/utils/src/record.cpp:2:\n/opt/tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/arm-linux-gnueabihf/include/c++/4.9.3/bits/basic_string.h: In function 'long int Loger::std::stol(const string&, Loger::std::size_t*, int)':"
  },
  {
    "objectID": "posts/cpp-template-log.html#猜测",
    "href": "posts/cpp-template-log.html#猜测",
    "title": "cpp模板编译踩坑记",
    "section": "猜测:",
    "text": "猜测:\n我怀疑是将#include &lt;iostream&gt;放在namespace中,就会导致std和别一些东西冲突.."
  },
  {
    "objectID": "posts/cpp-template-log.html#出现多重定义的错误",
    "href": "posts/cpp-template-log.html#出现多重定义的错误",
    "title": "cpp模板编译踩坑记",
    "section": "出现多重定义的错误",
    "text": "出现多重定义的错误\n我把定义放到外面之后编译出现\nCMakeFiles/myrecord.dir/utils/src/sock.cpp.o: In function `Loger::show_list()':\nsock.cpp:(.text+0x0): multiple definition of `Loger::show_list()'\nCMakeFiles/myrecord.dir/utils/src/record.cpp.o:record.cpp:(.text+0x0): first defined here\nCMakeFiles/myrecord.dir/xfly/src/quickasr.cpp.o: In function `Loger::show_list()':\nquickasr.cpp:(.text+0x0): multiple definition of `Loger::show_list()'\nCMakeFiles/myrecord.dir/utils/src/record.cpp.o:record.cpp:(.text+0x0): first defined here\nCMakeFiles/myrecord.dir/usr/src/main.cpp.o: In function `Loger::show_list()':\nmain.cpp:(.text+0x1cc): multiple definition of `Loger::show_list()'\nCMakeFiles/myrecord.dir/utils/src/record.cpp.o:record.cpp:(.text+0x0): first defined here\ncollect2: error: ld returned 1 exit status\nCMakeFiles/myrecord.dir/build.make:172: recipe for target '../bin/myrecord' failed\nmake[2]: *** [../bin/myrecord] Error 1\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/myrecord.dir/all' failed\nmake[1]: *** [CMakeFiles/myrecord.dir/all] Error 2\nMakefile:83: recipe for target 'all' failed\nmake: *** [all] Error 2"
  },
  {
    "objectID": "posts/cpp-template-log.html#猜测-1",
    "href": "posts/cpp-template-log.html#猜测-1",
    "title": "cpp模板编译踩坑记",
    "section": "猜测",
    "text": "猜测\n这个我找了半天没找到相关描述..但是我在那个函数前面加了inline就莫名的好了 😂\n最终代码:\n/*\n * @Author: Zheng Qihang\n * @Date: 2019-03-11 12:47:23\n * @Last Modified by: Zheng Qihang\n * @Last Modified time: 2019-04-01 15:01:21\n */\n#ifndef _LOGER_\n#define _LOGER_\n\n#include &lt;cstdlib&gt;\n#include &lt;iostream&gt;\n\nnamespace Loger {\n\n/* clang-format off */\n#define NONE                 \"\\e[0m\"\n#define BLACK                \"\\e[0;30m\"\n#define L_BLACK              \"\\e[1;30m\"\n#define RED                  \"\\e[0;31m\"\n#define L_RED                \"\\e[1;31m\"\n#define GREEN                \"\\e[0;32m\"\n#define L_GREEN              \"\\e[1;32m\"\n#define BROWN                \"\\e[0;33m\"\n#define YELLOW               \"\\e[1;33m\"\n#define BLUE                 \"\\e[0;34m\"\n#define L_BLUE               \"\\e[1;34m\"\n#define PURPLE               \"\\e[0;35m\"\n#define L_PURPLE             \"\\e[1;35m\"\n#define CYAN                 \"\\e[0;36m\"\n#define L_CYAN               \"\\e[1;36m\"\n#define GRAY                 \"\\e[0;37m\"\n#define WHITE                \"\\e[1;37m\"\n\n#define BOLD                 \"\\e[1m\"\n#define UNDERLINE            \"\\e[4m\"\n#define BLINK                \"\\e[5m\"\n#define REVERSE              \"\\e[7m\"\n#define HIDE                 \"\\e[8m\"\n#define CLEAR                \"\\e[2J\"\n#define CLRLINE              \"\\r\\e[K\"\n\n#define ERROR_C              \"\\e[0;31m[  ERR   ] \\e[0m\"\n#define OK_C                 \"\\e[0;32m[   OK   ] \\e[0m\"\n#define DEBUG_C              \"\\e[0;34m[ DEBUG  ] \\e[0m\"\n#define INFO_C               \"\\e[0;35m[  INFO  ] \\e[0m\"\n#define WAIT_C               \"\\e[0;33m[  WAIT  ] \\e[0m\"\n/* clang-format on */\n\n/**\n * @brief 处理边界条件\n *\n */\ninline void show_list() { std::cout &lt;&lt; std::endl; }\n\n/**\n * @brief 处理参数列表\n *\n * @tparam T\n * @tparam Args\n * @param var 参数1\n * @param args 变长参数\n */\ntemplate&lt;typename T, typename... Args&gt; void show_list(const T &var, const Args &... args) {\n    std::cout &lt;&lt; var;\n    show_list(args...);\n}\n\ntemplate&lt;typename... Args&gt; void info(const Args &... args) {\n    std::cout &lt;&lt; INFO_C;\n    show_list(args...);\n}\n\ntemplate&lt;typename... Args&gt; void err(const Args &... args) {\n    std::cout &lt;&lt; ERROR_C;\n    show_list(args...);\n}\n\ntemplate&lt;typename... Args&gt; void errexit(const Args &... args) {\n    std::cout &lt;&lt; ERROR_C;\n    show_list(args...);\n    exit(-1);\n}\n} // namespace Loger\n\n#endif"
  },
  {
    "objectID": "posts/cpp13.html",
    "href": "posts/cpp13.html",
    "title": "c++成员函数中static变量",
    "section": "",
    "text": "我最近在做郭炜老师的编程题目,这道题我实现的过程中出现了一些蛋疼的错误,进行一个记录."
  },
  {
    "objectID": "posts/cpp13.html#测试红色",
    "href": "posts/cpp13.html#测试红色",
    "title": "c++成员函数中static变量",
    "section": "测试红色",
    "text": "测试红色\n➜  exam cat test.txt | ./red.out\nCase:1\n000 red iceman 1 born with strength 5,1 iceman in red headquarter\n001 red lion 2 born with strength 6,1 lion in red headquarter\n002 red wolf 3 born with strength 7,1 wolf in red headquarter\n003 red headquarter stops making warriors"
  },
  {
    "objectID": "posts/cpp13.html#测试蓝色",
    "href": "posts/cpp13.html#测试蓝色",
    "title": "c++成员函数中static变量",
    "section": "测试蓝色",
    "text": "测试蓝色\n➜  exam cat test.txt | ./blue.out\nCase:1\n000 blue lion 1 born with strength 6,1 lion in blue headquarter\n001 blue dragon 2 born with strength 3,1 dragon in blue headquarter\n002 blue ninja 3 born with strength 4,1 ninja in blue headquarter\n003 blue iceman 4 born with strength 5,1 iceman in blue headquarter\n004 blue headquarter stops making warriors"
  },
  {
    "objectID": "posts/cpp13.html#测试全部",
    "href": "posts/cpp13.html#测试全部",
    "title": "c++成员函数中static变量",
    "section": "测试全部",
    "text": "测试全部\n➜  exam cat test.txt | ./all.out\nCase:1\n000 red iceman 1 born with strength 5,1 iceman in red headquarter\n001 red lion 2 born with strength 6,1 lion in red headquarter\n002 red wolf 3 born with strength 7,1 wolf in red headquarter\n003 red headquarter stops making warriors\n000 blue ninja 1 born with strength 4,1 ninja in blue headquarter\n001 blue dragon 2 born with strength 3,1 dragon in blue headquarter\n002 blue (null) 3 born with strength 0,1 (null) in blue headquarter\n003 blue (null) 4 born with strength 0,1 (null) in blue headquarter\n004 blue (null) 5 born with strength 0,1 (null) in blue headquarter\n[1]    17104 done                cat test.txt |\n       17105 segmentation fault  ./all.out"
  },
  {
    "objectID": "posts/cpp13.html#测试红色-1",
    "href": "posts/cpp13.html#测试红色-1",
    "title": "c++成员函数中static变量",
    "section": "测试红色",
    "text": "测试红色\n➜  exam cat test.txt | ./red.out\nCase:1\n000 red iceman 1 born with strength 5,1 iceman in red headquarter\nindex - seqcycle.begin(): 1\n001 red lion 2 born with strength 6,1 lion in red headquarter\nindex - seqcycle.begin(): 2\n002 red wolf 3 born with strength 7,1 wolf in red headquarter\nindex - seqcycle.begin(): 3\n003 red headquarter stops making warriors"
  },
  {
    "objectID": "posts/cpp13.html#测试两组",
    "href": "posts/cpp13.html#测试两组",
    "title": "c++成员函数中static变量",
    "section": "测试两组",
    "text": "测试两组\n➜  exam cat test.txt | ./all.out\nCase:1\n000 red iceman 1 born with strength 5,1 iceman in red headquarter\nindex - seqcycle.begin(): 1\n000 blue lion 1 born with strength 6,1 lion in blue headquarter\nindex - seqcycle.begin(): -8\n001 red wolf 2 born with strength 7,1 wolf in red headquarter\nindex - seqcycle.begin(): 3\n001 blue ninja 2 born with strength 4,1 ninja in blue headquarter\nindex - seqcycle.begin(): -6\n002 red dragon 3 born with strength 3,1 dragon in red headquarter\nindex - seqcycle.begin(): 5\n002 blue (null) 3 born with strength 0,1 (null) in blue headquarter\nindex - seqcycle.begin(): -4\n003 red (null) 4 born with strength 0,1 (null) in red headquarter\nindex - seqcycle.begin(): 7\n003 blue (null) 4 born with strength 0,1 (null) in blue headquarter\nindex - seqcycle.begin(): -2\nindex - seqcycle.begin(): 9\nindex - seqcycle.begin(): 10\nindex - seqcycle.begin(): 11\nindex - seqcycle.begin(): 12\nindex - seqcycle.begin(): 13\nindex - seqcycle.begin(): 14\nindex - seqcycle.begin(): 15\n[1]    32721 done                cat test.txt |\n       32722 segmentation fault  ./all.out\n终于看出问题了,应该是我在函数中使用的static auto index= seqcycle.begin();,这个静态的index是两个类所共有的,每次对index进行循环操作index= index == seqcycle.end() ? seqcycle.begin() : index + 1;,其中的seqcycle.begin()却是每个类独有的,就导致了每次 公共的index加1,两次同时操作就导致了index最终超出了seqcycle的范围,出现了数组越界."
  },
  {
    "objectID": "posts/csharp-invoke.html",
    "href": "posts/csharp-invoke.html",
    "title": "C# P/Invoke 总结",
    "section": "",
    "text": "关于C#调用本机lib时遇到的一些问题汇总."
  },
  {
    "objectID": "posts/csharp-invoke.html#dllimport",
    "href": "posts/csharp-invoke.html#dllimport",
    "title": "C# P/Invoke 总结",
    "section": "DllImport",
    "text": "DllImport\n可以通过为函数添加dllimport的属性来指明需要使用的动态链接库. 同时我们可以不写明后缀,dotnet会自动根据平台寻找. 比如我打包出来的动态链接库为libnncase_csharp.dylib, 我们只要写:\n[DllImport(\"libnncase_csharp\")]\nstatic extern unsafe void interpreter_load_model(byte* buffer_ptr, int size);\n这个方式适合于固定名字的动态库,并且加载了之后就不能卸载再重载."
  },
  {
    "objectID": "posts/csharp-invoke.html#nativelibrary.load",
    "href": "posts/csharp-invoke.html#nativelibrary.load",
    "title": "C# P/Invoke 总结",
    "section": "NativeLibrary.Load",
    "text": "NativeLibrary.Load\n目前我的方式就是动态的加载链接库,这种方式就是用起来比较麻烦, 首先我们要定义好对应的delegate签名,然后声明一系列的instance, 后面再加载对应的lib之后进行bind, 同时路径还得是固定的. 好处就是更新动态链接库不会受到影响.\ndelegate bool delegate_init();\ndelegate_init interpreter_init;\nTDelegate GetFFI&lt;TDelegate&gt;() =&gt; Marshal.GetDelegateForFunctionPointer&lt;TDelegate&gt;(NativeLibrary.GetExport(Handle, typeof(TDelegate).Name.Replace(\"delegate\", \"interpreter\")));\n\nHandle = NativeLibrary.Load(\"/Users/lisa/Documents/nncase/build/simulator/lib/libnncase_csharp.dylib\");\ninterpreter_init = GetFFI&lt;delegate_init&gt;();"
  },
  {
    "objectID": "posts/csharp-invoke.html#debug-csharp-with-cpp",
    "href": "posts/csharp-invoke.html#debug-csharp-with-cpp",
    "title": "C# P/Invoke 总结",
    "section": "debug csharp with cpp",
    "text": "debug csharp with cpp\n用dllimport的方式我尝试了各种方式去attch cpp的代码发现都不行,最终我还是在cpp中写了一个可执行程序然后用c#去起一个进程,然后再用这个时候直接attach到对应的可执行程序就可以debug了.这里还有个要注意点就是需要在可执行程序中加一个waitkey,不然程序等不到 attach就直接结束了."
  },
  {
    "objectID": "posts/csharp-invoke.html#inoutref关键字的处理",
    "href": "posts/csharp-invoke.html#inoutref关键字的处理",
    "title": "C# P/Invoke 总结",
    "section": "in/out/ref关键字的处理",
    "text": "in/out/ref关键字的处理\n在C#中,in, out, 和 ref 关键字用于控制参数如何在方法调用中传递.这些关键字在P/Invoke时也有特殊的意义,因为它们指示如何在托管代码和非托管代码之间传递数据.\n\nin 关键字\n\n当用于P/Invoke方法参数时,in 关键字指示参数应该从托管代码传递到非托管代码,但不期望从非托管代码返回数据到托管代码. in 关键字用于优化传递大型结构体或数组时的性能,因为它避免了在非托管代码执行后将数据复制回托管内存的需要. 该关键字对于基本数据类型通常是不必要的,因为它们默认按值传递,但对于结构体或数组,它可以防止不必要的内存复制.\n\nout 关键字\n\n使用out关键字时,参数被假定为未初始化,并且非托管函数负责填充它. 在P/Invoke调用完成后,参数的值会从非托管代码复制回托管代码. 这适用于那些只需要从非托管函数输出数据的情况,不需要传递初始化的数据给非托管代码.\n\nref 关键字\n\nref 关键字用于两个方向的数据传递:它既将数据从托管代码传递到非托管代码,也将非托管代码的修改传回托管代码. 这适用于需要在非托管代码中被修改,并且修改后的值需要返回给托管代码的情况. P/Invoke与这些关键字的交互\n当你在P/Invoke声明中使用这些关键字时,你告诉CLR(公共语言运行时)如何在托管和非托管之间封送(marshal)数据.封送是指在不同运行环境(如托管和非托管代码)之间转换数据的过程.\n对于in参数,CLR会创建数据的副本(如果需要)并将其传递到非托管代码,但在调用结束后不会检查非托管代码是否更改了数据. 对于out参数,CLR会跳过传递数据到非托管代码的步骤,但在调用结束后会从非托管代码读取数据并填充托管参数. 对于ref参数,CLR会传递数据到非托管代码,并在调用结束后读取可能的更改并更新托管参数. 这些关键字对于优化性能和确保数据正确传递非常重要.在使用这些关键字时,你应该清楚地了解非托管函数的预期行为,以便正确地选择使用 in, out, 或 ref.\n如果你有一个C函数,它的签名是这样的:\nvoid inc(int const *a);\n在C#中,你不能直接使用 in int a 来匹配 int const *a.相反,你应该使用 ref 关键字来传递一个 int 的引用,或者使用 IntPtr 来传递一个指针:\n// 使用 ref 关键字\n[DllImport(\"YourLibrary.dll\")]\npublic static extern void inc(ref int a);\n\n// 使用 IntPtr\n[DllImport(\"YourLibrary.dll\")]\npublic static extern void inc(IntPtr a);\n在调用时:\nint value = 5;\ninc(ref value); // 使用 ref 关键字\n\n// 或者,如果你使用 IntPtr\nIntPtr ptr = Marshal.AllocHGlobal(Marshal.SizeOf(typeof(int)));\nMarshal.WriteInt32(ptr, value);\ninc(ptr);\nMarshal.FreeHGlobal(ptr);\n使用 ref 关键字是最简单的方法,因为它允许CLR为你处理指针的创建和数据的封送.如果你使用 IntPtr,你需要手动分配和释放非托管内存,并且还需要手动读写该内存."
  },
  {
    "objectID": "posts/csharp-invoke.html#safehandle类",
    "href": "posts/csharp-invoke.html#safehandle类",
    "title": "C# P/Invoke 总结",
    "section": "SafeHandle类",
    "text": "SafeHandle类\n首先在csharp中有两种资源:\n\n托管资源 Managed Resources:\n这些是.NET环境下由公共语言运行时(CLR)管理的资源.托管资源主要指的是内存中的对象,如实例化的类或结构体.CLR负责分配和释放这些对象的内存,通常通过垃圾回收(GC)机制来完成.开发者通常不需要手动释放这些资源,因为GC会在对象不再被引用时自动清理它们.托管资源包括但不限于:\n\n对象实例(如类的实例)\n数组和集合\n委托\n事件处理器\n以及其他所有由CLR管理的内存分配.\n\n\n非托管资源 Unmanaged Resources:\n非托管资源是不由CLR直接管理的资源.它们通常是操作系统资源,如文件句柄,网络连接,数据库连接,图形界面句柄,以及任何其他需要通过平台调用(P/Invoke)或互操作服务(COM Interop)使用的资源.这些资源的分配和释放必须由开发者显式管理,因为垃圾回收器不会自动处理它们.常见的非托管资源包括:\n\n文件和流(FileStream,StreamReader,StreamWriter)\n网络套接字(Socket)\n数据库连接(SqlConnection)\n位图(Bitmap)等图形资源\n通过P/Invoke调用分配的内存\nCOM对象\n其他需要调用操作系统API来管理的资源.\n\n正确管理非托管资源是非常重要的,因为如果不释放这些资源,可能会导致内存泄漏或资源耗尽. 在.NET中,IDisposable接口和SafeHandle类是用来帮助开发者管理非托管资源的常用工具.当一个类实现了IDisposable接口,它应该提供一个Dispose方法,该方法负责释放类持有的所有非托管资源,以及可以释放的托管资源.而SafeHandle是一个专门设计用来封装非托管资源句柄的类,它提供了一个可靠的方式来确保非托管资源在不再需要时被正确释放.\n\n\nSafeHandle的资源释放逻辑\n它提供了两个重要的方法,Dispose 和终结器(由 ~SafeHandle() 表示),它们在不同的情况下被调用:\n\nDispose() 方法: Dispose 方法是 IDisposable 接口的一部分,需要在你的代码中显式调用.当你确定不再需要 SafeHandle 包装的资源时,你应该调用 Dispose 方法.这是一种确定性的方式来释放非托管资源,因为它允许你精确控制资源的释放时间.调用 Dispose 方法通常会导致 SafeHandle 调用其 ReleaseHandle 方法来释放其封装的非托管资源,并将其标记为无效.\nsafeHandle.Dispose();\n终结器(Finalizer): 终结器由 ~SafeHandle() 表示,并在垃圾回收器准备回收对象时自动调用.终结器的调用是非确定性的,因为你不能预知垃圾回收器何时会运行.当 SafeHandle 的实例不再有任何有效的托管引用时,垃圾回收器会在某个时间点回收它,并在此过程中调用终结器.终结器同样会尝试清理非托管资源,但它的执行时间是不可预测的.\n~SafeHandle() {\n    Dispose(false);\n}\n\n在 SafeHandle 的实现中,Dispose 方法通常是安全地处理托管和非托管资源的首选方法,而终结器是一种安全网,确保即使忘记调用 Dispose 方法,非托管资源也最终会被释放. 所以为了区分这两种情况, 基本上需要调用Dispose(bool disposing)方法来正确处理资源释放逻辑, 这个方法接受一个布尔值指示, 通常当 Dispose() 被显式调用时,disposing 是 true. 而终结器调用 Dispose(false) 时, 它只处理非托管资源, 因为托管资源可能已经被垃圾回收器清理了.\n但是其实在Dispose(bool disposing)方法中并没有关心disposing, 其实为了防止在 Dispose 被调用后终结器再次释放资源, 显式使用Dispose方法会调用 GC.SuppressFinalize(this), 这告诉垃圾回收器此对象的终结器不需要再被调用了,因为资源已经被显式清理了.\n如果没有显式使用Dispose方法, 这里的终结器还是会调用Dispose(false), 内部通过InternalRelease把资源释放掉. 在资源释放时, 首先通过检查state是否是open且只被引用一次且当前类构造的时候为_ownsHandle的形式, 检查成功后会将state设置为close状态, 这个时候要注意除非是使用DangerousRelease来释放, 否则必须会把state设置为Disposed. 等到state更新完毕, 才调用用户override的ReleaseHandle方法去处理非托管资源的释放.\n还有一点需要注意的是, 如果extern的释放函数是没法接受一个被close的SafeHandle 对象, 所以此时需要取出他的handle来调用资源释放函数. 释放后可以自行把handle设置为无效.\nnamespace System.Runtime.InteropServices\n{\n    // This implementation does not employ critical execution regions and thus cannot\n    // reliably guarantee handle release in the face of thread aborts.\n\n    /// &lt;summary&gt;Represents a wrapper class for operating system handles.&lt;/summary&gt;\n    public abstract partial class SafeHandle : CriticalFinalizerObject, IDisposable\n    {\n#if DEBUG && CORECLR\n        /// &lt;summary&gt;Indicates whether debug tracking and logging of SafeHandle finalization is enabled.&lt;/summary&gt;\n        private static readonly bool s_logFinalization = Environment.GetEnvironmentVariable(\"DEBUG_SafeHandle_FINALIZATION\") == \"1\";\n        /// &lt;summary&gt;Debug counter for the number of SafeHandles that have been finalized.&lt;/summary&gt;\n        private static long s_SafeHandlesFinalized;\n#endif\n\n        // IMPORTANT:\n        // - Do not add or rearrange fields as the EE depends on this layout,\n        //   as well as on the values of the StateBits flags.\n        // - The EE may also perform the same operations using equivalent native\n        //   code, so this managed code must not assume it is the only code\n        //   manipulating _state.\n\n#if DEBUG && CORECLR\n        private readonly string? _ctorStackTrace;\n#endif\n        /// &lt;summary&gt;Specifies the handle to be wrapped.&lt;/summary&gt;\n        protected IntPtr handle;\n        /// &lt;summary&gt;Combined ref count and closed/disposed flags (so we can atomically modify them).&lt;/summary&gt;\n        private volatile int _state;\n        /// &lt;summary&gt;Whether we can release this handle.&lt;/summary&gt;\n        private readonly bool _ownsHandle;\n        /// &lt;summary&gt;Whether constructor completed.&lt;/summary&gt;\n        private readonly bool _fullyInitialized;\n\n        /// &lt;summary&gt;Bitmasks for the &lt;see cref=\"_state\"/&gt; field.&lt;/summary&gt;\n        /// &lt;remarks&gt;\n        /// The state field ends up looking like this:\n        ///\n        ///  31                                                        2  1   0\n        /// +-----------------------------------------------------------+---+---+\n        /// |                           Ref count                       | D | C |\n        /// +-----------------------------------------------------------+---+---+\n        ///\n        /// Where D = 1 means a Dispose has been performed and C = 1 means the\n        /// underlying handle has been (or will be shortly) released.\n        /// &lt;/remarks&gt;\n        private static class StateBits\n        {\n            public const int Closed = 0b01;\n            public const int Disposed = 0b10;\n            public const int RefCount = unchecked(~0b11); // 2 bits reserved for closed/disposed; ref count gets 30 bits\n            public const int RefCountOne = 1 &lt;&lt; 2;\n        }\n\n        /// &lt;summary&gt;Creates a SafeHandle class.&lt;/summary&gt;\n        protected SafeHandle(IntPtr invalidHandleValue, bool ownsHandle)\n        {\n            handle = invalidHandleValue;\n            _state = StateBits.RefCountOne; // Ref count 1 and not closed or disposed.\n            _ownsHandle = ownsHandle;\n\n            if (!ownsHandle)\n            {\n                GC.SuppressFinalize(this);\n            }\n#if DEBUG && CORECLR\n            else if (s_logFinalization)\n            {\n                int lastError = Marshal.GetLastPInvokeError();\n                _ctorStackTrace = Environment.StackTrace;\n                Marshal.SetLastPInvokeError(lastError);\n            }\n#endif\n\n            Volatile.Write(ref _fullyInitialized, true);\n        }\n\n        ~SafeHandle()\n        {\n            if (_fullyInitialized)\n            {\n                Dispose(disposing: false);\n            }\n        }\n\n        internal bool OwnsHandle =&gt; _ownsHandle;\n\n        protected internal void SetHandle(IntPtr handle) =&gt; this.handle = handle;\n\n        public IntPtr DangerousGetHandle() =&gt; handle;\n\n        public bool IsClosed =&gt; (_state & StateBits.Closed) == StateBits.Closed;\n\n        public abstract bool IsInvalid { get; }\n\n        public void Close() =&gt; Dispose();\n\n        public void Dispose()\n        {\n            Dispose(disposing: true);\n            GC.SuppressFinalize(this);\n        }\n\n        protected virtual void Dispose(bool disposing)\n        {\n#if DEBUG && CORECLR\n            if (!disposing && _ctorStackTrace is not null)\n            {\n                long count = Interlocked.Increment(ref s_SafeHandlesFinalized);\n                Internal.Console.WriteLine($\"{Environment.NewLine}*** #{count} {GetType()} (0x{handle.ToInt64():x}) finalized! Ctor stack:{Environment.NewLine}{_ctorStackTrace}{Environment.NewLine}\");\n            }\n#endif\n            Debug.Assert(_fullyInitialized);\n            InternalRelease(disposeOrFinalizeOperation: true);\n        }\n\n        public void SetHandleAsInvalid()\n        {\n            Debug.Assert(_fullyInitialized);\n\n            Interlocked.Or(ref _state, StateBits.Closed);\n\n            GC.SuppressFinalize(this);\n        }\n\n        protected abstract bool ReleaseHandle();\n\n        public void DangerousAddRef(ref bool success)\n        {\n            Debug.Assert(_fullyInitialized);\n\n            int oldState, newState;\n            do\n            {\n                oldState = _state;\n                ObjectDisposedException.ThrowIf((oldState & StateBits.Closed) != 0, this);\n                newState = oldState + StateBits.RefCountOne;\n            } while (Interlocked.CompareExchange(ref _state, newState, oldState) != oldState);\n            success = true;\n        }\n\n        internal void DangerousAddRef()\n        {\n            bool success = false;\n            DangerousAddRef(ref success);\n        }\n\n        public void DangerousRelease() =&gt; InternalRelease(disposeOrFinalizeOperation: false);\n\n        private void InternalRelease(bool disposeOrFinalizeOperation)\n        {\n            Debug.Assert(_fullyInitialized || disposeOrFinalizeOperation);\n\n            bool performRelease;\n            int oldState, newState;\n            do\n            {\n                oldState = _state;\n\n                if (disposeOrFinalizeOperation && ((oldState & StateBits.Disposed) != 0))\n                {\n                    return;\n                }\n\n                ObjectDisposedException.ThrowIf((oldState & StateBits.RefCount) == 0, this);\n\n                performRelease = ((oldState & (StateBits.RefCount | StateBits.Closed)) == StateBits.RefCountOne) &&\n                                 _ownsHandle &&\n                                 !IsInvalid;\n                newState = oldState - StateBits.RefCountOne;\n                if ((oldState & StateBits.RefCount) == StateBits.RefCountOne)\n                {\n                    newState |= StateBits.Closed;\n                }\n                if (disposeOrFinalizeOperation)\n                {\n                    newState |= StateBits.Disposed;\n                }\n            } while (Interlocked.CompareExchange(ref _state, newState, oldState) != oldState);\n\n            if (performRelease)\n            {\n                int lastError = Marshal.GetLastPInvokeError();\n                ReleaseHandle();\n                Marshal.SetLastPInvokeError(lastError);\n            }\n        }\n    }\n}\n\n\nSafeHandle使用例子\nSafeHandle专门存储了一个指针, 在csharp中被释放的时候提供了各种callback让用户自己处理. 在P/invoke的过程中, 继承自safe handle的类就会被自动转换成一个指针对象. 并且下面这种结构体, 其实也可以被认为是一个指针, 也可以通过继承safe handle来处理.\nstruct MlirContext {\n  void *ptr;\n};\nSafeHandle如果是被extern的函数所构造, 那么必须要实现无参数的构造函数, 在调用extern的函数时, 实际上是先调用无参数构造函数然后再调用SetHandle把底层的指针传进去.\npublic sealed class Context : SafeHandle, IEquatable&lt;Context&gt;\n{\n    private static HashSet&lt;Context&gt; _liveContextSet = new();\n\n    public Context() : base(IntPtr.Zero, true)\n    {\n        System.Console.WriteLine(\"create\");\n    }\n\n    public static Context Create() =&gt; mlirContextCreate();\n}\n在csharp中, 只要任何被csharp对象引用的对象都不会被gc掉, 但是在mlir的python binding中, 有个很麻烦的事情就是他为了避免重复构造对象,使用Map&lt;ptr,object&gt;来存储context/module/operation. 但是这个是在python binding部分内部实现的, 在python中并不知道字典的value存在引用, 所以当python中释放对象的时候可以在c++中清理这个字典. 但是csharp中字典是知道module这个key value"
  },
  {
    "objectID": "posts/csharp-invoke.html#字符串处理",
    "href": "posts/csharp-invoke.html#字符串处理",
    "title": "C# P/Invoke 总结",
    "section": "字符串处理",
    "text": "字符串处理\n假设c中设计了StringRef进行字符串传递:\ntypedef struct MlirStringRef {\n  char *data;    ///&lt; Pointer to the first symbol.\n  size_t length; ///&lt; Length of the fragment.\n} StringRef;\n\nvoid libParseMlirStringRef(StringRef ref) {\n  printf(\"ref %s, %ld\\n\", ref.data, ref.length);\n}\n\nMlirStringRef MlirStringRefCreateFromCString(const char *str);\n我发现如果直接使用如下方法:\n    [DllImport(LibraryName)]\n    public static extern MlirStringRef mlirStringRefCreateFromCString([MarshalAs(UnmanagedType.LPStr)] string str);\n\n    public static unsafe MlirStringRef mlirStringRefCreate(char* str, ulong length)\n    {\n        return new MlirStringRef { data = str, length = length };\n    }\n\nvoid main() {\n    const string s = \"hello!\";\n    ParseMlirStringRef(mlirStringRefCreateFromCString(s));\n    ParseMlirStringRef(mlirStringRefCreateFromCString(\"hello!\"));\n\n    unsafe\n    {\n        var bytes = System.Text.Encoding.ASCII.GetBytes(s);\n        fixed (byte* ptr = bytes)\n        {\n            var sref = mlirStringRefCreate((char*)ptr, (ulong)bytes.Length);\n            ParseMlirStringRef(sref);\n        }\n    }\n}\n由于csharp默认的字符串编码只有Unicode和Ansi, 所以直接在底层使用没法直接使用string来构造, 只能先将string的编码转到ascii才能得到正确的结果:\nref �\u001e#m\u0001, 6\nref �\u001e#m\u0001, 6\nref hello!, 6\n或者其实还可以用一个更简单的方法, 这样也可以得到正确的调用结果:\n    [DllImport(LibraryName)]\n    public static extern MlirStringRef mlirStringRefCreateFromCString([MarshalAs(UnmanagedType.LPArray)] byte[] str);\n\nvoid main() {\n    const string s = \"hello!\";\n    ParseMlirStringRef(mlirStringRefCreateFromCString(System.Text.Encoding.ASCII.GetBytes(s)));\n    ParseMlirStringRef(mlirStringRefCreateFromCString(System.Text.Encoding.ASCII.GetBytes(\"hello!\")));\n}\n在pybind11中, 如果str转换到std::string那么是utf-8编码的.\n要注意一点, 如果c代码中直接返回一个char *指针, 然后C sharp中使用类似的方式:\n[DllImport(LibraryName)]\n[return: MarshalAs(UnmanagedType.LPStr)]\npublic static extern  string isl_basic_map_get_tuple_name(IntPtr bmap, dim_type type);\n这会引起潜在的问题, 因为csharp会自动管理并释放这个string, 如果这个内存是被c库中反复使用的话,就会出现指针异常."
  },
  {
    "objectID": "posts/csharp-invoke.html#csharp-无法观测到c对象之间的依赖",
    "href": "posts/csharp-invoke.html#csharp-无法观测到c对象之间的依赖",
    "title": "C# P/Invoke 总结",
    "section": "CSharp 无法观测到c对象之间的依赖",
    "text": "CSharp 无法观测到c对象之间的依赖\nisl中自己维护了一套引用计数的逻辑，但同时他还有一个context作为对象池，而用csharp调用时是看不到对象池的依赖关系的， 用两个例子可以很好的说明：\n\nkeep live ctx\n这里让ctx不提前释放，但是可以观察到对于c对象的释放还是在ctx之前：\npublic void TestSetMinAff{\n  using (var ctx = Isl.ctx.Create()) {\n    var set = new Isl.set(ctx, \"[N] → { [i,j,k]: 05 i &lt; 12 and 0 ≤ j&lt; Nand 0 ≤ k &lt; Nand O N &lt; 123 }\");\n    var aff = set.max_multi_pw_aff();\n    var min = aff.min_multi_val();\n    var max = aff.max_multi_val();\n    System.Console.WriteLine(min);\n    System.Console WriteLine(max);\n    GC.KeepAlive(ctx);\n  }\n}\n［11,0,0］\nisl_ctx.c:307: isl_ctx not freed as some objects still reference it\n［11,121,121］\n\n\nmanual free objects\npublic void TestSetMinAffFreeManualy()\n{\n    using (var ctx = Isl.ctx.Create())\n    {\n        var set = new Isl.set(ctx, \"[N] -&gt; { [i,j,k]: 0&lt;= i &lt; 12 and 0 &lt;= j &lt; N and 0 &lt;= k &lt; N and 0 &lt;= N &lt; 123 }\");\n        var aff = set.max_multi_pw_aff();\n        var min = aff.min_multi_val();\n        var max = aff.max_multi_val();\n        System.Console.WriteLine(min);\n        System.Console.WriteLine(max);\n        max.Dispose();\n        min.Dispose();\n        aff.Dispose();\n        set.Dispose();\n    }\n}\n这样才能把ctx的free放到最后处理。\n{[11,0,0]}\n{[11,121,121]}\nfree isl_multi_val 4307700432\nfree isl_multi_val 5510841360\nfree isl_multi_pw_aff 5510378384\nfree isl_set 5241109488\nfree isl_ctx 4307694592"
  },
  {
    "objectID": "posts/cstrcpy.html",
    "href": "posts/cstrcpy.html",
    "title": "c与c++字符串赋值",
    "section": "",
    "text": "最近用c++写的一个程序，我想用一个const char *p对一个char head[2]赋值，我使用strcpy赋值之后一直出现错误。我就写了个小程序去验证了一下。\n\n\n程序\n#include &lt;cstdint&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstring&gt;\nclass CHARGET {\n  private:\n    char _head[2];\n\n  public:\n    CHARGET(const char *p, int opt) {\n        memset(_head, 0, 2);\n        if (opt == 0) {\n            _head[0] = *p++;\n            _head[1] = *p++;\n        } else if (opt == 1) {\n            strcpy(_head, p);\n        } else if (opt == 2) {\n            strncpy(_head, p, 2);\n        } else if (opt == 3) {\n            strncpy(_head, p, 3);\n        } else {\n            return;\n        }\n    }\n\n    void show(void) { printf(\"%s%.2d\", _head, 2); }\n    ~CHARGET() {}\n};\n\nchar test[2];\n\nvoid CHARCPY(const char *p) {\n    test[0] = *p++;\n    test[1] = *p++;\n}\n\nint main(int argc, char const *argv[]) {\n    /* 类中 直接指针幅值 */\n    printf(\"类中直接指针幅值\\r\\n\");\n    CHARGET c1(\"HJ\", 0);\n    c1.show();\n    printf(\"\\n\\n\");\n\n    /* strncpy 两字节 */\n    printf(\"strcpy\\r\\n\");\n    CHARGET c2(\"HJ\", 1);\n    c2.show();\n    printf(\"\\n\\n\");\n\n    /* strncpy 两字节 */\n    printf(\"strncpy 两字节\\r\\n\");\n    CHARGET c3(\"HJ\", 2);\n    c3.show();\n    printf(\"\\n\\n\");\n\n    /* strncpy 三字节 */\n    printf(\"strncpy 三字节\\r\\n\");\n    CHARGET c4(\"HJ\", 3);\n    c4.show();\n    printf(\"\\n\\n\");\n\n    /* 类外 直接指针幅值 */\n    printf(\"类外直接指针幅值\\r\\n\");\n    CHARCPY(\"HJ\");\n    printf(\"%s%.2d\", test, 2);\n    printf(\"\\n\\n\");\n\n    return 0;\n}\n\n\n运行结果\n类中直接指针幅值\nHJ?T�+�02\n\nstrcpy\nHJ02\n\nstrncpy 两字节\nHJHJ02\n\nstrncpy 三字节\nHJ02\n\n类外直接指针幅值\nHJ02\n\n\n分析\n\n类中指针赋值\n直接通过指针进行赋值：\n    _head[0] = *p++;\n    _head[1] = *p++;\n输出： sh   HJ?T�+�02\nHJ之后带有一串乱码，应该是由于c++在指针赋值时没有将第三位赋值为\\0，导致打印输出时识别不到结束符一直输出。\nstrcpy赋值\n直接用strcpy，看起来好像还行，但是到了下一步就发现问题了。\nstrncpy两字节\n再新建一个类，然后对该类的_head进行strncpy两个字节。\n输出：\nHJHJ02\n应该是由于strncpy也没有在最后添加\\0，并且新创建的变量的地址正好在之前变量的地址之前，最后出现了这个现象。\nstrncpy三字节\n再新建一个类，然后对该类的_head进行strncpy三个字节。\n输出:\nHJ02\n说明这个时候是在字符串结尾添加了\\0。\n类外直接指针幅值\n不使用类的方式，直接用函数的方式，进行赋值。\n输出：\nHJ02\n是没有问题的。"
  },
  {
    "objectID": "posts/cvae.html",
    "href": "posts/cvae.html",
    "title": "条件VAE",
    "section": "",
    "text": "这几天时间自己把TensorFlow Probability里面的几个例子过了一遍，希望以后可以做出一些深度学习与概率论结合的成果。\n今天我试着用TensorFlow Probability把条件VAE实现一下。这个条件VAE通过控制传统VAE中的正态分布的均值来达到分类生成的效果，这样每个类别都有一个专属均值，可以通过这个专属均值来生成与此类相似的结果。"
  },
  {
    "objectID": "posts/cvae.html#结果",
    "href": "posts/cvae.html#结果",
    "title": "条件VAE",
    "section": "结果",
    "text": "结果\n\n生成类别8\n\n\n\n生成类别9"
  },
  {
    "objectID": "posts/design-gan.html",
    "href": "posts/design-gan.html",
    "title": "Design GAN",
    "section": "",
    "text": "DESIGN-GAN: CROSS-CATEGORY FASHION TRANSLATION DRIVENBY LANDMARK ATTENTION这是来自Alibaba的一篇论文，不过他投的会议，一共只有5页，感觉有的部分没有说清楚。这篇论文提出一种基于landmark 引导的注意力cyclegan，用于人物换装。"
  },
  {
    "objectID": "posts/design-gan.html#生成器部分",
    "href": "posts/design-gan.html#生成器部分",
    "title": "Design GAN",
    "section": "生成器部分",
    "text": "生成器部分\n\n利用基于HR-Net骨干的landmark回归网络生成landmark heatmap 文章说训练了两个模型，一个检测人体的关键点，一个检测服饰的关键点。然后通过反卷积生成多个通道的heatmap，一个通道对应一个landmark点，利用mseloss训练整个landmark heatmap生成器。\n利用特征提取器对服饰图像与原始图像提取服饰特征\n原文中说服饰图像是利用landmark回归网络生成landmark attention引导服饰区域，但是没有说明是怎么样引导的，我猜测是用物体的landmark将图像的区域进行连接，然后生成mask图像。\n特征提取器实际上应该是有两个，一个对原图进行提取。然后利用landmark生成的mask来提取特征。\n特征concat之后进行生成\n这里我认为他的实现上是两个独立的生成器或者中间带特征融合的多输出生成器，一个生成目标图像，另一个生成目标服饰的mask。"
  },
  {
    "objectID": "posts/design-gan.html#判别器部分",
    "href": "posts/design-gan.html#判别器部分",
    "title": "Design GAN",
    "section": "判别器部分",
    "text": "判别器部分\n\n特征提取 用相同的特征提取器进行特征提取\n利用landmark对人体特征进行加权\n回归出来的landmark heatmap原文说是尺度与原图像相同，这里又使用逐元素积加权，那么推测特征提取器的输出特征应该是3维的，甚至可能和原图大小也一样。\n特征融合后判别器判别"
  },
  {
    "objectID": "posts/design-gan.html#cyclegan-loss",
    "href": "posts/design-gan.html#cyclegan-loss",
    "title": "Design GAN",
    "section": "CycleGAN loss",
    "text": "CycleGAN loss\n还是几个基本的loss，lsgan loss+循环一致性 loss+身份映射 loss：\n\\[\n\\begin{aligned}\n\\mathcal{L}_{L S G A N}=& \\mathbb{E}_{(y, \\mathbf{b}) \\sim p_{\\text {data}}}\\left[\\left(D_{Y}(y, \\mathbf{b})-1\\right)^{2}\\right]+\\\\\n& \\mathbb{E}_{(x, \\mathbf{a}) \\sim p_{\\text {data}}}\\left[D_{Y}\\left(G_{X Y}(x, \\mathbf{a})\\right)^{2}\\right]\n\\end{aligned}\n\\]\n\\[\n\\begin{array}{r}\n\\mathcal{L}_{c y c}=\\mathbb{E}_{(x, \\mathbf{a}) \\sim p_{\\text {data}}}\\left[\\left\\|G_{Y X}\\left(G_{X Y}(x, \\mathbf{a})\\right)-(x, \\mathbf{a})\\right\\|_{1}\\right]+ \\\\\n\\mathbb{E}_{(y, \\mathbf{b}) \\sim p_{\\text {data}}}\\left[\\left\\|G_{X Y}\\left(G_{Y X}(y, \\mathbf{b})\\right)-(y, \\mathbf{b})\\right\\|_{1}\\right]\n\\end{array}\n\\]\n\\[\n\\begin{array}{r}\n\\mathcal{L}_{i d t}=\\mathbb{E}_{(y, \\mathbf{b}) \\sim p_{\\text {data}}}\\left[\\left\\|G_{X Y}(y, \\mathbf{b})-(y, \\mathbf{b})\\right\\|_{1}\\right]+ \\\\\n\\mathbb{E}_{(x, \\mathbf{a}) \\sim p_{\\text {data}}}\\left[\\left\\|G_{Y X}(x, \\mathbf{a})-(x, \\mathbf{a})\\right\\|_{1}\\right] .\n\\end{array}\n\\]"
  },
  {
    "objectID": "posts/design-gan.html#纹理损失和皮肤损失",
    "href": "posts/design-gan.html#纹理损失和皮肤损失",
    "title": "Design GAN",
    "section": "纹理损失和皮肤损失",
    "text": "纹理损失和皮肤损失\n对于生成前后服饰区域的图像，统一resize到相同大小，然后直接计算rgb的像素差异作为损失。这里有个比较好的点就是提出\\(w_{\\text {style}}\\)，应该类似center loss中的高斯heatmap的反向，越中心的权重越低，越边缘的区域权重越高，因为边缘区域的细节更加重要（比如裙子和背景的交界处）\n\\[\n\\mathcal{L}_{\\text {style}}=\\frac{1}{2 N} \\sum_{n=1}^{N} \\sum_{c=1}^{3}\\left\\|p(\\mathbf{a})_{i, j}-p\\left(\\mathbf{b}^{\\prime}\\right)_{i, j}\\right\\|_{2} \\circ w_{\\text {style}}\n\\]\n皮肤损失和上面类似，这里是使用了预先准备的皮肤纹理图像进行与原始图像中的皮肤进行匹配（根据landmark提取手臂关节处的皮肤，但文章没写穿长袖的时候怎么办。。）\n\\[\n\\mathcal{L}_{s k i n}=\\frac{1}{2 N} \\sum_{n=1}^{N} \\sum_{c=1}^{3}\\left\\|p\\left(\\mathbf{a}_{s}\\right)_{i, j}-p\\left(\\mathbf{b}_{s}^{\\prime}\\right)_{i, j}\\right\\|_{2} \\circ w_{s k i n}\n\\]\n最后多个损失合成一个损失。\n\\[\n\\begin{array}{r}\n\\mathcal{L}_{A l l}=\\mathcal{L}_{L S G A N}+\\lambda_{c y c} \\mathcal{L}_{c y c}+\\lambda_{i d t} \\mathcal{L}_{i d t}+ \\\\\n\\lambda_{s t y l e} \\mathcal{L}_{s t y l e}+\\lambda_{s k i n} \\mathcal{L}_{s k i n}\n\\end{array}\n\\]"
  },
  {
    "objectID": "posts/design-gan.html#纹理定制",
    "href": "posts/design-gan.html#纹理定制",
    "title": "Design GAN",
    "section": "纹理定制",
    "text": "纹理定制\n直接将纹理图像填充到上面描述的目标服饰的mask中，然后利用纹理损失就可以进行定制纹理了。"
  },
  {
    "objectID": "posts/dl-costmodel.html",
    "href": "posts/dl-costmodel.html",
    "title": "基于DL的CostModel",
    "section": "",
    "text": "调研一些使用机器学习/深度学习方法构造神经网络CostModel的论文.\n\n\nTLP: A Deep Learning-based Cost Model for Tensor Program Tuning\n他这里是把对源代码的schedule的类型进行onehot, 然后名字参数进行 tokenize, 数值参数不改变.\n\\[\n\\begin{aligned}\nF =  F_{1} (\\tau) (F_{2} (id) |F_{3} (num)) \\\\\nF_1 : \\text{PrimitiveType} \\rightarrow \\text{OnehotVector}  \\\\\nF_2 : \\text{NameParam} \\rightarrow \\text{Token}  \\\\\nF_3 : \\text{Number} \\rightarrow \\text{Number}  \\\\\n\\text{PrimitiveType} \\in { \\text{split}, \\text{reorder}, \\text{fuse} } \\\\\n\\text{NameParam} := \\text{id}\n\\end{aligned}\n\\]\n特征提取的流程图如下:\n\n他的模型基本上是基于transformer, 讲数据加载进来之后分为input[:setp_size,:feat_size], 这里setp_size,feat_size分别为25,22. 应该说默认一共调度25次, 以及每个调度的参数长22.\n\n\nEfficient Automatic Scheduling of Imaging and Vision Pipelines for the GPU\n这个是通过分析原始调度中的一系列特征值进行分类. 将pipeline_features, schedule_features送到两个输入头中, 然后分别进行全连接之后再concat之后继续全连接."
  },
  {
    "objectID": "posts/doublelist.html",
    "href": "posts/doublelist.html",
    "title": "循环链表",
    "section": "",
    "text": "我对这个循环链表的理解是一个节点既有指向前一个元素的指针，又有指向后一个元素的指针。 \n\n空链表的定义就如下： \n\n程序\n```c /  @Author: Zheng Qihang * @Date: 2018-07-03 21:33:00 * @Last Modified by: Zheng Qihang * @Last Modified time: 2018-11-08 16:36:43 */ #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;stdint.h&gt; #include &lt;time.h&gt;\ntypedef int ElementType; //data type #define Node ptrNode //Node defination #define List ptrNode //list defination typedef struct _Node //node defination { ElementType data; struct _Node *pre; struct _Node next; }  ptrNode; //Node is a pointer to _Node\nvoid logd(const char *c, int d) { printf(“%s is %d”, c, d); }\nList MakeEmpty(uint8_t Len, List L) { Node tepNode = NULL; Node last = NULL; // printf(“sizeof(List)=%d sizeof(Node)=%d”,sizeof(List),sizeof(Node)); // Test !!! malloc(sizeof(List)) and malloc(sizeof(Node)) does have different L = (List)malloc(sizeof(struct _Node)); //create a list header! srand((unsigned int)time(0)); L-&gt;data = rand() % 10; L-&gt;next = L; L-&gt;pre = L; last = L; while (Len–) { tepNode = (Node)malloc(sizeof(struct _Node)); last-&gt;next = tepNode; tepNode-&gt;data = rand() % 10; // logd(“tepNode-&gt;data”, tepNode-&gt;data); tepNode-&gt;next = L; tepNode-&gt;pre = last; last = tepNode; } L-&gt;pre=last; return L; }\nint IsEmpty(List L) { return (L-&gt;next == L-&gt;pre); }\nint IsLast(Node P, List L) { return (P-&gt;next == L); }\nNode Find(ElementType x, List L) { Node temnode = L-&gt;next; while (temnode-&gt;next != L) { if (temnode-&gt;data == x) { return temnode; } temnode = temnode-&gt;next; } return NULL; }\nvoid Delete(ElementType x, List L) { Node temnode = L-&gt;next; while (temnode-&gt;next != L) { if (temnode-&gt;data == x) { temnode-&gt;pre-&gt;next = temnode-&gt;next; temnode-&gt;next-&gt;pre = temnode-&gt;pre; printf(“delete the %p data is %d”, temnode, temnode-&gt;data); free(temnode); return; } temnode = temnode-&gt;next; } printf(“delete failed”); }\nvoid Insert(ElementType X, List L, Node P) { if (P != NULL) { Node newNode = (Node)malloc(sizeof(struct _Node)); newNode-&gt;data = X; newNode-&gt;pre = P; newNode-&gt;next = P-&gt;next; P-&gt;next-&gt;pre = newNode; P-&gt;next = newNode; } else { printf(“error:Node is null”); } }\nvoid DeleteList(List L) { if (L-&gt;next == L) { printf(“List already empty!”); return; } // printf(“%p”,L-&gt;pre); // printf(“%p”,L); // printf(“%p”,L-&gt;next); Node tmpNode = L-&gt;pre; do { tmpNode = tmpNode-&gt;pre; free(tmpNode-&gt;next); } while (tmpNode!=L); tmpNode-&gt;next=tmpNode; tmpNode-&gt;pre=tmpNode; }\nvoid PrintList(List L) { Node tep = L-&gt;next; printf(“——————————————————–”); printf(“| |”);\nfor (; tep != L; tep = tep-&gt;next)\n{\n    printf(\"|preaddr=0x%lX   addr=0x%lX   data=%d   nextaddr=0x%lX  |\\n\",\n           (unsigned long int)tep-&gt;pre & 0xFFF, (unsigned long int)tep & 0xFFF,\n           tep-&gt;data, (unsigned long int)tep-&gt;next & 0xFFF);\n}\n\nprintf(\"|                                                      |\\n\");\nprintf(\"--------------------------------------------------------\\n\");\n}\nint main(int argc, char const *argv[]) { int tempint; List MyList = NULL; Node pos = NULL; printf(“\n循环链表的基本操作：\n(a).make new linked list;\n(b).check the list is empty;\n(c).check the node is list’s tail;\n(d).find the data in list;\n(e).find the data in list and delete it;\n(f).Insert the data in List(first need (d));\n(g).Delete the all List:\n(p).print the all list;\n(q).quit;\n”); while (1) { switch (getchar()) { case ‘a’: printf(“please make sure list len:”); scanf(“%d”, &tempint); MyList = MakeEmpty((uint8_t)tempint, MyList);\n        if (MyList != NULL)\n        {\n            printf(\"Make a Len:%d List\\n\", tempint);\n        }\n\n        break;\n    case 'b':\n        if (IsEmpty(MyList))\n        {\n            printf(\"Is Empty!\\n\");\n        }\n        else\n        {\n            printf(\"Isn't Empty!\\n\");\n        }\n\n        break;\n    case 'c':\n        if (IsLast(pos, MyList))\n        {\n            printf(\"Is IsLast!\\n\");\n        }\n        else\n        {\n            printf(\"Isn't IsLast!\\n\");\n        }\n        break;\n    case 'd':\n        printf(\"please tell me which data you need find:\");\n        scanf(\"%d\", &tempint);\n        pos = Find(tempint, MyList);\n        printf(\"find the data in %p\\n\", pos);\n        break;\n    case 'e':\n        printf(\"please tell me which data you need delete:\");\n        scanf(\"%d\", &tempint);\n        Delete(tempint, MyList);\n        break;\n    case 'f':\n        printf(\"please tell me which data you need Insert:\");\n        scanf(\"%d\", &tempint);\n        Insert(tempint, MyList, pos);\n        break;\n    case 'g':\n        DeleteList(MyList);\n        break;\n    case 'p':\n        PrintList(MyList);\n        break;\n    case 'q':\n        exit(0);\n        break;\n    default:\n        break;\n    }\n}\n\nreturn 0;\n} ``\n`"
  },
  {
    "objectID": "posts/dtf-idft.html",
    "href": "posts/dtf-idft.html",
    "title": "声音信号处理-DTF与DTFT",
    "section": "",
    "text": "现在要做声音相关,所以先学习声音信号处理的内容,这个将会成为一个系列."
  },
  {
    "objectID": "posts/dtf-idft.html#例子-1",
    "href": "posts/dtf-idft.html#例子-1",
    "title": "声音信号处理-DTF与DTFT",
    "section": "例子 1",
    "text": "例子 1\n\\[\n\\begin{align}\ns^*_k&=e^{-j2\\pi k n / N}=cos(2 \\pi k n / N)-jsin(2 \\pi k n/N) \\\\\n如果 N&=4，那么n=0,1,2,3; k=0,1,2,3 \\\\\ns^*_0&=cos(2\\pi*0*n/4)-jsin(2\\pi*0*n/4)=[1,1,1,1] \\\\\ns^*_1&=cos(2\\pi*1*n/4)-jsin(2\\pi*1*n/4)=[1,-j,-1,j] \\\\\ns^*_2&=cos(2\\pi*2*n/4)-jsin(2\\pi*2*n/4)=[1,-1,1,-1] \\\\\ns^*_3&=cos(2\\pi*3*n/4)-jsin(2\\pi*3*n/4)=[1,j,-1,-j] \\\\\n\\end{align}\n\\] 可以观察下面的图,实际上DFT就是每个频率点与不同正弦波进行內积.如果內积为0那么说明特定频率的正弦波不存在与此波形中.\n# 例子 1\nN = 8\nplt.figure(1, figsize=(9.5, 6))\nfor k in range(N):\n    s = np.exp(-1j*2*np.pi*k/N*np.arange(N))\n    plt.subplot(N/2, 2, k+1)\n    plt.plot(np.real(s), 'b', lw=1.5)\n    plt.axis([0,N-1,-1.5,1.5])\n    plt.title(r\"$s^{*}_{%s}$\"%(k), fontsize=18)\n    plt.subplot(N/2, 2, k+1)\n    plt.plot(np.imag(s), 'g', lw=1.5)\n    plt.axis([0,N-1,-1.5,1.5])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/dtf-idft.html#例子2-标量乘积",
    "href": "posts/dtf-idft.html#例子2-标量乘积",
    "title": "声音信号处理-DTF与DTFT",
    "section": "例子2 : 标量乘积",
    "text": "例子2 : 标量乘积\n下面给定一个x=[1,1,1,1,-1,-1,-1,-1]进行计算\nx = np.array([1,1,1,1,-1,-1,-1,-1])\nN = 8\nmX = ()\npX = ()\nplt.figure(1, figsize=(9.5, 6))\n\nplt.subplot(3,1,1)\nplt.plot(x,marker='x',color='b', lw=1.5)\nplt.axis([0,N-1,-1.5,1.5])\nplt.title('x = [1,1,1,1,-1,-1,-1,-1]', fontsize=18)\n\nfor k in range(8):\n    s = np.exp(1j*2*np.pi*k/N*np.arange(N))\n    X = sum(x*np.conjugate(s))\n    mX = np.append(mX, np.abs(X))\n    pX = np.append(pX, np.angle(X))\n\nplt.subplot(3,1,2)\nplt.plot(mX, marker='x', color='r', lw=1.5)\nplt.title('$abs(&lt;x,s_k&gt;)$', fontsize=18) # 幅度谱\n\nplt.subplot(3,1,3)\nplt.plot(pX, marker='x', color='c', lw=1.5)\nplt.title('$angle(&lt;x,s_k&gt;)$', fontsize=18) # 相位谱\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/dtf-idft.html#例子-3",
    "href": "posts/dtf-idft.html#例子-3",
    "title": "声音信号处理-DTF与DTFT",
    "section": "例子 3",
    "text": "例子 3\n对一段音频数据进行DFT变换\nimport sys\nimport math\n\nsys.path.append('../software/models')\nimport utilFunctions as UF\nimport dftModel as DFT\n\n(fs, x) = UF.wavread('../sounds/oboe-A4.wav')\nw = np.hamming(511)\nN = 512\npin = 5000\nhM1 = int(math.floor((w.size+1)/2)) \nhM2 = int(math.floor(w.size/2))  \nx1 = x[pin-hM1:pin+hM2]\nmX, pX = DFT.dftAnal(x1, w, N)\n\nplt.figure(1, figsize=(9.5, 7))\n\nplt.subplot(311)\nplt.plot(np.arange(-hM1, hM2), x1, lw=1.5)\nplt.axis([-hM1, hM2, min(x1), max(x1)])\nplt.ylabel('amplitude')\nplt.title('x (oboe-A4.wav)')\n\nplt.subplot(3,1,2)\nplt.plot(np.arange(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,mX.size,min(mX),max(mX)])\nplt.title ('magnitude spectrum: mX = 20*log10(abs(X))')\nplt.ylabel('amplitude (dB)')\n\nplt.subplot(3,1,3)\nplt.plot(np.arange(mX.size), pX, 'c', lw=1.5)\nplt.axis([0,mX.size,min(pX),max(pX)])\nplt.title ('phase spectrum: pX=unwrap(angle(X))')\nplt.ylabel('phase (radians)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/dtf-idft.html#例子-1-1",
    "href": "posts/dtf-idft.html#例子-1-1",
    "title": "声音信号处理-DTF与DTFT",
    "section": "例子 1",
    "text": "例子 1\n\\[\n\\begin{align}\nX[k]&=[0,4,0,0];\\ \\ N=4 \\\\\nx[0]&=\\frac{1}{4}(X*x)[n=0]=\\frac{1}{4}(0*1+4*1+0*1+0*1)=1 \\\\\nx[1]&=\\frac{1}{4}(X*x)[n=1]=\\frac{1}{4}(0*1+4*1+0*(-1)+0*(-j))=j \\\\\nx[2]&=\\frac{1}{4}(X*x)[n=2]=\\frac{1}{4}(0*1+4*(-1)+0*1+0*(-1))=-1 \\\\\nx[3]&=\\frac{1}{4}(X*x)[n=3]=\\frac{1}{4}(0*1+4*(-j)+0*(-1)+0*j)=-j\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/dtf-idft.html#例子-2",
    "href": "posts/dtf-idft.html#例子-2",
    "title": "声音信号处理-DTF与DTFT",
    "section": "例子 2",
    "text": "例子 2\n真实信号的逆DFT \\[X[k]=|X[k]e^{j&lt;X[k]} and X[-K]=|X[k]|e^{-j&lt;X[k]}\\ \\ \\ \\ k=0,1,...,N/2-1\\]\nimport sys\n\nsys.path.append('../software/models/')\nimport dftModel as DFT\nimport math\n\nk0 = 8.5\nN = 64\nw = np.ones(N)\nx = np.cos(2*np.pi*k0/N*np.arange(-N/2,N/2))\nmX, pX = DFT.dftAnal(x, w, N)\ny = DFT.dftSynth(mX, pX, N)\n\nplt.figure(1, figsize=(9.5, 5))\nplt.subplot(311)\nplt.title('positive freq. magnitude spectrum in dB: mX')\nplt.plot(np.arange(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,mX.size, min(mX), max(mX)+1])\n\nplt.subplot(312)\nplt.title('positive freq. phase spectrum: pX')\nplt.plot(np.arange(pX.size), pX, 'c', lw=1.5)\nplt.axis([0, pX.size,-np.pi,np.pi])\n\nplt.subplot(313)\nplt.title('inverse spectrum: IDFT(X)')\nplt.plot(np.arange(-N/2, N/2), y,'b', lw=1.5)\nplt.axis([-N/2,N/2-1,min(y), max(y)])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/dtf-idft.html#复数sin波形的dft",
    "href": "posts/dtf-idft.html#复数sin波形的dft",
    "title": "声音信号处理-DTF与DTFT",
    "section": "复数sin波形的DFT",
    "text": "复数sin波形的DFT\nN=64 # 长度64\nk0=7 # 频率7\nx=np.exp(1j*2*np.pi*k0/N*np.arange(N))# 构造一个正弦信号\nX=np.array([])\nfor k in range(N):\n    s=np.exp(1j*2*np.pi*k/N*np.arange(N)) # 特定的sin曲线\n    X=np.append(X,sum(x*np.conjugate(s))) # x * sin曲线的复数共轭\n    \nplt.plot(np.arange(N),np.abs(X))\nplt.axis([0,N,0,N])\nplt.show()\n\n# 设置一个频率是7的sin信号\nN=64\nk0=7\nx=np.cos(2*np.pi*k0/N * np.arange(N))\nX=np.array([])\nfor k in range(N):\n    s=np.exp(1j*2*np.pi*k/N*np.arange(N))\n    X=np.append(X,sum(x*np.conjugate(s))) # conjugate 复数共轭\nplt.plot(np.arange(N),np.abs(X))\nplt.axis([0,N,0,N])\n[0, 64, 0, 64]"
  },
  {
    "objectID": "posts/dtf-idft.html#实数sin波形的dft",
    "href": "posts/dtf-idft.html#实数sin波形的dft",
    "title": "声音信号处理-DTF与DTFT",
    "section": "实数sin波形的DFT",
    "text": "实数sin波形的DFT\nN=64\nk0=7.5 # 频率 7.5\nx=np.cos(2*np.pi*k0/N * np.arange(N))\nX=np.array([])\nkv=np.arange(-N/2,N/2)\n\nfor k in kv:\n    s=np.exp(1j*2*np.pi*k/N*np.arange(N))\n    X=np.append(X,sum(x*np.conjugate(s))) # conjugate 复数共轭\n    \nplt.plot(kv,np.abs(X))\nplt.axis([-N/2,N/2-1,0,N])\nplt.show()"
  },
  {
    "objectID": "posts/easydist.html",
    "href": "posts/easydist.html",
    "title": "Alibaba EasyDist 浅析",
    "section": "",
    "text": "对于阿里巴巴开源的EasyDist: Automated Parallelization System and Infrastructure for Multiple Ecosystems代码解读, 主要关注IR设计与搜索域构造."
  },
  {
    "objectID": "posts/easydist.html#切分标注",
    "href": "posts/easydist.html#切分标注",
    "title": "Alibaba EasyDist 浅析",
    "section": "切分标注",
    "text": "切分标注\n首先引入切分的数据结构:\n\n这里ShardDim表示的是先chunk划分再切分的逻辑,比如[1, 1, 2, 2]可以先划分两块(chunk)-&gt; [1, 1] and [2, 2],再切分(shard)-&gt; [1, 2] | [1, 2]. 使用一维数组[ShardDim]表示一个输入参数所对应的切分, 多个输入时得到类似[[ShardDim], ...]的二维数组, 因此ShardAnnotation就是使用二维数组来存储切分信息, 来表示一个操作的切分输入切分信息.\n假设一个在两个gpu上运行的简单的小网络:\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.fc = torch.nn.Linear(32*32*3, 64, False)\n\n    def forward(self, x: torch.Tensor):0\n        v0 = torch.flatten(x, 1) # 128,32*32*3\n        v1 = self.fc(v0)\n        return v1\n以上计算的计算图如下:\nclass &lt;lambda&gt;(torch.nn.Module):\n    def forward(self, arg0, arg1, arg2, arg3, arg4):\n        arg0_1: f32[64, 3072], arg3_1, arg3_2: f32[128, 3, 32, 32], = fx_pytree.tree_flatten_spec([arg0, arg1, arg2, arg3, arg4], self._in_spec)\n        # No stacktrace found for following nodes\n        view: f32[128, 3072] = torch.ops.aten.view.default(arg3_2, [128, 3072]);  arg3_2 = None\n        t: f32[3072, 64] = torch.ops.aten.t.default(arg0_1)\n        mm: f32[128, 64] = torch.ops.aten.mm.default(view, t);  view = t = None\n        return pytree.tree_unflatten([arg0_1, None, mm], self._out_spec)\n通过一个继承了torch fx的Interpreter的类EDTorchShardingAnn来解释执行整个图, 一边执行一遍获取修改. 对于不同的节点类型, 执行不同的visitleaf函数:\n\nplaceholder\n\nplaceholder对应的应该输入节点, 如果检查到当前节点没有sharding annotation以及combination annotation, 那么会进入preset_meta_spmd函数进行统一处理.\npreset_meta_spmd函数通过不同的op类型分发到不同的rule中进行获取.\ndef preset_meta_spmd(meta_op, input_args=None):\n    if isinstance(meta_op, str):\n        args, kwargs = input_args\n        return PresetMetaSPMD.op_rules[meta_op](args, kwargs)\n    elif meta_op.name in PresetMetaSPMD.op_rules:\n        if input_args is None:\n            args, kwargs = meta_op.input_args\n        else:\n            args, kwargs = input_args\n        return PresetMetaSPMD.op_rules[meta_op.name](args, kwargs)\n    return None, None\n对于placeholder则进入meta_spmd_placeholder函数进行处理, 这里也是使用了统一的view_propagation函数构造sharding annotation以及combination annotation.\ndef meta_spmd_placeholder(args, kwargs):\n    input_shape = args.shape\n    output_shape = args.shape\n\n    view_ann = view_propagation(input_shape, output_shape, world_size=device_mesh_world_size())\n    return view_ann['sharding_ann'], view_ann['combination_ann']\nview_propagation的过程比较复杂, 但是对于输入shape和输出shape相同的节点来说还是比较简单的. 他这里会首先构造[[NoShardDim]* ShapeRank]作为sharding annotation, 比如输入shape为[128, 3, 32, 32], 那么构造出[[NoShardDim, NoShardDim, NoShardDim, NoShardDim]]表示全部不切分. 然后遍历每个维度, 如果当前维度大于world size(所有并行度), 则表示可以切分, 那么就构造一个ShardDim(id++), 对于这个例子将会构造出[[ShardDim(1), NoShardDim, NoShardDim, NoShardDim]].\n\ncall_function\n\n在visit call_function时, 构造出MetaOp后再使用meta_exec去执行. 然后检查是否有sharding_ann, combination_ann的cache,如果没有那么使用preset_meta_spmd来构造.\nargs, kwargs = materialize_args_kwargs(args_meta, kwargs_meta)\nmeta_op = MetaOp(func=target,\n                  input_args=(args, kwargs),\n                  shard_size=device_mesh_world_size(),\n                  name=ops_name)\n# user fake tensor here, maybe use shape/dtype info from `make_fx`\nmeta_out = meta_op.meta_exec(flat_meta_input=pytree.tree_flatten((args_meta,\n                                                                  kwargs_meta))[0])\n.\n.\n.\nsharding_ann, combination_ann = preset_meta_spmd(meta_op, (args_meta, kwargs_meta))\n对于expand来说, 并没有对应的切分信息构造, 所以他的sharding_ann为[[NoShardDim, NoShardDim, NoShardDim]]. 对于view, sharding_ann也是通过view_propagation来构造的. 对于矩阵乘, 通过meta_bmm来构造标注, 此时构造标注时除了需要检查是否可以切分, 同时还需要匹配切分的维度, 比如这里两个输入的batch维度使用相同的id, 矩阵的k维度用相同的id: ShardAnnotation([[ShardDim(1), ShardDim(2)], [ShardDim(2), ShardDim(3)]])\n在preset_meta_spmd中, 也有一些算子没有添加对应的rule, 所以此时就进入meta_op.sharding_discovery构造对应的shard annotation. 比如对于torch.ops.aten.t.default算子, 会在sharding_discovery中调用_try_sharding(self, fixed_annotation, subsequence_annotation, global_output, start_dim=0)函数获得对应的shard annotation. _try_sharding是一个递归的函数, 第一次进入时fixed_annotation=ShardAnnotation([]), subsequence_annotation=ShardAnnotation([[NoShardDim, NoShardDim]]). 然后在其中遍历所有的输入shape dim,逐一修改原始的subsequence_annotation[0], 获得try_annotation, 然后再进入_try_sharding.\nfor dim in range(start_dim, len(subsequence_annotation[0])):\n    if subsequence_annotation[0][dim].shard_dim_id != 0:\n        continue\n    try_annotation = copy.deepcopy(subsequence_annotation[0])\n    try_annotation[dim] = ShardDim.get_shard_dim(shard_dim_id_flag)\n    self._try_sharding(fixed_annotation + ShardAnnotation([try_annotation]),\n                        subsequence_annotation[1:], global_output)\n再次进入时此时shard annotation的subsequence为空, 首先使用当前的fixed_annotation执行self.exec, 获得切分过的sharded_output. 对于这个转置操作, 原本的输出为torch.Size([3072, 64]), 而在fixed_annotation=ShardAnnotation([[ShardDim(1), NoShardDim]])的条件下则获得到[torch.Size([3072, 32]),torch.Size([3072, 32])](转置操作的输入切分在dim 0上,则输出切分在dim 1上).\n进入try_combination(sharded_output, global_output)函数, 这里输入只有一个tensor, 所以进入try_combination_single, 他这里的逻辑就是遍历三种组合函数['try_combination_identity','try_combination_reduce', 'try_combination_gather'], 找到当前所需要的, 显然这个例子中尝试成功后得到的combination_func为functools.partial(&lt;function CombinationFunc.gather at 0x7f5bd501f0d0&gt;, dim=1). 因为将sharded_output gather就可以还原到global_output.\ndef try_combination_single(sharded_output_, global_output_):\n    # check all sharded tensor have equal dimension of global_output\n    for sharded_tensor in sharded_output_:\n        if len(sharded_tensor.shape) != len(global_output_.shape):\n            return None\n\n    for func_name in TRY_COMBINATION_FUNC:\n        combination_func = TRY_COMBINATION_FUNC[func_name](sharded_output_, global_output_)\n        if combination_func:\n            return combination_func\n\n    return None\nif len(subsequence_annotation) == 0:\n    try:\n        sharded_output = self.exec(shard_annotation=fixed_annotation,\n                                    priority_shard_dim_id=shard_dim_id_flag)\n    except RuntimeError as e:\n        logger.debug(f\"[{fixed_annotation}] {e}\")\n        return\n    except:\n        logger.debug(f\"[{fixed_annotation}] run op.exec failed\")\n        return\n\n    haloinfo = None\n    combination_func = try_combination(sharded_output, global_output)\n    .\n    .\n    .\n    if combination_func is not None and not isinstance(combination_func, HaloHint):\n        self.__combination_ann[shard_dim_id_flag] = combination_func\n        # inject haloinfo\n        fixed_annotation.inject_haloinfo(haloinfo, shard_dim_id_flag)\n\n        self.__sharding_annotion = copy.deepcopy(fixed_annotation)\n        self.__find_new_strategy = True\n    else:\n        logger.debug(f\"[{fixed_annotation}] combination failed\")\n    return"
  },
  {
    "objectID": "posts/easydist.html#图转换",
    "href": "posts/easydist.html#图转换",
    "title": "Alibaba EasyDist 浅析",
    "section": "图转换",
    "text": "图转换\n这里需要先描述一下meta ir的数据结构: \n在torch2meta_graph(fx_module: torch.fx.GraphModule, state_tensor_num, sharding_info, meta_info)函数中将原本的带有sharding annotation的fx graph转换为meta的graph, 同时将sharding info也转换为torch的SPMD表示. 具体流程如下:\n\n构造meta_graph\n遍历fx_module中所有的节点, 根据节点类型进行处理(每个fx graph的输出节点除了list/tuple都构造MetaVar, 除了getitem都构造MetaNode)\n\n\ncall_function\n\n构造MetaVar, 此时metavar表示为当前call的输出, MetaVar(name=node.name, shape=meta_info[node.name][\"shape\"], dtype=ABSTRACT_DTYPE[meta_info[node.name][\"dtype\"]])\n构造MetaNode, 此时outvars为当前的MetaVar, invars为空.\n记录Node到MetaGraph的Nodes中\n\nplaceholder或者get_attr\n\n构造MetaVar, 同上.\n构造MetaNode, MetaNode(name=node.name,op_name=node.op,invars=[],outvars=[meta_var],sharding_info=node_sharding_info,is_placeholder=True)\n记录Node到MetaGraph的Nodes和Inputs中\n\noutput\n\n获取output_names\n\n\n\n重新遍历fx module中的所有节点, 更新MetaNode和MetaVar的连接关系\n\n\ncall_function\n\n遍历当前call function对应的MetaNode的输入参数\n对于如果输入参数存在对应的MetaVar, 那么对调用meta_node.set_in_var(in_var, idx)来更新当前meta_node中的invars\n\n\n\n根据output_names把对应的meta var添加到meta graph中\n构造state_io_map, 遍历state_tensor_num, 将输入的node映射到输出的var上\n调用graph.coarsen函数来构造clusters, 这里根据不同的级别选择不同的构造方式, 默认使用build_cone_clusters函数来构造:\n\n遍历所有节点, 收集所有的cone root\n\n如果节点有多个下游节点或者没有下游节点,那么作为cone root.\n如果只有一个下游节点时, 有多个上游节点也是cone root, 有一个上游节点时则需要判断数据量, 如果输出数据量小于输入数据量那么也是cone root\n\n获取所有cone roots的id, 得到root_ids.\n遍历所有的cone roots构造Cluster\n\n构造MetaNodeCluster(unique_id=cluster_id), 其中cluster_id自动递增\n调用build_cone_cluster(cone_root, root_ids, cluster)填充cluster.\n\n将当前root添加到当前cluster中cluster.add_node(cone_root), 此时设定root node的cluster id\n遍历当前cone root的输入节点, 如果当前节点不在之前收集的cone roots中,那么递归的调用build_cone_cluster(up_node, root_ids, cluster)填充cluster\n\n调用finalize函数\n\n确定当前cluster的args和output node\n为当前cluster构造ClusterStrategyPool(self)\n获取输出节点的out_strtg_pool NodeSPMDStrategyPool,\n\n如果当前节点存在strtg_pool的属性那么直接返回\n否则通过sharding_info进行构造\n首先遍历shard ann, 将每一个可切分的维度都构造出一个NodeSPMDStrategy(这里构造则是将ShardDim的表示转换为SPMD的表示)\n比如对于一个[[ShardDim(1), ShardDim(2)]]的shard ann 将构造出三组切分策略NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([SHARD({'dim': 0})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([SHARD({'dim': 0})])])),  NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([SHARD({'dim': 1})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([SHARD({'dim': 1})])])),  NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE])]))\n然后再因为当前的DEVICE_MESH_1D==0, 再将每个NodeSPMDStrategy都进行扩展REPLICATE得到类似NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])]))\n这里我设置的gpu个数为两个, 但是实际上获取到的device mesh为(1,2), 所以应该是为了保证sbp的个数应该和拓扑结构的维度相同, 所以需要扩展VarSPMDStrategy的维度.\n\n遍历out_strtg_pool\n\n构造ClusterStrategy cluster_strtg\n给cluster_strtg设置当前的node_strategy\n使用back_build_strategy将当前的策略反向更新到cluster的其他节点\n将当前cluster_strtg添加到strategy_pool中\n\n\n\n\n\n最终得到的meta graph的一部分如下, 总的来说就是将原图中的节点转换为MetaNode以及MetaVar, 然后根据一定的策略划分出MetaNodeCluster的子图, 接下来对每个Cluster中的所有Node添加一系列的切分候选集:\n=====================\n[MetaIR]\n\ninput_list: [arg0_1, arg2_1, arg3_3, arg3_4]\n\n[arg0_1] &lt;--- [placeholder] --- []\n[arg2_1] &lt;--- [placeholder] --- []\n[arg3_3] &lt;--- [placeholder] --- []\n[arg3_4] &lt;--- [placeholder] --- []\n[view] &lt;--- [torch.ops.aten.view.default] --- [arg3_3]\n[t] &lt;--- [torch.ops.aten.t.default] --- [arg0_1]\n[mm] &lt;--- [torch.ops.aten.mm.default] --- [view, t]\n[_log_softmax] &lt;--- [torch.ops.aten._log_softmax.default] --- [mm]\n[getitem, getitem_1] &lt;--- [torch.ops.aten.nll_loss_forward.default] --- [_log_softmax, arg3_4]\n[ones_like] &lt;--- [torch.ops.aten.ones_like.default] --- [getitem]\n[nll_loss_backward] &lt;--- [torch.ops.aten.nll_loss_backward.default] --- [ones_like, _log_softmax, arg3_4, getitem_1]\n[_log_softmax_backward_data] &lt;--- [torch.ops.aten._log_softmax_backward_data.default] --- [nll_loss_backward, _log_softmax]\n[t_1] &lt;--- [torch.ops.aten.t.default] --- [_log_softmax_backward_data]\n[mm_1] &lt;--- [torch.ops.aten.mm.default] --- [t_1, view]\n[t_2] &lt;--- [torch.ops.aten.t.default] --- [mm_1]\n[t_3] &lt;--- [torch.ops.aten.t.default] --- [t_2]\n[mul_] &lt;--- [torch.ops.aten.mul_.Tensor] --- [arg2_1]\n[add_] &lt;--- [torch.ops.aten.add_.Tensor] --- [mul_, t_3]\n[add__1] &lt;--- [torch.ops.aten.add_.Tensor] --- [arg0_1, add_]\n\noutput_list: [add__1, add_, getitem]\n=====================\n\nnode clusters:\ncluster id: 0\n  nodes: arg0_1, \n  inputs: [[arg0_1, 0]]\n  output: arg0_1\nstrategies:\nnode strategies: \n{\n1: [NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])])),\n    NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 1})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 1})])])),\n    NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, REPLICATE])]))],\n}\nnode io strategies: \n{\n1: arg0_1\nin strategies: [[VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})]), VarSPMDStrategy([REPLICATE, SHARD({'dim': 1})]), VarSPMDStrategy([REPLICATE, REPLICATE])]]\noutput strategies: [[VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})]), VarSPMDStrategy([REPLICATE, SHARD({'dim': 1})]), VarSPMDStrategy([REPLICATE, REPLICATE])]],\n}\ncluster id: 1\n  nodes: arg3_4, \n  inputs: [[arg3_4, 0]]\n  output: arg3_4\nstrategies:\nnode strategies: \n{\n7: [NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])])),\n    NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, REPLICATE])]))],\n}\nnode io strategies: \n{\n7: arg3_4\nin strategies: [[VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})]), VarSPMDStrategy([REPLICATE, REPLICATE])]]\noutput strategies: [[VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})]), VarSPMDStrategy([REPLICATE, REPLICATE])]],\n}\ncluster id: 2\n  nodes: view, arg3_3, \n  inputs: [[arg3_3, 0]]\n  output: view\nstrategies:\nnode strategies: \n{\n9: [NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])])),\n    NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 1})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 1})])])),\n    NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, REPLICATE])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, REPLICATE])]))],\n5: [NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 0})])])),\n    NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 1})])]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, SHARD({'dim': 1})])])),\n    NodeSPMDStrategy(in_strtg_group: VarSPMDStrategyGroup([]), out_strtg_group: VarSPMDStrategyGroup([VarSPMDStrategy([REPLICATE, REPLICATE])]))],\n}\n.\n.\n."
  },
  {
    "objectID": "posts/easydist.html#约束模型与求解",
    "href": "posts/easydist.html#约束模型与求解",
    "title": "Alibaba EasyDist 浅析",
    "section": "约束模型与求解",
    "text": "约束模型与求解\n这里使用mip构造了一个约束模型, 对于enable_graph_coarsen的配置下, 会进入add_coarsen_graph来将meta graph转换为约束模型. 在这个函数中遍历所有meta graph中的cluster执行add_cluster.\n具体看一下add_cluster: 1. 获取cluster的strtg_pool 2. 遍历pool中的切分策略, 为每个切分策略构造出一个对应的binary var, 来表示是否选择这个切分. 3. 使用对应的pool与pick vars构造出mip_info = ClusterMipInfo(cluster, cluster_strtg_pool, mip_vars) 4. 遍历当前cluster的args进行加边self.add_cluster_edge(var, input_idx, down_node=input_node) 1. 对于一个有上下连接关系的cluster,需要进行加边 2. 获取他的输入node的outvar的切分策略up_out_strategy_list,获取他输出node的invar切分策略候选down_in_strategy_list 3. 构造出[up_out_strategy_list,down_in_strategy_list]大小的mip binary var矩阵, 用于标记每个连接是否选择 4. 再计算[up_out_strategy_list,down_in_strategy_list]大小的通信开销矩阵, 也就是对于每个切分的组合计算额外的通信开销 5. 再计算[up_out_strategy_list,down_in_strategy_list]大小的内存开销矩阵 5. 开始ilp_solve 1. 添加约束目标, 初始化comm_cost, mem_cost, 再遍历所有的cluster edge进行update 2. comm_cost += mip.xsum(mip_var[i][j] * comm_matrix[i][j] for i in range(shape_1) for j in range(shape_2)) 3. mem_cost += mip.xsum(mip_var[i][j] * mem_matrix[i][j] for i in range(shape_1) for j in range(shape_2)) 4. 遍历[up_out_strategy_list,down_in_strategy_list]大小的矩阵添加总cost中. 5. 再次遍历所有的cluster edge 6. 添加[up_out_strategy_list,down_in_strategy_list]的binary var矩阵和为1的约束, 这样确保只选择一种切分 7. 添加上下连接关系的约束, 也就是输入/输出选择某种切分, 那么矩阵中对应的节点也必须选择某种切分. 8. 遍历所有的cluster, 添加每个cluster只选择一个切分的约束 9. 添加目标为mip.minimize(comm_cost + 0.00000001 * mem_cost) 6. 求解并提取出新的graph"
  },
  {
    "objectID": "posts/egg.html",
    "href": "posts/egg.html",
    "title": "egg 浅析",
    "section": "",
    "text": "主要分析egraphs-good也就是egg这个库的实现机制.因为最近发现适配到基于relay的ir中存在一些问题,因此还是需要仔细研究一下他的实现细节.\n\n\n1. Language\nLanguage应该就是代表的是enode, 由一个op以及若干children组成. 他的构建机制每个dsl中自己实现from_op函数.\n\npub struct Symbol(u32);\n\npub struct Id(u32);\n\npub struct SymbolLang {\n    /// The operator for an enode\n    pub op: Symbol,\n    /// The enode's children `Id`s\n    pub children: Vec&lt;Id&gt;,\n}\n\nimpl FromOp for SymbolLang {\n    type Error = Infallible;\n\n    fn from_op(op: &str, children: Vec&lt;Id&gt;) -&gt; Result&lt;Self, Self::Error&gt; {\n        Ok(Self {\n            op: op.into(),\n            children,\n        })\n    }\n}\n此时SymbolLang中op对应的类型是symbol (u32),他的内部维护了一个string hashset,然后调用op.into()从hashset中取得对应的index作为symbol. 这里的children的类型是id (u32), 本意是表示eclass的id, 但如果没有加入egraph之前实际上是共用symobl的值.\n\n\n2. RecExpr\nRecExpr表示是一组由用户定义的language组成的递归的expression, 比如我构建输入a + b, 那么此时RecExpr的nodes由 [+, a, b],[a],[b]三个language节点组成. 即保存了输入表达式下的所有 language node.\npub struct RecExpr&lt;L&gt; {\n    nodes: Vec&lt;L&gt;,\n}\n他是通过递归的parser构建的,每次解析一个 language node然后放入到RecExpr中去.\nimpl&lt;L: FromOp&gt; FromStr for RecExpr&lt;L&gt; {\n    type Err = RecExprParseError&lt;L::Error&gt;;\n\n    fn from_str(s: &str) -&gt; Result&lt;Self, Self::Err&gt; {\n        use RecExprParseError::*;\n\n        fn parse_sexp_into&lt;L: FromOp&gt;(\n            sexp: &Sexp,\n            expr: &mut RecExpr&lt;L&gt;,\n        ) -&gt; Result&lt;Id, RecExprParseError&lt;L::Error&gt;&gt; {\n          ...\n        }\n\n        let mut expr = RecExpr::default();\n        let sexp = symbolic_expressions::parser::parse_str(s.trim()).map_err(BadSexp)?;\n        parse_sexp_into(&sexp, &mut expr)?;\n        Ok(expr)\n    }\n}\n\n\n3. EClass\nEClass定义.\npub struct EClass&lt;L, D&gt; {\n    /// 当前的eclass id\n    pub id: Id,\n    /// 所有等价的enode.\n    pub nodes: Vec&lt;L&gt;,\n    /// The analysis data associated with this eclass.\n    pub data: D,\n    /// eclass的反向连接 (也就是哪些enode使用了当前的eclass, 是为了在repair的时候进行递归).\n    pub(crate) parents: Vec&lt;(L, Id)&gt;,\n}\n\n\n4. EGraph\npub struct EGraph&lt;L: Language, N: Analysis&lt;L&gt;&gt; {\n    /// The `Analysis` given when creating this `EGraph`.\n    pub analysis: N,\n    /// The `Explain` used to explain equivalences in this `EGraph`.\n    pub(crate) explain: Option&lt;Explain&lt;L&gt;&gt;,\n    unionfind: UnionFind,\n    /// Stores each enode's `Id`, not the `Id` of the eclass.\n    /// Enodes in the memo are canonicalized at each rebuild, but after rebuilding new\n    /// unions can cause them to become out of date.\n    #[cfg_attr(feature = \"serde-1\", serde(with = \"vectorize\"))]\n    memo: HashMap&lt;L, Id&gt;, // memo 表示的就是hashcon, 即enode -&gt; eclass id\n    // pending 表示的是后续需要repair的enode节点和对应原始的eclass id(没有find过).\n    pending: Vec&lt;(L, Id)&gt;,\n    analysis_pending: IndexSet&lt;(L, Id)&gt;,\n    #[cfg_attr(\n        feature = \"serde-1\",\n        serde(bound(\n            serialize = \"N::Data: Serialize\",\n            deserialize = \"N::Data: for&lt;'a&gt; Deserialize&lt;'a&gt;\",\n        ))\n    )]\n    // 保存了eclass id 对应 eclass的map, 虽然eclass中已经保存了id, 但是在egraph中加一个字典后续更加方便.\n    classes: HashMap&lt;Id, EClass&lt;L, N::Data&gt;&gt;,\n    #[cfg_attr(feature = \"serde-1\", serde(skip))]\n    #[cfg_attr(feature = \"serde-1\", serde(default = \"default_classes_by_op\"))]\n    pub(crate) classes_by_op: HashMap&lt;std::mem::Discriminant&lt;L&gt;, HashSet&lt;Id&gt;&gt;,\n    /// 表示当前的egraph是否需要repair, 修复后clean=true.\n    #[cfg_attr(feature = \"serde-1\", serde(skip))]\n    pub clean: bool,\n}\n\n\n5. Rewrite\npub struct Rewrite&lt;L, N&gt; {\n    /// The name of the rewrite.\n    pub name: Symbol,\n    /// The searcher (left-hand side) of the rewrite.\n    pub searcher: Arc&lt;dyn Searcher&lt;L, N&gt; + Sync + Send&gt;,\n    /// The applier (right-hand side) of the rewrite.\n    pub applier: Arc&lt;dyn Applier&lt;L, N&gt; + Sync + Send&gt;,\n}\n\n\n5.1 rebuild\negraph 首先添加进入后都是没有clean的,所以需要rebuild一次\n\n\n5.1.2 收集class_by_op用于类型匹配.\negg的添加是,遍历这个eclass中所有的enode,然后enode把他所属的eclass id存入discriminant的key中.\nlet mut add = |n: &L| {\n    #[allow(clippy::mem_discriminant_non_enum)]\n    let key = std::mem::discriminant(n);\n    log::debug!(\"Add : {:?} class id : {:?} into key : {:?} \", n, class.id, key);\n    classes_by_op\n        .entry(key)\n        .or_default()\n        .insert(class.id)\n};\n\nlet mut nodes = class.nodes.iter();\nif let Some(mut prev) = nodes.next() {\n    add(prev);\n    for n in nodes { // 如果这个eclass中有多个enode, 检查后续的节点是否与之前的节点相同,不相同就继续添加id.\n        if !prev.matches(n) {\n            add(n);\n            prev = n;\n        }\n    }\n}\n因为rust的enum是可以提供完全不同结构的类型, 因此discriminant就是映射他的结构类型到int key, 他的好处就是你可以添加很多很多不同类型的ir,这样基于类型的enode匹配就很简单的从字典中获取一个入口eclass开始匹配即可. 虽然如下所示,App可能存在很多个enode,但是至少能从类型上消除很大一部分的候选了.\n[DEBUG egg::egraph] Add : App([93, 94]) class id : 54 into key : Discriminant(5) \n[DEBUG egg::egraph] Add : Let([45, 49, 53]) class id : 54 into key : Discriminant(7) \n[DEBUG egg::egraph] Add : Add([5, 47]) class id : 48 into key : Discriminant(3) \n[DEBUG egg::egraph] Add : Lambda([26, 41]) class id : 42 into key : Discriminant(6) \n[DEBUG egg::egraph] Add : App([35, 33]) class id : 36 into key : Discriminant(5)\n\n\n5.1 Rewrite Marco\n通过一个rewrite!的宏,将lhs,rhs构造成两部分.\npub struct Rewrite&lt;L, N&gt; {\n    /// The name of the rewrite.\n    pub name: Symbol,\n    /// 可以是从expr构建/ 也可以是自定义的匹配的方式\n    pub searcher: Arc&lt;dyn Searcher&lt;L, N&gt; + Sync + Send&gt;,\n    /// 获得对应的结果, 可以是expr也可以自定义构建\n    pub applier: Arc&lt;dyn Applier&lt;L, N&gt; + Sync + Send&gt;,\n}\n\n\n5.2 Pattern Match\n这里的匹配是调用rewriter的search进行搜索. 首先这里的searcher是从字符串构造, 首先通过字符串解析为PatternAst\npub type PatternAst&lt;L&gt; = RecExpr&lt;ENodeOrVar&lt;L&gt;&gt;;\n\nfn from_str(s: &str) -&gt; Result&lt;Self, Self::Err&gt; {\n    PatternAst::from_str(s).map(Self::from)\n}\n然后通过PatternAst构造出新的Pattern对象.\nimpl&lt;L: Language&gt; Pattern&lt;L&gt; {\n  /// Creates a new pattern from the given pattern ast.\n    pub fn new(ast: PatternAst&lt;L&gt;) -&gt; Self {\n      let ast = ast.compact();\n        let program = machine::Program::compile_from_pat(&ast);\n        Pattern { ast, program }\n    }\n\n    /// Returns a list of the [`Var`]s in this pattern.\n    pub fn vars(&self) -&gt; Vec&lt;Var&gt; {\n      let mut vars = vec![];\n        for n in self.ast.as_ref() {\n          if let ENodeOrVar::Var(v) = n {\n            if !vars.contains(v) {\n              vars.push(*v)\n                }\n            }\n        }\n        vars\n    }\n}\n这里要注意到有machine的机制, egg是写了一个类似于虚拟机的东西, 将pattern解析为匹配的指令码, 然后通过虚拟机执行指令从而完成匹配的功能.\n\nstruct Machine {\n    reg: Vec&lt;Id&gt;,\n    // a buffer to re-use for lookups\n    lookup: Vec&lt;Id&gt;,\n}\n\npub struct Program&lt;L&gt; {\n    instructions: Vec&lt;Instruction&lt;L&gt;&gt;,\n    subst: Subst,\n}\n\npub(crate) fn compile_from_pat(pattern: &PatternAst&lt;L&gt;) -&gt; Self {\n    let program = Compiler::new(pattern).compile();\n    log::debug!(\"Compiled {:?} to {:?}\", pattern.as_ref(), program);\n    program\n}\npattern的search是将自身的pattern ast转换为Enode,然后使用discriminant获取这个enode的op的类型,再从egraph中寻找所有的elass进行下一步匹配.\n但是我还是没懂discriminant是怎么获取的key是怎样的.官方文档上说此函数返回值只关心enum的类型,而不关心具体的值,这个就很奇怪了.\npub fn search(&self, egraph: &EGraph&lt;L, N&gt;) -&gt; Vec&lt;SearchMatches&lt;L&gt;&gt; {\n    self.searcher.search(egraph)\n}\n\nfn search(&self, egraph: &EGraph&lt;L, A&gt;) -&gt; Vec&lt;SearchMatches&lt;L&gt;&gt; {\n    match self.ast.as_ref().last().unwrap() {\n        ENodeOrVar::ENode(e) =&gt; { // 首先搜索的节点是一个enode\n            #[allow(clippy::mem_discriminant_non_enum)]\n            let key = std::mem::discriminant(e); // 获取当前enode的key\n            match egraph.classes_by_op.get(&key) {\n                None =&gt; vec![],\n                Some(ids) =&gt; ids\n                    .iter()\n                    .filter_map(|&id| self.search_eclass(egraph, id))\n                    .collect(),\n            }\n        }\n        ENodeOrVar::Var(_) =&gt; egraph\n            .classes()\n            .filter_map(|e| self.search_eclass(egraph, e.id))\n            .collect(),\n    }\n}"
  },
  {
    "objectID": "posts/face-recognition.html",
    "href": "posts/face-recognition.html",
    "title": "人脸识别方法总结",
    "section": "",
    "text": "要搞个人脸识别的应用，花了半天时间浏览一下，准备基于open face的模型来做移植。下面是对开源库face-recognition的使用指南进行一个翻译，看了一下基本知道了大致流程。不过我记得上次写过L softmx -&gt; A softmx -&gt; AM softmax的这些loss都是用在人脸识别里面的，但是如果基于softmax loss的话，每加一个人脸不都是要重新训练一波吗？不知道是不是这个情况，目前还没看到别的方式。"
  },
  {
    "objectID": "posts/face-recognition.html#deep-face-recognition-with-keras-dlib-and-opencv",
    "href": "posts/face-recognition.html#deep-face-recognition-with-keras-dlib-and-opencv",
    "title": "人脸识别方法总结",
    "section": "Deep face recognition with Keras, Dlib and OpenCV",
    "text": "Deep face recognition with Keras, Dlib and OpenCV\n面部识别识别面部图像或视频帧上的人。简而言之，人脸识别系统从输入人脸图像中提取特征，并将其与数据库中标记人脸的特征进行比较。比较基于特征相似性度量，并且最相似的数据库条目的标签用于标记输入图像。如果相似度值低于某个阈值，则输入图像标记为unknown。比较两个面部图像以确定它们是否显示同一个人被称为面部验证。\n该笔记本使用深度卷积神经网络（CNN）从输入图像中提取特征。它遵循1中描述的方法，其修改受OpenFace项目的启发。 Keras用于实现CNN，Dlib和OpenCV用于对齐面部在输入图像上。在LFW数据集的一小部分上评估面部识别性能，您可以将其替换为您自己的自定义数据集，例如：如果你想进一步试验这款笔记本，请附上你的家人和朋友的照片。在概述了CNN架构以及如何训练模型之后，将演示如何：\n\n在输入图像上检测，变换和裁剪面部。这可确保面部在进入CNN之前对齐。该预处理步骤对于神经网络的性能非常重要。\n使用CNN从对齐的输入图像中提取面部的128维表示或嵌入。在嵌入空间中，欧几里德距离直接对应于面部相似性的度量。\n将输入嵌入向量与数据库中标记的嵌入向量进行比较。这里，支持向量机（SVM）和KNN分类器，在标记的嵌入向量上训练，起到数据库的作用。在此上下文中的面部识别意味着使用这些分类器来预测标签，即新输入的身份。\n\n\nEnvironment setup 环境设置\nFor running this notebook, create and activate a new virtual environment and install the packages listed in requirements.txt with pip install -r requirements.txt. Furthermore, you’ll need a local copy of Dlib’s face landmarks data file for running face alignment:\n\n\nCNN architecture and training\n这里使用的CNN架构是初始架构2的变体。更确切地说，它是1中描述的NN4体系结构的变体，并标识为nn4.small2。这个笔记本使用该模型的Keras实现，其定义取自Keras-OpenFace项目。这里的体系结构细节并不太重要，只知道有一个完全连接的层，其中有128个隐藏单元，后面是卷积基础顶部的L2规范化层。这两个顶层被称为嵌入层，从中可以获得128维嵌入向量。完整模型在[model.py]（model.py）中定义，图形概述在[model.png]（model.png）中给出。可以使用create_model（）创建nn4.small2模型的Keras版本。\nfrom model import create_model\n\nnn4_small2 = create_model()\nW0801 21:29:26.376736 140043235366720 deprecation.py:506] From /home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n模型训练旨在学习嵌入\\(f(x)\\)图像\\(x\\)，使得相同身份的所有面部之间的平方L2距离较小，并且来自不同身份的一对面部之间的距离较大。当嵌入空间中的锚图像\\(x^a_i\\)和正图像\\(x^p_i\\)（相同身份）之间的距离小于两者之间的距离时，可以实现三元组损失 \\(L\\)。锚图像和负图像\\(x^n_i\\)（不同的身份）至少有一个边缘\\(\\alpha\\)。\n\\[\n\\begin{aligned}\nL = \\sum^{m}_{i=1} \\large[ \\small {\\mid \\mid f(x_{i}^{a}) - f(x_{i}^{p})) \\mid \\mid_2^2} - {\\mid \\mid f(x_{i}^{a}) - f(x_{i}^{n})) \\mid \\mid_2^2} + \\alpha \\large ] \\small_+\n\\end{aligned}\n\\]\n\\([z]_+\\)表示\\(\\max(z，0)\\)和\\(m\\)是训练集中三元组的数量。 Keras中的三重态损失最好用自定义层实现，因为损失函数不遵循通常的“损失（输入，目标）”模式。该层调用self.add_loss来安装三元组丢失：\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Input, Layer\nimport tensorflow.python as tf\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nK.set_session(tf.Session(config=config))\n\n# Input for anchor, positive and negative images\nin_a = Input(shape=(96, 96, 3))\nin_p = Input(shape=(96, 96, 3))\nin_n = Input(shape=(96, 96, 3))\n\n# Output for anchor, positive and negative embedding vectors\n# The nn4_small model instance is shared (Siamese network)\nemb_a = nn4_small2(in_a)\nemb_p = nn4_small2(in_p)\nemb_n = nn4_small2(in_n)\n\nclass TripletLossLayer(Layer):\n    def __init__(self, alpha, **kwargs):\n        self.alpha = alpha\n        super(TripletLossLayer, self).__init__(**kwargs)\n    \n    def triplet_loss(self, inputs):\n        a, p, n = inputs\n        p_dist = K.sum(K.square(a-p), axis=-1)\n        n_dist = K.sum(K.square(a-n), axis=-1)\n        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n    \n    def call(self, inputs):\n        loss = self.triplet_loss(inputs)\n        self.add_loss(loss)\n        return loss\n\n# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\ntriplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n\n# Model that can be trained with anchor, positive negative images\nnn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)\n在训练期间，选择正对\\((x^a_i,x^p_i)\\)和负对\\((x^a_i，x^n_i)\\)难以区分的三元组是很重要的，即它们在嵌入空间中的距离差异应该是低于间距\\(\\alpha\\)，否则，网络无法学习有用的嵌入。因此，每次训练迭代应该基于在前一次迭代中学习的嵌入来选择一批新的三元组。假设从triplet_generator（）调用返回的生成器可以在这些约束下生成三元组，可以通过以下方式训练网络：\nfrom data import triplet_generator\n\n# triplet_generator() creates a generator that continuously returns \n# ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch \n# and n_batch are batches of anchor, positive and negative RGB images \n# each having a shape of (batch_size, 96, 96, 3).\ngenerator = triplet_generator() \n\nnn4_small2_train.compile(loss=None, optimizer='adam')\nnn4_small2_train.fit_generator(generator, epochs=10, steps_per_epoch=100)\n\n# Please note that the current implementation of the generator only generates \n# random image data. The main goal of this code snippet is to demonstrate \n# the general setup for model training. In the following, we will anyway \n# use a pre-trained model so we don't need a generator here that operates \n# on real training data. I'll maybe provide a fully functional generator\n# later.\nW0801 21:29:38.732154 140043235366720 training_utils.py:1101] Output triplet_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_loss_layer.\nW0801 21:29:38.856654 140043235366720 deprecation.py:323] From /home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n\n\nEpoch 1/10\n100/100 [==============================] - 19s 191ms/step - loss: 0.8117\nEpoch 2/10\n100/100 [==============================] - 5s 46ms/step - loss: 0.7971\nEpoch 3/10\n100/100 [==============================] - 5s 46ms/step - loss: 0.8035\nEpoch 4/10\n100/100 [==============================] - 5s 46ms/step - loss: 0.8018\nEpoch 5/10\n100/100 [==============================] - 5s 46ms/step - loss: 0.8049\nEpoch 6/10\n100/100 [==============================] - 5s 46ms/step - loss: 0.8009\nEpoch 7/10\n100/100 [==============================] - 5s 47ms/step - loss: 0.8003\nEpoch 8/10\n100/100 [==============================] - 5s 48ms/step - loss: 0.7995\nEpoch 9/10\n100/100 [==============================] - 5s 46ms/step - loss: 0.8004\nEpoch 10/10\n100/100 [==============================] - 5s 46ms/step - loss: 0.7998\n\n\n\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7f5cda4385c0&gt;\n上面的代码片段应该只演示如何设置模型训练。但是，我们不是从头开始实际训练模型，而是使用预先训练的模型，因为从头开始的训练非常昂贵，并且需要庞大的数据集来实现良好的泛化性能。例如，[1]（https://arxiv.org/abs/1503.03832）使用包含大约8M身份的200M图像的数据集。\nOpenFace项目提供了预训练模型，这些模型使用公共人脸识别数据集FaceScrub进行训练,和CASIA-WebFace。 Keras-OpenFace项目将预先训练的nn4.small2.v1模型的权重转换为CSV文件，然后进行转换这里x为一个二进制格式，可由Keras用load_weights加载：\nnn4_small2_pretrained = create_model()\nnn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')\n\n\nCustom dataset 自定义数据集\n为了演示自定义数据集上的人脸识别，使用了LFW数据集的一小部分。它由10个身份的100个面部图像组成。每个图像的元数据（文件和身份名称）被加载到内存中以供以后处理。\nimport numpy as np\nimport os.path\n\nclass IdentityMetadata():\n    def __init__(self, base, name, file):\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path):\n    metadata = []\n    for i in sorted(os.listdir(path)):\n        for f in sorted(os.listdir(os.path.join(path, i))):\n            # Check file extension. Allow only jpg/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\nmetadata = load_metadata('images')\n\n\nFace alignment 面部对齐\nnn4.small2.v1模型使用对齐的面部图像进行训练，因此，自定义数据集中的面部图像也必须对齐。在这里，我们使用Dlib进行人脸检测，使用OpenCV进行图像变换和裁剪，以生成对齐的96x96 RGB人脸图像。通过使用OpenFace项目中的AlignDlib实用程序，这很简单：\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom align import AlignDlib\n\n%matplotlib inline\n\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]\n\n# Initialize the OpenFace face alignment utility\nalignment = AlignDlib('models/landmarks.dat')\n\n# Load an image of Jacques Chirac\njc_orig = load_image(metadata[77].image_path())\n\n# Detect face and return bounding box\nbb = alignment.getLargestFaceBoundingBox(jc_orig)\n\n# Transform image using specified face landmark indices and crop image to 96x96\njc_aligned = alignment.align(96, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n\n# Show original image\nplt.subplot(131)\nplt.imshow(jc_orig)\n\n# Show original image with bounding box\nplt.subplot(132)\nplt.imshow(jc_orig)\nplt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n\n# Show aligned image\nplt.subplot(133)\nplt.imshow(jc_aligned)\n\n如OpenFace 预训练模型中所述部分,模型nn4.small2.v1需要地标索引OUTER_EYES_AND_NOSE。让我们将面部检测，转换和裁剪实现为align_image函数，以便以后重用。\ndef align_image(img):\n    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), \n                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n\n\nEmbedding vectors 嵌入向量\n现在可以通过将对齐和缩放的图像馈送到预训练的网络中来计算嵌入向量。\nembedded = np.zeros((metadata.shape[0], 128))\n\nfor i, m in enumerate(metadata):\n    img = load_image(m.image_path())\n    img = align_image(img)\n    # scale RGB values to interval [0,1]\n    img = (img / 255.).astype(np.float32)\n    # obtain embedding vector for image\n    embedded[i] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\nLet’s verify on a single triplet example that the squared L2 distance between its anchor-positive pair is smaller than the distance between its anchor-negative pair.\n让我们在单个三元组示例上验证其锚定正对之间的平方L2距离小于其锚定负对之间的距离。\ndef distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))\n\ndef show_pair(idx1, idx2):\n    plt.figure(figsize=(8,3))\n    plt.suptitle(f'Distance = {distance(embedded[idx1], embedded[idx2]):.2f}')\n    plt.subplot(121)\n    plt.imshow(load_image(metadata[idx1].image_path()))\n    plt.subplot(122)\n    plt.imshow(load_image(metadata[idx2].image_path()))    \n\nshow_pair(77, 78)\nshow_pair(77, 50)\n\n\n正如预期的那样，Jacques Chirac的两幅图像之间的距离小于Jacques Chirac图像与GerhardSchröder图像之间的距离（0.30 &lt;1.12）。但是我们仍然不知道距离阈值\\(\\tau\\)是在相同身份和不同身份之间作出决定的最佳边界。\n\n\nDistance threshold 距离阈值\n要查找$ $的最佳值，必须在一系列距离阈值上评估面部验证性能。在给定阈值处，所有可能的嵌入向量对被分类为相同的身份或不同的身份并且与基础事实进行比较。因为我们正在处理偏斜的类（比正对更多的负对），我们使用F1得分作为评估指标而不是准确度\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndistances = [] # squared L2 distance between pairs\nidentical = [] # 1 if same identity, 0 otherwise\n\nnum = len(metadata)\n\nfor i in range(num - 1):\n    for j in range(1, num):\n        distances.append(distance(embedded[i], embedded[j]))\n        identical.append(1 if metadata[i].name == metadata[j].name else 0)\n        \ndistances = np.array(distances)\nidentical = np.array(identical)\n\nthresholds = np.arange(0.3, 1.0, 0.01)\n\nf1_scores = [f1_score(identical, distances &lt; t) for t in thresholds]\nacc_scores = [accuracy_score(identical, distances &lt; t) for t in thresholds]\n\nopt_idx = np.argmax(f1_scores)\n# Threshold at maximal F1 score\nopt_tau = thresholds[opt_idx]\n# Accuracy at maximal F1 score\nopt_acc = accuracy_score(identical, distances &lt; opt_tau)\n\n# Plot F1 score and accuracy as function of distance threshold\nplt.plot(thresholds, f1_scores, label='F1 score')\nplt.plot(thresholds, acc_scores, label='Accuracy')\nplt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\nplt.title(f'Accuracy at threshold {opt_tau:.2f} = {opt_acc:.3f}')\nplt.xlabel('Distance threshold')\nplt.legend()\n/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n  return f(*args, **kwds)\n/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n  return f(*args, **kwds)\n\n\\(\\tau\\) = 0.56的面部验证准确率为95.7％。对于总是预测不同身份（有980个pos。对和8821个neg。对）的分类器的基线为89％，这也不错，但由于nn4.small2.v1是一个相对较小的模型，它仍然小于最先进的模型（&gt; 99％）。\n以下两个直方图显示了正负对的距离分布和决策边界的位置。这些分布明显分开，这解释了网络的辨别性能。人们也可以发现正对中的一些强异常值，但这里不再进一步分析。\ndist_pos = distances[identical == 1]\ndist_neg = distances[identical == 0]\n\nplt.figure(figsize=(12,4))\n\nplt.subplot(121)\nplt.hist(dist_pos)\nplt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\nplt.title('Distances (pos. pairs)')\nplt.legend()\n\nplt.subplot(122)\nplt.hist(dist_neg)\nplt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\nplt.title('Distances (neg. pairs)')\nplt.legend()\n\n\n\nFace recognition 人脸识别\n给定距离阈值$ \\(的估计，人脸识别现在就像计算输入嵌入向量与数据库中所有嵌入向量之间的距离一样简单。如果输入小于\\) $或标签unknown，则为输入分配具有最小距离的数据库条目的标签（即标识）。此过程还可以扩展到大型数据库，因为它可以轻松并行化。它还支持一次性学习，因为仅添加新标识的单个条目可能足以识别该标识的新示例。\n更稳健的方法是使用数据库中的前$ k $评分条目标记输入，该条目基本上是KNN分类，具有欧几里德距离度量。或者，线性支持向量机可以用数据库条目训练并用于分类，即识别新输入。为了训练这些分类器，我们使用50％的数据集，用于评估其他50％。\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\n\ntargets = np.array([m.name for m in metadata])\n\nencoder = LabelEncoder()\nencoder.fit(targets)\n\n# Numerical encoding of identities\ny = encoder.transform(targets)\n\ntrain_idx = np.arange(metadata.shape[0]) % 2 != 0\ntest_idx = np.arange(metadata.shape[0]) % 2 == 0\n\n# 50 train examples of 10 identities (5 examples each)\nX_train = embedded[train_idx]\n# 50 test examples of 10 identities (5 examples each)\nX_test = embedded[test_idx]\n\ny_train = y[train_idx]\ny_test = y[test_idx]\n\nknn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\nsvc = LinearSVC()\n\nknn.fit(X_train, y_train)\nsvc.fit(X_train, y_train)\n\nacc_knn = accuracy_score(y_test, knn.predict(X_test))\nacc_svc = accuracy_score(y_test, svc.predict(X_test))\n\nprint(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')\nKNN accuracy = 0.96, SVM accuracy = 0.98\nKNN分类器在测试集上实现了96％的准确度，SVM分类器为98％。让我们使用SVM分类器来说明单个示例中的人脸识别。\nimport warnings\n# Suppress LabelEncoder warning\nwarnings.filterwarnings('ignore')\n\nexample_idx = 6\n\nexample_image = load_image(metadata[test_idx][example_idx].image_path())\nexample_prediction = svc.predict([embedded[test_idx][example_idx]])\nexample_identity = encoder.inverse_transform(example_prediction)[0]\n\nplt.imshow(example_image)\nplt.title(f'Recognized as {example_identity}')\n\n似乎合理:-)实际上应该检查分类结果是否（预测身份的数据库条目的一个子集）的距离小于\\(\\tau\\)，否则应该分配一个未知标签。此处跳过此步骤，但可以轻松添加。\n\n\nDataset visualization 数据集可视化\n为了将数据集嵌入到2D空间中以显示身份聚类，将t-distributed Stochastic Neighbor Embedding（t-SNE）应用于128维嵌入向量。除了一些异常值，身份集群很好地分开。\nfrom sklearn.manifold import TSNE\n\nX_embedded = TSNE(n_components=2).fit_transform(embedded)\n\nfor i, t in enumerate(set(targets)):\n    idx = targets == t\n    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n\nplt.legend(bbox_to_anchor=(1, 1))\n\n\n\nReferences\n\n[1] FaceNet: A Unified Embedding for Face Recognition and Clustering\n[2] Going Deeper with Convolutions"
  },
  {
    "objectID": "posts/fft-properties.html",
    "href": "posts/fft-properties.html",
    "title": "声音信号处理-FFT性质",
    "section": "",
    "text": "今天实践FFT的各种性质。"
  },
  {
    "objectID": "posts/fft-properties.html#对三角波进行fft",
    "href": "posts/fft-properties.html#对三角波进行fft",
    "title": "声音信号处理-FFT性质",
    "section": "对三角波进行fft",
    "text": "对三角波进行fft\nimport numpy as np\nfrom scipy.signal import triang\nfrom scipy.fftpack import fft\nimport matplotlib.pyplot as plt\n\nx=triang(15) # 生成一个15点三角波\nplt.figure(1, figsize=(9.5, 6.5))\nplt.subplot(4,1,1)\nplt.plot(x)\nplt.title('triang')\nx=fft(x)\nmX=np.abs(x)\npX=np.angle(x)\n\nplt.subplot(4,1,2)\nplt.plot(x)\nplt.title('triang after fft')\nplt.subplot(4,1,3)\nplt.plot(mX)\nplt.title('triang after fft abs')\nplt.subplot(4,1,4)\nplt.plot(pX)\nplt.title('triang after fft angle')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/fft-properties.html#位移三角波再进行fft",
    "href": "posts/fft-properties.html#位移三角波再进行fft",
    "title": "声音信号处理-FFT性质",
    "section": "位移三角波再进行fft",
    "text": "位移三角波再进行fft\nx=triang(15) # 生成一个15点三角波\nfftbuffer=np.zeros(15)\nfftbuffer[:8]=x[7:]\nfftbuffer[8:]=x[:7]\nplt.figure(1, figsize=(9.5, 6.5))\nplt.subplot(4,1,1)\nplt.plot(fftbuffer)\nplt.title('fftbuffer')\nx=fft(fftbuffer)\nmX=np.abs(x)\npX=np.angle(x)\n\nplt.subplot(4,1,2)\nplt.plot(x)\nplt.title('triang after fft')\nplt.subplot(4,1,3)\nplt.plot(mX)\nplt.title('triang after fft abs')\nplt.subplot(4,1,4)\nplt.plot(pX)\nplt.title('triang after fft angle')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/fft-properties.html#读取真实信号进行fft",
    "href": "posts/fft-properties.html#读取真实信号进行fft",
    "title": "声音信号处理-FFT性质",
    "section": "读取真实信号进行fft",
    "text": "读取真实信号进行fft\n取fftbuffer N=511\nimport utilFunctions as UF\n\nM=501 # 输入信号长度\nhM1=int(math.floor((M+1)/2)) # 计算窗宽\nhM2=int(math.floor(M/2))\nfs,x=UF.wavread('../sounds/soprano-E4.wav') \nx1=x[5000:5000+M]*np.hamming(M) # 加窗\n\nN=511\nfftbuffer=np.zeros(N) # 零填充\nfftbuffer[:hM1]=x1[hM2:] # \nfftbuffer[N-hM2:]=x1[:hM2]\n\nplt.figure(1, figsize=(9.5, 6.5))\nplt.subplot(4,1,1)\nplt.plot(fftbuffer)\nplt.title('fftbuffer')\nx=fft(fftbuffer)\nmX=np.abs(x)\npX=np.angle(x)\nplt.subplot(4,1,2)\nplt.plot(x)\nplt.title('after fft')\nplt.subplot(4,1,3)\nplt.plot(mX)\nplt.title('after fft abs')\nplt.subplot(4,1,4)\nplt.plot(pX)\nplt.title('after fft angle')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/fft-properties.html#读取真实信号进行fft-1",
    "href": "posts/fft-properties.html#读取真实信号进行fft-1",
    "title": "声音信号处理-FFT性质",
    "section": "读取真实信号进行fft",
    "text": "读取真实信号进行fft\n取fftbuffer N=1024\nimport utilFunctions as UF\n\nM=501 # 输入信号长度\nhM1=int(math.floor((M+1)/2)) # 计算窗宽\nhM2=int(math.floor(M/2))\nfs,x=UF.wavread('../sounds/soprano-E4.wav') \nx1=x[5000:5000+M]*np.hamming(M) # 加窗\n\nN=1024\nfftbuffer=np.zeros(N) # 零填充\nfftbuffer[:hM1]=x1[hM2:] # \nfftbuffer[N-hM2:]=x1[:hM2]\n\nplt.figure(1, figsize=(9.5, 6.5))\nplt.subplot(4,1,1)\nplt.plot(fftbuffer)\nplt.title('fftbuffer')\nx=fft(fftbuffer)\nmX=20*np.log10(np.abs(x))\npX=np.unwrap(np.angle(x))\nplt.subplot(4,1,2)\nplt.plot(x)\nplt.title('after fft')\nplt.subplot(4,1,3)\nplt.plot(mX)\nplt.title('after fft abs (db)')\nplt.subplot(4,1,4)\nplt.plot(pX)\nplt.title('after fft angle (unwrap)')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/fix-nvida-driver.html",
    "href": "posts/fix-nvida-driver.html",
    "title": "解决Ubuntu下nvidia驱动导致画面撕裂",
    "section": "",
    "text": "我最近装了这个RTX2060,虽然Tensorflow是编译成功安装了,但是我用的时候发现我的屏幕画面撕裂的太厉害了,我滚动代码都会导致花屏,把我气死.今天终于解决了.\n\n\n解决方案\n首先,什么强制同步啥的都没什么用.我的问题在于Linux的内核版本,如果是4.15内核的朋友赶紧升级一波:\nsudo apt install linux-generic-hwe-18.04-edge\n升级到4.18内核之后,重装了nvidia 4.15驱动.成功解决.\n                          ./+o+-       zqh@pc\n                  yyyyy- -yyyyyy+      OS: Ubuntu 18.04 bionic\n               ://+//////-yyyyyyo      Kernel: x86_64 Linux 4.18.0-16-generic\n           .++ .:/++++++/-.+sss/`      Uptime: 10m\n         .:++o:  /++++++++/:--:/-      Packages: 2671\n        o:+o+:++.`..```.-/oo+++++/     Shell: zsh 5.4.2\n       .:+o:+o/.          `+sssoo+/    Resolution: 3000x1920\n  .++/+:+oo+o:`             /sssooo.   DE: GNOME \n /+++//+:`oo+o               /::--:.   WM: GNOME Shell\n \\+/+o+++`o++o               ++////.   WM Theme: \n  .++.o+++oo+:`             /dddhhh.   GTK Theme: Adwaita [GTK2/3]\n       .+.o+oo:.          `oddhhhh+    Icon Theme: Adwaita\n        \\+.++o+o``-````.:ohdhhhhh+     Font: Cantarell 11\n         `:o+++ `ohhhhhhhhyo++os:      CPU: Intel Core i7-7700 @ 8x 4.2GHz [27.8°C]\n           .o:`.syhhhhhhh/.oo++o`      GPU: GeForce RTX 2060\n               /osyyyyyyo++ooo+++/     RAM: 2058MiB / 16003MiB\n                   ````` +oo+++o\\:    \n                          `oo++."
  },
  {
    "objectID": "posts/gccstm32.html",
    "href": "posts/gccstm32.html",
    "title": "linux stm32 开发",
    "section": "",
    "text": "最近换了双系统，发现还是linux下面写程序爽。windows还是比较适合打游戏233. 这篇文章记录一下linux下开发stm32的一些东西。\n\n\n安装vscode以及交叉编译器\n在官网下载即可\n\n\n安装交叉编译链\n下载地址 下载好后解压并且设置路径，可以看这篇文章中的安装交叉编译链部分。\n\n\n安装stm32cubemx\n下载地址 下载好后执行××.linux即可\n\n\n安装jlink驱动\n下载地址 Ubuntu的话双击即可安装。 安装成功后运行如下出现信息说明成功，输入q退出。\n➜  gitio JLinkExe\nSEGGER J-Link Commander V6.12j (Compiled Feb 15 2017 18:03:38)\nDLL version V6.12j, compiled Feb 15 2017 18:03:30\n\nConnecting to J-Link via USB...O.K.\nFirmware: J-Link ARM V8 compiled Jan 31 2018 18:34:52\nHardware version: V8.00\nS/N: 20080643\nLicense(s): RDI,FlashDL,FlashBP,JFlash,GDBFull\nVTref = 3.371V\n\n\nType \"connect\" to establish a target connection, '?' for help\nJ-Link&gt;\n\n\n使用cubemx建立一个makefile工程\n建立工程我就不赘述了。这里放一下vscode的配置\n{\n    \"configurations\": [\n        {\n            \"name\": \"Linux\",\n            \"includePath\": [\n                \"${workspaceFolder}/**\",\n                \"${workspaceFolder}/Inc\"\n            ],\n            \"defines\": [\n                \"USE_HAL_DRIVER\",\n                \"STM32L431xx\"\n            ],\n            \"compilerPath\": \"/opt/gccStm32/bin/arm-none-eabi-gcc\",\n            \"cStandard\": \"c99\",\n            \"cppStandard\": \"c++11\",\n            \"intelliSenseMode\": \"clang-x64\"\n        }\n    ],\n    \"version\": 4\n}\n\n\n编译\n编译只需要make即可在build文件下生成bin和hex文件\n➜  gcclight make\narm-none-eabi-gcc -c -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -DUSE_HAL_DRIVER-DSTM32L431xx -IInc -IHardware/BH1750 -IHardware/DHT11 -IHardware/GPS -IHardware/OLED -IDrivers/STM32L4xx_HAL_Driver/Inc -IDrivers/STM32L4xx_HAL_Driver/Inc/Legacy -IDrivers/CMSIS/Device/ST/STM32L4xx/Include -IDrivers/CMSIS/Include -Og -Wall -fdata-sections -ffunction-sections -g -gdwarf-2 -MMD -MP -MF\"build/main.d\" -Wa,-a,-ad,-alms=build/main.lst Src/main.c -o build/main.o\narm-none-eabi-gcc build/main.o build/gpio.o build/adc.o build/tim.o build/usart.o build/delay.o build/BH1750.o build/DHT11_BUS.o build/gps.o build/oled.o build/stm32l4xx_it.o build/stm32l4xx_hal_msp.o build/stm32l4xx_hal_adc.o build/stm32l4xx_hal_adc_ex.o build/stm32l4xx_hal_tim.o build/stm32l4xx_hal_tim_ex.o build/stm32l4xx_hal_uart.o build/stm32l4xx_hal_uart_ex.o build/stm32l4xx_hal.o build/stm32l4xx_hal_i2c.o build/stm32l4xx_hal_i2c_ex.o build/stm32l4xx_hal_rcc.o build/stm32l4xx_hal_rcc_ex.o build/stm32l4xx_hal_flash.o build/stm32l4xx_hal_flash_ex.o build/stm32l4xx_hal_flash_ramfunc.o build/stm32l4xx_hal_gpio.o build/stm32l4xx_hal_dma.o build/stm32l4xx_hal_dma_ex.o build/stm32l4xx_hal_pwr.o build/stm32l4xx_hal_pwr_ex.o build/stm32l4xx_hal_cortex.o build/system_stm32l4xx.o build/startup_stm32l431xx.o -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard-specs=nano.specs -TSTM32L431RBTx_FLASH.ld  -lc -lm -lnosys  -Wl,-Map=build/gcclight.map,--cref -Wl,--gc-sections -o build/gcclight.elf\narm-none-eabi-size build/gcclight.elf\n   text    data     bss     dec     hex filename\n  18160     120    2128   20408    4fb8 build/gcclight.elf\narm-none-eabi-objcopy -O ihex build/gcclight.elf build/gcclight.hex\narm-none-eabi-objcopy -O binary -S build/gcclight.elf build/gcclight.bin\n\n\n烧录\n连接到jlink 我使用如下脚本执行。\n#!/usr/bin/expect\n# 30s 超时\nset timeout 30\n# 执行\nspawn JLinkExe\n\n# 启动成功\nexpect \"J-Link&gt;\"\n# 连接\nsend \"connect\\r\"\n\n# 检测到默认设备\nexpect \"&lt;Default&gt;:\"\n# 确认设备\nsend \"\\r\"\n\n# 选择连接方式\nexpect \"TIF&gt;\"\n# 确认连接方式\nsend \"S\\r\"\n\n# 选择速度\nexpect \"Speed&gt;\"\n# 确认速度\nsend \"\\r\"\n\n# 用于测试\n# # 连接成功\n# expect \"J-Link&gt;\"\n# # 重启\n# send \"rx 20\\r\"\n\n# 烧录程序\n# 如果连接成功\nexpect \"identified.\"\n# 烧录程序\nsend \"loadbin build/gcclight.bin 0x8000000\\r\"\n\n# 如果烧录成功\nexpect \"O.K.\"\n# 重启\nsend \"rx 20\\r\"\n\n# 如果重启成功\nexpect \"J-Link&gt;\"\n# 运行程序\nsend \"g\\r\"\n\n\n# 如果运行成功\nexpect \"J-Link&gt;\"\n# 退出jlink\nsend \"qc\\r\"\n\n# 将控制权交给用户\ninteract\n终端输入如下\n➜  gcclight ./install.sh\nspawn JLinkExe\nSEGGER J-Link Commander V6.12j (Compiled Feb 15 2017 18:03:38)\nDLL version V6.12j, compiled Feb 15 2017 18:03:30\n\nConnecting to J-Link via USB...O.K.\nFirmware: J-Link ARM V8 compiled Jan 31 2018 18:34:52\nHardware version: V8.00\nS/N: 20080643\nLicense(s): RDI,FlashDL,FlashBP,JFlash,GDBFull\nVTref = 3.371V\n\n\nType \"connect\" to establish a target connection, '?' for help\nJ-Link&gt;connect\nPlease specify device / core. &lt;Default&gt;: STM32L431RB\nType '?' for selection dialog\nDevice&gt;\nPlease specify target interface:\n  J) JTAG (Default)\n  S) SWD\nTIF&gt;S\nSpecify target interface speed [kHz]. &lt;Default&gt;: 4000 kHz\nSpeed&gt;\nDevice \"STM32L431RB\" selected.\n\n\nFound SWD-DP with ID 0x2BA01477\nFound SWD-DP with ID 0x2BA01477\nAP-IDR: 0x24770011, Type: AHB-AP\nAHB-AP ROM: 0xE00FF000 (Base addr. of first ROM table)\nFound Cortex-M4 r0p1, Little endian.\nFPUnit: 6 code (BP) slots and 2 literal slots\nCoreSight components:\nROMTbl 0 @ E00FF000\nROMTbl 0 [0]: FFF0F000, CID: B105E00D, PID: 000BB00C SCS\nROMTbl 0 [1]: FFF02000, CID: B105E00D, PID: 003BB002 DWT\nROMTbl 0 [2]: FFF03000, CID: B105E00D, PID: 002BB003 FPB\nROMTbl 0 [3]: FFF01000, CID: B105E00D, PID: 003BB001 ITM\nROMTbl 0 [4]: FFF41000, CID: B105900D, PID: 000BB9A1 TPIU\nROMTbl 0 [5]: FFF42000, CID: B105900D, PID: 000BB925 ETM\nCortex-M4 identified.\nJ-Link&gt;loadbin build/gcclight.bin 0x8000000\nHalting CPU for downloading file.\nDownloading file [build/gcclight.bin]...\nComparing flash   [100%] Done.\nVerifying flash   [100%] Done.\nJ-Link: Flash download: Flash download skipped. Flash contents already match\nO.K.\nJ-Link&gt;rx 20\nReset delay: 20 ms\nReset type NORMAL: Resets core & peripherals via SYSRESETREQ & VECTRESET bit.\nJ-Link&gt;g\nqc\nJ-Link&gt;qc"
  },
  {
    "objectID": "posts/h5-to-pb.html",
    "href": "posts/h5-to-pb.html",
    "title": "H5模型转pb模型",
    "section": "",
    "text": "这个实际上是个伪需求，直接h5转tflite就好了，但是就是没办法，总有些东西不支持新的方法。下面记录一下怎样把tf2.0生成的h5模型转成tf1.10的pb模型。"
  },
  {
    "objectID": "posts/h5-to-pb.html#加载模型",
    "href": "posts/h5-to-pb.html#加载模型",
    "title": "H5模型转pb模型",
    "section": "加载模型",
    "text": "加载模型\n首先用老版本的tensorflow.keras加载模型：\nimport tensorflow as tf\nimport numpy as np\nfrom functools import reduce, wraps\nfrom typing import List\nimport os\nfrom tensorflow.contrib.lite.python import lite\nkl = tf.keras.layers\nk = tf.keras\nkr = tf.keras.regularizers\nK = tf.keras.backend\n\n\nh5_model = k.models.load_model('infer.h5')\n当然会碰到许多不兼容的地方：\n\nValueError: ('Unrecognized keyword arguments:', dict_keys(['ragged']))\n\n这个是因为老的k.Input不支持ragged参数，修改如下：\n  def __init__(self,\n               input_shape=None,\n               batch_size=None,\n               dtype=None,\n               input_tensor=None,\n               sparse=False,\n               name=None,\n               **kwargs):\n    if 'batch_input_shape' in kwargs:\n      batch_input_shape = kwargs.pop('batch_input_shape')\n      if input_shape and batch_input_shape:\n        raise ValueError('Only provide the input_shape OR '\n                         'batch_input_shape argument to '\n                         'InputLayer, not both at the same time.')\n      batch_size = batch_input_shape[0]\n      input_shape = batch_input_shape[1:]\n    # NOTE 注释这里：\n    # if kwargs:\n    #   raise ValueError('Unrecognized keyword arguments:', kwargs.keys())\n\nValueError: Unknown initializer: GlorotUniform\n\n老版本的tf.keras没有这个初始化类，从新的tensorflow里面拷贝一个来就好了。\nclass GlorotNormal(k.initializers.VarianceScaling):\n  def __init__(self, seed=None):\n    super(GlorotNormal, self).__init__(\n        scale=1.0, mode=\"fan_avg\", distribution=\"truncated_normal\", seed=seed)\n\n  def get_config(self):\n    return {\"seed\": self.seed}\n.\n.\n.\nh5_model = k.models.load_model('infer.h5', custom_objects={'GlorotUniform': GlorotNormal})\n\nTypeError: ('Keyword argument not understood:', 'threshold')\n\n这个是因为Relu层的实现不一样。\n先把tf.keras.backend.relu替换为如下：\n@tf_export('keras.backend.relu')\ndef relu(x, alpha=0., max_value=None, threshold=0):\n  if alpha != 0.:\n    if max_value is None and threshold == 0:\n      return nn.leaky_relu(x, alpha=alpha)\n\n    if threshold != 0:\n      negative_part = nn.relu(-x + threshold)\n    else:\n      negative_part = nn.relu(-x)\n\n  clip_max = max_value is not None\n\n  if threshold != 0:\n    # computes x for x &gt; threshold else 0\n    x = x * math_ops.cast(math_ops.greater(x, threshold), floatx())\n  elif max_value == 6:\n    # if no threshold, then can use nn.relu6 native TF op for performance\n    x = nn.relu6(x)\n    clip_max = False\n  else:\n    x = nn.relu(x)\n\n  if clip_max:\n    max_value = _to_tensor(max_value, x.dtype.base_dtype)\n    zero = _to_tensor(0., x.dtype.base_dtype)\n    x = clip_ops.clip_by_value(x, zero, max_value)\n\n  if alpha != 0.:\n    alpha = _to_tensor(alpha, x.dtype.base_dtype)\n    x -= alpha * negative_part\n  return x\n再把tf.keras.layers.advanced_activations里面的Relu替换为如下：\n@tf_export('keras.layers.ReLU')\nclass ReLU(Layer):\n\n  def __init__(self, max_value=None, negative_slope=0, threshold=0, **kwargs):\n    super(ReLU, self).__init__(**kwargs)\n    if max_value is not None and max_value &lt; 0.:\n      raise ValueError('max_value of Relu layer '\n                       'cannot be negative value: ' + str(max_value))\n    if negative_slope &lt; 0.:\n      raise ValueError('negative_slope of Relu layer '\n                       'cannot be negative value: ' + str(negative_slope))\n\n    self.support_masking = True\n    if max_value is not None:\n      max_value = K.cast_to_floatx(max_value)\n    self.max_value = max_value\n    self.negative_slope = K.cast_to_floatx(negative_slope)\n    self.threshold = K.cast_to_floatx(threshold)\n\n  def call(self, inputs):\n    # alpha is used for leaky relu slope in activations instead of\n    # negative_slope.\n    return K.relu(inputs,\n                  alpha=self.negative_slope,\n                  max_value=self.max_value,\n                  threshold=self.threshold)\n\n  def get_config(self):\n    config = {\n        'max_value': self.max_value,\n        'negative_slope': self.negative_slope,\n        'threshold': self.threshold\n    }\n    base_config = super(ReLU, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n  @tf_utils.shape_type_conversion\n  def compute_output_shape(self, input_shape):\n    return input_shape\n\nTypeError: ('Keyword argument not understood:', 'interpolation')\n\n这个是因为上采样层不同，替换tensorflow/python/keras/layers/convolutional.py里面的UpSampling2D：\n@tf_export('keras.layers.UpSampling2D')\nclass UpSampling2D(Layer):\n  def __init__(self,\n               size=(2, 2),\n               data_format=None,\n               interpolation='nearest',\n               **kwargs):\n    super(UpSampling2D, self).__init__(**kwargs)\n    self.data_format = conv_utils.normalize_data_format(data_format)\n    self.size = conv_utils.normalize_tuple(size, 2, 'size')\n    if interpolation not in {'nearest', 'bilinear'}:\n      raise ValueError('`interpolation` argument should be one of `\"nearest\"` '\n                       'or `\"bilinear\"`.')\n    self.interpolation = interpolation\n    self.input_spec = InputSpec(ndim=4)\n\n  def compute_output_shape(self, input_shape):\n    input_shape = tensor_shape.TensorShape(input_shape).as_list()\n    if self.data_format == 'channels_first':\n      height = self.size[0] * input_shape[\n          2] if input_shape[2] is not None else None\n      width = self.size[1] * input_shape[\n          3] if input_shape[3] is not None else None\n      return tensor_shape.TensorShape(\n          [input_shape[0], input_shape[1], height, width])\n    else:\n      height = self.size[0] * input_shape[\n          1] if input_shape[1] is not None else None\n      width = self.size[1] * input_shape[\n          2] if input_shape[2] is not None else None\n      return tensor_shape.TensorShape(\n          [input_shape[0], height, width, input_shape[3]])\n\n  def call(self, inputs):\n    return backend.resize_images(\n        inputs, self.size[0], self.size[1], self.data_format,\n        interpolation=self.interpolation)\n\n  def get_config(self):\n    config = {\n        'size': self.size,\n        'data_format': self.data_format,\n        'interpolation': self.interpolation\n    }\n    base_config = super(UpSampling2D, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n再替换tensorflow/python/keras/backend.py里面的resize_images：\n@tf_export('keras.layers.UpSampling2D')\nclass UpSampling2D(Layer):\n  def __init__(self,\n               size=(2, 2),\n               data_format=None,\n               interpolation='nearest',\n               **kwargs):\n    super(UpSampling2D, self).__init__(**kwargs)\n    self.data_format = conv_utils.normalize_data_format(data_format)\n    self.size = conv_utils.normalize_tuple(size, 2, 'size')\n    if interpolation not in {'nearest', 'bilinear'}:\n      raise ValueError('`interpolation` argument should be one of `\"nearest\"` '\n                       'or `\"bilinear\"`.')\n    self.interpolation = interpolation\n    self.input_spec = InputSpec(ndim=4)\n\n  def compute_output_shape(self, input_shape):\n    input_shape = tensor_shape.TensorShape(input_shape).as_list()\n    if self.data_format == 'channels_first':\n      height = self.size[0] * input_shape[\n          2] if input_shape[2] is not None else None\n      width = self.size[1] * input_shape[\n          3] if input_shape[3] is not None else None\n      return tensor_shape.TensorShape(\n          [input_shape[0], input_shape[1], height, width])\n    else:\n      height = self.size[0] * input_shape[\n          1] if input_shape[1] is not None else None\n      width = self.size[1] * input_shape[\n          2] if input_shape[2] is not None else None\n      return tensor_shape.TensorShape(\n          [input_shape[0], height, width, input_shape[3]])\n\n  def call(self, inputs):\n    return backend.resize_images(\n        inputs, self.size[0], self.size[1], self.data_format,\n        interpolation=self.interpolation)\n\n  def get_config(self):\n    config = {\n        'size': self.size,\n        'data_format': self.data_format,\n        'interpolation': self.interpolation\n    }\n    base_config = super(UpSampling2D, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))"
  },
  {
    "objectID": "posts/h5-to-pb.html#转换模型",
    "href": "posts/h5-to-pb.html#转换模型",
    "title": "H5模型转pb模型",
    "section": "转换模型",
    "text": "转换模型\n加载模型好了其实就简单了：\ndef test_convert_pb_2():\n\n  K.clear_session()\n  K.set_learning_phase(False)\n  sess = K.get_session()\n  init_graph = sess.graph\n  input_shapes = None\n  with init_graph.as_default():\n    h5_model = k.models.load_model(\n        'infer.h5', custom_objects={'GlorotUniform': GlorotNormal})\n    input_tensors = h5_model.inputs\n    output_tensors = h5_model.outputs\n    lite._set_tensor_shapes(input_tensors, input_shapes)\n    graph_def = lite._freeze_graph(sess, output_tensors)\n    tf.train.write_graph(graph_def, 'data_2', 'model.pb', as_text=False)\n    in_nodes = [inp.op.name for inp in h5_model.inputs]\n    out_nodes = [out.op.name for out in h5_model.outputs]\n    print(in_nodes)\n    print(out_nodes)\n    \ntest_convert_pb_2()\n得到如下：\n(vim3l) ➜  retinaface python ./test_model.py\n2020-03-18 00:02:18.150198: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n['input_1']\n['concatenate_3/concat', 'concatenate_4/concat', 'concatenate_5/concat']\n然后终于可以用他的**转换工具转换了"
  },
  {
    "objectID": "posts/halide-internal.html",
    "href": "posts/halide-internal.html",
    "title": "Halide 进阶",
    "section": "",
    "text": "主要分析halide内部机制."
  },
  {
    "objectID": "posts/halide-internal.html#func的构造",
    "href": "posts/halide-internal.html#func的构造",
    "title": "Halide 进阶",
    "section": "1. Func的构造",
    "text": "1. Func的构造\n首先我们分析如下代码:\nfloat *buffer_raw = new float[size];\nBuffer&lt;float&gt; buffer_host(buffer_raw, shape, name);\nFunc buffer_device(name + \"_device\"); // 开始定义的是一个空的function\nbuffer_device(c, i, j) = buffer_host(c, i, j);\nreturn buffer_device;\n\nRhs\n使用buffer(a,b,c)时,将会调用buffer_accessor构造出一个call expr进行返回.\ntemplate&lt;typename... Args&gt;\n    Expr operator()(const std::vector&lt;Expr&gt; &args) const {\n        return buffer_accessor(Buffer&lt;&gt;(*this), args); // Call::make(buf, int_args);\n    }\n返回的call的call_type是一个image, 这里halide的call其实不如tvm relay中的直观.\nCall *node = new Call;\nnode-&gt;type = type; // 返回类型 , 因为当前图像是f32,因此这里也是f32.\nnode-&gt;name = name; // 名字\nnode-&gt;args = args; // 参数\nnode-&gt;call_type = call_type; \n                  // Image,              A load from an input image\n                  //  Extern,            A call to an external C-ABI function, possibly with side-effects\n                  //  ExternCPlusPlus,   A call to an external C-ABI function, possibly with side-effects\n                  //  PureExtern,        A call to a guaranteed-side-effect-free external function\n                  //  Halide,            A call to a Func\n                  //  Intrinsic,         A possibly-side-effecty compiler intrinsic, which has special handling during codegen\n                  //  PureIntrinsic      A side-effect-free version of the above.\nnode-&gt;func = std::move(func); // 如果call type是halide func,那么需要存储函数指针\nnode-&gt;value_index = value_index; // 如果call的函数有很多个值, 那么需要保存call的那个index.\nnode-&gt;image = std::move(image); // 如果call type是image,那么需要存储image指针\nnode-&gt;param = std::move(param); // 如果call 是image param,那么需要存储param的指针\n\n\nLhs\n首先取使用buffer_device(a,b,c)时,将会调用operator()构造出一个FuncRef进行返回. 其实我觉得可以把它理解成FuncWrapper, 就是用来暂存一些dsl需要的信息的结构体.\nFuncRef Func::operator()(vector&lt;Expr&gt; args) const {\n    int placeholder_pos, count;\n    std::tie(placeholder_pos, count) = add_implicit_vars(args);\n    return FuncRef(func, args, placeholder_pos, count);\n}\n然后调用FuncRef的operator=去加载右边返回的call. 加载完成之后构造一个stage返回, 这里c++好的一点就是重载operator=是可以返回值的.\nStage FuncRef::operator=(const Tuple &e) {\n    func.define(expanded_args_str, e.as_vector());\n    return Stage(func, func.definition(), 0);\n}\n\n\nfunc 转换为expr\nfunc其实只是暂时定义好了数据流中的输入args以及输出ouputs,是一种游离在expr外的结构. 而当一个func要赋值给另一个func时, 才把这个funcion嵌入到数据流中,因此构造一个call expr返回, 再作为\nStage FuncRef::operator=(const FuncRef &e) {\n    if (e.size() == 1) {\n        return (*this) = Expr(e);\n    } else {\n        return (*this) = Tuple(e);\n    }\n}\n\nFuncRef::operator Expr() const {\n    user_assert(func.has_pure_definition() || func.has_extern_definition())\n        &lt;&lt; \"Can't call Func \\\"\" &lt;&lt; func.name() &lt;&lt; \"\\\" because it has not yet been defined.\\n\";\n\n    user_assert(func.outputs() == 1)\n        &lt;&lt; \"Can't convert a reference Func \\\"\" &lt;&lt; func.name()\n        &lt;&lt; \"\\\" to an Expr, because \" &lt;&lt; func.name() &lt;&lt; \" returns a Tuple.\\n\";\n\n    return Call::make(func, args);\n}"
  },
  {
    "objectID": "posts/halide-internal.html#func-lower",
    "href": "posts/halide-internal.html#func-lower",
    "title": "Halide 进阶",
    "section": "2. Func Lower",
    "text": "2. Func Lower\n下面看如何从Func转换到stmt.\nbuffer_device.compile_to_lowered_stmt(buffer_device.name() + \".julia\", {});\n\nPipeLine\n首先会把Funcion转换为PipeLine\nPipeline::Pipeline(const Func &output)\n    : contents(new PipelineContents) {\n    output.function().freeze();\n    contents-&gt;outputs.push_back(output.function());\n}\n\n\nModule\n然后把pipeline转换的module, 通过调用lower.\nModule lower(const vector&lt;Function&gt; &output_funcs,\n             const string &pipeline_name,\n             const Target &t,\n             const vector&lt;Argument&gt; &args,\n             const LinkageType linkage_type,\n             const vector&lt;Stmt&gt; &requirements,\n             bool trace_pipeline,\n             const vector&lt;IRMutator *&gt; &custom_passes) {\n    Module result_module{extract_namespaces(pipeline_name), t};\n    run_with_large_stack([&]() {\n        lower_impl(output_funcs, pipeline_name, t, args, linkage_type, requirements, trace_pipeline, custom_passes, result_module);\n    });\n    return result_module;\n}\n\nlower_impl\n首先构造一个全局的for循环\nstring root_var = LoopLevel::root().lock().to_string();\nStmt s = For::make(root_var, 0, 1, ForType::Serial, DeviceAPI::Host, Evaluate::make(0));\n得到了:\nbefore injector:\nfor (.__root, 0, 1) {\n 0\n}\n先是schedule_functions, 然后通过InjectFunctionRealization插入一系列的函数.\nInjectFunctionRealization injector(funcs, is_output_list, target, env);\ns = injector.mutate(s);\ninternal_assert(injector.found_store_level() && injector.found_compute_level());\n接着就是一堆的传统编译器pass对lowerd ir进行分析和优化..\n\n\npartition_loops\n比如我们写了一个卷积之后,halide可以把一个循环体划分为序言、稳态和尾声。通过寻找使用clamped ramps或likely的字段来找到稳定状态, 然后进行分离再对每个循环内部进行优化."
  },
  {
    "objectID": "posts/halide-internal.html#关于一些ir设计",
    "href": "posts/halide-internal.html#关于一些ir设计",
    "title": "Halide 进阶",
    "section": "3. 关于一些IR设计",
    "text": "3. 关于一些IR设计\n\n1. produce consume\nhalide是提供了一个ir,区别两种类型, 其中is_producer=true表示对于buffer可读可写,否则就是对于buffer只读.\nstruct ProducerConsumer : public StmtNode&lt;ProducerConsumer&gt; {\n    std::string name;\n    bool is_producer;\n    Stmt body;\n}\n然后构造的时候,如果有一个消费者对,那就构造一个block存储. 这个block也是专为producer和consumer设计的, 既可以保存配对信息,还能保证block中内容都必须要顺序执行, 不会出现执行问题.\nstruct Block : public StmtNode&lt;Block&gt; {\n    Stmt first, rest;\n}\n\nif (is_no_op(consumer)) {\n    // For the very first output to be scheduled, the consumer\n    // Stmt can be a no-op. No point in preserving it.\n    return producer;\n} else {\n    return Block::make(producer, consumer);\n}"
  },
  {
    "objectID": "posts/halide-note.html",
    "href": "posts/halide-note.html",
    "title": "Halide 入门",
    "section": "",
    "text": "Halide快速入门笔记。"
  },
  {
    "objectID": "posts/halide-note.html#func",
    "href": "posts/halide-note.html#func",
    "title": "Halide 入门",
    "section": "Func",
    "text": "Func\nFunc可以看作是一个数据流pipeline。"
  },
  {
    "objectID": "posts/halide-note.html#var",
    "href": "posts/halide-note.html#var",
    "title": "Halide 入门",
    "section": "Var",
    "text": "Var\nVar可以看成一个Func中计算时需要的变量，通常用Var表示计算时变化的坐标索引。"
  },
  {
    "objectID": "posts/halide-note.html#expr",
    "href": "posts/halide-note.html#expr",
    "title": "Halide 入门",
    "section": "Expr",
    "text": "Expr\nExpr是Var间的一系列操作，比如add = a+b，我觉得表达式的概念像是一个和一些Var绑定的Func，而Func呢算是一个更加通用的概念，可以和不同的输入参数Var进行绑定。"
  },
  {
    "objectID": "posts/halide-note.html#rdom",
    "href": "posts/halide-note.html#rdom",
    "title": "Halide 入门",
    "section": "RDom",
    "text": "RDom\nRDom（reduction domain）在官方教程中居然后来才介绍，可能一开始没有设计这个概念，我觉得RDom算是一个临时的Var，他的任务是用于reduce（也就是遍历后求和等），比如对于某一个维度进行reduce sum，此时用RDom可以方便的定义这个操作。"
  },
  {
    "objectID": "posts/halide-note.html#example",
    "href": "posts/halide-note.html#example",
    "title": "Halide 入门",
    "section": "example",
    "text": "example\n假设函数如下，默认将第一个元素放置到循环的最内层，也就是说如果是bchw的输入，在构造函数的时候最好应该构造为fuc(w,h,c,b)的形式。\ngradient(x, y) = x + y;\n\n  for y:\n    for x:\n      gradient(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#reorder",
    "href": "posts/halide-note.html#reorder",
    "title": "Halide 入门",
    "section": "reorder",
    "text": "reorder\n调整循环顺序,先计算行方向\ngradient.reorder(y, x);\n\n  for x:\n    for y:\n      gradient_col_major(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#split",
    "href": "posts/halide-note.html#split",
    "title": "Halide 入门",
    "section": "split",
    "text": "split\n把x分为内外循环，虽然实际上不会有什么改变，但是可以为别的优化方法提供基础。\nVar x_outer, x_inner;\ngradient.split(x, x_outer, x_inner, 2);\n  for y:\n    for x.v0:\n      for x.v1 in [0, 1]:\n        gradient_split(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#fuse",
    "href": "posts/halide-note.html#fuse",
    "title": "Halide 入门",
    "section": "fuse",
    "text": "fuse\n把两层循环合并，与split正好相反\nVar fused;\ngradient.fuse(x, y, fused);\nfor x.v2:\n  gradient_fused(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#tiling",
    "href": "posts/halide-note.html#tiling",
    "title": "Halide 入门",
    "section": "tiling",
    "text": "tiling\n通过以上介绍的方式对运算顺序进行组合，我们可以得到一个比较典型的优化策略tiling,就是把整块的运算拆分为多个小块，然后可以通过层面的并行去同步完成，整体的计算。\nVar x_outer, x_inner, y_outer, y_inner;\ngradient.split(x, x_outer, x_inner, 4);\ngradient.split(y, y_outer, y_inner, 4);\ngradient.reorder(x_inner, y_inner, x_outer, y_outer);\n\nfor y.v5:\n  for x.v3:\n    for y.v6 in [0, 3]:\n      for x.v4 in [0, 3]:\n        gradient_tiled(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#vectorized",
    "href": "posts/halide-note.html#vectorized",
    "title": "Halide 入门",
    "section": "vectorized",
    "text": "vectorized\n我们可以利用一些单指令多数据并行去同步处理多个数据，此时把数据进行向量化后加载就可以得到相当的大的加速。\nVar x_outer, x_inner;\ngradient.split(x, x_outer, x_inner, 4);\ngradient.vectorize(x_inner);\n\n  for y:\n    for x.v7:\n      vectorized x.v8 in [0, 3]:\n        gradient_in_vectors(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#unrolling",
    "href": "posts/halide-note.html#unrolling",
    "title": "Halide 入门",
    "section": "unrolling",
    "text": "unrolling\n循环展开，如果多个像素共享同个数据，这个可以把循环展开避免重复计算。\nVar x_outer, x_inner;\ngradient.split(x, x_outer, x_inner, 2);\ngradient.unroll(x_inner);\nfor y:\n  for x.v9:\n    unrolled x.v10 in [0, 1]:\n      gradient_unroll(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#splitting-by-factor",
    "href": "posts/halide-note.html#splitting-by-factor",
    "title": "Halide 入门",
    "section": "splitting by factor",
    "text": "splitting by factor\n如果按比例进行split，会出现一个问题，就是不满足比例的位置被重新计算了，比如7按3分割，那么后面就有两个元素被重复计算了。 \n这里是实际执行是会出现问题地方，正确的循环是执行2又1次，但是halide默认执行的是： 外循环 = 0 -&gt; (x_extent + factor - 1)/factor = (1+3-1) / 3 = 0 -&gt; 3 次 内循环 3次\n可以看到第三次外循环时对应的值被重复计算了，这个就非常蛋疼了。\nStore gradient_split_7x2.0(0, 0) = 0\nStore gradient_split_7x2.0(1, 0) = 1\nStore gradient_split_7x2.0(2, 0) = 2\nStore gradient_split_7x2.0(3, 0) = 3\nStore gradient_split_7x2.0(4, 0) = 4\nStore gradient_split_7x2.0(5, 0) = 5\nStore gradient_split_7x2.0(4, 0) = 4\nStore gradient_split_7x2.0(5, 0) = 5\nStore gradient_split_7x2.0(6, 0) = 6"
  },
  {
    "objectID": "posts/halide-note.html#fusing-tiling-parallelizing.",
    "href": "posts/halide-note.html#fusing-tiling-parallelizing.",
    "title": "Halide 入门",
    "section": "Fusing + tiling + parallelizing.",
    "text": "Fusing + tiling + parallelizing.\n融合多个优化策略，首先把图像8x8的图像tiling到2x2个4x4，然后把2x2融合为4，相当于4个4x4，再并行4个线程计算\nVar x_outer, y_outer, x_inner, y_inner, tile_index;\ngradient.tile(x, y, x_outer, y_outer, x_inner, y_inner, 4, 4);\ngradient.fuse(x_outer, y_outer, tile_index);\ngradient.parallel(tile_index);"
  },
  {
    "objectID": "posts/halide-note.html#融合多种trick",
    "href": "posts/halide-note.html#融合多种trick",
    "title": "Halide 入门",
    "section": "融合多种trick",
    "text": "融合多种trick\n首先64x64作为tiling\nVar x_outer, y_outer, x_inner, y_inner, tile_index;\ngradient_fast\n    .tile(x, y, x_outer, y_outer, x_inner, y_inner, 64, 64)\n    .fuse(x_outer, y_outer, tile_index)\n    .parallel(tile_index);\n然后对每个内部的再tiling到4x2，然后每个x的循环4直接作为向量化计算，y的循环2直接展开。\nVar x_inner_outer, y_inner_outer, x_vectors, y_pairs;\ngradient_fast\n    .tile(x_inner, y_inner, x_inner_outer, y_inner_outer, x_vectors, y_pairs, 4, 2)\n    .vectorize(x_vectors)\n    .unroll(y_pairs);\n虽然再tiling到64x64的时候会出现一些重复计算的问题，但是总体来说加速还是很明显的。"
  },
  {
    "objectID": "posts/halide-note.html#origin",
    "href": "posts/halide-note.html#origin",
    "title": "Halide 入门",
    "section": "origin",
    "text": "origin\n首先用最原始的方法进行计算，即先遍历一次做x方向的平均，然后再遍历一次做y方向的平均。\n  {\n    Buffer&lt;uint8_t&gt; input =\n        load_image(\"/home/workspace/Halide/tutorial/images/rgb.png\");\n    Func blur_x(\"blur_x\"), blur_y(\"blur_y\");\n    blur_x(x, y, c) = cast&lt;uint16_t&gt;(input(x - 1, y, c) + input(x, y, c) +\n                                     input(x + 1, y, c)) /\n                      3;\n    blur_y(x, y, c) = cast&lt;uint8_t&gt;(\n        (blur_x(x, y - 1, c) + blur_x(x, y, c) + blur_x(x, y + 1, c)) / 3);\n\n    blur_x.compute_root();\n    blur_y.compute_root();\n\n    blur_x.print_loop_nest();\n    printf(\"-------------\\n\");\n    blur_y.print_loop_nest();\n\n    Buffer&lt;uint8_t&gt; output(input.width() - 2, input.height() - 2, 3);\n    output.set_min(1, 1);\n    blur_y.realize(output);\n\n    save_image(output, \"blur_1.png\");\n  }\n通过compute root函数进行AOT编译，对pure func进行实例化，得到输出，可以发现计算对图像遍历了两次。\nproduce blur_x:\n  for c:\n    for y:\n      for x:\n        blur_x(...) = ...\nconsume blur_x:\n  produce blur_y:\n    for c:\n      for y:\n        for x:\n          blur_y(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#tile",
    "href": "posts/halide-note.html#tile",
    "title": "Halide 入门",
    "section": "tile",
    "text": "tile\n这里对blur y的进行tile加速,然后把blur x的计算时机挪到blur y进行第内循环的时候。（这里要注意的就是halide的顺序问题，使用变量时最内侧的循环放在最前面，表达计算流程时最后结果也放在前面，也就是作为root，所以compute_at是在计算blur_x的时候调用的）\n    blur_y.compute_root().tile(x, y, xi, yi, 128, 24);\n    blur_x.compute_root().compute_at(blur_y, x);\n得到结果\nproduce blur_y:\n  for c:\n    for y.y:\n      for x.x:\n        produce blur_x:\n          for y:\n            for x:\n              blur_x(...) = ...\n        consume blur_x:\n          for y.yi in [0, 23]:\n            for x.xi in [0, 127]:\n              blur_y(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#parallel",
    "href": "posts/halide-note.html#parallel",
    "title": "Halide 入门",
    "section": "parallel",
    "text": "parallel\n我们还可以继续把tile出来的块并行计算然后内部用向量化来算:\n    blur_y.compute_root()\n        .tile(x, y, xi, yi, 128, 24)\n        .parallel(yi)\n        .vectorize(xi);\n    blur_x.compute_root().compute_at(blur_y, x);\n我感觉halide应该是有一些方法直接绑定两个func到同一个level进行计算的，但是目前还不太清楚怎么弄，通过观察他的运算顺序可以发现我们需要对于每个Func都指定好相同的策略（如果我们想放到同一个looplevel进行计算的话)\nproduce blur_y:\n  for c:\n    for y.y:\n      for x.x:\n        produce blur_x:\n          for y:\n            for x:\n              blur_x(...) = ...\n        consume blur_x:\n          parallel y.yi in [0, 23]:\n            vectorized x.xi in [0, 127]:\n              blur_y(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#parallel-2",
    "href": "posts/halide-note.html#parallel-2",
    "title": "Halide 入门",
    "section": "parallel 2",
    "text": "parallel 2\n\n详解titling\n对于tiling来说，我们传入的xi和yi分别是内部循环中的变量，可以说halide把中间过程的变量表示作为临时变量了。\n    blur_x.compute_root()\n        .tile(x, y, xi, yi, 128, 24)\n得到\nproduce blur_x:\n  for c:\n    for y.y:\n      for x.x:\n        for y.yi in [0, 23]:\n          for x.xi in [0, 127]:\n            blur_x(...) = ...\n\n\nparallel的位置\n上面的y.y和x.x就是被忽略的变量。对于我们有用的就是xi和yi。假设我们要对多个titling的区域进行并行化，那么应该把参数加在这里：\n    blur_x.compute_root()\n        .tile(x, y, xi, yi, 128, 24).parallel(yi);\nproduce blur_x:\n  for c:\n    for y.y:\n      for x.x:\n        parallel y.yi in [0, 23]:\n          for x.xi in [0, 127]:\n            blur_x(...) = ...\n\n\n最终结果\n这里我们指定blur y计算的loop level为xi，那么可以省去几行代码，但是问题来了，这里的的blur x内部还是在循环y x，不知道生成代码的时候是不是还是重复计算的！这个需要继续去实践才知道。\n    blur_y.compute_root()\n        .tile(x, y, xi, yi, 128, 24)\n        .parallel(yi, 8)\n        .vectorize(xi, 32);\n    blur_x.compute_root().compute_at(blur_y, xi);\n运算顺序输出\nproduce blur_y:\n  for c:\n    for y.y:\n      for x.x:\n        parallel y.yi in [0, 23]:\n          for x.xi.xi in [0, 3]:\n            produce blur_x:\n              for y:\n                for x:\n                  blur_x(...) = ...\n            consume blur_x:\n              vectorized x.xi.v0 in [0, 31]:\n                blur_y(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#definition-update",
    "href": "posts/halide-note.html#definition-update",
    "title": "Halide 入门",
    "section": "definition update",
    "text": "definition update\n\n直接定义\n有时候我们需要手动指定一些索引按我们需要的方式来更新，或者说直接指定索引计算结果，但是如果在halide中这样做的问题就是他默认会帮你重新循环：\n假设我们自定义了两次计算如下：\n        Func g(\"g\");\n        g(x, y) = x + y;    // Pure definition\n        g(2, 1) = 42;       // First update definition\n        g(x, 0) = g(x, 1);  // Second update definition\n可以发现每次update的定义会重新生成一次循环，但是明显我们的操作是可以和原来的循环融合的。\nproduce g:\n  for y:\n    for x:\n      g(...) = ...\n  g(...) = ...\n  for x:\n    g(...) = ...\n\n\nreduction\n用RDom进行一些规约也会出现同样的情况\n    Func f;\n    f(x, y) = (x + y) / 100.0f;\n    RDom r(0, 50);\n    f(x, r) = f(x, r) * f(x, r);\n可以发现还是重新生成了一次循环，本来RDom的操作应该在x的内部的。\nproduce f5:\n  for y:\n    for x:\n      f5(...) = ...\n  for x:\n    for r19 in [0, 49]:\n      f5(...) = ...\n\n\nScheduling update steps\n接下来对udpate step进行一些调度，来一个例子：\nFunc f;\nf(x, y) = x * y;\n// Set row zero to each row 8\nf(x, 0) = f(x, 8);\n// Set column zero equal to column 8 plus 2\nf(0, y) = f(8, y) + 2;\n计算流程：\nproduce f10:\n  for y:\n    for x:\n      f10(...) = ...\n  for x:\n    f10(...) = ...\n  for y:\n    f10(...) = ...\n如果需要进行schedule，我们可以对每一个stage进行调整：\nf.vectorize(x, 4).parallel(y);\n则第一个阶段f的计算将会被调整：\nproduce f10:\n  parallel y:\n    for x.x:\n      vectorized x.v3 in [0, 3]:\n        f10(...) = ...\n  for x:\n    f10(...) = ...\n  for y:\n    f10(...) = ...\n如果需要调整第二个阶段的stage怎么办？可以调用update函数获取某个阶段的句柄，然后进行schedule：\nf.update(0).vectorize(x, 4);\n可以看到第一次update的循环被schedule了\nproduce f10:\n  parallel y:\n    for x.x:\n      vectorized x.v3 in [0, 3]:\n        f10(...) = ...\n  for x.x:\n    vectorized x.v4 in [0, 3]:\n      f10(...) = ...\n  for y:\n    f10(...) = ...\n\n\nProducer-Consumer Case 0\n上面的例子展示了如何对任意阶段的代码进行schedule，但是如何把两个update放到一个循环中呢,halide把这种情况统一为生产者-消费者模型。 比如一个update的写法，如果把它写成两个func，halide就会自动把他们放在同一个循环内部：\nFunc newf;\nnewf(x) = x * 2;\nnewf(x) += 10;\nnewf(x) = 2 * newf(x);\n\nFunc producer, consumer;\nproducer(x) = x * 2;\nproducer(x) += 10;\nconsumer(x) = 2 * producer(x);\n对比计算顺序就可以看到明显的区别：\nproduce f15:\n  for x:\n    f15(...) = ...\n  for x:\n    f15(...) = ...\n  for x:\n    f15(...) = ...\n\nproduce f14:\n  for x:\n    produce f13:\n      f13(...) = ...\n      f13(...) = ...\n    consume f13:\n      f14(...) = ...\n\n\nCase 1 ： 消费者只在pure func阶段消费\n接下来继续更加复杂的例子，有Producer-Consumer的同时，Consumer只操作pure func，再对Consumer做update。\nproducer(x) = x * 17;\nconsumer(x) = 2 * producer(x);\nconsumer(x) += 50;\nproducer.compute_at(consumer, x);\n在compute_at之前其实halide已经把Producer-Consumer pair放到了一个循环级别内部。经过一次compute_at，把循环重新声明之后，可以看到更加详细的内容。这里还要注意到consumer(x) += 50;这个update操作都是被放到另一次循环了。\nproduce f21:\n  for x:\n    f21(...) = ...\n  for x:\n    f21(...) = ...\n------------\nproduce f21:\n  for x:\n    produce f20:\n      f20(...) = ...\n    consume f20:\n      f21(...) = ...\n  for x:\n    f21(...) = ...\n\n\nCase 2 ： 消费者update之后使用生产者的结果\n首先执行producer，但是在消费之前先update一次consumer：\nproducer(x) = x * 17;\nconsumer(x) = 100 - x * 10;\nconsumer(x) += producer(x);\nproducer.compute_at(consumer, x);\n然后我们producer的运算放到consumer update之后进行运算（这里不用写producer.compute_at(consumer.update(0), x),因为schedule是根据变量来调度的，这里的变量级别即可以确定）。\nproduce f27:\n  for x:\n    f27(...) = ...\n  for x:\n    produce f26:\n      f26(...) = ...\n    consume f26:\n      f27(...) = ...\n\n\nCsae 3 : 消费update时多次共享producer\n下面生产一次，然后使用两次producer的值\nproducer(x) = x * 17;\nconsumer(x) = 170 - producer(x); // update\nconsumer(x) += producer(x) / 2;\nproducer.compute_at(consumer, x);\n这里还是因为update了两次，所以没有把producer给共用。\nproduce f33:\n  for x:\n    produce f32:\n      f32(...) = ...\n    consume f32:\n      f33(...) = ...\n  for x:\n    produce f32:\n      f32(...) = ...\n    consume f32:\n      f33(...) = ..."
  },
  {
    "objectID": "posts/halide-note.html#compute-at",
    "href": "posts/halide-note.html#compute-at",
    "title": "Halide 入门",
    "section": "compute at",
    "text": "compute at\ncompute at是指当前函数依赖于哪一个变量来计算，比如下面这个例子，当函数依赖于x计算，那么会在最内层的x循环计算4个g到f。如果是依赖于循环y来计算的话，对于x就需要两次循环了。\nimport halide as hl\n\nf = hl.Func('f')\ng = hl.Func('g')\nx = hl.Var('x')\ny = hl.Var('y')\ng[x, y] = x * y\nf[x, y] = g[x, y] + g[x, y + 1] + g[x + 1, y] + g[x + 1, y + 1]\n\ng.compute_at(f, x)\nf.print_loop_nest()\n\"\"\" \nproduce f:\n  for y:\n    for x:\n      produce g:\n        for y:\n          for x:\n            g(...) = ...\n      consume g:\n        f(...) = ...\n\"\"\"\n\ng.compute_at(f, y)\nf.print_loop_nest()\n\"\"\" \nproduce f:\n  for y:\n    produce g:\n      for y:\n        for x:\n          g(...) = ...\n    consume g:\n      for x:\n        f(...) = ...\n\"\"\""
  },
  {
    "objectID": "posts/halide-note.html#生成不依赖halidelib的代码",
    "href": "posts/halide-note.html#生成不依赖halidelib的代码",
    "title": "Halide 入门",
    "section": "生成不依赖halidelib的代码",
    "text": "生成不依赖halidelib的代码\n生成代码部分可以去看他的教程，我们可以在生成代码时加上一些参数:\nno_runtime,  ///&lt; Do not include a copy of the Halide runtime in any generated object file or assembly\nno_asserts,       ///&lt; Disable all runtime checks, for slightly tighter code.\n生成c代码和头文件之后，我们一般添加头文件HalideBuffer.H就够了。如果生成的是no_runtime的话，生成的c代码里面是不带runtime的，所以我们需要手动添加头文件HalideRuntime.h。"
  },
  {
    "objectID": "posts/halide-note.html#替换数据类型",
    "href": "posts/halide-note.html#替换数据类型",
    "title": "Halide 入门",
    "section": "替换数据类型",
    "text": "替换数据类型\n生成代码的时候我们只能选择halide内置类型，但是我们实际调用的时候要用自己的类型时候就比较麻烦了，折腾了半天，我才发现可以自定义内置数据，他是构造buffer的时候去检查类型，如果没有对应的类型就直接报错了，但是我们把自己类型注册进去就好了。 下面这个例子就是halide自带的bfloat16类型的组成过程。\ntemplate&lt;&gt;\nHALIDE_ALWAYS_INLINE halide_type_t halide_type_of&lt;Halide::bfloat16_t&gt;() {\n    return halide_type_t(halide_type_bfloat, 16);\n}"
  },
  {
    "objectID": "posts/halide-note.html#生成时选择",
    "href": "posts/halide-note.html#生成时选择",
    "title": "Halide 入门",
    "section": "生成时选择",
    "text": "生成时选择\nhalide的一个缺点就是tiling的时候不会自动判断是否能整除（当然也可以选择tiling的策略，但是通过自动补齐、if判断并不适用于所有情况），比如现在要在固定的tile上面计算，所以他的方案是对多个不同的size进行specialize tiling，如果全部都不符合，再fallback到一个通用的处理上。\n// 计算出当前的accumulators，然后按倍数进行fallback，也就是4 -&gt; 2 -&gt; 1\nconst int accumulators = get_accumulator_count(target);\nstd::vector&lt;std::pair&lt;int, int&gt;&gt; tile_sizes;\nconst int min_tile_c = 1;\nconst int max_tile_c = 4;\nfor (int tile_c = max_tile_c; tile_c &gt;= min_tile_c; tile_c /= 2) {\n    int tile_x = std::min(8, accumulators / tile_c);\n    tile_sizes.emplace_back(tile_c, tile_x);\n}\ntile_sizes.emplace_back(max_tile_c, 1);\nVar xo(\"xo\");\nExpr output_channels = output_.dim(0).extent();\nExpr output_width = output_.dim(1).extent();\nfor (auto i : tile_sizes) {\n    const int tile_c = i.first;\n    const int tile_x = i.second;\n    output_\n        .specialize(output_channels % (tile_c * accum_vector_size) == 0 && output_width &gt;= tile_x)\n        .split(c, co, c, tile_c * accum_vector_size, TailStrategy::RoundUp)\n        .split(x, xo, x, tile_x, TailStrategy::ShiftInwards)\n        .reorder(x, c, co, xo, y, b)\n        .vectorize(c)\n        .unroll(x);\n}"
  },
  {
    "objectID": "posts/halide-note.html#tiling的策略",
    "href": "posts/halide-note.html#tiling的策略",
    "title": "Halide 入门",
    "section": "tiling的策略",
    "text": "tiling的策略\n当输入为20，tiling 8的时候，不同的tiling策略的底层做法\n\nRoundUp\n\nRoundUp就是直接取最小满足的整数倍，直接取到8的整数倍，这也是最直接的方法，当然报错也来的很直接，因为8 * 3 = 24 &gt; 20 sh   terminate called after throwing an instance of 'Halide::RuntimeError'   what():  Error: Input buffer b0 is accessed at 23, which is beyond the max (19) in dimension 1\n\nGuardWithIf\n\n自动生成一个special的版本进行fallback sh   if ((gemm.extent.1 % 8) != 0) {    let t114 = (gemm.extent.0 + 15)/16    let t115 = gemm.extent.1 % 8    let t116 = 0 - (gemm.min.1*gemm.stride.1)    let t117 = ((gemm.extent.1/8)*8) + gemm.min.1    .    .    .   }\n\nShiftInwards\n\n重新补齐然后按tiling的size进行eval，但是这只能对prue func有效果\n\nPredicate\n\n这是在内循环中做fallback，和GuardWithIf类似。并且他没有重复eval以及不限制输入尺寸，但也是由于分离增加了代码大小尾tile的处理。"
  },
  {
    "objectID": "posts/hf-llama.html",
    "href": "posts/hf-llama.html",
    "title": "hugging face llama使用",
    "section": "",
    "text": "记录一下使用hugging face llama推理时遇到的问题.\n\n首先使用如下代码进行推理:\nfrom transformers import AutoConfig, AutoModel\nfrom transformers import LlamaModel, LlamaConfig, LlamaTokenizerFast, LlamaForCausalLM\nfrom transformers import AutoTokenizer\nfrom torchsummary import summary\nimport logging\nlogging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\nlogger = logging.getLogger('transformers')\nlogger.setLevel(logging.DEBUG)\n\ntokenizer = LlamaTokenizerFast.from_pretrained(\"/root/workspace/llama_test/llama-tokenizer\")\nprompt = \"My name is Mariama, my favorite\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\nprint(inputs)\n\nconfig = AutoConfig.from_pretrained(\"/data/llama-65b-hf/\") # \nconfig.torch_dtype = \"float32\"\nconfig.use_cache = False\nprint(config)\n\nmodel = LlamaForCausalLM.from_pretrained(\"/data/llama-65b-hf/\", config=config)\n\nprint(\"model init!\")\ngenerate_ids = model.generate(inputs.input_ids, max_new_tokens=32)\nprint(generate_ids)\n这里得到的输入为tensor([[    1,  1619,  1024,   338,  1085,  2829, 29874, 29892,   590, 25448]]), attention_mask为tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\ngenerate过程\n\n加载模型的generation.json GenerationConfig {       \"bos_token_id\": 0,       \"eos_token_id\": 1,       \"max_new_tokens\": 32,       \"pad_token_id\": -1,       \"use_cache\": false     }\n配置最大长度\ngeneration_config.max_length = generation_config.max_new_tokens + input_ids_length目前是32+10 = 42\ngreedy_search\n根据输出策略, 进入greedy_search进行推理.\n\n循环推理\n准备输入 'input_ids':      tensor([[    1,  1619,  1024,   338,  1085,  2829, 29874, 29892,   590, 25448]])      'position_ids':      tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])      'past_key_values':      None      'use_cache':      False      'attention_mask':      tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n进入forward\ndecode layers. 输出hidden_states为torch.Size([1, 10, 8192])\n执行lm head, 得到logits为torch.Size([1, 10, 32000])\n取logits最后一个torch.Size([1, 32000])求最大概率 这里我得到的是color. 而整个输出得到是# name is Ktha and and I friends color. 本来以为是输入&lt;s&gt; xxx 会得到xxxy这样, 然后取y作为一个输出. 现在看来其实前面部分也会被脑补一些."
  },
  {
    "objectID": "posts/infixtoendfix.html",
    "href": "posts/infixtoendfix.html",
    "title": "中缀表达式转后缀表达式",
    "section": "",
    "text": "我用c语言写了个中缀表达式转后缀表达式代码。。。网上教程多的不要不要的，不过我觉得还是看数据结构-c语言实现是最舒服的。\n\n\n程序\n这个程序只支持个位数的数字运算！(我偷懒不想写太多~~)\n/*\n * @Author: Zheng Qihang\n * @Date: 2018-07-05 20:14:48\n * @Last Modified by: Zheng Qihang\n * @Last Modified time: 2018-11-08 16:37:02\n */\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;time.h&gt;\n\ntypedef enum {\n    ADD = 0,\n    SUBT = 0,\n    MULT = 1,\n    DIVD = 1,\n    LEFTB = -1,\n    RIGHTB =-1,\n    NLL = -2,\n} Operator;\n\ntypedef char ElementType; // data type\n#define Node ptrNode      // Node defination\n#define Stack ptrNode     // list defination\ntypedef struct _Node      // node defination\n{\n    ElementType data;\n    struct _Node *next;\n} * ptrNode; // Node is a pointer to _Node\n\nvoid CheckStack(Stack S) {\n    if (S == NULL) {\n        printf(\"stack is invaild!\\n\");\n    }\n}\n\nint IsEmpty(Stack S) {\n    CheckStack(S);\n    return S-&gt;next == NULL;\n}\n\nStack CreateStack(void) {\n    Stack S = (Stack)malloc(sizeof(struct _Node));\n    CheckStack(S);\n    S-&gt;next = NULL;\n    return S;\n}\n\nElementType Pop(Stack S) {\n    int temp;\n    CheckStack(S);\n    Node tmepNode = S-&gt;next;\n    if (tmepNode != NULL) {\n        temp = tmepNode-&gt;data;\n        S-&gt;next = tmepNode-&gt;next;\n        free(tmepNode);\n        return temp;\n    } else {\n        return -1;\n    }\n}\n\nvoid MakeEmpty(Stack S) {\n    CheckStack(S);\n    while (S-&gt;next != NULL) {\n        Pop(S);\n    }\n}\nvoid DisposeStack(Stack S) {\n    CheckStack(S);\n    MakeEmpty(S);\n    free(S);\n}\n\nvoid Push(ElementType X, Stack S) {\n    CheckStack(S);\n    Node tempNode = S-&gt;next;\n    Node newNode = (Node)malloc(sizeof(struct _Node));\n    newNode-&gt;data = X;\n    // printf(\"S-&gt;next %p\\n\",S-&gt;next);\n    if (tempNode != NULL) {\n        newNode-&gt;next = tempNode;\n        // printf(\"%p = %p\\n\",newNode-&gt;next,tempNode);\n    } else {\n        newNode-&gt;next = NULL;\n    }\n    S-&gt;next = newNode;\n    // printf(\"S-&gt;next %p\\n\",S-&gt;next);\n}\n\nElementType Top(Stack S) {\n    CheckStack(S);\n    if (S-&gt;next != NULL) {\n        return S-&gt;next-&gt;data;\n    } else {\n        return -1;\n    }\n}\n\nvoid PrintOutPut(Stack S) {\n    Node tep = S-&gt;next;\n    char *str=(char*)malloc(30*sizeof(char));\n    int cnt=0;\n    CheckStack(S);\n    while (tep != NULL) {\n        *(str+(cnt++))=tep-&gt;data;//add char to string;\n        tep = tep-&gt;next;\n    }\n    while(cnt--){\n        printf(\"%c\",*(str+cnt));//Reverse the string and print\n    }\n    free(str);//avoid memory leaks!!\n}\n\nvoid PrintStack(Stack S) {\n    Node tep = S-&gt;next;\n    while (tep != NULL) {\n        printf(\"addr=0x%3lX  data=%d  nextaddr=0x%3lX\\n\",\n               (unsigned long int)tep & 0xFFF, tep-&gt;data,\n               (unsigned long int)tep-&gt;next & 0xFFF);\n        tep = tep-&gt;next;\n    }\n}\n\nOperator ChToOpt(char C) {\n    Operator temp;\n    switch (C) {\n    case '+':\n        temp = ADD;\n        break;\n    case '-':\n        temp = SUBT;\n        break;\n    case '*':\n        temp = MULT;\n        break;\n    case '/':\n        temp = DIVD;\n        break;\n    case '(':\n        temp = LEFTB;\n        break;\n    case ')':\n        temp = RIGHTB;\n        break;\n    default:\n        temp = NLL;\n        break;\n    }\n    return temp;\n}\n\nvoid printfCovt(Stack stack, Stack out, int i) {\n    printf(\"-------STEP  %d---------\\n\", i);\n    printf(\"stack :  \");\n    PrintOutPut(stack);\n    printf(\"   &lt;--\\n\");\n    printf(\"output:  \");\n    PrintOutPut(out);\n    printf(\"   &lt;--\\n\");\n    printf(\"-----------------------\\n\\n\");\n}\n\nint main(int argc, char const *argv[]) {\n    int tempint;\n    Stack MyStack, output = NULL;\n    uint8_t i = 0;\n    char *func = \"1+2*3+(4*5+6)*7\";\n    Operator curOp;\n    Operator TopOp;\n    MyStack = CreateStack(); // make new stack;\n    output = CreateStack();\n    for (i = 0; i &lt; strlen(func); i++) {\n\n        switch (*(func + i)) {\n        case '+':\n        case '-':\n        case '*':\n        case '/':\n            do {\n                curOp = ChToOpt(*(func + i));\n                TopOp = ChToOpt(Top(MyStack));\n                // printf(\"topop:%d curop :%d\\n\", TopOp, curOp);\n                if (TopOp &gt;= curOp) {\n                    Push(Pop(MyStack), output);    // move stack to output\n                    TopOp = ChToOpt(Top(MyStack)); // refresh TopOperater\n                }\n            } while (TopOp &gt;= curOp);\n            Push(*(func + i), MyStack); // add Current Operater in stack\n            break;\n        case '(':\n            Push(*(func + i), MyStack); // add left brackets in stack\n            break;\n        case ')':\n            while (Top(MyStack) != '(') {\n                // move stack to output utill top is '('\n                Push(Pop(MyStack), output); \n            }\n            Pop(MyStack); // pop the '('\n            break;\n        default: // when func[i]=0~9\n            Push(*(func + i), output);\n            break;\n        }\n        printfCovt(MyStack, output, i);\n    }\n\n    while (!IsEmpty(MyStack)) {\n        Push(Pop(MyStack), output); // move stack to output\n    }\n    printfCovt(MyStack, output, i++);\n\n    return 0;\n}\n\n\n运行结果\n-------STEP  0---------\nstack :     &lt;--\noutput:  1   &lt;--\n-----------------------\n\n-------STEP  1---------\nstack :  +   &lt;--\noutput:  1   &lt;--\n-----------------------\n\n-------STEP  2---------\nstack :  +   &lt;--\noutput:  12   &lt;--\n-----------------------\n\n-------STEP  3---------\nstack :  +*   &lt;--\noutput:  12   &lt;--\n-----------------------\n\n-------STEP  4---------\nstack :  +*   &lt;--\noutput:  123   &lt;--\n-----------------------\n\n-------STEP  5---------\nstack :  +   &lt;--\noutput:  123*+   &lt;--\n-----------------------\n\n-------STEP  6---------\nstack :  +(   &lt;--\noutput:  123*+   &lt;--\n-----------------------\n\n-------STEP  7---------\nstack :  +(   &lt;--\noutput:  123*+4   &lt;--\n-----------------------\n\n-------STEP  8---------\nstack :  +(*   &lt;--\noutput:  123*+4   &lt;--\n-----------------------\n\n-------STEP  9---------\nstack :  +(*   &lt;--\noutput:  123*+45   &lt;--\n-----------------------\n\n-------STEP  10---------\nstack :  +(+   &lt;--\noutput:  123*+45*   &lt;--\n-----------------------\n\n-------STEP  11---------\nstack :  +(+   &lt;--\noutput:  123*+45*6   &lt;--\n-----------------------\n\n-------STEP  12---------\nstack :  +   &lt;--\noutput:  123*+45*6+   &lt;--\n-----------------------\n\n-------STEP  13---------\nstack :  +*   &lt;--\noutput:  123*+45*6+   &lt;--\n-----------------------\n\n-------STEP  14---------\nstack :  +*   &lt;--\noutput:  123*+45*6+7   &lt;--\n-----------------------\n\n-------STEP  15---------\nstack :     &lt;--\noutput:  123*+45*6+7*+   &lt;--\n-----------------------"
  },
  {
    "objectID": "posts/jax-reshard.html",
    "href": "posts/jax-reshard.html",
    "title": "探究jax reshard优化",
    "section": "",
    "text": "Google在分布式系统上有非常深厚的积累，本文主要尝试检查jax的行为来探究数据重分布reshard算子的优化方案。\n配置当前环境下的device/mesh："
  },
  {
    "objectID": "posts/jax-reshard.html#ir解读",
    "href": "posts/jax-reshard.html#ir解读",
    "title": "探究jax reshard优化",
    "section": "IR解读",
    "text": "IR解读\n检查spmd的ir，先查看module信息：\nHloModule jit_reshard_1, is_scheduled=true, entry_computation_layout={(f32[1024,2048]{1,0})-&gt;f32[512,1024]{1,0}}, num_partitions=8\n这里有一个num_partitions是定义程序所执行的分区数量，在hlo中还有一个num_replicas定义的是副本数， 我理解num_replicas是用于描述数据并行的，虽然在sharding的描述中看起来数据并行也是切分，但是他们属于不同的数据了，所以hlo这里会有一些特殊优化存在。\n然后查看主函数参数为:\n%param = f32[1024,2048]{1,0} parameter(0), sharding={devices=[2,1,4]&lt;=[8] last_tile_dim_replicate}, metadata={op_name=\"x\"}\n这表示了在spmd的ir下，并不会使用global view来表示分布式的数据。 同时这里f32[1024, 2048]{1, 0}中花括号部分表示的是数据的layout， 就表明此时的param为列主序。\n然后查看下面的三个constant和函数bitcast_dynamic-slice_fusion调用， 并将其简化:\n%partition-id = u32[] partition-id()\n%slice_8 = dynamic-slice({0,   0 , 512, 512 , 1024, 1024, 1536, 1536}, %partition-id)\n%slice_7 = dynamic-slice({0,   0 ,  0 ,  0  , 1024, 1024, 1024, 1024}, %partition-id)\n%slice_6 = dynamic-slice({0, 1024,  0 , 1024,   0 , 1024,   0 , 1024}, %partition-id)\n%sub_3 = subtract(%slice_8, %slice_7)\n%slice_5 = f32[512,1024]{1,0} dynamic-slice(%param_0, %sub_3, %slice_6)\n可以发现xla通过分析reshard，把需要进行数据传输的范围进行了提前计算，一共8个设备并行，然后每个设备上对应的slice 范围为：\n0 =&gt; dynamic_slice(input=f32[1024, 2048], start=(0 - 0  =       0 ,      0 ), size=(512, 1024))\n1 =&gt; dynamic_slice(input=f32[1024, 2048], start=(0 - 0  =       0 ,    1024), size=(512, 1024))\n2 =&gt; dynamic_slice(input=f32[1024, 2048], start=(512 - 0 =     512,      0 ), size=(512, 1024))\n3 =&gt; dynamic_slice(input=f32[1024, 2048], start=(512 - 0 =     512,    1024), size=(512, 1024))\n4 =&gt; dynamic_slice(input=f32[1024, 2048], start=(1024 - 1024 =  0 ,      0 ), size=(512, 1024))\n5 =&gt; dynamic_slice(input=f32[1024, 2048], start=(1024 - 1024 =  0 ,    1024), size=(512, 1024))\n6 =&gt; dynamic_slice(input=f32[1024, 2048], start=(1536 - 1024 = 512,      0 ), size=(512, 1024))\n7 =&gt; dynamic_slice(input=f32[1024, 2048], start=(1536 - 1024 = 512,    1024), size=(512, 1024))\n因为mesh的维度 Y = 2X, 所以上述操作在每个设备上对数据在M，N维度在此切分即可得到分布为[M @ Y,N @ X]的数据f32[512,1024], 但数据的位置还是错乱的，因此还需要调用一次高性能的primitive: collective-permute，数据设备上交错之后就可以得到最终所需要的分布式布局：\nROOT %collective-permute = f32[512,1024]{1,0} collective-permute(%bitcast_dynamic-slice_fusion), channel_id=1, source_target_pairs={{0,0},{1,4},{2,1},{3,5},{4,2},{5,6},{6,3},{7,7}}, metadata={op_name=\"jit(reshard_1)/reshard\"}"
  },
  {
    "objectID": "posts/k210-double-core.html",
    "href": "posts/k210-double-core.html",
    "title": "k210_双核测试",
    "section": "",
    "text": "k210由两个核组成，我想测试一下两个核的运行情况。编写了以下程序：\n\n/*\n * @Author: Zheng Qihang \n * @Date: 2018-11-11 20:10:46 \n * @Last Modified by:   Zheng Qihang \n * @Last Modified time: 2018-11-11 20:10:46 \n */\n\n#include \"bsp.h\"\n#include &lt;stdio.h&gt;\n\nvolatile int core0_value= 0;\nvolatile int core1_value= 0;\nvolatile int global_value= 0;\n\nint core1_function(void *ctx) {\n    uint64_t core= current_coreid();\n\n    printf(\"Core %ld Hello world\\n\", core);\n    while (1) {\n        core1_value++;\n        global_value++;\n        msleep(500);\n        if (core1_value % 5 == 0) {\n            printf(\"core1 %d global %d\\n\", core1_value, global_value);\n        }\n    };\n}\n\nint main() {\n    uint64_t core= current_coreid();\n    printf(\"Core %ld Hello world\\n\", core);\n    register_core1(core1_function, NULL);\n    while (1) {\n        core0_value++;\n        global_value--;\n        msleep(500);\n        if (core0_value % 5 == 0) {\n            printf(\"core0 %d global %d\\n\", core0_value, global_value);\n        }\n    };\n}\n\n编译运行\n➜  build cmake .. -DPROJ=double_core && make && make clean\n➜  build python3 isp.py -p /dev/ttyUSB0 -b 115200 double_core.bin\n\n[INFO] Rebooting...\nCore 0 Hello world\nCore 1 Hello world\ncore0 5 global 0\ncore1 5 global -1\ncore0 10 global 0\ncore1 10 global -1\ncore0 15 global 0\ncore1 15 global -1\ncore0 20 global 0\ncore1 20 global -1\n\n\n思考\n发现两个核应该是以同一个速度进行运行的，并且是同时运行的，对全局变量具有相同的权限。但是对于同一个外设，同一时间的优先级还是未知的。"
  },
  {
    "objectID": "posts/k210-gpiohs-irq.html",
    "href": "posts/k210-gpiohs-irq.html",
    "title": "k210 高速gpio与中断",
    "section": "",
    "text": "高速gpio拥有更快的反转能力，并且一共有32个io，足够我们使用。\n\n\nK210外部中断\n可以将任意的外部中断分配到cpu中。\n\n\n代码\n```c\n#include “fpioa.h” #include “gpiohs.h” #include “sysctl.h” #include &lt;stdio.h&gt; #include &lt;unistd.h&gt;\nint irq_flag= 1; /* clang-format off / #define PIN_LED 12 #define PIN_KEY 14 / clang-format on */\nvoid irq_gpiohs2(void gp) { irq_flag= gpiohs_get_pin(2); / 进入中断后读取KEY值 */\nprintf(\"IRQ The PIN is %d\\n\", irq_flag);\n\nif (irq_flag) /* 设置LED状态 */\n    gpiohs_set_pin(3, GPIO_PV_LOW);\nelse\n    gpiohs_set_pin(3, GPIO_PV_HIGH);\n}\nint main(void) { plic_init(); /* 初始化中断 / sysctl_enable_irq(); / 使能系统中断 */\nfpioa_set_function(PIN_LED, FUNC_GPIOHS3); /* LED映射高速GPIO3 */\ngpiohs_set_drive_mode(3, GPIO_DM_OUTPUT);  /* 设置GPIOHS3 输出 */\ngpio_pin_value_t value= GPIO_PV_HIGH;      /* 初始化GPIO状态 */\ngpiohs_set_pin(3, value);                  /* GPIO状态设置 */\n\nfpioa_set_function(PIN_KEY, FUNC_GPIOHS2); /* KEY映射GPIOHS2 */\ngpiohs_set_drive_mode(2, GPIO_DM_INPUT);   /* 设置GPIOHS2 输入 */\ngpiohs_set_pin_edge(2, GPIO_PE_BOTH);      /* 设置双沿触发 */\ngpiohs_set_irq(2, 1, irq_gpiohs2); /* 设置回调函数，优先级1 */\n\nwhile (1) {\n    sleep(1);\n    if (irq_flag) { gpiohs_set_pin(3, value= !value); }\n    int val= gpiohs_get_pin(2); /* 正常状态读取KEY值 */\n    printf(\"The PIN is %d\\n\", val);\n}\nreturn 0;\n}\n``\n`"
  },
  {
    "objectID": "posts/k210-windows.html",
    "href": "posts/k210-windows.html",
    "title": "k210环境搭建_Windows",
    "section": "",
    "text": "首先我们去官网下载需要的文件,我的板子是绿色板子.\n我们找到官网,进入其中的资源下载\n\n\n我们下载以下文件:  还要下载烧录软件! \n\n安装交叉编译器\n之前我们下载了RISC-V 64bit toolchain for Kendryte K210_win32交叉编译器。现在我们首先需要对他进行安装。\n\n解压Toolchain\n我将交叉编译器放到了I:\\Kendryte210\\kendryte-toolchain.\n设置环境变量\n打开系统的环境变量并将添加如下: \n\n\n\n开始编译\n\n首先解压sdk\n我把sdk解压到了I:\\Kendryte210\\kendryte-standalone-sdk-0.5.0\n安装Cmake\n我们要编译,首先得安装Cmake.去cmake官网下载安装.如果网速不给力可以去这里下载. 记得按照时选择添加到系统路径\n生成Makefile\n在sdk中打开终端powershell运行：\nPS I:\\Kendryte210\\kendryte-standalone-sdk-0.5.0&gt; mkdir build\n\n\n    目录: I:\\Kendryte210\\kendryte-standalone-sdk-0.5.0\n\n\nMode                LastWriteTime         Length Name\n----                -------------         ------ ----\nd-----        2018/11/1     15:38                build\n\n\nPS I:\\Kendryte210\\kendryte-standalone-sdk-0.5.0&gt; cd build\nPS I:\\Kendryte210\\kendryte-standalone-sdk-0.5.0\\build&gt; cmake .. -DPROJ=hello_world  -G \"MinGW Makefiles\"\nPROJ = hello_world\n-- Check for RISCV toolchain ...\n-- Using I:/Kendryte210/kendryte-toolchain/bin RISCV toolchain\n-- The C compiler identification is GNU 7.2.0\n-- The CXX compiler identification is GNU 7.2.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\nSOURCE_FILES=I:/Kendryte210/kendryte-standalone-sdk-0.5.0/src/hello_world/main.c\n\nProject: hello_world\nLIST_FILE=I:/Kendryte210/kendryte-standalone-sdk-0.5.0/cmake/executable.cmake\nTOOLCHAIN=I:/Kendryte210/kendryte-toolchain/bin\nKENDRYTE_IDE=\nBUILDING_SDK=yes\n\nCMAKE_BUILD_TYPE=Debug\nCMAKE_C_COMPILER=I:/Kendryte210/kendryte-toolchain/bin/riscv64-unknown-elf-gcc.exe\nCMAKE_CXX_COMPILER=I:/Kendryte210/kendryte-toolchain/bin/riscv64-unknown-elf-g++.exe\nCMAKE_LINKER=I:/Kendryte210/kendryte-toolchain/bin/riscv64-unknown-elf-ld.exe\nCMAKE_OBJCOPY=I:/Kendryte210/kendryte-toolchain/bin/riscv64-unknown-elf-objcopy.exe\nCMAKE_OBJDUMP=I:/Kendryte210/kendryte-toolchain/bin/riscv64-unknown-elf-objdump.exe\nCMAKE_MAKE_PROGRAM=I:/Kendryte210/kendryte-toolchain/bin/mingw32-make.exe\n\nCMAKE_C_FLAGS= -mcmodel=medany -fno-common -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -fno-zero-initialized-in-bss -Os -ggdb -std=gnu11 -Wno-pointer-to-int-cast -Wall -Werror=all -Wno-error=unused-function -Wno-error=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Werror=frame-larger-than=65536 -Wno-unused-parameter -Wno-sign-compare -Wno-error=missing-braces -Wno-error=return-type -Wno-error=pointer-sign -Wno-missing-braces -Wno-strict-aliasing -Wno-implicit-fallthrough -Wno-missing-field-initializers -Wno-old-style-declaration\nCMAKE_CXX_FLAGS= -mcmodel=medany -fno-common -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -fno-zero-initialized-in-bss -Os -ggdb -std=gnu++17 -Wall -Werror=all -Wno-error=unused-function -Wno-error=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Werror=frame-larger-than=65536 -Wno-unused-parameter -Wno-sign-compare -Wno-error=missing-braces -Wno-error=return-type -Wno-error=pointer-sign -Wno-missing-braces -Wno-strict-aliasing -Wno-implicit-fallthrough -Wno-missing-field-initializers\nLDFLAGS= -nostartfiles -static -Wl,--gc-sections -Wl,-static -Wl,--start-group -Wl,--whole-archive -Wl,--no-whole-archive -Wl,--end-group -Wl,-EL -T I:/Kendryte210/kendryte-standalone-sdk-0.5.0/lds/kendryte.ld\nCMAKE_BINARY_DIR=I:/Kendryte210/kendryte-standalone-sdk-0.5.0/build\nMakefile created.\n\n\n-- Configuring done\n-- Generating done\n-- Build files have been written to: I:/Kendryte210/kendryte-standalone-sdk-0.5.0/build\n可以看到build下已经有几个文件生成. \n编译工程\n依旧是在那个目录下，执行mingw32-make命令。\nPS I:\\Kendryte210\\kendryte-standalone-sdk-0.5.0\\build&gt; mingw32-make\nScanning dependencies of target kendryte\n[  2%] Building C object lib/CMakeFiles/kendryte.dir/bsp/entry.c.obj\n[  5%] Building C object lib/CMakeFiles/kendryte.dir/bsp/entry_user.c.obj\n[  8%] Building C object lib/CMakeFiles/kendryte.dir/bsp/interrupt.c.obj\n[ 11%] Building C object lib/CMakeFiles/kendryte.dir/bsp/printf.c.obj\n[ 14%] Building C object lib/CMakeFiles/kendryte.dir/bsp/sleep.c.obj\n[ 17%] Building C object lib/CMakeFiles/kendryte.dir/bsp/syscalls.c.obj\n[ 20%] Building C object lib/CMakeFiles/kendryte.dir/drivers/aes.c.obj\n[ 23%] Building C object lib/CMakeFiles/kendryte.dir/drivers/clint.c.obj\n[ 26%] Building C object lib/CMakeFiles/kendryte.dir/drivers/common.c.obj\n[ 29%] Building C object lib/CMakeFiles/kendryte.dir/drivers/dmac.c.obj\n[ 32%] Building C object lib/CMakeFiles/kendryte.dir/drivers/dvp.c.obj\n[ 35%] Building C object lib/CMakeFiles/kendryte.dir/drivers/fft.c.obj\n[ 38%] Building C object lib/CMakeFiles/kendryte.dir/drivers/fpioa.c.obj\n[ 41%] Building C object lib/CMakeFiles/kendryte.dir/drivers/gpio.c.obj\n[ 44%] Building C object lib/CMakeFiles/kendryte.dir/drivers/gpiohs.c.obj\n[ 47%] Building C object lib/CMakeFiles/kendryte.dir/drivers/i2c.c.obj\n[ 50%] Building C object lib/CMakeFiles/kendryte.dir/drivers/i2s.c.obj\n[ 52%] Building C object lib/CMakeFiles/kendryte.dir/drivers/kpu.c.obj\n[ 55%] Building C object lib/CMakeFiles/kendryte.dir/drivers/plic.c.obj\n[ 58%] Building C object lib/CMakeFiles/kendryte.dir/drivers/pwm.c.obj\n[ 61%] Building C object lib/CMakeFiles/kendryte.dir/drivers/rtc.c.obj\n[ 64%] Building C object lib/CMakeFiles/kendryte.dir/drivers/sha256.c.obj\n[ 67%] Building C object lib/CMakeFiles/kendryte.dir/drivers/spi.c.obj\n[ 70%] Building C object lib/CMakeFiles/kendryte.dir/drivers/sysclock.c.obj\n[ 73%] Building C object lib/CMakeFiles/kendryte.dir/drivers/sysctl.c.obj\n[ 76%] Building C object lib/CMakeFiles/kendryte.dir/drivers/timer.c.obj\n[ 79%] Building C object lib/CMakeFiles/kendryte.dir/drivers/uart.c.obj\n[ 82%] Building C object lib/CMakeFiles/kendryte.dir/drivers/uarths.c.obj\n[ 85%] Building C object lib/CMakeFiles/kendryte.dir/drivers/utils.c.obj\n[ 88%] Building C object lib/CMakeFiles/kendryte.dir/drivers/wdt.c.obj\n[ 91%] Building C object lib/CMakeFiles/kendryte.dir/bsp/crt.S.obj\n[ 94%] Linking C static library libkendryte.a\n[ 94%] Built target kendryte\nScanning dependencies of target hello_world\n[ 97%] Building C object CMakeFiles/hello_world.dir/src/hello_world/main.c.obj\n[100%] Linking C executable hello_world\nGenerating .bin file ...\n[100%] Built target hello_world\nPS I:\\Kendryte210\\kendryte-standalone-sdk-0.5.0\\build&gt;\n现在程序就被编译了出来.\n\n\n\n烧录程序\n此芯片暂时我只看到通过串口烧录程序的.我先解压下的K-Flash-V0.3.0到文件夹中. 运行如下(选择好串口通道和波特率以及二进制文件): \n提示 如果是黑色板子，那么没有问题，但是我这个是绿色板子初代版本，自动下载电路没有完善，所以需要手动拉低boot。 我们需要将IO16的跳线帽拔掉，并且在靠近芯片那一端接一根线出来连接到GND，然后按一下RESET！！，进入isp模式，才可以下载程序。 如图所示(最终我是在115200下载成功)： \n\n\n运行效果\n现在我们打开串口调试助手，重启板子，就可以看到串口输出啦～～"
  },
  {
    "objectID": "posts/k210core1.html",
    "href": "posts/k210core1.html",
    "title": "k210中使用core1",
    "section": "",
    "text": "记录一下在k210中使用core1遇到的错误.\n\n\n1.无法结束kpu_run_kmodel\n我想把kpu task放在第二个核去运行,但是一直无法结束这个任务,然后发现原来第二个核也要添加中断使能sysctl_enable_irq()…"
  },
  {
    "objectID": "posts/k210上手.html",
    "href": "posts/k210上手.html",
    "title": "k210环境搭建_Linux",
    "section": "",
    "text": "首先我使用的Deepin15.6系统,我拿到的板子是绿色的原版\n我们找到官网,进入其中的资源下载 \n我首先下载Kendryte KD233 Board Schematic V01原理图。\n再下载一个Kendryte K210 Standalone SDK裸机sdk。\n当然还需要一个工具链Kendryte OpenOCD for Ubuntu x86_64\n以及交叉编译器RISC-V 64bit toolchain for Kendryte K210_ubuntu_amd64\n\n\n安装交叉编译器\n之前我们下载了RISC-V 64bit toolchain for Kendryte K210_ubuntu_amd64交叉编译器。现在我们首先需要对他进行安装。\n➜  ~ cd ~/Downloads/         # 进入你下载到文件的目录\n➜  Downloads sudo tar -xvf kendryte-toolchain.tar.gz -C /opt/   # 使用-C参数将解压出来文件放入/opt目录中，大家自己随意\n➜  Downloads cd /opt/kendryte-toolchain/bin         # 解压完成进入目录\n➜  bin realpath .                                   # 获取当前目录路径，为了添加环境变量\n/opt/kendryte-toolchain/bin\n➜  bin sudo vi /etc/profile                         # 编辑profile文件，添加环境变量\nPATH=\"$PATH:/opt/kendryte-toolchain/bin\"            #  在文件末尾添加上编译器可执行文件目录\n➜  bin source /etc/profile                          # 同步环境变量以生效\n$ riscv64-unknown-elf-                               # 输入riscv64 之后按tab键 出现一大串说明是安装成功了\nriscv64-unknown-elf-addr2line   riscv64-unknown-elf-gcov-tool \nriscv64-unknown-elf-ar          riscv64-unknown-elf-gdb       \nriscv64-unknown-elf-as          riscv64-unknown-elf-gprof     \nriscv64-unknown-elf-c++         riscv64-unknown-elf-ld        \nriscv64-unknown-elf-c++filt     riscv64-unknown-elf-ld.bfd    \nriscv64-unknown-elf-cpp         riscv64-unknown-elf-nm        \nriscv64-unknown-elf-elfedit     riscv64-unknown-elf-objcopy   \nriscv64-unknown-elf-g++         riscv64-unknown-elf-objdump   \nriscv64-unknown-elf-gcc         riscv64-unknown-elf-ranlib    \nriscv64-unknown-elf-gcc-7.2.0   riscv64-unknown-elf-readelf   \nriscv64-unknown-elf-gcc-ar      riscv64-unknown-elf-run       \nriscv64-unknown-elf-gcc-nm      riscv64-unknown-elf-size      \nriscv64-unknown-elf-gcc-ranlib  riscv64-unknown-elf-strings   \nriscv64-unknown-elf-gcov        riscv64-unknown-elf-strip     \nriscv64-unknown-elf-gcov-dump       \n\n$ riscv64-unknown-elf-gcc -v                        # deepin下执行也是有效果的\nUsing built-in specs.\nCOLLECT_GCC=riscv64-unknown-elf-gcc\nCOLLECT_LTO_WRAPPER=/opt/kendryte-toolchain/bin/../libexec/gcc/riscv64-unknown-elf/7.2.0/lto-wrapper\nTarget: riscv64-unknown-elf\nConfigured with: /opt/toolchain/riscv-gnu-toolchain/riscv-gcc/configure --target=riscv64-unknown-elf --prefix=/usr/local --disable-shared --disable-threads --enable-languages=c,c++ --with-system-zlib --enable-tls --with-newlib --with-sysroot=/usr/local/riscv64-unknown-elf --with-native-system-header-dir=/include --disable-libmudflap --disable-libssp --disable-libquadmath --disable-libgomp --disable-nls --src=.././riscv-gcc --enable-checking=yes --disable-multilib --with-abi=lp64d --with-arch=rv64imafdc 'CFLAGS_FOR_TARGET=-Os  -mcmodel=medany'\nThread model: single\ngcc version 7.2.0 (GCC) \n\n\n安装openbcd调试\n➜  ~ mkdir Kendryte_K210    # 首先我在家目录下建立了一个文件用于存放本开发版相关资料\n➜  ~ cd Downloads           # 进入下载目录\n➜  Downloads tar -xvf kendryte-openocd-0.1.3-ubuntu64.tar.gz -C ~/Kendryte_K210/   # 解压openbcd到之前建立的文件夹\n➜  Downloads cd ~/Kendryte_K210/kendryte-openocd/ \n➜  kendryte-openocd sudo apt install libusb-dev -y          # 根据openbcd的readme文件，先安装libusb库\n➜  kendryte-openocd ./bin/openocd -v                        # 运行此命令 输出两行如下，说明可以使用\nKendryte Open On-Chip Debugger For RISC-V v0.1.3 (20180912)\nLicensed under GNU GPL v2\n\n\n开始编译\n\n首先我使用的vscode写代码，所以首先使用vscode打开工程目录\n➜  ~ code Kendryte_K210/kendryte-standalone-sdk-0.5.0  # 启动vscode\n\n\n\n开启图片\n\n\n配置c++插件\n可以看到我的图片里面有绿色波浪线，说明我的代码存在问题，这是由于没有配置好c++插件导致找不到头文件的问题。所以修改左上方的.vscode/c_cpp_properties为：\n{\n\"configurations\": [\n    {\n        \"name\": \"Linux\",\n        \"includePath\": [\n            \"${workspaceFolder}/**\",   \n            \"${workspaceFolder}/lib/bsp/include\",  \n            \"${workspaceFolder}/lib/drivers/include\",\n            \"/opt/kendryte-toolchain/riscv64-unknown-elf/include\"  //依据自己编译器安装位置修改\n        ],\n        \"defines\": [],\n        \"compilerPath\": \"/opt/kendryte-toolchain/bin/riscv64-unknown-elf-gcc\",\n        \"cStandard\": \"c11\",\n        \"cppStandard\": \"c++17\",\n        \"intelliSenseMode\": \"clang-x64\"\n    }\n],\n\"version\": 4\n}\n配置成功后如图，就没有错误信息了。 \n创建build目录\n打开终端运行：\n➜  kendryte-standalone-sdk-0.5.0 mkdir build\n➜  kendryte-standalone-sdk-0.5.0 cd build\n\n\n\n配置成功\n\n\n生成对应工程的Makefile 因为我们已经配置过了编译器的环境变量，所以现在我们使用如下命令生成Makefile\n➜  build cmake .. -DPROJ=hello_world\nPROJ = hello_world\n-- Check for RISCV toolchain ...\n-- Using /opt/kendryte-toolchain/bin RISCV toolchain\nSOURCE_FILES=/home/zqh/Kendryte_K210/kendryte-standalone-sdk-0.5.0/src/hello_world/main.c\n\nProject: hello_world  LIST_FILE=/home/zqh/Kendryte_K210/kendryte-standalone-sdk-0.5.0/cmake/executable.cmake  TOOLCHAIN=/opt/kendryte-toolchain/bin\nKENDRYTE_IDE=  BUILDING_SDK=yes\nCMAKE_BUILD_TYPE=Debug  CMAKE_C_COMPILER=/opt/kendryte-toolchain/bin/riscv64-unknown-elf-gcc\nCMAKE_CXX_COMPILER=/opt/kendryte-toolchain/bin/riscv64-unknown-elf-g++\nCMAKE_LINKER=/opt/kendryte-toolchain/bin/riscv64-unknown-elf-ld\nCMAKE_OBJCOPY=/opt/kendryte-toolchain/bin/riscv64-unknown-elf-objcopy\nCMAKE_OBJDUMP=/opt/kendryte-toolchain/bin/riscv64-unknown-elf-objdump\nCMAKE_MAKE_PROGRAM=/usr/bin/make\n\nCMAKE_C_FLAGS= -mcmodel=medany -fno-common -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -fno-zero-initialized-in-bss -Os -ggdb -std=gnu11 -Wno-pointer-to-int-cast -Wall -Werror=all -Wno-error=\nunused-function -Wno-error=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Werror=frame-larger-than=65536 -Wno-unused-parameter -Wno-sign-compare -Wno-error=missing-b\nraces -Wno-error=return-type -Wno-error=pointer-sign -Wno-missing-braces -Wno-strict-aliasing -Wno-implicit-fallthrough -Wno-missing-field-initializers -Wno-old-style-declaration\nCMAKE_CXX_FLAGS= -mcmodel=medany -fno-common -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -fno-zero-initialized-in-bss -Os -ggdb -std=gnu++17 -Wall -Werror=all -Wno-error=unused-function -Wno-\nerror=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Werror=frame-larger-than=65536 -Wno-unused-parameter -Wno-sign-compare -Wno-error=missing-braces -Wno-error=retu\nrn-type -Wno-error=pointer-sign -Wno-missing-braces -Wno-strict-aliasing -Wno-implicit-fallthrough -Wno-missing-field-initializers\nLDFLAGS= -nostartfiles -static -Wl,--gc-sections -Wl,-static -Wl,--start-group -Wl,--whole-archive -Wl,--no-whole-archive -Wl,--end-group -Wl,-EL -T /home/zqh/Kendryte_K210/kendryte-standalone-sdk-0.5.0/lds/kend\nryte.ld  \nCMAKE_BINARY_DIR=/home/zqh/Kendryte_K210/kendryte-standalone-sdk-0.5.0/build\nMakefile created.\n出现Makefile created.说明我们已经可以开始编译咯\n编译工程\n依旧是在那个目录下，执行make命令。\n➜  build make\n[  2%] Building C object lib/CMakeFiles/kendryte.dir/bsp/entry.c.obj\n[  5%] Building C object lib/CMakeFiles/kendryte.dir/bsp/entry_user.c.obj\n[  8%] Building C object lib/CMakeFiles/kendryte.dir/bsp/interrupt.c.obj\n[ 11%] Building C object lib/CMakeFiles/kendryte.dir/bsp/printf.c.obj\n[ 14%] Building C object lib/CMakeFiles/kendryte.dir/bsp/sleep.c.obj\n[ 17%] Building C object lib/CMakeFiles/kendryte.dir/bsp/syscalls.c.obj\n[ 20%] Building C object lib/CMakeFiles/kendryte.dir/drivers/aes.c.obj\n[ 23%] Building C object lib/CMakeFiles/kendryte.dir/drivers/clint.c.obj\n[ 26%] Building C object lib/CMakeFiles/kendryte.dir/drivers/common.c.obj\n[ 29%] Building C object lib/CMakeFiles/kendryte.dir/drivers/dmac.c.obj\n[ 32%] Building C object lib/CMakeFiles/kendryte.dir/drivers/dvp.c.obj\n[ 35%] Building C object lib/CMakeFiles/kendryte.dir/drivers/fft.c.obj\nIn file included from /home/zqh/Kendryte_K210/kendryte-standalone-sdk-0.5.0/lib/drivers/fft.c:15:0:\n/home/zqh/Kendryte_K210/kendryte-standalone-sdk-0.5.0/lib/drivers/include/fft.h:222:29: error: unknown type name 'dmac_channel_number_t'\nvoid fft_complex_uint16_dma(dmac_channel_number_t dma_send_channel_num,\n                            ^~~~~~~~~~~~~~~~~~~~~\n/home/zqh/Kendryte_K210/kendryte-standalone-sdk-0.5.0/lib/drivers/include/fft.h:223:29: error: unknown type name 'dmac_channel_number_t'\n                            dmac_channel_number_t dma_receive_channel_num,\n                            ^~~~~~~~~~~~~~~~~~~~~\n/home/zqh/Kendryte_K210/kendryte-standalone-sdk-0.5.0/lib/drivers/include/fft.h:225:52: error: unknown type name 'size_t'\n                            const uint64_t *input, size_t point_num,\n                                                    ^~~~~~\nmake[2]: *** [lib/CMakeFiles/kendryte.dir/build.make:206：lib/CMakeFiles/kendryte.dir/drivers/fft.c.obj] 错误 1\nmake[1]: *** [CMakeFiles/Makefile2:123：lib/CMakeFiles/kendryte.dir/all] 错误 2\nmake: *** [Makefile:84：all] 错误 2\n这里我使用的sdk是kendryte-standalone-sdk-0.5.0，可能这个工程还是有点问题的。\n问题解决\n这里看到错误提示为dmac_channel_number_t未定义，我进入fft.h查看了一下，发现这个文件可能少加了个头文件。所以在/kendryte-standalone-sdk-0.5.0/lib/drivers/include/fft.h中添加一行 #include \"dmac.h\"\n\n\n\n修改\n\n\n编译成功\n现在重新运行make命令即可编译成功。 \n\n\n\n烧录程序\n此芯片暂时我只看到通过串口烧录程序的，并且由于我是linux系统，linux下串口烧录程序的文件在官网暂时没有。所以我在qq群中找到Python下载脚本。脚本内容放在最后，大家复制即可。 将脚本保存为isp_auto.revA.02.py后执行如下：\n➜  build python3 isp_auto.revA.02.py -p /dev/ttyUSB0 -b 115200 hello_world.bin\n[INFO] COM Port Selected Manually:  /dev/ttyUSB0\n[INFO] Selected Baudrate:  115200\n[INFO] Trying to Enter the ISP Mode...\n\n[INFO] Greeting Message Detected, Start Downloading ISP\nDownloading ISP: |██████████████████████████████████████████████████| 100.0% Complete\n[INFO] Booting From 0x80000000\n[INFO] Wait For 1sec for ISP to Boot\n[INFO] Boot to Flashmode Successfully\n[INFO] Selected Flash:  On-Board\nDownloading Program: |██████████████████████████████████████████████████| 100.0% Complete\n[INFO] Rebooting...\n提示 如果是黑色板子，那么没有问题，但是我这个是绿色板子初代版本，自动下载电路没有完善，所以需要手动拉低boot。 我们需要将IO16的跳线帽拔掉，并且在靠近芯片那一端接一根线出来连接到GND，然后按一下RESET！！，进入isp模式，才可以下载程序。 如图所示： \n\n\n运行效果\n现在我们打开串口调试助手，重启板子，就可以看到串口输出啦～～ \n\n\n附件\nisp_auto.revA.02.py\n#!env python3\nimport sys\n\nBASH_TIPS = dict(NORMAL='\\033[0m',BOLD='\\033[1m',DIM='\\033[2m',UNDERLINE='\\033[4m',\n                    DEFAULT='\\033[39', RED='\\033[31m', YELLOW='\\033[33m', GREEN='\\033[32m',\n                    BG_DEFAULT='\\033[49m', BG_WHITE='\\033[107m')\n\nERROR_MSG   = BASH_TIPS['RED']+BASH_TIPS['BOLD']+'[ERROR]'+BASH_TIPS['NORMAL']\nWARN_MSG    = BASH_TIPS['YELLOW']+BASH_TIPS['BOLD']+'[WARN]'+BASH_TIPS['NORMAL']\nINFO_MSG    = BASH_TIPS['GREEN']+BASH_TIPS['BOLD']+'[INFO]'+BASH_TIPS['NORMAL']\n\nVID_LIST_FOR_AUTO_LOOKUP = \"(1A86)|(0403)|(067B)|(10C4)\"\n#                            WCH    FTDI    PL     CL\ntimeout = 0.5\n\nclass TimeoutError(Exception): pass\n\nimport time\nimport zlib\n\ntry:\n    import serial\n    import serial.tools.list_ports\nexcept ImportError:\n    print(ERROR_MSG,'PySerial must be installed, run '+BASH_TIPS['GREEN']+'`pip3 install pyserial`',BASH_TIPS['DEFAULT'])\n    sys.exit(1)\n\ntry:\n    if sys.platform != 'win32':\n        from Crypto.Cipher import AES\nexcept ImportError:\n    print(ERROR_MSG,'Crypto must be installed, run '+BASH_TIPS['GREEN']+'`pip3 install crypto`',BASH_TIPS['DEFAULT'])\n    sys.exit(1)\n\nimport struct\nfrom enum import Enum\nimport binascii\nimport hashlib\nimport argparse\n\nimport time, math\n\nISP_PROG = '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'\n\nISP_PROG = binascii.unhexlify(ISP_PROG)\n\n#print('ISP_FLASH progam size (compressed)', len(ISP_PROG))\nISP_PROG = zlib.decompress(ISP_PROG)\n#print('ISP_FLASH progam size (decompressed)', len(ISP_PROG))\n\ndef printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n    \"\"\"\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        length      - Optional  : character length of bar (Int)\n        fill        - Optional  : bar fill character (Str)\n    \"\"\"\n    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n    filledLength = int(length * iteration // total)\n    bar = fill * filledLength + '-' * (length - filledLength)\n    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n    # Print New Line on Complete\n    if iteration == total: \n        print()\n\ndef slip_reader(port):\n    partial_packet = None\n    in_escape = False\n\n    while True:\n        waiting = port.inWaiting()\n        read_bytes = port.read(1 if waiting == 0 else waiting)\n        if read_bytes == b'':\n            raise Exception(\"Timed out waiting for packet %s\" % (\"header\" if partial_packet is None else \"content\"))\n        for b in read_bytes:\n\n            if type(b) is int:\n                b = bytes([b])  # python 2/3 compat\n\n            if partial_packet is None:  # waiting for packet header\n                if b == b'\\xc0':\n                    partial_packet = b\"\"\n                else:\n                    raise Exception('Invalid head of packet (%r)' % b)\n            elif in_escape:  # part-way through escape sequence\n                in_escape = False\n                if b == b'\\xdc':\n                    partial_packet += b'\\xc0'\n                elif b == b'\\xdd':\n                    partial_packet += b'\\xdb'\n                else:\n                    raise Exception('Invalid SLIP escape (%r%r)' % (b'\\xdb', b))\n            elif b == b'\\xdb':  # start of escape sequence\n                in_escape = True\n            elif b == b'\\xc0':  # end of packet\n                yield partial_packet\n                partial_packet = None\n            else:  # normal byte in packet\n                partial_packet += b\n\n\nclass ISPResponse:\n    class ISPOperation(Enum):\n        ISP_ECHO = 0xC1\n        ISP_NOP = 0xC2\n        ISP_MEMORY_WRITE = 0xC3\n        ISP_MEMORY_READ = 0xC4\n        ISP_MEMORY_BOOT = 0xC5\n        ISP_DEBUG_INFO = 0xD1\n\n    class ErrorCode(Enum):\n        ISP_RET_DEFAULT = 0\n        ISP_RET_OK = 0xE0\n        ISP_RET_BAD_DATA_LEN = 0xE1\n        ISP_RET_BAD_DATA_CHECKSUM = 0xE2\n        ISP_RET_INVALID_COMMAND = 0xE3\n\n    @staticmethod\n    def parse(data):\n        op = data[0]\n        reason = data[1]\n        text = ''\n        try:\n            if ISPResponse.ISPOperation(op) == ISPResponse.ISPOperation.ISP_DEBUG_INFO:\n                text = data[2:].decode()\n        except ValueError:\n            print('Warning: recv unknown op', op)\n\n        return (op, reason, text)\n\n\nclass FlashModeResponse:\n    class Operation(Enum):\n        ISP_DEBUG_INFO = 0xD1\n        ISP_NOP = 0xD2\n        ISP_FLASH_ERASE = 0xD3\n        ISP_FLASH_WRITE = 0xD4\n        ISP_REBOOT = 0xD5\n        ISP_UARTHS_BAUDRATE_SET = 0xD6\n        FLASHMODE_FLASH_INIT = 0xD7\n\n    class ErrorCode(Enum):\n        ISP_RET_DEFAULT = 0\n        ISP_RET_OK = 0xE0\n        ISP_RET_BAD_DATA_LEN = 0xE1\n        ISP_RET_BAD_DATA_CHECKSUM = 0xE2\n        ISP_RET_INVALID_COMMAND = 0xE3\n\n    @staticmethod\n    def parse(data):\n        op = data[0]\n        reason = data[1]\n        text = ''\n        if FlashModeResponse.Operation(op) == FlashModeResponse.Operation.ISP_DEBUG_INFO:\n            text = data[2:].decode()\n\n        return (op, reason, text)\n\n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n\n\nclass MAIXLoader:\n    def change_baudrate(self, baudrate):\n        self._port = serial.Serial(\n            port=port,\n            baudrate=baudrate,\n            parity=serial.PARITY_NONE,\n            stopbits=serial.STOPBITS_ONE,\n            bytesize=serial.EIGHTBITS,\n            timeout=0.1\n        )\n\n    def __init__(self, port='/dev/ttyUSB1', baudrate=115200):\n        # configure the serial connections (the parameters differs on the device you are connecting to)\n        self._port = serial.Serial(\n            port=port,\n            baudrate=baudrate,\n            parity=serial.PARITY_NONE,\n            stopbits=serial.STOPBITS_ONE,\n            bytesize=serial.EIGHTBITS,\n            timeout=0.1\n        )\n        print(INFO_MSG,\"Selected Baudrate: \",baudrate, BASH_TIPS['DEFAULT'])\n\n        self._port.isOpen()\n        self._slip_reader = slip_reader(self._port)\n\n    \"\"\" Read a SLIP packet from the serial port \"\"\"\n\n    def read(self):\n        return next(self._slip_reader)\n\n    \"\"\" Write bytes to the serial port while performing SLIP escaping \"\"\"\n\n    def write(self, packet):\n        buf = b'\\xc0' \\\n              + (packet.replace(b'\\xdb', b'\\xdb\\xdd').replace(b'\\xc0', b'\\xdb\\xdc')) \\\n              + b'\\xc0'\n        #print('[WRITE]', binascii.hexlify(buf))\n        return self._port.write(buf)\n\n    def read_loop(self):\n        out = b''\n        # while self._port.inWaiting() &gt; 0:\n        #     out += self._port.read(1)\n\n        # print(out)\n        while 1:\n            sys.stdout.write('[RECV] raw data: ')\n            sys.stdout.write(binascii.hexlify(self._port.read(1)).decode())\n            sys.stdout.flush()\n\n    def recv_one_return(self):\n        timeout_init = time.time()\n        data = b''\n        # find start boarder\n        #sys.stdout.write('[RECV one return] raw data: ')\n        while 1:\n            if time.time() - timeout_init &gt; timeout:\n                raise TimeoutError\n            c = self._port.read(1)\n            #sys.stdout.write(binascii.hexlify(c).decode())\n            sys.stdout.flush()\n            if c == b'\\xc0':\n                break\n\n        in_escape = False\n        while 1:\n            if time.time() - timeout_init &gt; timeout:\n                raise TimeoutError\n            c = self._port.read(1)\n            #sys.stdout.write(binascii.hexlify(c).decode())\n            sys.stdout.flush()\n            if c == b'\\xc0':\n                break\n\n            elif in_escape:  # part-way through escape sequence\n                in_escape = False\n                if c == b'\\xdc':\n                    data += b'\\xc0'\n                elif c == b'\\xdd':\n                    data += b'\\xdb'\n                else:\n                    raise Exception('Invalid SLIP escape (%r%r)' % (b'\\xdb', b))\n            elif c == b'\\xdb':  # start of escape sequence\n                in_escape = True\n\n            data += c\n\n        #sys.stdout.write('\\n')\n        return data\n\n    def reset_to_isp_kd233(self):\n        self._port.dtr = False\n        self._port.rts = False\n        time.sleep(0.01)\n        #print('-- RESET to LOW, IO16 to HIGH --')\n        # Pull reset down and keep 10ms\n        self._port.dtr = True\n        self._port.rts = False\n        time.sleep(0.01)\n        #print('-- IO16 to LOW, RESET to HIGH --')\n        # Pull IO16 to low and release reset\n        self._port.rts = True\n        self._port.dtr = False\n        time.sleep(0.01)\n\n    def reset_to_isp_dan(self):\n        self._port.dtr = False\n        self._port.rts = False\n        time.sleep(0.01)\n        #print('-- RESET to LOW, IO16 to HIGH --')\n        # Pull reset down and keep 10ms\n        self._port.dtr = False\n        self._port.rts = True\n        time.sleep(0.01)\n        #print('-- IO16 to LOW, RESET to HIGH --')\n        # Pull IO16 to low and release reset\n        self._port.rts = False\n        self._port.dtr = True\n        time.sleep(0.01)\n\n    def reset_to_boot(self):\n        self._port.dtr = False\n        self._port.rts = False\n        time.sleep(0.01)\n        #print('-- RESET to LOW --')\n        # Pull reset down and keep 10ms\n        self._port.dtr = True\n        self._port.rts = False\n        time.sleep(0.01)\n        #print('-- RESET to HIGH, BOOT --')\n        # Pull IO16 to low and release reset\n        self._port.rts = False\n        self._port.dtr = False\n        time.sleep(0.01)\n\n    def greeting(self):\n        self._port.write(b'\\xc0\\xc2\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0')\n        op, reason, text = ISPResponse.parse(self.recv_one_return())\n\n        #print('MAIX return op:', ISPResponse.ISPOperation(op).name, 'reason:', ISPResponse.ErrorCode(reason).name)\n        \n\n    def flash_greeting(self):\n        while 1:\n            self._port.write(b'\\xc0\\xd2\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0')\n            op, reason, text = FlashModeResponse.parse(self.recv_one_return())\n            #print('MAIX return op:', FlashModeResponse.Operation(op).name, 'reason:',\n            #      FlashModeResponse.ErrorCode(reason).name)\n            if FlashModeResponse.Operation(op) == FlashModeResponse.Operation.ISP_NOP:\n                print(INFO_MSG,\"Boot to Flashmode Successfully\",BASH_TIPS['DEFAULT'])\n                break\n\n            print('wait 1 sec')\n            time.sleep(1)\n\n    def boot(self, address=0x80000000):\n        print(INFO_MSG,\"Booting From \" + hex(address),BASH_TIPS['DEFAULT'])\n        out = struct.pack('II', address, 0)\n\n        crc32_checksum = struct.pack('I', binascii.crc32(out) & 0xFFFFFFFF)\n\n        out = struct.pack('HH', 0xc5, 0x00) + crc32_checksum + out  # op: ISP_MEMORY_WRITE: 0xc3\n        self.write(out)\n\n    def recv_debug(self):\n        op, reason, text = ISPResponse.parse(self.recv_one_return())\n        #print('[RECV] op:', ISPResponse.ISPOperation(op).name, 'reason:', ISPResponse.ErrorCode(reason).name)\n        if text:\n            print('-' * 30)\n            print(text)\n            print('-' * 30)\n        if ISPResponse.ErrorCode(reason) not in (ISPResponse.ErrorCode.ISP_RET_DEFAULT, ISPResponse.ErrorCode.ISP_RET_OK):\n            print('Failed, retry, errcode=', hex(reason))\n            return False\n        return True\n\n    def flash_recv_debug(self):\n        op, reason, text = FlashModeResponse.parse(self.recv_one_return())\n        #print('[Flash-RECV] op:', FlashModeResponse.Operation(op).name, 'reason:',\n        #      FlashModeResponse.ErrorCode(reason).name)\n        if text:\n            print('-' * 30)\n            print(text)\n            print('-' * 30)\n\n        if FlashModeResponse.ErrorCode(reason) not in (FlashModeResponse.ErrorCode.ISP_RET_OK, FlashModeResponse.ErrorCode.ISP_RET_OK):\n            print('Failed, retry')\n            return False\n        return True\n\n    def init_flash(self, chip_type):\n        chip_type = int(chip_type)\n        print(INFO_MSG,\"Selected Flash: \",(\"In-Chip\", \"On-Board\")[chip_type],BASH_TIPS['DEFAULT'])\n        out = struct.pack('II', chip_type, 0)\n        crc32_checksum = struct.pack('I', binascii.crc32(out) & 0xFFFFFFFF)\n\n        out = struct.pack('HH', 0xd7, 0x00) + crc32_checksum + out\n\n        sent = self.write(out)\n        op, reason, text = FlashModeResponse.parse(self.recv_one_return())\n        #print('MAIX return op:', FlashModeResponse.Operation(op).name, 'reason:',\n        #      FlashModeResponse.ErrorCode(reason).name)\n\n    def flash_dataframe(self, data, address=0x80000000):\n        DATAFRAME_SIZE = 1024\n        data_chunks = chunks(data, DATAFRAME_SIZE)\n        #print('[DEBUG] flash dataframe | data length:', len(data))\n        total_chunk = math.ceil(len(data)/DATAFRAME_SIZE)\n\n        for n, chunk in enumerate(data_chunks):\n            while 1:\n                #print('[INFO] sending chunk', i, '@address', hex(address), 'chunklen', len(chunk))\n                out = struct.pack('II', address, len(chunk))\n\n                crc32_checksum = struct.pack('I', binascii.crc32(out + chunk) & 0xFFFFFFFF)\n\n                out = struct.pack('HH', 0xc3, 0x00) + crc32_checksum + out + chunk  # op: ISP_MEMORY_WRITE: 0xc3\n                sent = self.write(out)\n                #print('[INFO]', 'sent', sent, 'bytes', 'checksum', binascii.hexlify(crc32_checksum).decode())\n\n                address += len(chunk)\n                \n                if self.recv_debug():\n                    break\n            printProgressBar(n+1, total_chunk, prefix = 'Downloading ISP:', suffix = 'Complete', length = 50)\n\n    def dump_to_flash(self, data, address=0):\n        '''\n        typedef struct __attribute__((packed)) {\n            uint8_t op;\n            int32_t checksum; // 下面的所有字段都要参与checksum的计算\n            uint32_t address;\n            uint32_t data_len;\n            uint8_t data_buf[1024];\n        } isp_request_t;\n        '''\n\n        DATAFRAME_SIZE = 4096\n        data_chunks = chunks(data, DATAFRAME_SIZE)\n        #print('[DEBUG] flash dataframe | data length:', len(data))\n\n       \n\n        for n, chunk in enumerate(data_chunks):\n            #print('[INFO] sending chunk', i, '@address', hex(address))\n            out = struct.pack('II', address, len(chunk))\n\n            crc32_checksum = struct.pack('I', binascii.crc32(out + chunk) & 0xFFFFFFFF)\n\n            out = struct.pack('HH', 0xd4, 0x00) + crc32_checksum + out + chunk\n            #print(\"[$$$$]\", binascii.hexlify(out[:32]).decode())\n            sent = self.write(out)\n            #print('[INFO]', 'sent', sent, 'bytes', 'checksum', crc32_checksum)\n            self.flash_recv_debug()\n            address += len(chunk)\n            \n            \n\n    def flash_erase(self):\n        #print('[DEBUG] erasing spi flash.')\n        self._port.write(b'\\xc0\\xd3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0')\n        op, reason, text = FlashModeResponse.parse(self.recv_one_return())\n        #print('MAIX return op:', FlashModeResponse.Operation(op).name, 'reason:',\n        #      FlashModeResponse.ErrorCode(reason).name)\n\n    def install_flash_bootloader(self, data):\n        # 1. 刷入 flash bootloader\n        self.flash_dataframe(data, address=0x80000000)\n\n    def flash_firmware(self, firmware_bin: bytes, aes_key: bytes = None):\n        #print('[DEBUG] flash_firmware DEBUG: aeskey=', aes_key)\n\n        # 固件加上头\n        # 格式: SHA256(after)(32bytes) + AES_CIPHER_FLAG (1byte) + firmware_size(4bytes) + firmware_data\n\n        aes_cipher_flag = b'\\x01' if aes_key else b'\\x00'\n\n        # 加密\n        if aes_key:\n            firmware_bin = AES.new(key=aes_key, mode=AES.MODE_CBC, iv=b'\\x00' * 16).encrypt(firmware_bin)\n\n        firmware_len = len(firmware_bin)\n\n        total_chunk = math.ceil(firmware_len/4096)\n\n        data = aes_cipher_flag + struct.pack('I', firmware_len) + firmware_bin\n\n        sha256_hash = hashlib.sha256(data).digest()\n\n        firmware_with_header = data + sha256_hash\n\n        # 3. 分片刷入固件\n        data_chunks = chunks(firmware_with_header, 4096)  # 4kb for a sector\n\n        for n, chunk in enumerate(data_chunks):\n            chunk = chunk.ljust(4096, b'\\x00')  # align by 4kb\n\n            # 3.1 刷入一个dataframe\n            #print('[INFO]', 'Write firmware data piece')\n            self.dump_to_flash(chunk, address=n * 4096)\n            printProgressBar(n+1, total_chunk, prefix = 'Downloading Program:', suffix = 'Complete', length = 50)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-p\", \"--port\", help=\"COM Port\", default=\"DEFAULT\")\n    parser.add_argument(\"-c\", \"--chip\", help=\"SPI Flash type, 1 for in-chip, 0 for on-board\", default=1)\n    parser.add_argument(\"-b\", \"--baudrate\", type=int, help=\"UART baudrate for uploading firmware\", default=115200)\n    parser.add_argument(\"-l\", \"--bootloader\", help=\"bootloader bin path\", required=False, default=None)\n    parser.add_argument(\"-k\", \"--key\", help=\"AES key in hex, if you need encrypt your firmware.\", required=False, default=None)\n    parser.add_argument(\"-v\", \"--verbose\", help=\"increase output verbosity\", default=False,\n                        action=\"store_true\")\n    parser.add_argument(\"firmware\", help=\"firmware bin path\")\n\n    args = parser.parse_args()\n    if args.port == \"DEFAULT\":\n        try:\n            list_port_info = next(serial.tools.list_ports.grep(VID_LIST_FOR_AUTO_LOOKUP)) #Take the first one within the list\n            print(INFO_MSG,\"COM Port Auto Detected, Selected \",list_port_info.device,BASH_TIPS['DEFAULT'])\n            _port = list_port_info.device\n        except StopIteration:\n            print(ERROR_MSG,\"No vaild COM Port found in Auto Detect, Check Your Connection or Specify One by\"+BASH_TIPS['GREEN']+'`--port/-p`',BASH_TIPS['DEFAULT'])\n            sys.exit(1)\n    else:\n        _port = args.port\n        print(INFO_MSG,\"COM Port Selected Manually: \",_port,BASH_TIPS['DEFAULT'])\n\n    loader = MAIXLoader(port=_port, baudrate=args.baudrate)\n\n    # 1. Greeting.\n    print(INFO_MSG,\"Trying to Enter the ISP Mode...\",BASH_TIPS['DEFAULT'])\n    \n    retryCount = 0\n\n    while 1:\n        retryCount = retryCount + 1\n        if retryCount &gt; 15:\n            print(\"\\n\" + ERROR_MSG,\"No vaild Kendryte K210 found in Auto Detect, Check Your Connection or Specify One by\"+BASH_TIPS['GREEN']+'`-p '+('/dev/ttyUSB0', 'COM3')[sys.platform == 'win32']+'`',BASH_TIPS['DEFAULT'])\n            sys.exit(1)\n        try:\n            print('.', end='')\n            loader.reset_to_isp_dan()\n            loader.greeting()\n            break\n        except TimeoutError:\n            pass\n            \n        try:\n            print('_', end='')\n            loader.reset_to_isp_kd233()\n            loader.greeting()\n            break\n        except TimeoutError:\n            pass\n    timeout = 3  \n    print()\n    print(INFO_MSG,\"Greeting Message Detected, Start Downloading ISP\",BASH_TIPS['DEFAULT'])\n    # 2. flash bootloader and firmware\n    firmware_bin = open(args.firmware, 'rb')\n\n    # install bootloader at 0x80000000\n    if args.bootloader:\n        loader.install_flash_bootloader(open(args.bootloader, 'rb').read())\n    else:\n        loader.install_flash_bootloader(ISP_PROG)\n\n    loader.boot()\n\n    print(INFO_MSG,\"Wait For 1sec for ISP to Boot\",BASH_TIPS['DEFAULT'])\n\n    time.sleep(2)\n\n    loader.flash_greeting()\n\n    loader.init_flash(args.chip)\n\n    if args.key:\n        aes_key = binascii.a2b_hex(args.key)\n        if len(aes_key) != 16:\n            raise ValueError('AES key must by 16 bytes')\n\n        loader.flash_firmware(firmware_bin.read(), aes_key=aes_key)\n    else:\n        loader.flash_firmware(firmware_bin.read())\n\n    # 3. boot\n    loader.reset_to_boot()\n    print(INFO_MSG,\"Rebooting...\",BASH_TIPS['DEFAULT'])\n\n    try:\n        while 1:\n            out = b''\n            while loader._port.inWaiting() &gt; 0:\n                out += loader._port.read(1)\n            print(\"\".join(map(chr, out)), end='')\n    except KeyboardInterrupt:\n        sys.exit(0)"
  },
  {
    "objectID": "posts/keras-loss-reduction.html",
    "href": "posts/keras-loss-reduction.html",
    "title": "tf.keras损失函数聚合测试",
    "section": "",
    "text": "使用tf.keras自定义损失函数的时候,他的reduction的文档解释太少,所以写个代码测试一下预期的行为."
  },
  {
    "objectID": "posts/keras-loss-reduction.html#代码",
    "href": "posts/keras-loss-reduction.html#代码",
    "title": "tf.keras损失函数聚合测试",
    "section": "代码",
    "text": "代码\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.python.keras as k\nimport tensorflow.python.keras.layers as kl\nfrom tensorflow.python.keras.utils.losses_utils import ReductionV2\n\nx = np.reshape(np.arange(10000), (1000, 10))\ny = np.tile(np.array([[1, 2, 3]], np.float32), [1000, 1])\n\nmodel = k.Sequential([kl.Dense(3, input_shape=[10])])  # type:k.Model\n\nclass test_Loss(k.losses.Loss):\n    def __init__(self, reduction=ReductionV2.SUM_OVER_BATCH_SIZE, name=None):\n        super().__init__(reduction=reduction, name=name)\n\n    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):\n        return y_true + 0 * y_pred\n\nprint('AUTO : ')\nmodel.compile(k.optimizers.Adam(), [test_Loss(ReductionV2.AUTO)])\nmodel.fit(x, y, 100)\nprint('NONE : ')\nmodel.compile(k.optimizers.Adam(), [test_Loss(ReductionV2.NONE)])\nmodel.fit(x, y, 100)\nprint('SUM : ')\nmodel.compile(k.optimizers.Adam(), [test_Loss(ReductionV2.SUM)])\nmodel.fit(x, y, 100)\nprint('SUM_OVER_BATCH_SIZE : ')\nmodel.compile(k.optimizers.Adam(), [test_Loss(ReductionV2.SUM_OVER_BATCH_SIZE)])\nmodel.fit(x, y, 100)"
  },
  {
    "objectID": "posts/keras-loss-reduction.html#结果",
    "href": "posts/keras-loss-reduction.html#结果",
    "title": "tf.keras损失函数聚合测试",
    "section": "结果",
    "text": "结果\nAUTO : \n1000/1000 [==============================] - 1s 757us/sample - loss: 2.0000\nNONE : \n1000/1000 [==============================] - 0s 79us/sample - loss: 2.0000\nSUM : \n1000/1000 [==============================] - 0s 88us/sample - loss: 600.0000\nSUM_OVER_BATCH_SIZE : \n1000/1000 [==============================] - 0s 105us/sample - loss: 2.0000"
  },
  {
    "objectID": "posts/keras-loss-reduction.html#解析",
    "href": "posts/keras-loss-reduction.html#解析",
    "title": "tf.keras损失函数聚合测试",
    "section": "解析",
    "text": "解析\n主要就是SUM与SUM_OVER_BATCH_SIZE,这里的SUM_OVER_BATCH_SIZE其实是先求和再除以的batch size是total loss中的元素个数,并不是训练的batch size."
  },
  {
    "objectID": "posts/keras-sn.html",
    "href": "posts/keras-sn.html",
    "title": "tf.keras实现Spectral Normalization",
    "section": "",
    "text": "最近准备把自己写的训练框架全部升级到支持分布式以及混合精度训练，发现如果其中对于自定义层的改动还真不少。这里分享一个支持分布式以及混合精度训练的Spectral Normalization实现。\nNOTE: 这里遇到一个问题，发现混合精度训练之后GPU使用率只有20%不到，查找一番之后发现果然有issue，貌似对于DepthwiseConv2d的支持不好，可能要等到tf2.2之后才可以更新了。 \n\n代码\n分布式训练的情况下不能直接使用var.assgin来更新变量，所以参考了tf.keras的batchnorm写法进行内部的变量更新。但是tf2.1下的batchnorm中的滑动平均等变量目前还是无法支持多设备的同步，在tf2.2是实现了tf.keras.layers.experimental.SyncBatchNormalization，因为太麻烦我就没搞了。同时还有几个坑点，比如self.add_update需要放在call里面。对于谱归一化我没找到合适的参考，不知道是不是需要对training状态做出判别。。\nimport tensorflow as tf\nk = tf.keras\nK = tf.keras.backend\nkl = tf.keras.layers\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.keras.engine.input_spec import InputSpec\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.keras.utils import conv_utils, tf_utils\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.keras.layers.convolutional import Conv\n\n\nclass InstanceNormalization(kl.Layer):\n  \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n\n  def __init__(self, epsilon=1e-5):\n    super(InstanceNormalization, self).__init__()\n    self.epsilon = epsilon\n\n  def build(self, input_shape):\n    self.scale = self.add_weight(\n        name='scale',\n        shape=input_shape[-1:],\n        initializer=tf.random_normal_initializer(1., 0.02),\n        trainable=True)\n\n    self.offset = self.add_weight(\n        name='offset',\n        shape=input_shape[-1:],\n        initializer='zeros',\n        trainable=True)\n\n  def call(self, x):\n    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n    inv = tf.math.rsqrt(variance + self.epsilon)\n    normalized = (x-mean) * inv\n    return self.scale * normalized + self.offset\n\n  def get_config(self):\n    config = {\n        'epsilon': self.epsilon,\n    }\n    base_config = super().get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n\nclass ConvSpectralNormal(Conv):\n\n  def build(self, input_shape):\n    input_shape = tensor_shape.TensorShape(input_shape)\n    input_channel = self._get_input_channel(input_shape)\n    kernel_shape = self.kernel_size + (input_channel, self.filters)\n\n    self.kernel = self.add_weight(\n        name='kernel',\n        shape=kernel_shape,\n        initializer=self.kernel_initializer,\n        regularizer=self.kernel_regularizer,\n        constraint=self.kernel_constraint,\n        trainable=True,\n        dtype=self.dtype)\n\n    try:\n      # Disable variable partitioning when creating the variable\n      if hasattr(self, '_scope') and self._scope:\n        partitioner = self._scope.partitioner\n        self._scope.set_partitioner(None)\n      else:\n        partitioner = None\n\n      self.u = self.add_weight(\n          name='sn_u',\n          shape=(1, tf.reduce_prod(kernel_shape[:-1])),\n          dtype=self.dtype,\n          initializer=tf.keras.initializers.ones,\n          synchronization=tf.VariableSynchronization.ON_READ,\n          trainable=False,\n          aggregation=tf.VariableAggregation.MEAN)\n    finally:\n      if partitioner:\n        self._scope.set_partitioner(partitioner)\n\n    if self.use_bias:\n      self.bias = self.add_weight(\n          name='bias',\n          shape=(self.filters,),\n          initializer=self.bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n    channel_axis = self._get_channel_axis()\n    self.input_spec = InputSpec(\n        ndim=self.rank + 2, axes={channel_axis: input_channel})\n\n    self._build_conv_op_input_shape = input_shape\n    self._build_input_channel = input_channel\n    self._padding_op = self._get_padding_op()\n    self._conv_op_data_format = conv_utils.convert_data_format(\n        self.data_format, self.rank + 2)\n    self._convolution_op = nn_ops.Convolution(\n        input_shape,\n        filter_shape=self.kernel.shape,\n        dilation_rate=self.dilation_rate,\n        strides=self.strides,\n        padding=self._padding_op,\n        data_format=self._conv_op_data_format)\n    self.built = True\n\n  def call(self, inputs, training=None):\n    # Check if the input_shape in call() is different from that in build().\n    # If they are different, recreate the _convolution_op to avoid the stateful\n    # behavior.\n    if training is None:\n      training = K.learning_phase()\n\n    call_input_shape = inputs.get_shape()\n    recreate_conv_op = (\n        call_input_shape[1:] != self._build_conv_op_input_shape[1:])\n\n    if recreate_conv_op:\n      self._convolution_op = nn_ops.Convolution(\n          call_input_shape,\n          filter_shape=self.kernel.shape,\n          dilation_rate=self.dilation_rate,\n          strides=self.strides,\n          padding=self._padding_op,\n          data_format=self._conv_op_data_format)\n\n    # Apply causal padding to inputs for Conv1D.\n    if self.padding == 'causal' and self.__class__.__name__ == 'Conv1D':\n      inputs = array_ops.pad(inputs, self._compute_causal_padding())\n\n    # Update SpectralNormalization variable\n    u, v, w = self.calc_u(self.kernel)\n\n    def u_update():\n      # TODO u_update maybe need `training control`\n      return tf_utils.smart_cond(training, lambda: self._assign_new_value(\n          self.u, u), lambda: array_ops.identity(u))\n\n    # NOTE add update must in call function scope\n    self.add_update(u_update)\n\n    sigma = self.calc_sigma(u, v, w)\n    new_kernel = tf_utils.smart_cond(\n        training, lambda: self.kernel / sigma, lambda: self.kernel)\n\n    outputs = self._convolution_op(inputs, new_kernel)\n\n    if self.use_bias:\n      if self.data_format == 'channels_first':\n        if self.rank == 1:\n          # nn.bias_add does not accept a 1D input tensor.\n          bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n          outputs += bias\n        else:\n          outputs = nn.bias_add(outputs, self.bias, data_format='NCHW')\n      else:\n        outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')\n\n    if self.activation is not None:\n      return self.activation(outputs)\n    return outputs\n\n  def calc_u(self, w):\n    w = K.reshape(w, (-1, w.shape[-1]))\n    v = K.l2_normalize(K.dot(self.u, w))\n    u = K.l2_normalize(K.dot(v, K.transpose(w)))\n    return u, v, w\n\n  def calc_sigma(self, u, v, w):\n    return K.sum(K.dot(K.dot(u, w), K.transpose(v)))\n\n  def _assign_new_value(self, variable, value):\n    with K.name_scope('AssignNewValue') as scope:\n      with ops.colocate_with(variable):\n        return state_ops.assign(variable, value, name=scope)\n\n\nclass Conv2DSpectralNormal(ConvSpectralNormal):\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               strides=(1, 1),\n               padding='valid',\n               data_format=None,\n               dilation_rate=(1, 1),\n               activation=None,\n               use_bias=True,\n               kernel_initializer='glorot_uniform',\n               bias_initializer='zeros',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super().__init__(\n        rank=2,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=dilation_rate,\n        activation=tf.keras.activations.get(activation),\n        use_bias=use_bias,\n        kernel_initializer=tf.keras.initializers.get(kernel_initializer),\n        bias_initializer=tf.keras.initializers.get(bias_initializer),\n        kernel_regularizer=tf.keras.regularizers.get(kernel_regularizer),\n        bias_regularizer=tf.keras.regularizers.get(bias_regularizer),\n        activity_regularizer=tf.keras.regularizers.get(activity_regularizer),\n        kernel_constraint=tf.keras.constraints.get(kernel_constraint),\n        bias_constraint=tf.keras.constraints.get(bias_constraint),\n        **kwargs)"
  },
  {
    "objectID": "posts/l-softmax.html",
    "href": "posts/l-softmax.html",
    "title": "L softmx -> A softmx -> AM softmax",
    "section": "",
    "text": "本篇文章是对Large Margin Softmax loss,Angular Margin to Softmax Loss,Additive Margin Softmax Loss的学习记录。公式我尽量按照原文来写，并加入一点注释。"
  },
  {
    "objectID": "posts/l-softmax.html#modified-softmax",
    "href": "posts/l-softmax.html#modified-softmax",
    "title": "L softmx -> A softmx -> AM softmax",
    "section": "modified softmax",
    "text": "modified softmax\n这个就是作者让传统的softmax的权重\\(\\boldsymbol{W}\\)归一化:\\(\\parallel \\boldsymbol{W}_j\\parallel =1\\)(必须是没有bias的),得到了modified softmax loss: \\[ \\begin{align}\nLoss=\\frac{1}{N} \\sum_{i}-\\log \\left(\\frac{e^{\\left\\|\\boldsymbol{x}_{i}\\right\\| \\cos \\left(\\theta_{y_{i}, i}\\right)}}{\\sum_{j} e^{\\left\\|\\boldsymbol{x}_{i}\\right\\| \\cos \\left(\\theta_{j, i}\\right)}}\\right)\n\\end{align}\n\\]\n当然我不知道这个归一化有什么好处,从作者给出的结果上来看准确率提高了1%."
  },
  {
    "objectID": "posts/l-softmax.html#angular-margin-to-softmax-loss",
    "href": "posts/l-softmax.html#angular-margin-to-softmax-loss",
    "title": "L softmx -> A softmx -> AM softmax",
    "section": "Angular Margin to Softmax Loss",
    "text": "Angular Margin to Softmax Loss\n对于上面的modified softmax loss在二分类问题中,当\\(\\cos(\\theta_1)&gt;\\cos(\\theta_2)\\)可以确定类别为1,但是两个类别的决策面是\\(\\cos(\\theta_1)=\\cos(\\theta_2)\\),这样的决策面间隔太小,为了让决策面之间的间距更大一些,作者提出做两个决策面:\n类别1的决策面为\\(\\cos(m\\theta_1)=\\cos(\\theta_2)\\); 类别2的决策面为\\(\\cos(\\theta_1)=\\cos(m\\theta_2)\\); 其中\\(m\\geq2,m\\in N\\),\\(m\\)取整数可以利用倍角公式;\n这样的话,也就是说预测出来的\\(\\boldsymbol{x}\\)与他对应的类的夹角必须要小于他与其他类最小的夹角的\\(m\\)倍,比如2分类问题,其实就有三个决策面,中间两个决策面之间的就是决策间距.然后推导出A softmax loss: \\[ \\begin{aligned}\nLoss = \\frac{1}{N}\\sum_i-\\log(\\frac{\\exp(\\|x_i\\|\\cos(m\\theta_{yi,i}))}{\\exp(\\|x_i\\|\\cos(m\\theta_{yi,i}))+\\sum_{j\\neq y_i}\\exp(\\|x_i\\|\\cos(\\theta_{j,i}))})\n\\end{aligned} \\]\n其中\\(\\theta_{yi,i}\\in[0, \\frac{\\pi}{m}]\\),这就是因为\\(cos\\)的性质决定,我在上面也提到了,当\\(m\\theta_{yi,i}&gt;\\pi\\)时,会使得\\(m\\theta_{yi,i}&gt;\\theta_{j,i}\\ ,j\\neq y_i\\),但\\(cos(m\\theta_{1})&gt;cos(\\theta_2)\\)也会成立,这就与之前的假设相反.\n为了避免\\(cos\\)的问题,就设计 \\[\n\\begin{aligned}\n\\psi(\\theta_{y_i,i})=(-1)^k\\cos(m\\theta_{y_i,i})-2k\\ \\ \\ \\theta_{y_i,i}\\in[\\frac{k\\pi}{m},\\frac{(k+1)\\pi}{m}],且k\\in[0,m-1]    \n\\end{aligned}\\]\n来代替.这样使得\\(\\psi\\)随着\\(\\theta_{y_i,i}\\)单调递减,如果\\(m\\theta_{y_i,i}&gt;\\theta_{j,i},j\\neq y_i\\)那么必有\\(\\psi(\\theta_{y_i,i})&lt;\\cos(\\theta_{j,i})\\),这里我们看一下\\(\\psi(\\theta)\\)函数的图像(完全符合单独递减的要求,并且是连续函数可导):\n\n最终得出损失函数如下: \\[ \\begin{aligned}\nL_{ang} = \\frac{1}{N}\\sum_i-\\log(\\frac{\\exp(\\|x_i\\|\\psi(\\theta_{yi,i}))}{\\exp(\\|x_i\\|\\psi(\\theta_{yi,i}))+\\sum_{j\\neq y_i}\\exp(\\|x_i\\|\\cos(\\theta_{j,i}))})\n\\end{aligned} \\]\n这里对比一下三个loss的不同决策界:\n\n\n\n\n\n\n\n损失函数\n决策面\n\n\n\n\nSoftmax Loss\n\\(\\boldsymbol{W}_1-\\boldsymbol{W}_2+b1-b2=0\\)\n\n\nModified Softmax Loss\n\\(\\parallel\\boldsymbol{x}(cos(\\theta_1)-cos(\\theta_2))\\parallel\\)=0\n\n\nA Softmax Loss\n\\[\\begin{aligned}   \\parallel\\boldsymbol{x}\\parallel(cos(m\\theta_1)-cos(\\theta_2)=0\\ \\text{for class 1}\\\\ \\parallel\\boldsymbol{x}\\parallel(cos(\\theta_1)-cos(m\\theta_2)=0\\ \\text{for class 2} \\end{aligned} \\]\n\n\n\n注: 写到这里我发现这tm就是一个作者的两篇文章,到这里A softmax还是与L softmax的区别就在于是否对\\(\\parallel\\boldsymbol{W}\\parallel\\)进行归一化.这样的话对于分类来说可以观察到A softmax的分类结果都是长度会趋近相同,L softmax的分类长度会不相同.\n\n\n\nA softmax\nL softmax"
  },
  {
    "objectID": "posts/l-softmax.html#定义-1",
    "href": "posts/l-softmax.html#定义-1",
    "title": "L softmx -> A softmx -> AM softmax",
    "section": "定义",
    "text": "定义\n实际上看了上面的Loss函数,所有的变化点其实就在\\(e^{?}\\)做文章.这里首先替换了\\(\\psi(\\theta)\\)函数(\\(m\\)是偏移): \\[ \\begin{aligned}\n    \\psi(\\theta)=cos(\\theta)-m\n\\end{aligned} \\]\n然后又把\\(W,x\\)都归一化: \\[ \\begin{aligned}\n    Loss_i=-log(\\frac{e^{\\parallel \\boldsymbol{W}_{j_i}\\parallel \\parallel x_i\\parallel cos(\\theta_{y_i})}}{\\sum_j e^{\\parallel \\boldsymbol{W}_j\\parallel \\parallel x_i\\parallel cos(\\theta_j)}})\\ \\ \\ \\ \\parallel\\boldsymbol{W}\\parallel=1,\\parallel x\\parallel=1\n\\end{aligned} \\]\n这样內积结果就是: \\[ \\begin{aligned}\n    &lt;\\boldsymbol{W_{y_i}},x&gt;=cos(\\theta_{y_i})\n\\end{aligned} \\]\n接着再来一个偏移\\(m,\\ m&gt;0\\)与缩放因子\\(s\\),得到最后的损失函数: \\[ \\begin{aligned}\n    Loss_i = - \\log \\frac{e^{s\\cdot(\\cos\\theta_{y_i} -m)}}{e^{s\\cdot (\\cos\\theta_{y_i} -m)}+\\sum^c_{j=1,i\\neq t}  e^{s\\cdot\\cos\\theta_j }}\n\\end{aligned} \\]\n按照原论文,取\\(m=0.35\\),\\(s=30\\).然后就结束了… 😄\n再补张图(AM softmax没有乘\\(s\\)):"
  },
  {
    "objectID": "posts/l-softmax.html#a-softmax-loss",
    "href": "posts/l-softmax.html#a-softmax-loss",
    "title": "L softmx -> A softmx -> AM softmax",
    "section": "A softmax loss",
    "text": "A softmax loss\n首先是几个代码要注意的点,\\(cos(\\theta)\\)可以通过向量除计算,\\(cos(m\\theta)\\)可以通过倍角公式计算.: \\[\n\\begin{split}\n\\cos \\theta_{i,j} &= \\frac{\\vec{x_i}\\cdot\\vec{W_j}}{\\|\\vec{x_i}\\|\\cdot\\|\\vec{W_j}\\|} \\frac{\\vec{x_i}\\cdot\\vec{W_{norm_j}}}{\\|\\vec{x_i}\\|} \\cr\n\\cos 2\\theta &= 2\\cos^2 \\theta -1 \\cr\n\\cos 3\\theta &= 4\\cos^2 \\theta -3 \\cos \\theta \\cr\n\\cos 4\\theta &= 8\\cos^4 \\theta -8\\cos^2 \\theta + 1 \\cr\n\\end{split}\n\\]\n然后还有\\(k\\)的取值,先利用\\(sign\\)函数判断\\(cos(\\theta)\\)属于哪一个区间,再确定\\(k\\)的值: \\[ \\begin{aligned}\n    sign_0&=sign(cos(\\theta))\\\\\n    sign_3&=sign(cos(2\\theta))*sign_0\\\\\n    sign_4&=2*sign_0+sign_3-3 \\\\\n    \\psi(\\theta)&=sign_3*cos(4\\theta)+sign_4 \\\\\n    &=sign_3*(8\\cos^4 \\theta -8\\cos^2 \\theta + 1)+sign_4\n\\end{aligned} \\]\n下面是\\(m=4\\)时的\\(Loss\\)计算函数.\nimport tensorflow.python as tf\nfrom tensorflow import contrib\ndef Angular_Softmax_Loss(embeddings, labels, margin=4):\n        \"\"\"\n        Note:(about the value of margin)\n        as for binary-class case, the minimal value of margin is 2+sqrt(3)\n        as for multi-class  case, the minimal value of margin is 3\n\n        the value of margin proposed by the author of paper is 4.\n        here the margin value is 4.\n        \"\"\"\n        l = 0.\n        embeddings = tf.random_normal((2, 10))\n        labels = tf.convert_to_tensor([[1], [2]], dtype=tf.int64)\n        x_norm = tf.norm(embeddings, axis=1)\n\n        with tf.variable_scope(\"softmax\"):\n            weights = tf.get_variable(name='embedding_weights',\n                                      shape=[embeddings.get_shape().as_list()[-1], 10],\n                                      initializer=contrib.layers.xavier_initializer())\n            W = tf.nn.l2_normalize(weights, axis=0)\n            # cacualting the cos value of angles between embeddings and W\n            orgina_logits = tf.matmul(embeddings, W)\n            N = embeddings.get_shape()[0]  # get batch_size\n            single_sample_label_index = tf.concat([tf.constant(list(range(N)), tf.int64, shape=(N, 1)), labels], axis=1)\n            # N = 128, labels = [1,0,...,9]\n            # single_sample_label_index:\n            # [ [0,1],\n            #   [1,0],\n            #   ....\n            #   [128,9]]\n            # 这里就是F_y_i,根据有目标的位置来选取需要计算的loss位置.\n            f_y_i = tf.gather_nd(orgina_logits, single_sample_label_index)\n            # NOTE 因为 \\parallel W\\parallel =1 所以 cos(theta)=f_y_i/x_norm\n            cos_theta = tf.div(f_y_i, x_norm)\n            cos_theta_2 = tf.pow(cos_theta, 2)\n            cos_theta_4 = tf.pow(cos_theta, 4)\n\n            sign0 = tf.sign(cos_theta)\n            sign3 = tf.multiply(tf.sign(2 * cos_theta_2 - 1), sign0)\n            sign4 = 2 * sign0 + sign3 - 3\n            result = sign3 * (8 * cos_theta_4 - 8 * cos_theta_2 + 1) + sign4\n\n            margin_logits = tf.multiply(result, x_norm)\n            f = 1.0 / (1.0 + l)\n            ff = 1.0 - f\n            combined_logits = tf.add(orgina_logits,\n                                     tf.scatter_nd(single_sample_label_index,\n                                                   tf.subtract(margin_logits, f_y_i),\n                                                   orgina_logits.get_shape()))\n            updated_logits = ff * orgina_logits + f * combined_logits\n            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits=updated_logits, labels=tf.reshape(labels, (-1,))))\n            pred_prob = tf.nn.softmax(logits=updated_logits)\n            return pred_prob, loss"
  },
  {
    "objectID": "posts/l-softmax.html#additive-margin-softmax-1",
    "href": "posts/l-softmax.html#additive-margin-softmax-1",
    "title": "L softmx -> A softmx -> AM softmax",
    "section": "Additive Margin Softmax",
    "text": "Additive Margin Softmax\n这个比前面那个简单多了:\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import *\nimport tensorflow.python.keras.backend as K\nfrom tensorflow.python.keras.constraints import unit_norm\n\n(train_x, train_y), (test_x, test_y) = keras.datasets.fashion_mnist.load_data()\n\ntrain_x = K.reshape(train_x, (-1, 784))\ntrain_y = keras.utils.to_categorical(train_y, 10)\n\n\nmodel = keras.Sequential([Input(shape=(784,)),\n                          Dense(512, keras.activations.relu),\n                          Dense(256, keras.activations.relu),\n                          Dense(128, keras.activations.relu),\n                          Lambda(lambda x: K.l2_normalize(x, 1)),\n                          Dense(10, use_bias=False, kernel_constraint=unit_norm())])\n\n\ndef am_softmax_loss(y_true, y_pred, scale=30, margin=0.35):\n    # NOTE 预测出来的x就是归一化后的,并且W也是归一化后的,所以y_pred就是cos(𝜃)\n    y_pred = (y_true * (y_pred - margin) + (1 - y_true) * y_pred) * scale\n    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n\n\nmodel.compile(loss=am_softmax_loss, optimizer=keras.optimizers.Adam(),\n              metrics=[keras.metrics.CategoricalAccuracy()])\n\nmodel.fit(x=train_x, y=train_y, batch_size=128, epochs=5)\n对于人脸识别问题，还是需要稀疏版本的Additive Margin Softmax实现以节省显存，这里提供一个实现：\nclass Sparse_AmsoftmaxLoss(kls.Loss):\n\n  def __init__(self,\n               batch_size: int,\n               scale: int = 30,\n               margin: int = 0.35,\n               reduction='auto',\n               name=None):\n    \"\"\" sparse addivate margin softmax\n\n        Parameters\n        ----------\n\n        scale : int, optional\n\n            by default 30\n\n        margin : int, optional\n\n            by default 0.35\n\n        \"\"\"\n    super().__init__(reduction=reduction, name=name)\n    self.scale = scale\n    self.margin = margin\n    self.batch_idxs = tf.expand_dims(tf.range(0, batch_size, dtype=tf.int32),\n                                     1)  # shape [batch,1]\n\n  def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\" loss calc\n\n        Parameters\n        ----------\n        y_true : tf.Tensor\n\n            shape = [batch,1] type = tf.int32\n\n        y_pred : tf.Tensor\n\n            shape = [batch,class num] type = tf.float32\n\n        Returns\n        -------\n\n        tf.Tensor\n\n            loss\n        \"\"\"\n    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], 1)\n    y_true_pred = tf.gather_nd(y_pred, idxs)\n    y_true_pred = tf.expand_dims(y_true_pred, 1)\n    y_true_pred_margin = y_true_pred - self.margin\n    _Z = tf.concat([y_pred, y_true_pred_margin], 1)\n    _Z = _Z * self.scale\n    logZ = tf.math.reduce_logsumexp(_Z, 1, keepdims=True)\n    logZ = logZ + tf.math.log(1 - tf.math.exp(self.scale * y_true_pred - logZ))\n    return -y_true_pred_margin * self.scale + logZ\n稀疏版本的Additive Margin Softmax代码中最后两步的推导过程如下： \\[\n\\begin{aligned}\n    \\text{Let}\\ \\ p&=y_{pred}\\\\\n    log_Z&=\\log\\left[e^{s*p_0}+\\ldots+e^{s*p_c}+\\ldots+e^{s*p_{y_i}}+e^{s*(p_{y_i}-m)}\\right]\\\\\n    log_Z&=log_Z+\\log\\left[1-e^{s*p_{y_i}-log_Z}  \\right]\\\\\n    &=log_Z+\\log\\left[1-\\frac{e^{s*p_{y_i}}}{e^{log_Z}}  \\right]\\\\\n    &=log_Z+\\log\\left[1-\\frac{e^{s*p_{y_i}}}{e^{\\log\\left[e^{s*p_0}+\\ldots+e^{s*p_c}+\\ldots+e^{s*p_{y_i}}+e^{s*(p_{y_i}-m)}\\right]}}  \\right]\\\\\n    &=\\log\\left[e^{s*p_0}+\\ldots+e^{s*p_c}+e^{s*(p_{y_i}-m)}\\right]\\\\\n    \\mathcal{L}&=-\\log\\left[\\frac{e^{s*(p_{y_i}-m)}}{e^{s*p_0}+\\ldots+e^{s*p_c}+e^{s*(p_{y_i}-m)}} \\right]\\\\\n    &=-\\log e^{s*(p_{y_i}-m)}+log_Z\\\\\n    &=-s*(p_{y_i}-m)+log_Z\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/libmpfrerr.html",
    "href": "posts/libmpfrerr.html",
    "title": "libmpfr错误",
    "section": "",
    "text": "我重装了Ubuntu之后去编译k210的程序发现编译不了了。蛋疼。\n\n\n错误输出\n➜  build make\n[  3%] Building C object lib/CMakeFiles/kendryte.dir/bsp/entry.c.obj\n/opt/kendryte-toolchain-7.2.0/bin/../libexec/gcc/riscv64-unknown-elf/7.2.0/cc1: error while\nloading shared libraries: libmpfr.so.4: cannot open shared object file: No such file or dire\nctory\nlib/CMakeFiles/kendryte.dir/build.make:62: recipe for target 'lib/CMakeFiles/kendryte.dir/bs\np/entry.c.obj' failed\nmake[2]: *** [lib/CMakeFiles/kendryte.dir/bsp/entry.c.obj] Error 1\nCMakeFiles/Makefile2:122: recipe for target 'lib/CMakeFiles/kendryte.dir/all' failed\nmake[1]: *** [lib/CMakeFiles/kendryte.dir/all] Error 2\nMakefile:83: recipe for target 'all' failed\nmake: *** [all] Error 2\n发现这个他是说这个lib找不到了，我继续看输出\n➜  ~ ldd /opt/kendryte-toolchain-7.2.0/bin/../libexec/gcc/riscv64-unknown-elf/7.2.0/cc1\n    linux-vdso.so.1 (0x00007ffd235ea000)\n    libmpc.so.3 =&gt; /usr/lib/x86_64-linux-gnu/libmpc.so.3 (0x00007f6ea5ba5000)\n    libmpfr.so.4 =&gt; not found\n    libgmp.so.10 =&gt; /usr/lib/x86_64-linux-gnu/libgmp.so.10 (0x00007f6ea5924000)\n    libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6ea5720000)\n    libz.so.1 =&gt; /lib/x86_64-linux-gnu/libz.so.1 (0x00007f6ea5503000)\n    libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6ea5165000)\n    libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6ea4d74000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f6ea5dbd000)\n    libmpfr.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libmpfr.so.6 (0x00007f6ea4af4000)\n这个libmpfr.so.4的确找不到，我看看ldconfig\n➜  ~ ldconfig -p | grep mpfr\n    libmpfr.so.6 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libmpfr.so.6\n    libmpfr.so (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libmpfr.so\n我尝试安装这个libmpfr.so.4\n➜  ~ sudo apt-get install libmpfr4   \n[sudo] password for zqh: \nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nE: Unable to locate package libmpfr4\n没有这个包。md\n\n\n解决\n我决定直接链接两个包完事\nsudo ln -s /usr/lib/x86_64-linux-gnu/libmpfr.so.6 /usr/lib/x86_64-linux-gnu/libmpfr.so.4\n➜  x86_64-linux-gnu l | grep libmpfr \n-rw-r--r--   1 root root  1.1M 2月   8  2018 libmpfr.a\nlrwxrwxrwx   1 root root    16 2月   8  2018 libmpfr.so -&gt; libmpfr.so.6.0.1\nlrwxrwxrwx   1 root root    38 12月  1 15:50 libmpfr.so.4 -&gt; /usr/lib/x86_64-linux-gnu/libmpfr.so.6\nlrwxrwxrwx   1 root root    16 11月 28 10:19 libmpfr.so.6 -&gt; libmpfr.so.6.0.1\n-rw-r--r--   1 root root  512K 2月   8  2018 libmpfr.so.6.0.1\n然后编译成功.."
  },
  {
    "objectID": "posts/linuxWpa使用.html",
    "href": "posts/linuxWpa使用.html",
    "title": "linux wpa wifi自动连接",
    "section": "",
    "text": "我使用的板子是OrangePI zero Plus2，基于全志H5，安装好了armbian系统。 现在要使用wpa进行自动wifi连接，并且固定ip。"
  },
  {
    "objectID": "posts/linuxWpa使用.html#编辑连接信息",
    "href": "posts/linuxWpa使用.html#编辑连接信息",
    "title": "linux wpa wifi自动连接",
    "section": "编辑连接信息",
    "text": "编辑连接信息\n在 /etc/network/ 目录下创建 wifi 热点配置文件 wpa_supplication.conf,添加内容如下：\nctrl_interface=/var/run/wpa_supplicant\nctrl_interface_group=0\nap_scan=1\nnetwork={\n    ssid=\"wifi名字\"\n    scan_ssid=1\n    key_mgmt=WPA-EAP WPA-PSK IEEE8021X NONE\n    pairwise=TKIP CCMP\n    group=CCMP TKIP WEP104 WEP40\n    psk=\"wifi密码\"\n    priority=5\n}"
  },
  {
    "objectID": "posts/linuxWpa使用.html#添加静态地址",
    "href": "posts/linuxWpa使用.html#添加静态地址",
    "title": "linux wpa wifi自动连接",
    "section": "添加静态地址",
    "text": "添加静态地址\n编辑/etc/network/interfaces:\n# Network is managed by Network manager\nauto lo\niface lo inet loopback\n+ auto wlan0\n+ iface wlan0 inet static\n+ address 10.42.0.143\n+ netmask 255.0.0.0"
  },
  {
    "objectID": "posts/linuxWpa使用.html#连接wifi",
    "href": "posts/linuxWpa使用.html#连接wifi",
    "title": "linux wpa wifi自动连接",
    "section": "连接wifi",
    "text": "连接wifi\nroot@H5:~# wpa_supplicant -Dnl80211 -iwlan0 -c /etc/network/wpa_supplication.conf -B"
  },
  {
    "objectID": "posts/linuxblue5.html",
    "href": "posts/linuxblue5.html",
    "title": "Raspi蓝牙:播放与录音",
    "section": "",
    "text": "距离上一篇文章已经过去太久了,现在尝试使用树莓派的蓝牙录音.\n\n\n添加用户到蓝牙组\nsudo usermod -G bluetooth -a pi \nsudo reboot \n\n\n安装软件包\nsudo apt-get install pulseaudio pulseaudio-module-bluetooth\n\n\n搜索配对\n其中power on是必须的,否则是无法connect成功的.\nbluetoothctl\npower on\nagent on\ndefault-agent\nscan on\n开始搜索后,先quit然后启动pulseaudio:\nquit\nsudo killall bluealsa\npulseaudio --start\n然后回来链接设备,可能还要输入下scan on:\npair xx:xx:xx:xx:xx:xx\ntrust xx:xx:xx:xx:xx:xx\nconnect xx:xx:xx:xx:xx:xx\nscan off\n\n\n配置A2DP\npacmd list-cards\npacmd set-card-profile bluez_card.xx_xx_xx_xx_xx_xx a2dp_sink\npacmd set-default-sink bluez_sink.xx_xx_xx_xx_xx_xx.a2dp_sink\n然后使用paplay播放声音\n\n\n配置HSP\n这里要用官方的命令输入到蓝牙中才可以正确配置.\nsudo hcitool cmd 0x3F 0x01C 0x01 0x02 0x00 0x01 0x01\npacmd set-card-profile bluez_card.xx_xx_xx_xx_xx_xx headset_head_unit\npacmd set-default-sink bluez_sink.xx_xx_xx_xx_xx_xx.headset_head_unit\npacmd set-default-source bluez_source.xx_xx_xx_xx_xx_xx.headset_head_unit\n\n\n录音以及播放~\nparecord -v voice.wav\npaplay -v voice.wav\n然后就成功了.接下来准备做一下蓝牙录音编程相关的工作."
  },
  {
    "objectID": "posts/low-cost-metric.html",
    "href": "posts/low-cost-metric.html",
    "title": "tf.keras中优化metric计算(提取loss至metric)",
    "section": "",
    "text": "有的时候我们的loss函数是一个复合函数，但是在tf.keras中，loss函数只能返回一个标量，这个时候我们如果想要观察loss中子部分的值就只能写个metric去重新计算，但是这样是很浪费计算资源的，所以最好直接将loss中的值提取至metric。\n\n代码如下所示，只要我们自定义loss的时候给出一个变量去保存子损失的值，那么我们在metric可以直接对这个变量进行读值操作。\n这里还是有个小问题，就是必须要对变量的操作符进行调用才可以获得到正确的值，我已经在tensorflow中提交issue了，现在用了一个取巧的方法避开这个问题。\nimport tensorflow as tf\nimport tensorflow.keras as k\nimport tensorflow.keras.layers as kl\nimport tensorflow.keras.metrics as km\nfrom tensorflow.keras.datasets import fashion_mnist\n\ntfcfg = tf.ConfigProto()\ntfcfg.gpu_options.allow_growth = True\nsess = tf.Session(config=tfcfg)\nk.backend.set_session(sess)\n\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\nx_train = x_train.reshape((-1, 28, 28, 1))\nx_test = x_test.reshape((-1, 28, 28, 1))\ny_train = k.utils.to_categorical(y_train, 10)\ny_test = k.utils.to_categorical(y_test, 10)\n\n\nmodel = k.Sequential([\n    kl.Conv2D(32, 3, 1, input_shape=[28, 28, 1]),\n    kl.BatchNormalization(),\n    kl.LeakyReLU(),\n    kl.Conv2D(64, 3, 1),\n    kl.BatchNormalization(),\n    kl.LeakyReLU(),\n    kl.Conv2D(128, 3, 1),\n    kl.BatchNormalization(),\n    kl.LeakyReLU(),\n    kl.LeakyReLU(),\n    kl.Flatten(),\n    kl.Dense(512),\n    kl.BatchNormalization(),\n    kl.LeakyReLU(),\n    kl.Dense(10)\n])\n\n\nclass Metric_HIGH_COST(km.Metric):\n    def __init__(self, name=None, dtype=None, **kwargs):\n        super().__init__(name=name, dtype=dtype, **kwargs)\n        self.ce = self.add_weight('ce', initializer=tf.zeros_initializer)\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.ce.assign(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)))\n\n    def result(self):\n        return self.ce\n\n\nclass Metric_LOW_COST(km.Metric):\n    def __init__(self, cross_entropy: tf.Variable, name='CE', dtype=None):\n        \"\"\" yolo landmark error metric\n\n        Parameters\n        ----------\n        MeanMetricWrapper : [type]\n\n        landmark_error : ResourceVariable\n            a variable from yoloalign loss\n        name : str, optional\n            by default 'LE'\n        dtype : [type], optional\n            by default None\n        \"\"\"\n        super().__init__(name=name)\n        self.ce = cross_entropy\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.ce\n\n    def result(self):\n        return self.ce.read_value()\n\n\nclass Myloss(k.losses.Loss):\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.ce = tf.get_variable('ce', (), tf.float32, tf.zeros_initializer)  # type:tf.RefVariable\n\n    def call(self, y_true, y_pred):\n        ce_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n\n        # ! method 1 got zero output :\n        # self.ce.assign(ce_loss)\n        # return ce_loss\n\n        # ! method 2 get correct output :\n        return ce_loss + 0 * self.ce.assign(ce_loss)\n\n\nmyloss = Myloss()\nhigh_cost_metric = Metric_HIGH_COST('high_cost_ce')\nlow_cost_metric = Metric_LOW_COST(myloss.ce, 'low_cost_ce')\n\nsess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n\nmodel.compile(k.optimizers.Adam(), [myloss], [high_cost_metric, low_cost_metric])\n\nmodel.fit(x_train, y_train, 100, 10)"
  },
  {
    "objectID": "posts/mac-amx_en.html",
    "href": "posts/mac-amx_en.html",
    "title": "Explore AMX instructions: Unlock the performance of Apple Silicon",
    "section": "",
    "text": "Since 2020, Apple has published M1/M2/M3. They have at least four different ways to perform high-intensity computing tasks.\nIf we use ARM NEON instructions to accelerate the sgemm kernel on the single core of the M1 Max, It can achieve a performance of around 102 GFLOPS. But if use AMX instructions it can achieve 1475 GFLOPS!\nIn this article, I will introduce how you can leverage the AMX instructions to unlock the potential performance of Apple Silicon. And the all code I used in here (Verified on M2 Pro). This article refers to the work of Peter Cawley et al, which contains more instructions and usage methods."
  },
  {
    "objectID": "posts/mac-amx_en.html#computation-performance",
    "href": "posts/mac-amx_en.html#computation-performance",
    "title": "Explore AMX instructions: Unlock the performance of Apple Silicon",
    "section": "3.1 Computation Performance",
    "text": "3.1 Computation Performance\nFrom the previous section, we clearly know that the Z register group is divided into 16 groups in float32 datatype, and a computation results in a column in each group, so the ALU is actually divided into 4 groups. Here is to verify the peak performance of different ALU number enabled:\nperf_func = [&z_nums]() {\n  constexpr uint64_t a = matfp().dtype_mode(matfp_dtype_t::f32f32).z_row(0);\n  constexpr uint64_t b = matfp().dtype_mode(matfp_dtype_t::f32f32).z_row(1);\n  constexpr uint64_t c = matfp().dtype_mode(matfp_dtype_t::f32f32).z_row(2);\n  constexpr uint64_t d = matfp().dtype_mode(matfp_dtype_t::f32f32).z_row(3);\n  AMX_MATFP(a);\n  if (z_nums &gt; 1)\n    AMX_MATFP(b);\n  if (z_nums &gt; 2)\n    AMX_MATFP(c);\n  if (z_nums &gt; 3)\n    AMX_MATFP(d);\n};\nThe results as following:\n\n\n\nALU Nums\nGflop/s\n\n\n\n\n1\n405.530\n\n\n2\n826.912\n\n\n3\n1244.570\n\n\n4\n1666.952\n\n\n\nThis shows that although each group of ALUs is individually configured and emitted, but they can be executed in parallel."
  },
  {
    "objectID": "posts/mac-amx_en.html#load-performance",
    "href": "posts/mac-amx_en.html#load-performance",
    "title": "Explore AMX instructions: Unlock the performance of Apple Silicon",
    "section": "3.2 Load Performance",
    "text": "3.2 Load Performance\nHere is the load performance test case I designed, where reg nums represents the whether to load data into the X and Y register at the same time. near indicates whether to read consecutive addresses in memory. In M2 Pro, the l1 Dcache size is 65536, so K is designed larger than the l1 Dcache size. When near == 0, it needs to cross double cache size to load data. width indicates the number of registers used to read at once, The maximum number is 4 in M2.\nconstexpr size_t K = (65536 / 4 / (16 * 4)) * 4;\nfloat M[K * 2][16 * 4]{};\nfloat N[K * 2][16 * 4]{};\nperf_func = [&M, &N, &near, &reg_num, &x_width, &y_width]() {\n  auto ldx = ldxy().register_index(0);\n  auto ldy = ldxy().register_index(0);\n  if (x_width &gt;= 2)\n    ldx = ldx.multiple();\n  if (x_width &gt;= 4)\n    ldx = ldx.multiple_four();\n  if (reg_num &gt; 1) {\n    if (y_width &gt;= 2)\n      ldy = ldy.multiple();\n    if (y_width &gt;= 4)\n      ldy = ldy.multiple_four();\n  }\n\n  if (near) {\n    for (size_t i = 0; i &lt; K; i++) {\n      AMX_LDX(ldx.bind(M[i]));\n      if (reg_num &gt; 1) {\n        AMX_LDY(ldy.bind(N[i]));\n      }\n    }\n  } else {\n    for (size_t i = 0; i &lt; K / 2; i++) {\n      for (size_t j = 0; j &lt; 2; j++) {\n        AMX_LDX(ldx.bind(M[j * K + i]));\n        if (reg_num &gt; 1) {\n          AMX_LDY(ldy.bind(N[j * K + i]));\n        }\n      }\n    }\n  }\n};\nAfter running this test, we have collected load performance metrics table:\n\n\n\nReg Nums\nNear\nX Width\nY Width\nGB/s\n\n\n\n\n1\n1\n1\n0\n87.1489\n\n\n1\n1\n2\n0\n213.164\n\n\n1\n1\n4\n0\n456.332\n\n\n1\n0\n1\n0\n120.796\n\n\n1\n0\n2\n0\n260.115\n\n\n1\n0\n4\n0\n483.285\n\n\n2\n1\n1\n1\n134.33\n\n\n2\n1\n1\n2\n162.084\n\n\n2\n1\n1\n4\n297.15\n\n\n2\n1\n2\n1\n201.658\n\n\n2\n1\n2\n2\n214.772\n\n\n2\n1\n2\n4\n350.554\n\n\n2\n1\n4\n1\n384.614\n\n\n2\n1\n4\n2\n349.528\n\n\n2\n1\n4\n4\n476.722\n\n\n2\n0\n1\n1\n130.604\n\n\n2\n0\n1\n2\n163.91\n\n\n2\n0\n1\n4\n254.922\n\n\n2\n0\n2\n1\n195.612\n\n\n2\n0\n2\n2\n213.61\n\n\n2\n0\n2\n4\n298.603\n\n\n2\n0\n4\n1\n310.308\n\n\n2\n0\n4\n2\n302.767\n\n\n2\n0\n4\n4\n325.193\n\n\n\nWe can get some analysis from the table above. 1. increasing the width can double the bandwidth, so this is a free lunch. 2. consecutive reads is fast than non-consecutive reads, indicating we should optimize the data layout. 3. loading two registers and loading two groups in same register pool at the same time also not result in bandwidth reduction, indicating that the A/B matrix can be loaded at the same time."
  },
  {
    "objectID": "posts/mac-amx_en.html#store-performance",
    "href": "posts/mac-amx_en.html#store-performance",
    "title": "Explore AMX instructions: Unlock the performance of Apple Silicon",
    "section": "3.3 Store Performance",
    "text": "3.3 Store Performance\nSame like above, in store performance test also has reg_num,near,width options. But notice that the STZ instruction will store 16 groups from the Z register pool at the same time.\nconstexpr size_t K = (65536 / 4 / (16 * 4)) * 2;\nfloat CNear[16][16 * 4]{};\nfloat C[16][K]{};\nperf_func = [&C, &CNear, &near, &z_num, &width]() {\n  auto ldst = width == 2 ? ldstz().multiple() : ldstz();\n  for (size_t z = 0; z &lt; z_num; z++) {\n    for (size_t m = 0; m &lt; 16; m++) {\n      AMX_STZ(ldst.row_index(m * 4 + z * width)\n                  .bind(near ? CNear[m] + 16 * z * width\n                              : C[m] + 16 * z * width));\n    }\n  }\n};\nResult:\n\n\n\nReg Nums\nNear\nWidth\nGB/s\n\n\n\n\n1\n1\n1\n10.3769\n\n\n1\n1\n2\n8.93052\n\n\n1\n0\n1\n12.9423\n\n\n1\n0\n2\n12.3377\n\n\n2\n1\n1\n5.69731\n\n\n2\n1\n2\n12.3658\n\n\n2\n0\n1\n7.55092\n\n\n2\n0\n2\n13.0133\n\n\n3\n1\n1\n6.58085\n\n\n3\n0\n1\n11.4118\n\n\n4\n1\n1\n8.8847\n\n\n4\n0\n1\n9.85956\n\n\n\nIt can be found that using multiple registers does not increase bandwidth, indicating that it is basically a serial store, but twice the width can still effectively increase bandwidth. However, the overall speed is several times slower than computing and loading, indicating that we cannot load and store Z frequently."
  },
  {
    "objectID": "posts/mac-amx_en.html#strategy-1-load-1m-and-4n",
    "href": "posts/mac-amx_en.html#strategy-1-load-1m-and-4n",
    "title": "Explore AMX instructions: Unlock the performance of Apple Silicon",
    "section": "4.1 Strategy 1: Load 1M and 4N",
    "text": "4.1 Strategy 1: Load 1M and 4N\nIn my M2 Pro, AMX can load up to 4 * 64 bytes at a time, so it can load 1 group of M and 4 groups of N, and use the full 4 ALU for to get M * 4N, and then switch different K in the next loop. In total, it is loaded once and calculated 4 times, and can only be cached once more due to the size limit of the X register pool.\n\nLooking up the table, we can see that the loading bandwidth is 297.15 GB/s, and the compute time and load time are respectively: \\[\n\\begin{aligned}\nFLOPs &= 2 * M * N * ALU = 2 * 16 * 16 * 4 = 2048 \\\\\nCompute Time &= FLOPs / 1666  = 1.229~NanoSeconds \\\\\nBytes &= (1 * M + 4 * N) * 4 = 320 \\\\\nLoad Time &= Bytes / 297.15 = 1.076~NanoSeconds\n\\end{aligned}\n\\]\nThe difference between they times is only 0.15ns, which is not enough to load the data required for the next loop, causing the ALU to fail to working continuously."
  },
  {
    "objectID": "posts/mac-amx_en.html#strategy-2-load-4m-and-1n",
    "href": "posts/mac-amx_en.html#strategy-2-load-4m-and-1n",
    "title": "Explore AMX instructions: Unlock the performance of Apple Silicon",
    "section": "4.2 Strategy 2: Load 4M and 1N",
    "text": "4.2 Strategy 2: Load 4M and 1N\nSimilar to the previous strategy, but there is an additional problem here that the column direction of the A matrix is non-contiguous, and it needs to be transposed 4 times M by the CPU before loading, so it is not considered."
  },
  {
    "objectID": "posts/mac-amx_en.html#strategy-3-load-2m-and-2n",
    "href": "posts/mac-amx_en.html#strategy-3-load-2m-and-2n",
    "title": "Explore AMX instructions: Unlock the performance of Apple Silicon",
    "section": "4.3 Strategy 3: Load 2M and 2N",
    "text": "4.3 Strategy 3: Load 2M and 2N\nIn order to balance the storage usage of X/Y registers, we consider loading M/N of the same size, that is, loading 2 groups. We can calculate 4 times to get the 2M * 2N output.\nIn my M2 Pro, the maximum loaded data is 4 * 64 bytesat a time, so if load 2M and 2N of two different K, it means that can actually be calculated 8 times, maximizing the utilization of ALU. And we can prepare the next loop’s input data in a free space of the X/Y register pools.\n\nLook up the table and get a bandwidth of 476.722 GB/s, which is taken into the formula to calculate: \\[\n\\begin{aligned}\nFLOPs &= 2 * M * N * 4 * 2 = 4096 \\\\\nCompute Time &= FLOPs / 1666  = 2.458~NanoSeconds \\\\\nBytes &= (4 * M + 4 * N) * 4 \\\\\nLoad Time &= Bytes / 476.722 = 1.074~NanoSeconds\n\\end{aligned}\n\\]\nThe compute time minus the load time is 1.384 ns, which is greater than the next load time of 1.074 ns, indicating that we can perfectly stream the computation, thus exerting the maximum computing performance."
  },
  {
    "objectID": "posts/mac-amx_en.html#verify-strategy-3",
    "href": "posts/mac-amx_en.html#verify-strategy-3",
    "title": "Explore AMX instructions: Unlock the performance of Apple Silicon",
    "section": "4.4 Verify Strategy 3",
    "text": "4.4 Verify Strategy 3\nIn order to achieve peak data loading performance, we need to optimize the layout of the matrix, that is, pack the A/B matrix with a width of 32, so that the loading is satisfied with contiguous memory reading, and the result of 2 * M * N can be calculated at one time. The specific iteration flowchart is as follows:\n\nFor the simplicity, I assume that M/K/N are multiples of the smallest computing unit, and the input A/B matrix is required to be optimized for layout. Here is the code:\ntemplate &lt;bool LoadC, bool StoreC, size_t KTile, size_t M, size_t N&gt;\nvoid gepdot(size_t curM, size_t curK, size_t curN,\n            const float packedA[KTile][32], const float packedB[KTile][32],\n            float C[M][N]) {\n  static_assert(KTile % 4 == 0, \"not support k%4!=0\");\n\n  if constexpr (LoadC) {\n    // load acc value.\n    for (size_t om = 0; om &lt; 2; om++) {\n      for (size_t m = 0; m &lt; 16; m++) {\n        AMX_STZ(\n            ldstz().row_index(m * 4 + om * 2).multiple().bind(C[om * 16 + m]));\n      }\n    }\n  }\n\n  for (size_t k = curK; k &lt; curK + KTile; k += 4) {\n    for (size_t ok = k; ok &lt; k + 4; ok += 2) {\n      // load [m0,k0], [m1,k0], [m0,k1], [m1,k1]\n      AMX_LDY(ldxy().register_index(0).multiple().multiple_four().bind(\n          (void *)packedA[ok]));\n      // load [n0,k0], [n1,k0], [n0,k1], [n1,k1]\n      AMX_LDX(ldxy().register_index(0).multiple().multiple_four().bind(\n          (void *)packedB[ok]));\n      // compute 8 times.\n      // [[m0,n0],[m0,n1],\n      //  [m1,n0],[m1,n1]]\n      for (size_t ik = ok; ik &lt; ok + 2; ik++) {\n        auto fma = ik == 0 ? fma32().skip_z() : fma32();\n        for (size_t m = 0; m &lt; 2; m++) {\n          for (size_t n = 0; n &lt; 2; n++) {\n            AMX_FMA32(fma.z_row(m * 2 + n)\n                          .y_offset((ik - ok) * 2 * 16 * 4 + m * 16 * 4)\n                          .x_offset((ik - ok) * 2 * 16 * 4 + n * 16 * 4));\n          }\n        }\n      }\n    }\n  }\n\n  // last time need store C.\n  if constexpr (StoreC) {\n    for (size_t om = 0; om &lt; 2; om++) {\n      for (size_t m = 0; m &lt; 16; m++) {\n        AMX_STZ(\n            ldstz().row_index(m * 4 + om * 2).multiple().bind(C[om * 16 + m]));\n      }\n    }\n  }\n}\nTested under the conditions of M = 16 * 2, K = 8192, N = 16 * 2, we obtained 1632.13 Gflop/s performance, reaching 97.9% of the peak performance.\n[----------] 1 test from test_amx\n[ RUN      ] test_amx.test_gepdot\n             Gflop/s: 1632.13\n[       OK ] test_amx.test_gepdot (1032 ms)\n[----------] 1 test from test_amx (1032 ms total)"
  },
  {
    "objectID": "posts/make-use-shell.html",
    "href": "posts/make-use-shell.html",
    "title": "Makefile中使用shell赋值变量",
    "section": "",
    "text": "今天我为了makefile的方便起见,将一些变量通过脚本的形式给到makefile中,但是通过shell命令给makefile变量赋值让我头疼了一波.. 🙄\n\n\nshell中执行方式\n在shell中赋值非常简单\n➜ PRE_CKPT=log/20190405-174159        \n➜ NUM=`python3 tools/get_trian_num.py ${PRE_CKPT}`\n➜ echo ${NUM}\n4700\n\n\n在makefile中实现\n\n第一次写法\n\nfreeze: \n    NUM=`python3 tools/get_trian_num.py ${PRE_CKPT}`\n    echo ${NUM}\n执行,发现什么都没有输出\n➜ make freeze PRE_CKPT=log/20190405-174159\nNUM=`python3 tools/get_trian_num.py log/20190405-174159`\necho \n这里的问题是,我们通过shell命令赋值的是shell的变量,这个变量还不是makefile的变量.所以我们需要通过$$VAR的方式调用这个变量.\n\n第二次写法\n\nfreeze: \n    NUM=`python3 tools/get_trian_num.py ${PRE_CKPT}`\n    echo $$NUM\n执行,发现还是没有输出\n➜ make freeze PRE_CKPT=log/20190405-174159\nNUM=`python3 tools/get_trian_num.py log/20190405-174159`\necho $NUM\n这里是因为makefile中命令如果没有使用;\\来连接,是无法共享变量的.所以还得修改\n\n第三次写法\n\nfreeze: \n    @NUM=`python3 tools/get_trian_num.py ${PRE_CKPT}`; \\\n    echo $$NUM\n终于有了我想要的输出 😊\n➜  make freeze PRE_CKPT=log/20190405-174159\n4700"
  },
  {
    "objectID": "posts/maml.html",
    "href": "posts/maml.html",
    "title": "Model-Agnostic Meta-Learning",
    "section": "",
    "text": "Model-Agnostic Meta-Learning(MAML)是元学习中经典算法之一，今天准备来实现一下。"
  },
  {
    "objectID": "posts/maml.html#task-base-learning",
    "href": "posts/maml.html#task-base-learning",
    "title": "Model-Agnostic Meta-Learning",
    "section": "task base learning",
    "text": "task base learning\nMAML中以一个task作为训练的最小样本，每一个task使用nway与kshot描述它，比如如果使用omniglot数据集，此数据集中有许多个文件夹，每个文件夹表示一个字符，其中包含了20个不同的人对这一字符的书写。在数据生成的过程中，随机选取nway个文件夹作为类别，每个文件夹采样kshot个样本。\n如下图所示,nway=5,kshot=1时的采样结果，即一共5个类别，每个类别采样kshot×2个样本。这里采样kshot×2个样本是为了将一半的样本作为support set(一个batch中的训练集)用于maml方法自适应，另一半作为query set(一个batch中的测试集)用于常规的梯度下降。\n\n\n\nMAML Sample"
  },
  {
    "objectID": "posts/maml.html#compute-graph",
    "href": "posts/maml.html#compute-graph",
    "title": "Model-Agnostic Meta-Learning",
    "section": "compute graph",
    "text": "compute graph\nmaml需要找到一个在全局的task下具备较优泛化能力\\(\\theta\\)。设我们一个batch中得到了support set为\\(S = \\{x_s, y_s\\}\\)，query set为\\(Q = \\{x_q, y_q\\}\\)。那么开始执行计算图：\n首先maml算法将原始参数\\(\\theta\\)设置为当前网络的参数，并设置当前内循环迭代次数\\(i=0\\)，即\\(\\theta_0=\\theta\\)。然后开始自适应的过程，使用参数为\\(\\theta_{i}\\)的神经网络\\(f(\\theta_{i})\\)对support set进行推理，并对预测结果与\\(y_s\\)进行损失得到\\(L_{i}^s\\)，对此损失进行求导并利用maml独立的学习率\\(\\alpha\\)对参数\\(\\theta_{i}\\)进行更新，得到新的\\(\\theta_{i+1}\\)。当内循环迭代次数大于阈值\\(N\\)时，结束自适应过程后使用query set对原始参数进行一次梯度下降并更新原始参数。\nNOTE:原论文默认保留计算图，即后续的梯度下降是需要求二阶导数的。并且原论文中N默认为1，即只自适应一次。\n\n\n\nCompute Graph\n\n\n根据计算图，我们可以了解到，MAML实际上是要求最终的参数\\(\\theta\\)进行\\(N\\)次自适应后能对task较好拟合。"
  },
  {
    "objectID": "posts/maml.html#implement",
    "href": "posts/maml.html#implement",
    "title": "Model-Agnostic Meta-Learning",
    "section": "Implement",
    "text": "Implement\n我本来打算参考learn2learn的方法在megengine中进行实现，但是实现过程中还是碰到不少问题，比如megengine其实是底层默认自动构建图的，也就是和tensorflow 2有点类似，因此强制替换模型参数会破坏计算图。但是我又不想按原论文的实现那样，将所有参数手动指定，这样根本没办法随意修改模型，目前我想找一个能不破坏模型构建过程的方法将模型参数托管到全局字典中，前向推理的过程中再取对应参数计算，但是找不到。。希望能找到一些灵感。"
  },
  {
    "objectID": "posts/mel-augments.html",
    "href": "posts/mel-augments.html",
    "title": "Augmentation For Mel Spectrogram",
    "section": "",
    "text": "对音频数据训练我认为还是对数Mel谱图的方式比较好一下，需要一个音频版的RandAugment，借此机会把一下Mel谱图的增强方式汇总一下。\n\n\n数据增强函数\n代码如下：\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n\ndef power_to_db(magnitude, ref=1.0, amin=1e-10, top_db=80.0):\n  ref_value = tf.abs(ref)\n  log_spec = 10.0 * (tf.math.log(tf.maximum(amin, magnitude)) / tf.math.log(10.))\n  log_spec -= 10.0 * (tf.math.log(tf.maximum(amin, ref_value)) / tf.math.log(10.))\n  log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n  return log_spec\n\n\ndef freq_mask(mel: tf.Tensor, factor: float = 0.1, times: int = 1) -&gt; tf.Tensor:\n  \"\"\" mel spectogram freq mask (row mask)\n    \n    Args:\n            mel (tf.Tensor): [freq, time] float32\n            factor (tf.Tensor): mask factor (0. ~  1.)\n            times (int): int, default = 1\n    \n    Returns:\n            tf.Tensor: [freq, time] float32\n    \"\"\"\n  freq_max, time_max = mel.shape\n\n  def body(idx, mel):\n    max_w = tf.cast(factor * tf.cast(freq_max, tf.float32) / 2, tf.int32)\n    coord = tf.random.uniform([], 0, freq_max, tf.int32)\n    mask_w = tf.random.uniform([], 0, tf.maximum(max_w, 1), tf.int32)\n    cut = tf.stack([coord - mask_w, coord + mask_w])\n    cut = tf.clip_by_value(cut, 0, freq_max)\n    mel = tf.concat(\n        [mel[:cut[0]],\n         tf.zeros_like(mel[cut[0]:cut[1]]), mel[cut[1]:]], 0)\n    return idx + 1, mel\n\n  cond = lambda idx, mel: (idx &lt; times)\n  init_idx = tf.constant(0)\n  _, aug_mel = tf.while_loop(\n      cond,\n      body, [init_idx, mel],\n      shape_invariants=[init_idx.shape,\n                        tf.TensorShape((None, time_max))])\n  return aug_mel\n\n\ndef time_mask(mel: tf.Tensor, factor: float = 0.1, times: int = 1) -&gt; tf.Tensor:\n  \"\"\" mel spectogram time mask (cloum mask)\n    \n    Args:\n            mel (tf.Tensor): [freq, time] float32\n            factor (tf.Tensor): mask factor (0. ~  1.)\n            times (int): int, default = 1\n    \n    Returns:\n            tf.Tensor: [freq, time] float32\n    \"\"\"\n  freq_max, time_max = mel.shape\n\n  def body(idx, mel):\n    max_w = tf.cast(factor * tf.cast(time_max, tf.float32) / 2, tf.int32)\n    coord = tf.random.uniform([], 0, time_max, tf.int32)\n    mask_w = tf.random.uniform([], 0, tf.maximum(max_w, 1), tf.int32)\n    cut = tf.stack([coord - mask_w, coord + mask_w])\n    cut = tf.clip_by_value(cut, 0, time_max)\n    mel = tf.concat(\n        [mel[:, :cut[0]],\n         tf.zeros_like(mel[:, cut[0]:cut[1]]), mel[:, cut[1]:]], 1)\n    return idx + 1, mel\n\n  cond = lambda idx, mel: (idx &lt; times)\n  init_idx = tf.constant(0)\n  _, aug_mel = tf.while_loop(\n      cond,\n      body, [init_idx, mel],\n      shape_invariants=[init_idx.shape,\n                        tf.TensorShape((freq_max, None))])\n  return aug_mel\n\n\ndef freq_rescale(mel: tf.Tensor, factor: float = 0.1) -&gt; tf.Tensor:\n  \"\"\"rescale mel freq axis\n    \n    Args:\n            mel (tf.Tensor): [freq, time] float32\n            factor (float, optional): rescle factor. Defaults to 0.1.\n    \n    Returns:\n            tf.Tensor: [freq, time] float32\n    \"\"\"\n  freq_max, time_max = mel.shape\n  choosen_factor = tf.random.uniform([], 1 - factor, 1 + factor)\n\n  new_freq_size = tf.cast(\n      tf.cast(freq_max, tf.float32) * choosen_factor, tf.int32)\n\n  mel_aug = tf.squeeze(\n      tf.image.resize(tf.expand_dims(mel, -1), [new_freq_size, time_max]), -1)\n\n  def fn():\n    pad_offset = tf.random.uniform([], 0, freq_max - new_freq_size, tf.int32)\n    return tf.pad(mel_aug,\n                  [[pad_offset, freq_max - new_freq_size - pad_offset], [0, 0]])\n\n  mel_aug = tf.cond(\n      choosen_factor &lt; 1., lambda: fn(), lambda: mel_aug[0:freq_max,])\n  return mel_aug\n\n\ndef time_rescale(mel: tf.Tensor, factor: tf.Tensor = 0.1) -&gt; tf.Tensor:\n  \"\"\"rescale mel time axis\n    \n    Args:\n            mel (tf.Tensor): [freq, time] float32\n            factor (tf.Tensor, optional): rescle factor. Defaults to 0.1.\n    \n    Returns:\n            tf.Tensor: [freq, time] float32\n    \"\"\"\n  freq_max, time_max = mel.shape\n  choosen_factor = tf.random.uniform([], 1 - factor, 1 + factor)\n\n  new_time_size = tf.cast(\n      tf.cast(time_max, tf.float32) * choosen_factor, tf.int32)\n\n  mel_aug = tf.squeeze(\n      tf.image.resize(tf.expand_dims(mel, -1), [freq_max, new_time_size]), -1)\n\n  def fn():\n    pad_offset = tf.random.uniform([], 0, time_max - new_time_size, tf.int32)\n    return tf.pad(mel_aug,\n                  [[0, 0], [pad_offset, time_max - new_time_size - pad_offset]])\n\n  mel_aug = tf.cond(\n      choosen_factor &lt; 1., lambda: fn(), lambda: mel_aug[:, 0:time_max])\n  return mel_aug\n\n\ndef mel_dropout(mel: tf.Tensor, drop_prob: int = 0.05) -&gt; tf.Tensor:\n  \"\"\" mel drop out\n    \n    Args:\n            mel (tf.Tensor): [freq, time] float32, float32\n            drop_prob (int, optional): keep prob. Defaults to 0.05.\n    \n    Returns:\n            tf.Tensor: [freq, time] float32, float32\n    \"\"\"\n  return tf.nn.dropout(mel, rate=1 - drop_prob)\n\n\ndef time_warping(mel: tf.Tensor, factor: float = 0.1,\n                 npoints: int = 1) -&gt; tf.Tensor:\n  \"\"\" mel time warp use by `image_sparse_warp`\n        choice source point       from `[time//4, time - time//4]` \n        choice warped time width  from `[- factor/2 * time, factor/2 * time]`\n        \n    \n    Args:\n            mel (tf.Tensor): [freq, time] float32\n            factor (float, optional): NOTE factor should be [0., 1.]. Defaults to 0.1.\n            npoints (int, optional): disort point num NOTE don't set npoints &gt; 5, it will be terrible. Defaults to 1.\n            \n    Returns:\n            tf.Tensor: [freq, time] float32\n    \"\"\"\n\n  freq_max, time_max = mel.shape\n\n  freq_max = tf.cast(freq_max, tf.float32)\n  time_max = tf.cast(time_max, tf.float32)\n\n  # random choice some point, NOTE don't choose boundary\n  src_pt_y = tf.random.shuffle(tf.range(freq_max - 1) + 1)[:npoints]\n  tau_4 = tf.math.floordiv(time_max, 4)\n  src_pt_x = tf.random.shuffle(tf.range(tau_4, time_max - tau_4))[:npoints]\n  src_pt = tf.stack([src_pt_y, src_pt_x], -1)\n\n  disort_width = tf.random.uniform([npoints], -time_max * factor / 2,\n                                   time_max * factor / 2)\n  dest_pt_y = src_pt_y\n  dest_pt_x = src_pt_x + disort_width\n  dest_pt = tf.stack([dest_pt_y, dest_pt_x], -1)\n  # NOTE num_boundary_points=1 keep image boundary will not be disort\n  mel_aug, _ = tfa.image.sparse_image_warp(\n      mel[None, ..., None],\n      src_pt[None, ...],\n      dest_pt[None, ...],\n      num_boundary_points=1)\n  return mel_aug[0, ..., 0]\n\n\ndef freq_warping(mel: tf.Tensor, factor: float = 0.1,\n                 npoints: int = 1) -&gt; tf.Tensor:\n  \"\"\" mel freq warp use by `image_sparse_warp`\n        choice source point       from `[freq//4, freq - freq//4]` \n        choice warped time width  from `[- factor/2 * freq, factor/2 * freq]`\n        \n    \n    Args:\n            mel (tf.Tensor): [freq, time] float32\n            factor (float, optional): NOTE factor should be [0., 1.]. Defaults to 0.1.\n            npoints (int, optional): disort point num NOTE don't set npoints &gt; 5, it will be terrible. Defaults to 1.\n            \n    Returns:\n            tf.Tensor: [freq, time] float32\n    \"\"\"\n\n  freq_max, time_max = mel.shape\n  freq_max = tf.cast(freq_max, tf.float32)\n  # random choice some point, NOTE don't choose boundary\n  freq_4 = tf.math.floordiv(freq_max, 4)\n  src_pt_x = tf.random.shuffle(\n      tf.range(tf.cast(time_max, tf.float32), dtype=tf.float32))[:npoints]\n  src_pt_y = tf.random.shuffle(tf.range(freq_4, freq_max - freq_4))[:npoints]\n  src_pt = tf.stack([src_pt_y, src_pt_x], -1)\n\n  disort_width = tf.random.uniform([npoints], -freq_max * factor / 2,\n                                   freq_max * factor / 2)\n  dest_pt_y = src_pt_y + disort_width\n  dest_pt_x = src_pt_x\n  dest_pt = tf.stack([dest_pt_y, dest_pt_x], -1)\n  # NOTE num_boundary_points=1 keep image boundary will not be disort\n  mel_aug, _ = tfa.image.sparse_image_warp(\n      mel[None, ..., None],\n      src_pt[None, ...],\n      dest_pt[None, ...],\n      num_boundary_points=1)\n  return mel_aug[0, ..., 0]\n\n\ndef mel_loudness(mel: tf.Tensor, factor: float = 0.1) -&gt; tf.Tensor:\n  \"\"\" mel spectrogram loudness control\n    \n    \n    Args:\n            mel (tf.Tensor): [freq, time] float32\n            factor (float, optional): [0. ~ 1.]. Defaults to 0.1.\n    \n    Returns:\n            tf.Tensor: [freq, time] float32\n    \"\"\"\n  min_v = tf.reduce_min(mel)\n  return (mel-min_v) * tf.abs(1 - tf.random.uniform([], 0., factor)) + min_v\n\n\n预期效果\n\nfreq_mask\n\n频率维度mask\n\n\ntime_mask\n\n时间维度mask\n\n\nfreq_rescale\n\n频率维度拉伸(或缩放)\n\n\ntime_rescale\n\n时间维度拉伸(或缩放)\n\n\nfreq_warping\n\n频率维度扭曲，这里的扭曲函数用的是tensorflow addons中的，写的过程中踩了一些小坑，实现的时候我没有选择做大面积的平行变形(感觉这样对于Mel谱图太强烈)，我是选择数个点都进行随机变形(在5个点以下我认为基本是符合先验的)\n\n\ntime_warping\n\n时间维度扭曲\n\n\nmel_dropout\n\ndropout就不说了\n\n\nmel_loudness\n\n响度变化，减去最小值后乘上比例再加最小值。\n\n\n\nRandAugment\n代码如下,是适配tf.data的：\nimport tensorflow as tf\nimport transforms.audio.transform as ops\n\nNAME_TO_FUNC = {\n    'Identity': tf.identity,\n    'FreqMask': ops.freq_mask,\n    'TimeMask': ops.time_mask,\n    'FreqRescale': ops.freq_rescale,\n    'TimeRescale': ops.time_rescale,\n    'FreqWarping': ops.freq_warping,\n    'TimeWarping': ops.time_warping,\n    'Dropout': ops.mel_dropout,\n    'Loudness': ops.mel_loudness,\n}\n\n\ndef _ignore_level_to_arg(level):\n  del level\n  return ()\n\n\ndef _mask_level_to_arg(level):\n  # level = [0~1]\n  # Note factor loop in [0. ~ 0.2]\n  limit = tf.constant(0.2, tf.float32)\n  factor = tf.math.mod(level, limit)\n  factor = tf.cond(tf.equal(factor, 0.), lambda: limit, lambda: factor)\n  times = tf.cast(tf.math.floordiv(level, limit), tf.int32) + 1\n  return (\n      factor,\n      times,\n  )\n\n\ndef _rescale_level_to_arg(level):\n  # level = [0~1]\n  factor = level * 0.5\n  return (factor,)\n\n\ndef _warping_level_to_arg(level):\n  # level = [0~1]\n  # Note factor loop in [0. ~ 0.2]\n  factor = tf.math.mod(level, 0.2)\n  factor = tf.cond(tf.equal(factor, 0.), lambda: 0.2, lambda: factor)\n\n  npoints = tf.cast(tf.math.floordiv(level, 0.2), tf.int32) + 1\n  return (\n      factor,\n      npoints,\n  )\n\n\ndef _dropout_level_to_arg(level):\n  # level = [0~1]\n  drop_prob = level * 0.3\n  return (drop_prob,)\n\n\ndef _loudness_level_to_arg(level):\n  # level = [0~1]\n  factor = level * 0.4\n  return (factor,)\n\n\nLEVEL_TO_ARG = {\n    'Identity': _ignore_level_to_arg,\n    'FreqMask': _mask_level_to_arg,\n    'TimeMask': _mask_level_to_arg,\n    'FreqRescale': _rescale_level_to_arg,\n    'TimeRescale': _rescale_level_to_arg,\n    'FreqWarping': _warping_level_to_arg,\n    'TimeWarping': _warping_level_to_arg,\n    'Dropout': _dropout_level_to_arg,\n    'Loudness': _loudness_level_to_arg,\n}\n\nAUG_OPS = [\n    'Identity',\n    'FreqMask',\n    'TimeMask',\n    'FreqRescale',\n    'TimeRescale',\n    'FreqWarping',\n    'TimeWarping',\n    'Dropout',\n    'Loudness',\n]\n\n\nclass RandAugment(object):\n  \"\"\"Random augment with fixed magnitude.\"\"\"\n\n  def __init__(self,\n               num_layers: int = 2,\n               prob_to_apply: float = None,\n               num_levels: int = 10):\n    \"\"\"Initialized rand augment.\n    \n    Args:\n        num_layers (int, optional): how many times to do augmentation. Defaults to 2.\n        prob_to_apply (float, optional): probability to apply on each layer.\n        If None then always apply. Defaults to None.\n        num_levels (int, optional): number of levels for quantization of the magnitude. Defaults to 10.\n    \"\"\"\n    self.num_layers = num_layers\n    self.prob_to_apply = (\n        float(prob_to_apply) if prob_to_apply is not None else None)\n    self.num_levels = int(num_levels) if num_levels else None\n\n  def _get_level(self):\n    level = tf.random.uniform([], 1, self.num_levels + 1, tf.int32)\n    return (tf.cast(level, tf.float32) / self.num_levels)\n\n  def _apply_one_layer(self, data):\n    \"\"\"Applies one level of augmentation to the data.\"\"\"\n    level = self._get_level()\n    branch_fns = []\n    for augment_op_name in AUG_OPS:\n      augment_fn = NAME_TO_FUNC[augment_op_name]\n      level_to_args_fn = LEVEL_TO_ARG[augment_op_name]\n\n      def _branch_fn(data=data,\n                     augment_fn=augment_fn,\n                     level_to_args_fn=level_to_args_fn):\n        args = [data] + list(level_to_args_fn(level))\n        return augment_fn(*args)\n\n      branch_fns.append(_branch_fn)\n\n    branch_index = tf.random.uniform(\n        shape=[], maxval=len(branch_fns), dtype=tf.int32)\n    aug_data = tf.switch_case(branch_index, branch_fns, default=lambda: data)\n    if self.prob_to_apply is not None:\n      return tf.cond(\n          tf.random.uniform(shape=[], dtype=tf.float32) &lt;\n          self.prob_to_apply, lambda: aug_data, lambda: data)\n    else:\n      return aug_data\n\n  def __call__(self, data: tf.Tensor, aug_key='data') -&gt; tf.Tensor:\n    output_dict = {}\n    org_shape = data.shape\n\n    if aug_key is not None:\n      aug_data = data\n      for _ in range(self.num_layers):\n        aug_data = self._apply_one_layer(aug_data)\n        # NOTE must set shape for while_loop !\n        aug_data.set_shape(org_shape)\n      output_dict[aug_key] = aug_data\n\n    if aug_key != 'data':\n      output_dict['data'] = data\n\n    return output_dict\n\n测试效果如下,感觉好像有点夸张："
  },
  {
    "objectID": "posts/meta-pratice-1.html",
    "href": "posts/meta-pratice-1.html",
    "title": "模板元编程实战(第一章)",
    "section": "",
    "text": "模板元编程实战,第一章节."
  },
  {
    "objectID": "posts/meta-pratice-1.html#类型元函数",
    "href": "posts/meta-pratice-1.html#类型元函数",
    "title": "模板元编程实战(第一章)",
    "section": "类型元函数",
    "text": "类型元函数\n如果说上面的函数是操作\\(y=f(x)\\),那么他的输入是一个数值.但其实在c++中我们可以把类型看作是一种数值,对类型进行计算.\n\ntemplate &lt;typename T&gt;\nstruct Func_\n{\n  using type = T;\n};\n\ntemplate &lt;&gt;\nstruct Func_&lt;int&gt;\n{\n  using type = uint64_t;\n};\n\ntemplate &lt;&gt;\nstruct Func_&lt;uint32_t&gt;\n{\n  using type = uint64_t;\n};\n不过上面的元函数表述方法太过繁琐,我们可以用更加简化的方式来调用,由于using的时候默认会认为你在声明namespace,所以需要加上typename修饰来表明这是一个类型.\ntemplate &lt;typename T&gt;\nusing Fun = typename Func_&lt;T&gt;::type;\n\nTEST(chapter1, _1_1_2)\n{\n  // NOTE 我们构建的类型映射,把int 或者 uint32都转换到了uint64,然后利用比较他的类型是否和uint64相同.\n  Fun&lt;int&gt; a = 0x1;\n  ic(std::is_same&lt;decltype(a), uint64_t&gt;::value);\n}"
  },
  {
    "objectID": "posts/meta-pratice-1.html#模板类型参数与容器模板",
    "href": "posts/meta-pratice-1.html#模板类型参数与容器模板",
    "title": "模板元编程实战(第一章)",
    "section": "模板类型参数与容器模板",
    "text": "模板类型参数与容器模板\n模板元编程最重要的就是把类型也看作是一种数据,要知道我们编写的程序在编译时必然被编译器存储,那么代码的类型也是一种变量存储在编译器中的,因此我们合理地调用类型数据,可以发挥更大的作用.\n\n模板作为元函数的输入\nNOTE 我们可以传入一个模版类型,这个模板类型可以接收多个一个或多个模板类型的,此时对应的数学表达式类似于:\n\\[\n\\text{Func}(T_1,t_2)=T_1(t_2)\n\\]\n\ntemplate &lt;template &lt;typename&gt; class T1, typename T2&gt;\nstruct TypeCall_\n{\n  using type = typename T1&lt;T2&gt;::type;\n};\ntemplate &lt;template &lt;typename&gt; class T1, typename T2&gt;\nusing TypeCall = typename TypeCall_&lt;T1, T2&gt;::type;\n\nTEST(chapter1, _1_2_1)\n{\n  TypeCall&lt;std::remove_reference, int &&gt; h = 3;\n  ic(h);\n}\n\n\n模板作为元函数的输出\nNOTE 其实我个人觉得这只能算是多个元函数的compose,元函数中很\n\ntemplate &lt;int AddorRemoveRef&gt;\nstruct OptFunc_;\n\ntemplate &lt;&gt;\nstruct OptFunc_&lt;0&gt;\n{\n  template &lt;typename T&gt;\n  using type = std::add_lvalue_reference&lt;T&gt;;\n};\n\ntemplate &lt;&gt;\nstruct OptFunc_&lt;1&gt;\n{\n  template &lt;typename T&gt;\n  using type = std::remove_reference&lt;T&gt;;\n};\n\ntemplate &lt;typename T, int AddorRemoveRef&gt;\nusing OptFunc = typename OptFunc_&lt;AddorRemoveRef&gt;::template type&lt;T&gt;;\n\nTEST(chapter1, _1_2_2)\n{\n  OptFunc&lt;int, 1&gt;::type h = 1;\n  ic(h);\n}\n\n\n容器模板\n容器模板就是一种可以保存数值数据或者类型数据的一个容器.他就是一个类型,但是他可以保存以上两种数据.\ntemplate &lt;int... Vals&gt;\nstruct IntContainer\n{\n  // NOTE 即IntContainer这个类型中存储了一系列int值\n};\n\ntemplate &lt;typename... Types&gt;\nstruct TypeContainer\n{\n  // NOTE 存储了一系列类型\n};\n\n// 以下两个是比较复杂的情况, ①保存了一系列的模板类型\ntemplate &lt;template &lt;typename&gt; typename... Types&gt;\nstruct TemplateContainer\n{\n};\n\ntemplate &lt;template &lt;typename...&gt; typename... Types&gt;\nstruct TemplateContainer2\n{\n};"
  },
  {
    "objectID": "posts/meta-pratice-1.html#编译期实现分支循环",
    "href": "posts/meta-pratice-1.html#编译期实现分支循环",
    "title": "模板元编程实战(第一章)",
    "section": "编译期实现分支、循环",
    "text": "编译期实现分支、循环\n\n典型的顺序执行元函数\ntemplate &lt;typename T&gt;\nstruct RemoveCV_\n{\nprivate:\n  using inner_type = typename std::remove_reference&lt;T&gt;::type;\n\npublic:\n  using type = typename std::remove_reference&lt;inner_type&gt;::type;\n};\n\ntemplate &lt;typename T&gt;\nusing RemoveCV = typename RemoveCV_&lt;T&gt;::type;\n\nTEST(chapter1, _1_3_1)\n{\n  RemoveCV&lt;const int &&gt; h = 1;\n  ic(h);\n}\n\n\n分支执行的代码\nNOTE 其实分支执行的方式有好多,我自己都能写出好几个,但是找到一个比较通用优雅的写法可能还挺难\n\n通过conditional实现分支, 这种方法就是用在结构体模板继承时使用的(通过选择来继承类型,然后得到当前的值)\n\ntemplate &lt;int T&gt;\nstruct IsOdd_ : std::conditional_t&lt;(T % 2) == 1, std::true_type, std::false_type&gt;\n{\n};\n\ntemplate &lt;int T&gt;\nconstexpr bool IsOdd = IsOdd_&lt;T&gt;::value;\n\n通过特化匹配来实现分支,比如我们设计了一个isfloat结构体,默认都是false,通过类型分发的可以自定义不同类型是否是float.\n\ntemplate &lt;typename T&gt;\nstruct isFloat_ : std::false_type\n{\n};\n\ntemplate &lt;&gt;\nstruct isFloat_&lt;float&gt; : std::true_type\n{\n};\n\ntemplate &lt;&gt;\nstruct isFloat_&lt;uint64_t&gt; : std::true_type\n{\n};\n\ntemplate &lt;typename T&gt;\nconstexpr auto isFloat = isFloat_&lt;T&gt;::value;\n\nstd::enable_if来实现分支,这个是比较好用的,下面就是一个简单的应用. 首先利用enable_if来匹配当前参数的大致类型,然后对于array类型,写了一个traits去获得他的size,然后再for循环.这其实就是任意类型打印的雏形了.\n\ntemplate &lt;typename T&gt;\nstruct is_array : std::false_type\n{\n};\ntemplate &lt;typename T, size_t N&gt;\nstruct is_array&lt;std::array&lt;T, N&gt;&gt; : std::true_type\n{\n};\n\ntemplate &lt;typename T&gt;\nconstexpr bool is_array_v = is_array&lt;T&gt;::value;\n\ntemplate &lt;typename T&gt;\nstruct array_traits\n{\n};\ntemplate &lt;typename T, size_t N&gt;\nstruct array_traits&lt;std::array&lt;T, N&gt;&gt;\n{\n  constexpr static size_t size = N;\n};\n\ntemplate &lt;typename T&gt;\nstd::enable_if_t&lt;is_array_v&lt;T&gt;, void&gt; print_any(const T &v)\n{\n  std::cout &lt;&lt; \"arr :\";\n  for (size_t i = 0; i &lt; array_traits&lt;T&gt;::size; i++)\n  {\n    std::cout &lt;&lt; v[i] &lt;&lt; \" , \";\n  }\n  std::cout &lt;&lt; std::endl;\n}\n\ntemplate &lt;typename T&gt;\nstd::enable_if_t&lt;std::is_arithmetic_v&lt;T&gt;, void&gt; print_any(const T &v)\n{\n  std::cout &lt;&lt; \"value :\" &lt;&lt; v &lt;&lt; std::endl;\n}\n\nTEST(chapter1, _1_3_2)\n{\n  // 1\n  ic(IsOdd&lt;1&gt;, IsOdd&lt;2&gt;);\n  // 2\n  ic(isFloat&lt;float&gt;, isFloat&lt;uint64_t&gt;, isFloat&lt;double&gt;);\n  // 3\n  std::array&lt;float, 10&gt; arr = {1, 2, 3, 4, 5, 6, 7};\n  print_any(arr);\n  print_any(true);\n}\n\n\n循环执行的代码\n通常我们需要用递归的方式进行执行\ntemplate &lt;size_t Input&gt;\nconstexpr size_t Onescount = (Input % 2) + Onescount&lt;(Input / 2)&gt;;\n\ntemplate &lt;&gt;\nconstexpr size_t Onescount&lt;0&gt; = 0;\n\nTEST(chapter1, _1_3_3)\n{\n  constexpr size_t res = Onescount&lt;45&gt;;\n  ic(res);\n}"
  },
  {
    "objectID": "posts/meta-pratice-1.html#练习",
    "href": "posts/meta-pratice-1.html#练习",
    "title": "模板元编程实战(第一章)",
    "section": "练习",
    "text": "练习\n\n练习1\n构造一个输入为类型输出为值的元函数\ntemplate &lt;typename T&gt;\nstruct get_type_size : std::integral_constant&lt;size_t, sizeof(T)&gt;\n{\n};\n\nTEST(chapter1, practice_1)\n{\n  ic(get_type_size&lt;std::tuple&lt;int, float, double&gt;&gt;::value);\n  ic(get_type_size&lt;float&gt;::value);\n}\n\n\n练习2\n元函数的输入参数甚至可以是类型与数值混合的。尝试构造个元函数,其输入参数为一个类型以及一个整数。如果该类型所对应对象的大小等于该整数,那么返回true,否则返回 false。\ntemplate &lt;typename T, size_t N&gt;\nstruct get_type_size2 : std::bool_constant&lt;sizeof(T) == N&gt;\n{\n};\n\nTEST(chapter1, practice_2)\n{\n  ic(get_type_size2&lt;std::tuple&lt;int, float, double&gt;, 16&gt;::value);\n  ic(get_type_size2&lt;float, 16&gt;::value);\n  ic(get_type_size2&lt;uint64_t, 8&gt;::value);\n}\n\n\n练习3\n其他的元函数表现形式\n如果我们所有的操作都是在操作类型,我们可以用继承的方式把类型进行传递,这样就不需要中间变量. 当然在类型操作不足的时候,我们可以利用一些操作补足他们,比如下面这个例子就是先利用一个constexpr函数求值,此时这个值的类型是integral_constant类型,我们再decltype得到他的类型,再获取他的值.(这是个简单的例子,可能看不出这样有什么方便的)\ntemplate &lt;size_t A, size_t B&gt;\nconstexpr auto add()\n{\n  return std::integral_constant&lt;size_t, A + B&gt;();\n}\n\ntemplate &lt;size_t A, size_t B&gt;\nstruct add_ : decltype(add&lt;A, B&gt;())\n{\n};\n\nTEST(chapter1, question_3)\n{\n  ic(add_&lt;1, 2&gt;::value);\n}\n\n\n练习4\n构造一个元函数,返回另一个元函数\ntemplate &lt;typename T&gt;\nstruct reduce\n{\n  using type = std::integral_constant&lt;size_t, 0&gt;;\n};\n\ntemplate &lt;size_t A, size_t B&gt;\nstruct reduce&lt;std::index_sequence&lt;A, B&gt;&gt;\n{\n  using type = std::integral_constant&lt;size_t, A + B&gt;;\n};\n\ntemplate &lt;size_t A, size_t B, size_t... Ns&gt;\nstruct reduce&lt;std::index_sequence&lt;A, B, Ns...&gt;&gt;\n{\n\n  using type = typename reduce&lt;std::index_sequence&lt;A + B, Ns...&gt;&gt;::type;\n};\n\nTEST(chapter1, question_4)\n{\n  ic(reduce&lt;std::index_sequence&lt;1, 2, 3, 4&gt;&gt;::type::value);\n}\n\n\n练习5\n使用 SFINAE构造一个元函数:输入一个类型T,当T存在子类型type时该元函数返回true,否则返回 false\ntemplate &lt;typename T, typename = void&gt;\nstruct has_type : std::false_type\n{\n};\n\ntemplate &lt;typename T&gt;\nstruct has_type&lt;T, std::void_t&lt;typename T::type&gt;&gt; : std::true_type\n{\n};\n\nTEST(chapter1, question_5)\n{\n  ic(has_type&lt;reduce&lt;std::index_sequence&lt;1, 2, 3, 4&gt;&gt;&gt;::value);\n  ic(has_type&lt;int&gt;::value);\n}\n\n\n练习6\n使用在本章中学到的循环代码书写方式,编写一个元函数,输入一个类型数组,输出一个无符号整型数组,输出数组中的每个元素表示了输入数组中相应类型变量的大小。\ntemplate &lt;typename... TArgs&gt;\nstruct get_sizes\n{\n  constexpr static std::array&lt;size_t, sizeof...(TArgs)&gt; arr = {sizeof(TArgs)...};\n};\n\nTEST(chapter1, question_6)\n{\n  ic(get_sizes&lt;int, float, double, int8_t, uint32_t&gt;::arr);\n}\n\n\n练习7\n使用分支短路逻辑实现一个元函数,给定一个整数序列,判断其中是否存在值为1 的元素。如果存在,就返回true,否则返回 false\n\ntemplate &lt;size_t V&gt;\nconstexpr bool is_zero = (V == 0);\n\ntemplate &lt;bool cur, typename TNext&gt;\nconstexpr static bool AndValue = false;\n\ntemplate &lt;typename TNext&gt;\nconstexpr static bool AndValue&lt;true, TNext&gt; = TNext::value;\n\ntemplate &lt;typename T&gt;\nstruct has_one\n{\n  constexpr static bool value = false;\n};\n\ntemplate &lt;size_t V&gt;\nstruct has_one&lt;std::index_sequence&lt;V&gt;&gt;\n{\n  constexpr static bool value = is_zero&lt;V&gt;;\n};\n\ntemplate &lt;size_t V, size_t... Ns&gt;\nstruct has_one&lt;std::index_sequence&lt;V, Ns...&gt;&gt;\n{\n  constexpr static bool cur_is_zero = is_zero&lt;V&gt;;\n  constexpr static bool value = AndValue&lt;cur_is_zero, has_one&lt;std::index_sequence&lt;Ns...&gt;&gt;&gt;;\n};\n\nTEST(chapter1, question_7)\n{\n  ic(has_one&lt;std::index_sequence&lt;0, 0, 0, 0, 0, 0, 0, 0, 1&gt;&gt;::value);\n  ic(has_one&lt;std::index_sequence&lt;0, 0, 0, 0, 0, 0, 0, 0&gt;&gt;::value);\n  ic(has_one&lt;std::index_sequence&lt;0, 1, 0, 0, 0, 0, 0, 0&gt;&gt;::value);\n}"
  },
  {
    "objectID": "posts/mindspore-tf.html",
    "href": "posts/mindspore-tf.html",
    "title": "mindspore vs tensorflow",
    "section": "",
    "text": "尝试用了一下mindspore，这里给出一个dcgan的demo对比一下两个框架。 我使用mindspore 0.7，tensorflow 2.2，megengine 0.6，其他参数均相同。\n\n\nmindspore版\nfrom mindspore.nn.wrap.grad_reducer import DistributedGradReducer\nfrom mindspore.train.parallel_utils import ParallelMode\nfrom mindspore.parallel._utils import (_get_device_num, _get_mirror_mean, _get_parallel_mode)\n\nimport mindspore as ms\nimport mindspore.context as context\nimport mindspore.nn.wrap as mwp\nimport mindspore.nn.layer as ml\nimport mindspore.train.callback as callback\nimport mindspore.nn.loss as mls\nimport mindspore.nn.optim as moptim\nfrom mindspore.nn import Cell\nimport mindspore.ops.functional as F\nimport mindspore.ops.operations as P\nimport mindspore.ops.composite as C\nfrom mindspore.common import initializer as minit\nimport mindspore.dataset as ds\nimport mindspore.dataset.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport urllib.request\nfrom urllib.parse import urlparse\nimport gzip\nimport time\n\n\ndef unzipfile(gzip_path):\n  \"\"\"unzip dataset file\n  Args:\n      gzip_path: dataset file path\n  \"\"\"\n  open_file = open(gzip_path.replace('.gz', ''), 'wb')\n  gz_file = gzip.GzipFile(gzip_path)\n  open_file.write(gz_file.read())\n  gz_file.close()\n\n\ndef download_dataset():\n  \"\"\"Download the dataset from http://yann.lecun.com/exdb/mnist/.\"\"\"\n  train_path = \"./MNIST_Data/train/\"\n  test_path = \"./MNIST_Data/test/\"\n  train_path_check = os.path.exists(train_path)\n  test_path_check = os.path.exists(test_path)\n  if train_path_check == False and test_path_check == False:\n    os.makedirs(train_path)\n    os.makedirs(test_path)\n  else:\n    return\n  print(\"******Downloading the MNIST dataset******\")\n  train_url = {\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n               \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"}\n  test_url = {\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n              \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"}\n  for url in train_url:\n    url_parse = urlparse(url)\n    # split the file name from url\n    file_name = os.path.join(train_path, url_parse.path.split('/')[-1])\n    if not os.path.exists(file_name.replace('.gz', '')):\n      file = urllib.request.urlretrieve(url, file_name)\n      unzipfile(file_name)\n      os.remove(file_name)\n  for url in test_url:\n    url_parse = urlparse(url)\n    # split the file name from url\n    file_name = os.path.join(test_path, url_parse.path.split('/')[-1])\n    if not os.path.exists(file_name.replace('.gz', '')):\n      file = urllib.request.urlretrieve(url, file_name)\n      unzipfile(file_name)\n      os.remove(file_name)\n\n\ndef create_dataset(data_path, noise_dim, batch_size=32, repeat_size=1,\n                   num_parallel_workers=1):\n  \"\"\" create dataset for train or test\n  Args:\n      data_path: Data path\n      batch_size: The number of data records in each group\n      repeat_size: The number of replicated data records\n      num_parallel_workers: The number of parallel workers\n  \"\"\"\n  # define dataset\n  mnist_ds = ds.MnistDataset(data_path)\n\n  hwc2chw_op = transforms.vision.c_transforms.HWC2CHW()\n  # apply map operations on images\n  mnist_ds = (mnist_ds.map(operations=lambda x: ((x - 127.5) / 127.5).astype('float32'), input_columns=\"image\",\n                           num_parallel_workers=num_parallel_workers)\n              .map(operations=hwc2chw_op, input_columns=\"image\",\n                   num_parallel_workers=num_parallel_workers)\n              .map(operations=lambda x: (x, np.random.randn(noise_dim).astype('float32')),\n                   input_columns=\"image\",\n                   output_columns=[\"image\", \"noise\"],\n                   columns_order=[\"image\", \"noise\"],\n                   num_parallel_workers=num_parallel_workers))\n  # apply DatasetOps\n  buffer_size = 60000\n  mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)  # 10000 as in LeNet train script\n  mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n  mnist_ds: ds.MindDataset = mnist_ds.repeat(repeat_size)\n  # print(mnist_ds.output_())\n  # print(mnist_ds.output_types())\n  return mnist_ds\n\n\nclass Reshape(Cell):\n  def __init__(self, shape: list) -&gt; None:\n    super().__init__()\n    self.shape = shape\n\n  def construct(self, x):\n    return F.reshape(x, self.shape)\n\n\ndef make_generator_model(noise_dim):\n  model = ml.SequentialCell(\n      ml.Dense(noise_dim, 7 * 7 * 256, has_bias=False),\n      Reshape((-1, 256, 7, 7)),\n      ml.BatchNorm2d(256),\n      ml.LeakyReLU(),\n      # assert model.output_shape == (None, 7, 7, 256)  # 注意：batch size 没有限制\n      ml.Conv2dTranspose(256, 128, (5, 5), stride=(1, 1), pad_mode='same', has_bias=False),\n      # assert model.output_shape == (None, 7, 7, 128)\n      ml.BatchNorm2d(128),\n      ml.LeakyReLU(),\n      ml.Conv2dTranspose(128, 64, (5, 5), stride=(2, 2), pad_mode='same', has_bias=False),\n      # assert model.output_shape == (None, 14, 14, 64)\n      ml.BatchNorm2d(64),\n      ml.LeakyReLU(),\n      ml.Conv2dTranspose(64, 1, (5, 5), stride=(2, 2),\n                         pad_mode='same', has_bias=False),\n      ml.Tanh()\n      # assert model.output_shape == (None, 28, 28, 1)\n  )\n  return model\n\n\ndef make_discriminator_model():\n  model = ml.SequentialCell(\n      ml.Conv2d(1, 64, (5, 5), stride=(2, 2), pad_mode='same'),\n      ml.LeakyReLU(),\n      ml.Dropout(0.3),\n      ml.Conv2d(64, 128, (5, 5), stride=(2, 2), pad_mode='same'),\n      ml.LeakyReLU(),\n      ml.Dropout(0.3),\n      ml.Flatten(),\n      ml.Dense(128 * 7 * 7, 1),\n      ml.Sigmoid()\n  )\n\n  return model\n\n\nclass GANBaseNet(Cell):\n  def __init__(self, noise_dim) -&gt; None:\n    super().__init__(auto_prefix=True)\n    self.generator = make_generator_model(noise_dim)\n    self.discriminator = make_discriminator_model()\n\n  def construct(self, images, noise):\n    generated_images = self.generator(noise)\n    real_output = self.discriminator(images)\n    fake_output = self.discriminator(generated_images)\n    return real_output, fake_output\n\n\nclass GANWithLoss(Cell):\n  def __init__(self, base_net: GANBaseNet) -&gt; None:\n    super().__init__(auto_prefix=True)\n    self.base_net = base_net\n    self.cross_entropy = P.BinaryCrossEntropy()  # 是否需要sigmoid是个问题\n\n  def discriminator_loss(self, real_output, fake_output, weight):\n    real_loss = self.cross_entropy(real_output, F.ones_like(real_output), weight)\n    fake_loss = self.cross_entropy(fake_output, F.zeros_like(fake_output), weight)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\n  def generator_loss(self, fake_output, weight):\n    return self.cross_entropy(fake_output, F.ones_like(fake_output), weight)\n\n  def construct(self, images, noise):\n    real_output, fake_output = self.base_net(images, noise)\n    weight = F.ones_like(real_output)\n    gen_loss = self.generator_loss(fake_output, weight)\n    disc_loss = self.discriminator_loss(real_output, fake_output, weight)\n    return gen_loss, disc_loss\n\n\nclass IthOutputCell(Cell):\n  \"\"\" 显式指定反向传播图 \"\"\"\n\n  def __init__(self, network, output_index):\n    super(IthOutputCell, self).__init__()\n    self.network = network\n    self.output_index = output_index\n\n  def construct(self, image, noise):\n    predict = self.network(image, noise)[self.output_index]\n    return predict\n\n\nclass TrainStepWrap(Cell):\n  def __init__(self, network: GANWithLoss, g_optimizer: moptim.Optimizer, d_optimizer: moptim.Optimizer, sens=1.0):\n    # NOTE 这里必须要设置auto_prefix，否则两个优化器的参数将冲突\n    super(TrainStepWrap, self).__init__(auto_prefix=True)\n    self.network = network\n    self.network.set_grad()\n    self.network.add_flags(defer_inline=True)\n    self.g_weights = g_optimizer.parameters\n    self.g_optimizer = g_optimizer\n    self.d_weights = d_optimizer.parameters\n    self.d_optimizer = d_optimizer\n    self.g_grad = C.GradOperation('g_grad', get_by_list=True, sens_param=True)\n    self.d_grad = C.GradOperation('d_grad', get_by_list=True, sens_param=True)\n\n    self.g_loss_net = IthOutputCell(network, output_index=0)\n    self.d_loss_net = IthOutputCell(network, output_index=1)\n\n    self.sens = sens\n    self.reducer_flag = False\n    self.grad_reducer = None\n    parallel_mode = _get_parallel_mode()\n    if parallel_mode in (ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL):\n      self.reducer_flag = True\n    if self.reducer_flag:\n      mean = _get_mirror_mean()\n      degree = _get_device_num()\n      self.g_grad_reducer = DistributedGradReducer(g_optimizer.parameters, mean, degree)\n      self.d_grad_reducer = DistributedGradReducer(d_optimizer.parameters, mean, degree)\n\n  def update_model(self, image, noise, loss, loss_net, grad, optimizer, weights, grad_reducer):\n    sens = F.fill(F.dtype(loss), F.shape(loss), self.sens)\n    grads = grad(loss_net, weights)(image, noise, sens)\n    if self.reducer_flag:\n      # apply grad reducer on grads\n      grads = grad_reducer(grads)\n    return F.depend(loss, optimizer(grads))\n\n  def construct(self, image, noise):\n    g_loss, d_loss = self.network(image, noise)\n    g_out = self.update_model(image, noise, g_loss, self.g_loss_net, self.g_grad,\n                              self.g_optimizer, self.g_weights, self.g_grad_reducer)\n    d_out = self.update_model(image, noise, d_loss, self.d_loss_net, self.d_grad,\n                              self.d_optimizer, self.d_weights, self.d_grad_reducer)\n    return g_out, d_out\n\n\nclass GANLossMonitor(callback.LossMonitor):\n  def step_end(self, run_context):\n    cb_params = run_context.original_args()\n    g_loss, d_loss = cb_params.net_outputs\n    g_loss: ms.Tensor\n\n    g_loss = np.mean(g_loss.asnumpy())\n    d_loss = np.mean(d_loss.asnumpy())\n\n    cur_step_in_epoch = (cb_params.cur_step_num - 1) % cb_params.batch_num + 1\n\n    if isinstance(g_loss, float) and (np.isnan(g_loss) or np.isinf(g_loss)):\n      raise ValueError(\"epoch: {} step: {}. Invalid loss, terminating training.\".format(\n          cb_params.cur_epoch_num, cur_step_in_epoch))\n    if self._per_print_times != 0 and cb_params.cur_step_num % self._per_print_times == 0:\n      print(\"epoch: %s step: %s, g_loss %s d_loss %s\" %\n            (cb_params.cur_epoch_num, cur_step_in_epoch, g_loss, d_loss), flush=True)\n\n\nclass GANImageSave(callback.Callback):\n  def __init__(self, generator: Cell, noise_dim) -&gt; None:\n    super().__init__()\n    self.generator = generator\n    self.seed = ms.Tensor(np.random.randn(16, noise_dim), ms.float32)\n    if not os.path.exists('./log'):\n      os.mkdir('./log')\n\n  def epoch_end(self, run_context):\n    cb_params = run_context.original_args()\n    # self.generator.set_train(False) NOTE 暂时不知道是否需要设置\n    predictions: ms.Tensor = self.generator(self.seed)\n    predictions = predictions.asnumpy()\n\n    fig = plt.figure(figsize=(4, 4))\n\n    for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i + 1)\n      plt.imshow(predictions[i, 0, :, :] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n    plt.savefig('./log/image_at_epoch_{:04d}.png'.format(cb_params.cur_epoch_num))\n\n\nclass Timer(callback.Callback):\n\n  def epoch_begin(self, run_context):\n    self.start = time.time()\n\n  def epoch_end(self, run_context):\n    cb_params = run_context.original_args()\n    print('Time for epoch {} is {} sec'.format(cb_params.cur_epoch_num, time.time() - self.start))\n\n\nif __name__ == \"__main__\":\n  EPOCHS = 50\n  NOISE_DIM = 100\n  BATCH_SIZE = 256\n  num_examples_to_generate = 16\n  context.set_context(mode=context.GRAPH_MODE, device_target='GPU')\n  sink_mode = True\n\n  \"\"\" set dataset ~ \"\"\"\n  download_dataset()\n  mnist_path = \"./MNIST_Data\"\n  ds_train = create_dataset(os.path.join(mnist_path, \"train\"), NOISE_DIM,\n                            BATCH_SIZE, 1)\n\n  \"\"\" define model ~ \"\"\"\n  net = GANBaseNet(NOISE_DIM)\n  net_loss = GANWithLoss(net)\n  generator_optimizer = moptim.Adam(net.generator.trainable_params(), 1e-4)\n  discriminator_optimizer = moptim.Adam(net.discriminator.trainable_params(), 1e-4)\n  net_train_step = TrainStepWrap(net_loss, generator_optimizer, discriminator_optimizer)\n  model = ms.train.Model(net_train_step, amp_level='O2')\n\n  \"\"\" trianing ~ \"\"\"\n  model.train(EPOCHS, ds_train,\n              callbacks=[Timer(), GANImageSave(net.generator, NOISE_DIM)],\n              dataset_sink_mode=sink_mode)\n  \"\"\" make gif ~ \"\"\"\n  anim_file = 'dcgan.gif'\n  import imageio\n  import glob\n  with imageio.get_writer(anim_file, mode='I') as writer:\n    filenames = glob.glob('./log/image*.png')\n    filenames = sorted(filenames)\n    last = -1\n    for i, filename in enumerate(filenames):\n      frame = 2 * (i**0.5)\n      if round(frame) &gt; round(last):\n        last = frame\n      else:\n        continue\n      image = imageio.imread(filename)\n      writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)\n\"\"\" 14.3 sec/epoch , GPU mem 1544Mb \"\"\"\n输出：\nTime for epoch 1 is 14.746528148651123 sec\nTime for epoch 2 is 13.69857907295227 sec\nTime for epoch 3 is 13.860252380371094 sec\nTime for epoch 4 is 13.879372358322144 sec\nTime for epoch 5 is 13.845653057098389 sec\nTime for epoch 6 is 13.994170665740967 sec\nTime for epoch 7 is 13.880078554153442 sec\n\n\ntensorflow版\nimport tensorflow as tf\n\nimport glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom tensorflow.keras import layers\nimport time\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nassert len(physical_devices) &gt; 0, \"Not enough GPU hardware devices available\"\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\n(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\ntrain_images = (train_images - 127.5) / 127.5  # 将图片标准化到 [-1, 1] 区间内\nBUFFER_SIZE = 60000\nBATCH_SIZE = 256\n# 批量化和打乱数据\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\n\ndef make_generator_model():\n  model = tf.keras.Sequential()\n  model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100,)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.LeakyReLU())\n\n  model.add(layers.Reshape((7, 7, 256)))\n  assert model.output_shape == (None, 7, 7, 256)  # 注意：batch size 没有限制\n\n  model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n  assert model.output_shape == (None, 7, 7, 128)\n  model.add(layers.BatchNormalization())\n  model.add(layers.LeakyReLU())\n\n  model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n  assert model.output_shape == (None, 14, 14, 64)\n  model.add(layers.BatchNormalization())\n  model.add(layers.LeakyReLU())\n\n  model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2),\n                                   padding='same', use_bias=False, activation='tanh'))\n  assert model.output_shape == (None, 28, 28, 1)\n\n  return model\n\n\ndef make_discriminator_model():\n  model = tf.keras.Sequential()\n  model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                          input_shape=[28, 28, 1]))\n  model.add(layers.LeakyReLU())\n  model.add(layers.Dropout(0.3))\n\n  model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n  model.add(layers.LeakyReLU())\n  model.add(layers.Dropout(0.3))\n\n  model.add(layers.Flatten())\n  model.add(layers.Dense(1))\n\n  return model\n\n\ngenerator = make_generator_model()\ndiscriminator = make_discriminator_model()\n# 该方法返回计算交叉熵损失的辅助函数\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n\ndef discriminator_loss(real_output, fake_output):\n  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n  total_loss = real_loss + fake_loss\n  return total_loss\n\n\ndef generator_loss(fake_output):\n  return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\nEPOCHS = 50\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n\n# 我们将重复使用该种子（因此在动画 GIF 中更容易可视化进度）\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\n\n# 注意 `tf.function` 的使用\n# 该注解使函数被“编译”\n\n\n@tf.function\ndef train_step(images):\n  noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    generated_images = generator(noise, training=True)\n\n    real_output = discriminator(images, training=True)\n    fake_output = discriminator(generated_images, training=True)\n\n    gen_loss = generator_loss(fake_output)\n    disc_loss = discriminator_loss(real_output, fake_output)\n\n  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n  discriminator_optimizer.apply_gradients(\n      zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n\ndef generate_and_save_images(model, epoch, test_input):\n  # 注意 training` 设定为 False\n  # 因此，所有层都在推理模式下运行（batchnorm）。\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4, 4))\n\n  for i in range(predictions.shape[0]):\n    plt.subplot(4, 4, i + 1)\n    plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n    plt.axis('off')\n\n  plt.savefig('/tmp/image_at_epoch_{:04d}.png'.format(epoch))\n\n\ndef train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))\n\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)\n\n\ntrain(train_dataset, EPOCHS)\nanim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('/tmp/image_at_epoch_*.png')\n  filenames = sorted(filenames)\n  last = -1\n  for i, filename in enumerate(filenames):\n    frame = 2 * (i**0.5)\n    if round(frame) &gt; round(last):\n      last = frame\n    else:\n      continue\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\n\"\"\" 7.74 sec/epoch, total 183.87s, GPU mem 2655Mb \"\"\"\n输出：\nTime for epoch 1 is 10.126373767852783 sec\nTime for epoch 2 is 7.418195009231567 sec\nTime for epoch 3 is 7.2069251537323 sec\nTime for epoch 4 is 7.063368797302246 sec\nTime for epoch 5 is 7.035956144332886 sec\n\n\nmegengine版\nimport megengine as mge\nimport megengine.functional as F\nimport megengine.module as M\nimport numpy as np\nfrom megengine.data import DataLoader\nfrom megengine.data.dataset import MNIST\nfrom megengine.data import SequentialSampler\nfrom megengine.data.transform import Normalize, ToMode, Compose, VisionTransform\nfrom megengine.jit import trace\nfrom megengine import optimizer as optim\nfrom megengine.core.tensor_factory import ones\nimport time\n\n\ndef ones_like(inp):\n  return ones(inp.shapeof()).astype(inp.dtype)\n\n\nbatch_size = 256\n\n\nclass Noise(VisionTransform):\n  def __init__(self, noise_dim=100, *, order=None) -&gt; None:\n    super().__init__(order)\n    self.noise_dim = noise_dim\n\n  def _apply_image(self, image):\n    return image, np.random.randn(self.noise_dim).astype('float32')\n\n\nmnist_train_dataset = MNIST(root=\"./MNIST\", train=True, download=False)\nsequential_sampler = SequentialSampler(dataset=mnist_train_dataset, batch_size=batch_size)\nmnist_train_dataloader = DataLoader(dataset=mnist_train_dataset,\n                                    sampler=sequential_sampler,\n                                    transform=Compose([\n                                        Normalize(127.5, 127.5),\n                                        ToMode('CHW'),\n                                        Noise()]))\n\n\nclass Reshape(M.Module):\n  def __init__(self, shape: tuple) -&gt; None:\n    super().__init__()\n    self.shape = shape\n\n  def forward(self, inputs):\n    return F.reshape(inputs, self.shape)\n\n\ngenerator = M.Sequential(\n    M.Linear(100, 7 * 7 * 256, bias=False),\n    Reshape((-1, 256, 7, 7)),\n    M.BatchNorm2d(256),\n    M.LeakyReLU(),\n    M.ConvTranspose2d(256, 128, 5, stride=1, padding=2, bias=False),\n    M.BatchNorm2d(128),\n    M.LeakyReLU(),\n    M.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False),\n    M.BatchNorm2d(64),\n    M.LeakyReLU(),\n    M.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=False),\n    M.Elemwise('TANH')\n)\n\n\ndiscriminator = M.Sequential(\n    M.Conv2d(1, 64, 5, stride=2, padding=1),\n    M.LeakyReLU(),\n    M.Dropout(0.3),\n    M.Conv2d(64, 128, 5, stride=2, padding=1),\n    M.LeakyReLU(),\n    M.Dropout(0.3),\n    Reshape((-1, 128 * 6 * 6)),\n    M.Linear(128 * 6 * 6, 1),\n    M.Sigmoid()\n)\n\n# iters = iter(mnist_train_dataloader)\n# (img, noise), _ = next(iters)\n# discriminator(img).shape\n# # 4608\n# 128*6*6\n\n\ndef discriminator_loss(real_output, fake_output):\n  real_loss = F.binary_cross_entropy(real_output, ones_like(real_output))\n  fake_loss = F.binary_cross_entropy(fake_output, F.zeros_like(fake_output))\n  total_loss = real_loss + fake_loss\n  return total_loss\n\n\ndef generator_loss(fake_output):\n  return F.binary_cross_entropy(fake_output, ones_like(fake_output))\n\n\n@trace(symbolic=True)\ndef train_step(images, noise, *, g_opt, d_opt, g_net, d_net):\n  generator_optimizer.zero_grad()\n  discriminator_optimizer.zero_grad()\n  generated_images = g_net(noise)\n\n  real_output = d_net(images)\n  fake_output = d_net(generated_images)\n\n  gen_loss = generator_loss(fake_output)\n  disc_loss = discriminator_loss(real_output, fake_output)\n  g_opt.backward(gen_loss)\n  d_opt.backward(disc_loss)\n  return gen_loss, disc_loss\n\n\ngenerator_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\ndiscriminator_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4)\n\n\ntrace.enabled = True  # 开启trace，使用静态图模式\n\nEPOCHS = 50\n\n\ngenerator.train()\ndiscriminator.train()\nfor epoch in range(EPOCHS):\n  start = time.time()\n  for (imgs, noise), _ in mnist_train_dataloader:\n\n    gen_loss, disc_loss = train_step(imgs, noise, g_opt=generator_optimizer,\n                                     d_opt=discriminator_optimizer,\n                                     g_net=generator, d_net=discriminator)\n  # generate_and_save_images(generator,\n  #                          epoch + 1,\n  #                          seed)\n\n  print('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))\n\n\"\"\" 8.47 sec/epoch, GPU mem 1299Mb \"\"\"\n输出:\n21 21:17:30 process the raw files of train set...\n100%|██████████████████████████████████| 60000/60000 [00:02&lt;00:00, 21099.52it/s]\n100%|████████████████████████████████| 60000/60000 [00:00&lt;00:00, 1832574.11it/s]\nTime for epoch 1 is 8.129057168960571 sec\nTime for epoch 2 is 7.941509008407593 sec\nTime for epoch 3 is 7.948836326599121 sec\nTime for epoch 4 is 7.97221827507019 sec\nTime for epoch 5 is 7.969634771347046 sec\nTime for epoch 6 is 7.968409776687622 sec\nTime for epoch 7 is 7.994731187820435 sec\nTime for epoch 8 is 8.000129699707031 sec\nTime for epoch 9 is 8.010562181472778 sec\nTime for epoch 10 is 8.037590265274048 sec\n\n\n总结\n\ntensorflow占显存为2655Mb，转静态图之后的速度真的还是非常快的。虽然我吐槽tf，但强还是强的啊。\nmindspore虽然占显存为1544Mb，但是明明是静态图运行，速度居然比tf慢一倍，这个真的让人难以接受，并且我还是开启了他的自动半精度，并没有什么用处。此外我还尝试了扩大数据载入的线程，然而4线程的时候一个周期反而需要55s，这是让我没想到的。\nmegengine是我之前最不看好的，因为我感觉就像全抄pytorch的一样，肯定不会快很多。没想到结果还挺香，显存只需要1299Mb，速度还挺快."
  },
  {
    "objectID": "posts/mlc-llm.html",
    "href": "posts/mlc-llm.html",
    "title": "mlc-llm 浅析",
    "section": "",
    "text": "学习tvm是如何解决LLM推理问题.\n\n\n1. Model Arch Generator\nLLM有一个特点就是其动态与自回归的特性, 传统CNN的模型的计算通路都保存在模型中, 对于DL Compiler来说只需要将固定shape下的模型进行编译优化即可, 而LLM的计算通路并没有体现在模型中, 万幸的是没有多少厂商会大改LLM的模型结构, 所以DL Compiler的前端去手动去处理也问题不大.\n使用mlc.build对模型进行编译, 进入build_model_from_args函数:\ndef build_model_from_args(args: argparse.Namespace):\n    # 各种配置处理\n\n    # 选择模型进行解析\n    model_generators = {\n        \"llama\": llama,\n        \"mistral\": llama,\n        \"stablelm_epoch\": stablelm_3b,\n        \"gpt_neox\": gpt_neox,\n        \"gpt_bigcode\": gpt_bigcode,\n        \"minigpt\": minigpt,\n        \"gptj\": gptj,\n        \"rwkv\": rwkv,\n        \"rwkv_world\": rwkv,\n        \"chatglm\": chatglm,\n    }\n\n    # \n目前tvm是基于relax的分支支持LLM的, 构建模型的过程主要就是使用relax的主要特性按原始模型结构重新构造了一遍tvm的ir module:\n首先是构造BlockBuilder的scope, 然后在其中构造整个模型运行的每个阶段.\ndef get_model(args, hf_config):\n    # 处理配置...\n    param_manager = ParamManager()\n    bb = relax.BlockBuilder()\n\n    if sep_embed:\n        create_embed_func(bb, param_manager, config, args.quantization)\n    # 省略batching的构造...\n    create_prefill_func_for_single_seq(bb, param_manager, config, args.quantization, sep_embed)\n    create_decoding_func_for_single_seq(bb, param_manager, config, args.quantization)\n    create_kv_cache_func(bb, config)\n    create_softmax_func_for_single_seq(bb, config)\n\n    create_metadata_func(\n        bb,\n        model_name=model_name,\n        max_window_size=config.max_sequence_length,\n        stop_tokens=[2],\n        add_prefix_space=False,\n    )\n    # 设定动态dim的上下界\n    mod = bb.get()\n    for gv in mod.functions:\n        func = mod[gv]\n        if isinstance(func, relax.Function):\n            mod[gv] = func.with_attr( \"tir_var_upper_bound\", { \"n\": config.max_sequence_length, \"m\": config.max_sequence_length, }, )\n\n    if args.build_model_only:\n        return mod, param_manager, None, config\n\n    return setup_params(mod, param_manager, dtype, config, args)\n在relax中支持同时包含构造relay的数据流以及tir, 所以下面会使用nn.emit以及nn.emit_te, 同时还可以使用一些手动优化的vm函数relax.extern(\"vm.builtin.paged_attention_kv_cache_append\")以及直接编写的prim_func.\nclass Linear(nn.Module):\n    # ...\n    def forward(self, input: relax.Expr) -&gt; relax.Var:\n        return nn.emit(relax.op.linear(input, self.weight, self.bias))\n\ndef apply_rotary_pos_emb(q, k, position_embedding_base, offset: int = 0):\n    def f_rotary_embedding(tensor, offset):\n        def rotary_compute(*idx):\n            pos = (offset + idx[-3]).astype(\"float32\")\n            return rotary_modulate_by_freq(\n                tensor,\n                idx,\n                pos,\n                position_embedding_base,\n            )\n\n        return tvm.te.compute(tensor.shape, rotary_compute, name=\"rotary\")\n\n    q_embed = nn.emit_te(f_rotary_embedding, q, offset, primfunc_name_hint=\"rotary_embedding\")\n    k_embed = nn.emit_te(f_rotary_embedding, k, offset, primfunc_name_hint=\"rotary_embedding\")\n    return q_embed, k_embed\n\nclass LlamaPagedAttention(LlamaAttentionBase):\n    # ...\n    def attention_fwd(\n        self,\n        query_states: relax.Expr,\n        key_states: relax.Expr,\n        value_states: relax.Expr,\n        past_key_values: relax.Expr,\n        batch_size: tir.PrimExpr,\n        q_len: tir.PrimExpr,\n        **kwargs,\n    ) -&gt; Tuple[relax.Expr, relax.Expr]:\n        assert \"layer_id\" in kwargs and isinstance(kwargs[\"layer_id\"], int)\n        layer_id = kwargs[\"layer_id\"]\n\n        f_kv_cache_append = relax.extern(\"vm.builtin.paged_attention_kv_cache_append\")\n        past_key_values = nn.emit(\n            relax.call_pure_packed(\n                f_kv_cache_append,\n                past_key_values,\n                self.kv_cache_transpose_append,\n                key_states,\n                value_states,\n                relax.PrimValue(layer_id),\n                sinfo_args=relax.ObjectStructInfo(),\n            )\n        )\n        # ...\n        return attn_output, past_key_values\n\ndef emit_paged_kv_cache_op(bb: relax.BlockBuilder, dtype: str) -&gt; None:\n    from tvm.script import tir as T\n\n    # fmt: off\n    @T.prim_func\n    def kv_cache_transpose_append(\n        var_pages: T.handle,\n        var_k_data: T.handle,\n        var_v_data: T.handle,\n        var_page_table_indptr: T.handle,\n        var_page_table_values: T.handle,\n        var_last_page_offset: T.handle,\n        var_append_length_indptr: T.handle,\n        var_pos2seqidx: T.handle,\n        layer_id: T.int32,\n    ):\n        # 省略buffer构造...\n        for global_pos, h, f in T.grid(ntoken, nhead, nfeat):\n            with T.block(\"k_transpose_append\"):\n                vgpos, vh, vf = T.axis.remap(\"SSS\", [global_pos, h, f])\n                seq_idx = pos2seqidx[vgpos]\n                seqlen: T.int32 = (page_table_indptr[seq_idx + 1] - page_table_indptr[seq_idx] - 1) * page_size + last_page_offset[seq_idx]\n                pages[\n                    page_table_values[page_table_indptr[seq_idx] + T.floordiv(seqlen - (append_length_indptr[seq_idx + 1] - vgpos), page_size)],\n                    layer_id,\n                    0,\n                    vh,\n                    T.floormod(seqlen - (append_length_indptr[seq_idx + 1] - vgpos), page_size),\n                    vf,\n                ] = k_data[vgpos, vh, vf]\n            with T.block(\"v_transpose_append\"):\n                vgpos, vh, vf = T.axis.remap(\"SSS\", [global_pos, h, f])\n                seq_idx = pos2seqidx[vgpos]\n                seqlen: T.int32 = (page_table_indptr[seq_idx + 1] - page_table_indptr[seq_idx] - 1) * page_size + last_page_offset[seq_idx]\n                pages[\n                    page_table_values[page_table_indptr[seq_idx] + T.floordiv(seqlen - (append_length_indptr[seq_idx + 1] - vgpos), page_size)],\n                    layer_id,\n                    1,\n                    vh,\n                    T.floormod(seqlen - (append_length_indptr[seq_idx + 1] - vgpos), page_size),\n                    vf,\n                ] = v_data[vgpos, vh, vf]\n    # fmt: on\n\n    bb.add_func(kv_cache_transpose_append, \"kv_cache_transpose_append\")\n    # Todo: integrating attention TIR func/kernel.\n    bb.add_func(relax.extern(\"attention_func\"), \"attention\")\n在源代码中检索了一下, 发现是在vm中是直接实现了kv cache, 同时将kv cache的接口进行了封装, 让relax可以进行调用.\nclass AttentionKVCacheObj : public Object {\n public:\n  /*!\n   * \\brief Underlying support data.\n   */\n  NDArray data;\n\n  /*!\n   * \\brief number of slots already filled.\n   */\n  int64_t fill_count{0};\n\n  /*!\n   * \\brief View all current cached values as one array.\n   * \\param shape The cached values.\n   */\n  NDArray View(const ShapeTuple& shape) {\n    // ..\n  }\n\n  /** Clear the cache */\n  void Clear() { /* ... */ }\n\n  /** pop n entries */\n  void PopN(size_t n) {\n    // ...\n  }\n\n  void Update(NDArray value) {\n    // ...\n  }\n\n  /*!\n   * \\brief Append value to the cache.\n   * \\param value The value to be appended.\n   */\n  void Append(NDArray value) {\n    // ...\n  }\n\n  static constexpr const uint32_t _type_index = TypeIndex::kDynamic;\n  static constexpr const char* _type_key = \"relax.vm.AttentionKVCache\";\n  TVM_DECLARE_FINAL_OBJECT_INFO(AttentionKVCacheObj, Object);\n};\n\n// register\nTVM_REGISTER_GLOBAL(\"vm.builtin.attention_kv_cache_create\")\nTVM_REGISTER_GLOBAL(\"vm.builtin.attention_kv_cache_create_multiple\")\nTVM_REGISTER_GLOBAL(\"vm.builtin.attention_kv_cache_update\")\nTVM_REGISTER_GLOBAL(\"vm.builtin.attention_kv_cache_append\")\nTVM_REGISTER_GLOBAL(\"vm.builtin.attention_kv_cache_view\")\nTVM_REGISTER_GLOBAL(\"vm.builtin.attention_kv_cache_array_popn\")\nTVM_REGISTER_GLOBAL(\"vm.builtin.attention_kv_cache_array_clear\")\n其实tvm这种直接在module中构造操作的方式也是很方便的, 如果是传统的编译流程对于每个层还需要写pattern去切子图, 并且一些kv cache相关的优化可能还需要通过一些选项去在某些位置强行添加.\nmod_after_get_model.py\n\n\n2. Module Transform\n如果开启了量化还需要更新全部的参数, 然后对构造好的IR.Module进行优化, 这里也是一些比较有针对性的优化pass:\ndef mod_transform_before_build(\n    mod: tvm.IRModule,\n    param_manager: param_manager.ParamManager,\n    args: argparse.Namespace,\n    config: Dict,\n) -&gt; tvm.IRModule:\n  # \n  mod = param_manager.transform_dequantize()(mod)\n  mod = relax.transform.BundleModelParams()(mod)\n  use_ft_quant = args.quantization.name in [\"q4f16_ft\", \"q8f16_ft\"]\n  mod = mlc_llm.transform.FuseDecodeTranspose(skip_gemm=not use_ft_quant)(mod)\n\n  if max_seq_len:\n    num_key_value_heads = config.get_num_key_value_heads()\n    mod = fuse_split_rotary_embedding(\n            config.num_attention_heads // args.num_shards,\n            num_key_value_heads // args.num_shards,\n            config.hidden_size // args.num_shards,\n            config.position_embedding_base,\n        )(mod)\n  if args.target_kind == \"cuda\":\n    # ...\n  mod = mlc_llm.transform.FuseTransposeMatmul()(mod)\n  mod = relax.pipeline.get_pipeline()(mod)  # pylint: disable=no-value-for-parameter\n  mod = mlc_llm.transform.FuseDecodeMatmulEwise()(mod)\n  mod = mlc_llm.transform.FuseDecodeTake()(mod)\n  mod = relax.transform.DeadCodeElimination(model_names)(mod)\n  mod = mlc_llm.transform.CleanUpTIRAttrs()(mod)\n  mod_deploy = mod\n  return mod_deploy\n修改后的Module如下, 相比原本的Module多了许多Fused的算子.\nmod_depoly.py\n\n\n3. Module Build\nbuild的过程就是调用原本tvm中的编译下降进行处理, 这里我的target为m1-metal:\ndef build(mod_deploy: tvm.IRModule, args: argparse.Namespace) -&gt; None:\n    # dump ...\n    if target_kind != \"cpu\":\n        dispatch_target = (\n            args.target\n            if args.target_kind != \"webgpu\"\n            else tvm.target.Target(\"apple/m1-gpu-restricted\")\n        )\n        with dispatch_target:\n            mod_deploy = dl.ApplyDefaultSchedule(  # pylint: disable=not-callable\n                dl.gpu.Matmul(),\n                dl.gpu.GEMV(),\n                dl.gpu.Reduction(),\n                dl.gpu.GeneralReduction(),\n                dl.gpu.Fallback(),\n            )(mod_deploy)\n            mod_deploy = (\n                mlc_llm.transform.LiftTIRGlobalBufferAlloc()(  # pylint: disable=not-callable\n                    mod_deploy\n                )\n            )\n            mod_deploy = tvm.tir.transform.ForceNarrowIndexToInt32()(mod_deploy)\n\n    # 省略使用cuda...\n    args.lib_path = os.path.join(args.artifact_path, output_filename)\n    ex.export_library(args.lib_path, **args.export_kwargs)\n    print(f\"Finish exporting to {args.lib_path}\")\nrelax中的原生支持动态shape, 所以在decode过程中是通过dataflow的形式来执行:\n@R.function\n    def decode(input_ids1: R.Tensor((1, 1), dtype=\"int32\"), all_seq_len: R.Shape([\"n\"]), kv_cache:...):\n        cls = Module\n        with R.dataflow():\n          # ...\n          lv1897 = R.call_tir(cls.transpose5, (lv1894,), out_sinfo=R.Tensor((1, 32, n, 80), dtype=\"float16\"))\n          lv1898 = R.call_tir(cls.transpose5, (lv1895,), out_sinfo=R.Tensor((1, 32, n, 80), dtype=\"float16\"))\n          lv722 = R.call_tir(cls.fused_NT_matmul7_divide2_maximum1_minimum1_cast9, (lv1896, lv1897, lv1871), out_sinfo=R.Tensor((1, 32, 1, n), dtype=\"float32\"))\n          # ...\n例如decode中的transpose5函数在before build阶段, tir中是以动态的方式进行构造的:\n    @T.prim_func(private=True)\n    def transpose5(var_A: T.handle, var_T_transpose: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(var_A, (T.int64(1), n, T.int64(32), T.int64(80)), \"float16\")\n        T_transpose = T.match_buffer(var_T_transpose, (T.int64(1), T.int64(32), n, T.int64(80)), \"float16\")\n        # with T.block(\"root\"):\n        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), n, T.int64(80)):\n            with T.block(\"T_transpose\"):\n                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n                T.reads(A[v_ax0, v_ax2, v_ax1, v_ax3])\n                T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])\n                T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax2, v_ax1, v_ax3]\n在after build阶段, 经过编译下降之后的block中的iterVar被映射到了thread和block两个层级. 我估计在tvm中对于动态申请的内存默认都是连续的, 所以这里match buffer也没有特别的stride.\n    @T.prim_func(private=True)\n    def transpose5(var_A: T.handle, var_T_transpose: T.handle):\n        T.func_attr({\"tir.is_scheduled\": 1, \"tir.noalias\": T.bool(True)})\n        n = T.int32()\n        A = T.match_buffer(var_A, (1, n, 32, 80), \"float16\")\n        T_transpose = T.match_buffer(var_T_transpose, (1, 32, n, 80), \"float16\")\n        # with T.block(\"root\"):\n        for ax0_ax1_ax2_fused_0 in T.thread_binding((n * 2560 + 1023) // 1024, thread=\"blockIdx.x\"):\n            for ax0_ax1_ax2_fused_1 in T.thread_binding(1024, thread=\"threadIdx.x\"):\n                with T.block(\"T_transpose\"):\n                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) // (80 * n))\n                    v1 = T.axis.spatial(n, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (80 * n) // 80)\n                    v2 = T.axis.spatial(80, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % 80)\n                    T.where(ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1 &lt; n * 2560)\n                    T.reads(A[0, v1, v0, v2])\n                    T.writes(T_transpose[0, v0, v1, v2])\n                    T_transpose[0, v0, v1, v2] = A[0, v1, v0, v2]\n编译后的模型如下:\nmod_build_stage.py\n\n\n4. Chat\nchat 其实经过之前的编译过程后会非常的精简, 只需要获取对应编译后模型的packed func然后反复调用即可.\nclass ChatModule {\n public:\n  /*!\n   * \\brief Constructor\n   * \\param device the device to run the chat on.\n   */\n  explicit ChatModule(const DLDevice& device) {\n    this-&gt;chat_mod_ = mlc::llm::CreateChatModule(device);\n    this-&gt;prefill_ = this-&gt;chat_mod_-&gt;GetFunction(\"prefill\");\n    this-&gt;decode_ = this-&gt;chat_mod_-&gt;GetFunction(\"decode\");\n    this-&gt;stopped_ = this-&gt;chat_mod_-&gt;GetFunction(\"stopped\");\n    this-&gt;get_message_ = this-&gt;chat_mod_-&gt;GetFunction(\"get_message\");\n    this-&gt;reload_ = this-&gt;chat_mod_-&gt;GetFunction(\"reload\");\n    this-&gt;get_role0_ = this-&gt;chat_mod_-&gt;GetFunction(\"get_role0\");\n    this-&gt;get_role1_ = this-&gt;chat_mod_-&gt;GetFunction(\"get_role1\");\n    this-&gt;runtime_stats_text_ = this-&gt;chat_mod_-&gt;GetFunction(\"runtime_stats_text\");\n    this-&gt;verbose_runtime_stats_text_ = this-&gt;chat_mod_-&gt;GetFunction(\"verbose_runtime_stats_text\");\n    this-&gt;reset_chat_ = this-&gt;chat_mod_-&gt;GetFunction(\"reset_chat\");\n    this-&gt;process_system_prompts_ = this-&gt;chat_mod_-&gt;GetFunction(\"process_system_prompts\");\n    this-&gt;lib_path_ = \"\";\n    this-&gt;executable_ = tvm::runtime::Module(nullptr);\n    ICHECK(prefill_ != nullptr);\n    ICHECK(decode_ != nullptr);\n    ICHECK(stopped_ != nullptr);\n    ICHECK(get_message_ != nullptr);\n    ICHECK(reload_ != nullptr);\n    ICHECK(get_role0_ != nullptr);\n    ICHECK(get_role1_ != nullptr);\n    ICHECK(runtime_stats_text_ != nullptr);\n    ICHECK(verbose_runtime_stats_text_ != nullptr);\n    ICHECK(reset_chat_ != nullptr);\n  }"
  },
  {
    "objectID": "posts/mlircsharp.html",
    "href": "posts/mlircsharp.html",
    "title": "MLIRSharp",
    "section": "",
    "text": "记录一下MLIRSharp的开发总结."
  },
  {
    "objectID": "posts/mlircsharp.html#context管理",
    "href": "posts/mlircsharp.html#context管理",
    "title": "MLIRSharp",
    "section": "1. context管理",
    "text": "1. context管理\npython中对于mlircontext做了复杂的包装, 需要详细的了解我才能知道在csharp中应该如何实现.\n这里是PyMlirContext的类定义:\nclass PyMlirContext {\npublic:\n  PyMlirContext() = delete;\n  PyMlirContext(const PyMlirContext &) = delete;\n  PyMlirContext(PyMlirContext &&) = delete;\n\n  /// For the case of a python __init__ (py::init) method, pybind11 is quite\n  /// strict about needing to return a pointer that is not yet associated to\n  /// an py::object. Since the forContext() method acts like a pool, possibly\n  /// returning a recycled context, it does not satisfy this need. The usual\n  /// way in python to accomplish such a thing is to override __new__, but\n  /// that is also not supported by pybind11. Instead, we use this entry\n  /// point which always constructs a fresh context (which cannot alias an\n  /// existing one because it is fresh).\n  static PyMlirContext *createNewContextForInit();\n\n  /// Returns a context reference for the singleton PyMlirContext wrapper for\n  /// the given context.\n  static PyMlirContextRef forContext(MlirContext context);\n  ~PyMlirContext();\n\n  /// Accesses the underlying MlirContext.\n  MlirContext get() { return context; }\n\n  /// Gets a strong reference to this context, which will ensure it is kept\n  /// alive for the life of the reference.\n  PyMlirContextRef getRef() {\n    return PyMlirContextRef(this, pybind11::cast(this));\n  }\n\n  /// Gets a capsule wrapping the void* within the MlirContext.\n  pybind11::object getCapsule();\n\n  /// Creates a PyMlirContext from the MlirContext wrapped by a capsule.\n  /// Note that PyMlirContext instances are uniqued, so the returned object\n  /// may be a pre-existing object. Ownership of the underlying MlirContext\n  /// is taken by calling this function.\n  static pybind11::object createFromCapsule(pybind11::object capsule);\n\n  /// Gets the count of live context objects. Used for testing.\n  static size_t getLiveCount();\n\n  /// Gets the count of live operations associated with this context.\n  /// Used for testing.\n  size_t getLiveOperationCount();\n\n  /// Clears the live operations map, returning the number of entries which were\n  /// invalidated. To be used as a safety mechanism so that API end-users can't\n  /// corrupt by holding references they shouldn't have accessed in the first\n  /// place.\n  size_t clearLiveOperations();\n\n  /// Gets the count of live modules associated with this context.\n  /// Used for testing.\n  size_t getLiveModuleCount();\n\n  /// Enter and exit the context manager.\n  pybind11::object contextEnter();\n  void contextExit(const pybind11::object &excType,\n                   const pybind11::object &excVal,\n                   const pybind11::object &excTb);\n\n  /// Attaches a Python callback as a diagnostic handler, returning a\n  /// registration object (internally a PyDiagnosticHandler).\n  pybind11::object attachDiagnosticHandler(pybind11::object callback);\n\n  /// Controls whether error diagnostics should be propagated to diagnostic\n  /// handlers, instead of being captured by `ErrorCapture`.\n  void setEmitErrorDiagnostics(bool value) { emitErrorDiagnostics = value; }\n  struct ErrorCapture;\n\nprivate:\n  PyMlirContext(MlirContext context);\n  // Interns the mapping of live MlirContext::ptr to PyMlirContext instances,\n  // preserving the relationship that an MlirContext maps to a single\n  // PyMlirContext wrapper. This could be replaced in the future with an\n  // extension mechanism on the MlirContext for stashing user pointers.\n  // Note that this holds a handle, which does not imply ownership.\n  // Mappings will be removed when the context is destructed.\n  using LiveContextMap = llvm::DenseMap&lt;void *, PyMlirContext *&gt;;\n  static LiveContextMap &getLiveContexts();\n\n  // Interns all live modules associated with this context. Modules tracked\n  // in this map are valid. When a module is invalidated, it is removed\n  // from this map, and while it still exists as an instance, any\n  // attempt to access it will raise an error.\n  using LiveModuleMap =\n      llvm::DenseMap&lt;const void *, std::pair&lt;pybind11::handle, PyModule *&gt;&gt;;\n  LiveModuleMap liveModules;\n\n  // Interns all live operations associated with this context. Operations\n  // tracked in this map are valid. When an operation is invalidated, it is\n  // removed from this map, and while it still exists as an instance, any\n  // attempt to access it will raise an error.\n  using LiveOperationMap =\n      llvm::DenseMap&lt;void *, std::pair&lt;pybind11::handle, PyOperation *&gt;&gt;;\n  LiveOperationMap liveOperations;\n\n  bool emitErrorDiagnostics = false;\n\n  MlirContext context;\n  friend class PyModule;\n  friend class PyOperation;\n};\n接下来根据几个在python中实际的场景进行讲解.\n\n1.1 构造context\n这里其实是给用户两个选择来管理context, 也可以通过with的方式来管理context. 总的来说就是只要构造了一个context就必然会把底层alloc出来的context指针存放到LiveModuleMap中. 当使用with语法的时候, 会调用__enter__函数将当前的context作为current.\nctx1 = Context()\ntry:\n    curr = Context.current\nexcept:\n    print(\"no current\")\n    print(\"live:\", Context._get_live_count())\nwith Context() as ctx2:\n  print(Context.current)\n  print(\"live:\", Context._get_live_count())\nTEST: testInsertionPointEnterExit\nno current\nlive: 1\n&lt;mlir._mlir_libs._site_initialize.&lt;locals&gt;.Context object at 0x100a05400&gt;\nlive: 2\n\n\n1.2 location/insert point\nctx1 = Context()\nwith Location.unknown(ctx1) as loc1:\n    assert Context.current is ctx1\n    assert Location.current is loc1\n\nm = Module.create(Location.unknown(ctx1))\nip = InsertionPoint(m.body)\n\nwith ip:\n    assert InsertionPoint.current is ip\n这里的location/以及insert point同样调用的是默认的push方式, 维护了一个存储context/insertionPoint/location的栈, 如果在新的scope中, context不变的话可以\nvoid PyThreadContextEntry::push(FrameKind frameKind, py::object context,\n                                py::object insertionPoint,\n                                py::object location) {\n  auto &stack = getStack();\n  stack.emplace_back(frameKind, std::move(context), std::move(insertionPoint),\n                     std::move(location));\n  // If the new stack has more than one entry and the context of the new top\n  // entry matches the previous, copy the insertionPoint and location from the\n  // previous entry if missing from the new top entry.\n  if (stack.size() &gt; 1) {\n    auto &prev = *(stack.rbegin() + 1);\n    auto &current = stack.back();\n    if (current.context.is(prev.context)) {\n      // Default non-context objects from the previous entry.\n      if (!current.insertionPoint)\n        current.insertionPoint = prev.insertionPoint;\n      if (!current.location)\n        current.location = prev.location;\n    }\n  }\n}\n\n\n1.3 block/operation管理\n这里应该算是最复杂的部分, 主要问题就是在mlir的内部实现中, operation以及记录了他内部的block等信息, 但是在other language binding的时候, 是不知道内部已经记录了这些内容. 比如如果我使用safe handle去实现operation/block的时候, 此时operation可以get block, 拿到他内部的block, 但是如果直接通过调用extern的函数来获得这个block, 对于charp来说他会是一个新的block对象. 也就是如果使用charp的block去构造operation, 再通过operation去获取block, 拿到的block并不是原来block对象.\n我觉得可以有两个解决方法:\n\n在csharp的类中再维护一套引用的关系, 保证优先返回csharp中已经构造好的对象.\n在构造csharp类的时候区分好是新创建还是引用创建, 然后为csharp的对象重写比较, 只需要handle相同就是相等的. 但是这样每次都重新创建了csharp的类不知道会不会有隐藏的问题.\n\n先看看python binding的数据关系处理: \n\n\n1.4 python wrapper\n在python中,对于context又做了一层包装, 也就是每个context构造的时候都直接加载所有的dialect了.\ndef _site_initialize():\n    import importlib\n    import itertools\n    import logging\n    from ._mlir import ir\n\n    logger = logging.getLogger(__name__)\n    registry = ir.DialectRegistry()\n    post_init_hooks = []\n\n    def process_initializer_module(module_name):\n        try:\n            m = importlib.import_module(f\".{module_name}\", __name__)\n        except ModuleNotFoundError:\n            return False\n        except ImportError:\n            message = (\n                f\"Error importing mlir initializer {module_name}. This may \"\n                \"happen in unclean incremental builds but is likely a real bug if \"\n                \"encountered otherwise and the MLIR Python API may not function.\"\n            )\n            logger.warning(message, exc_info=True)\n\n        logger.debug(\"Initializing MLIR with module: %s\", module_name)\n        if hasattr(m, \"register_dialects\"):\n            logger.debug(\"Registering dialects from initializer %r\", m)\n            m.register_dialects(registry)\n        if hasattr(m, \"context_init_hook\"):\n            logger.debug(\"Adding context init hook from %r\", m)\n            post_init_hooks.append(m.context_init_hook)\n        return True\n\n    # If _mlirRegisterEverything is built, then include it as an initializer\n    # module.\n    process_initializer_module(\"_mlirRegisterEverything\")\n\n    # Load all _site_initialize_{i} modules, where 'i' is a number starting\n    # at 0.\n    for i in itertools.count():\n        module_name = f\"_site_initialize_{i}\"\n        if not process_initializer_module(module_name):\n            break\n\n    class Context(ir._BaseContext):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.append_dialect_registry(registry)\n            for hook in post_init_hooks:\n                hook(self)\n            # TODO: There is some debate about whether we should eagerly load\n            # all dialects. It is being done here in order to preserve existing\n            # behavior. See: https://github.com/llvm/llvm-project/issues/56037\n            self.load_all_available_dialects()\n\n    ir.Context = Context\n\n    class MLIRError(Exception):\n        \"\"\"\n        An exception with diagnostic information. Has the following fields:\n          message: str\n          error_diagnostics: List[ir.DiagnosticInfo]\n        \"\"\"\n\n        def __init__(self, message, error_diagnostics):\n            self.message = message\n            self.error_diagnostics = error_diagnostics\n            super().__init__(message, error_diagnostics)\n\n        def __str__(self):\n            s = self.message\n            if self.error_diagnostics:\n                s += \":\"\n            for diag in self.error_diagnostics:\n                s += (\n                    \"\\nerror: \"\n                    + str(diag.location)[4:-1]\n                    + \": \"\n                    + diag.message.replace(\"\\n\", \"\\n  \")\n                )\n                for note in diag.notes:\n                    s += (\n                        \"\\n note: \"\n                        + str(note.location)[4:-1]\n                        + \": \"\n                        + note.message.replace(\"\\n\", \"\\n  \")\n                    )\n            return s\n\n    ir.MLIRError = MLIRError"
  },
  {
    "objectID": "posts/model-driven-optimization.html",
    "href": "posts/model-driven-optimization.html",
    "title": "Model Driven Optimization",
    "section": "",
    "text": "关于Model-Driven Optimization For Tensor Computations论文的阅读笔记.\n\n\n1. 单内存层级建模\nfor (i2 = 0; i2 &lt; Ni; i2+=Ti1)\n  for (j2 = 0; j2 &lt; Nj; j2+=Tj1)\n    for (k2 = 0; k2 &lt; Nk; k2+=Tk1)\n      for (i1 = 0; i1 &lt; Ti1; i1++)\n        for (j1 = 0; j1 &lt; Tj1; j1++)\n          for (k1 = 0; k1 &lt; Tk1; k1++)\n            C[i1+i2][j1+j2] += A[i1+i2][k1+k2] * B[k1+k2][j1+j2];\n以这样的矩阵乘作为例子, 进行单层内存层级的tiling建模. 首先循环变量表示为\\(i_{1},i_{2},\\ldots,i_{l+1}\\), 其中\\(l\\)为tiling层级(我理解这个应该就是循环层级), \\(l==0\\)表示的是statement, \\(l+1\\)表示的就是最外层循环,\\(l==1\\)表示最内层循环. tilesize变量表示为\\(T_{i_{1}},T_{i_{2}},\\ldots,T{i_{l+1}}\\).\n对于一个固定的循环order, 我们可以用上面定义的变量来建模数据移动. 令\\(DF(A,i)\\)表示的是数组\\(A\\)从循环\\(i\\)开始的被访问过的不同元素的数量(Data Footprint).令\\(DM(A,i)\\)表示在循环\\(i\\)处数组\\(A\\)从主存到cache的数据移动(Data Movement). 下面这段代码展示令如何计算这两个不同的项目, 其实他们的区别也就是在于存在数据复用的时候, 内存足迹是不改变的, 但是内存移动量在内存足迹大于cache size的时候是需要重复load的. 根据这个计算方式, 也就是表示如果有一个维度足够小, 可以让数据完整的放在cache中, 那么他的就不需要重复load.\nforeach (loop i in Loops.Reverse())  {\n  if (i == 0) { // statement level\n    foreach(tensor A in Tensors) {\n      DM(A, i) = DF(A, i) = 1;\n    }\n  }\n  else {\n    foreach(tensor A in Tensors) {\n      if (A.Indices.Contains(i)) {\n        DM(A, i) = DM(A, i − 1) * range(i);\n        DF(A, i) = DF(A, i − 1) * range(i);\n      } else {\n        DF(A, i) = DF(A, i − 1);\n        if (Sum(Tensors, A =&gt; DF(A, i − 1)) &lt; CacheCapacity) {\n          DM(A, i) = DM(A, i − 1);\n        } else {\n          DM(A, i) = DM(A, i − 1) * range(i);\n        }\n      }\n    }\n  }\n}\n\n\n2. 多内存层级建模\n目前的现代处理架构通常有多个内存层级, 更高的内存层级会有更大的存储以及更小的带宽. 假设每一个循环只能tile到其中一个内存层级, 对于一个两级内存层级的tiling示例代码如下:\nfor (i3 = 0; i3 &lt; Ni; i3+=Ti2)\n  for (j3 = 0; j3 &lt; Nj; j3+=Tj2)\n    for (k3 = 0; k3 &lt; Nk; k3+=Tk2)\n      for (i2 = 0; i2 &lt; Ti2; i2+=Ti1)\n        for (j2 = 0; j2 &lt; Tj2; j2+=Tj1)\n          for (k2 = 0; k2 &lt; Tk2; k2+=Tk1)\n            for (i1 = 0; i1 &lt; Ti1; i1++)\n              for (j1 = 0; j1 &lt; Tj1; j1++)\n                for (k1 = 0; k1 &lt; Tk1; k1++)\n                  C[i1+i2+i3][j1+j2+j3] += A[i1+i2+i3][k1+k2+k3] * B[k1+k2+k3][j1+j2+j3];\n为了适配多内存层级, 需要修改\\(DM\\)的定义为\\(DM(A,i,l)\\)表示数组\\(A\\)在循环\\(i\\)上从内存层级\\(l\\)到\\(l+1\\)的数据移动, 但原始论文只说了更新建模的代码从Sum(Tensors, A =&gt; DF(A, i − 1)) &lt; CacheCapacity到Sum(Tensors, A =&gt; DF(A, i − 1)) &lt; CacheCapacity(l), 并没有给出完整的实现. 这里实际上每一个出现DM的地方都需要添加\\(l\\)参数, 那么可能需要在最外层再添加关于内存层级的循环, 但是从直觉上来说内存层级的选择这里是多分支的, 不是简单的双循环就可以构造出来的.\n\n\n3. 估计执行时间\n这里假设访存是主要的瓶颈, 那么通过统计每个内存层级的数据移动量以及对应的带宽大小来估计总时间. 令\\(L\\)表示内存层级编号, \\(C_1,\\ldots,C_{L}\\)表示各个内层层级, 而\\(C_0\\)表示计算单元,\\(C_{L+1}\\)表示主存. 令\\(BW_1,\\ldots,BW_{L+1}\\)表示各个内存层级的带宽. 令\\(C\\_DM_(l)\\)表示从内存层级\\(l\\)到\\(l-1\\)的数据移动量,\\(C\\_Time(\\mathcal{P}, l)\\)在特定的循环排列下从内存层级\\(l\\)到\\(l-1\\)的数据移动时间. 那么给定一个固定的循环排列\\(\\mathcal{P}\\),对应的时间为: \\[\n\\begin{aligned}\n  C\\_Time(\\mathcal{P}, l) &= C\\_DM(l) / BW_l \\\\\n  TotTime(\\mathcal{P}) &= \\max_{l = 1}^{L+1} \\left( C\\_Time(\\mathcal{P}, l) \\right)\n\\end{aligned}\n\\]\n\n\n4. tilesize与循环排列选择\n首先对于一个固定循环排列下的tile size选择就变成了一个约束求解问题: \\[\n\\begin{aligned}\n  \\arg \\min_{tile-sizes}(TotTime(\\mathcal{P})) = \\arg \\min_{tile-sizes}\\left(\\max_{l = 1}^{L+1} \\left( C\\_Time(\\mathcal{P}, l) \\right)\\right)\n\\end{aligned}\n\\]\n为了减少搜索空间, 限制对于内存层级\\(l\\)的所有内存移动量\\(C\\_DM(l)\\)必须小于等于内存层级的容量, 令\\(group\\_outer(l)\\)表示属于内存层级\\(l\\)的最外层循环, 构建容量约束如下: \\[\n\\begin{aligned}\n  \\forall l \\in [1,L],\\ \\sum_{A\\in Tensors} DM(A, group\\_outer(l)) \\leq CacheCapacity(l)\n\\end{aligned}\n\\]\n但这样还是没有考虑tile size对于不同层级的访存速度影响, 需要构建一个多级约束. 令\\(T\\)表示所有的tile变量集合, \\(T_l\\)表示会影响内存层级\\(l\\)的总数据移动量\\(C\\_DM(l)\\)的tile变量子集. 也就是多个循环可以对应同一个内存层级, 那么这些循环对应的tile变量组成的子集, 这些变量每个都会影响当前内存层级的数据移动量.\n假设\\(j\\)是最大瓶颈的内存层级, 也就是他的时间大于其他内存层级时间: \\[\n\\begin{aligned}\n  \\forall i \\in [1, L+1], \\ (C\\_DM(j)/C\\_BW(j)) \\geq (C\\_DM(i)/C\\_BW (i)\n\\end{aligned}\n\\]\n当固定了\\(j\\)层级的tile size后, 下一个瓶颈的内存层级可以使用如下公式找到: \\[\n\\begin{aligned}\n  \\argmin_{T-T_j} (\\max_{l\\in[1,L+1] - T_j} C\\_Time(\\mathcal{P},l))\n\\end{aligned}\n\\]\n但是论文中并没有详细描述怎么进行多级优化, 这里的约束应该都还是变量, 除非是求解器可以通过某种方式添加优化指导.\n接下来就是考虑不同的循环排列, 对于两级tiling, 就有9个循环那么排列方式就有9!种. 但其实只需要考虑每个tile出来的循环内部进行排序,也就是\\(3!\\times3!\\times3! = 216\\)种即可. 最终的解为: \\[\n\\begin{aligned}\nfinal\\_solution =  \\argmin_{\\mathcal{P}\\in \\mathcal{R}} (\\argmin_{tile\\_sizes}(TotTime(\\mathcal{P})))\n\\end{aligned}\n\\]\n\n\n5. micro kernel\n对于包含SIMD的指令集的处理器来说, 需要考虑设计一个最大硬件利用率的micro kernel, 这里需要对micro kernel也进行建模. 首先是MaxIssue表示一个时钟周期最大可以发射的指令数, WordPerVec表示指令集宽度, Latency表示一个指令周期所需要的时钟周期数. 假设在一个指令周期中每个时钟周期都可以发射MaxIssue个指令数, 并且他们之间是没有数据依赖的, 那么MaxIssue * Latency则是可以保证流水线打满的最小指令数, MaxIssue * Latency * WordPerVec则表示最小寄存器容量. 比如在BLIS库中,通常使用外积的方式来设计micro kernel, 但是这个kernel它需要一块布局优化后的数据块才可以开始计算, 这就要求进行packing.\npacking的一个好处是可以减少Conflict Miss,现代处理器中cache的存放策略通常是set-associative, 整个缓存被划分为多个set,而这些set内部进一步被细分为lines/ways. 一个映射函数决定内存地址到set的映射. 在每个set内,一个给定的内存地址可能出现在任意一个cache line上. 当在多个不同的内存地址由于缓存映射规则而被迫存储在同一个cache line时, 由于每个缓存行只能存储一个数据块, 当一个新的数据块需要被存储到这个缓存行时, 原有的数据块必须被替换掉, 在理想情况下, 被替换的应该是最近最少使用(LRU)的数据块, 然而在某些情况下, 由于映射规则的限制, 一个不是LRU的数据块也可能被替换, 这就是发生了Conflict Miss.\n通过选择合适tile大小,并依赖于packing设计,可以将即数据元素的排列顺序与它们将被访问的顺序相同,从而避免Conflict Miss. 虽然packed buffer在内存中是连续的存储. 但是实际上他们分散在cache中各个地方, 由于大部分的cache是无法编程的, 加载新的数据会将其他tensor从cache中驱除, 为了避免这种情况, 每个tensor拥有的cache line的数量要被小心的控制, 对于矩阵乘的例子这里计算分配给各个tensor的cache line数量: \\[\n\\begin{aligned}\nlineA = [DF(A,l) / (NumOfSets(l)*lineSize(l))] \\\\\nlineB = [DF(B,l) / (NumOfSets(l)*lineSize(l))] \\\\\nlineC = [DF(C,l) / (NumOfSets(l)*lineSize(l))] \\\\\ns.t Line_A(l)+Line_B(l)+Line_C(l) \\leq Associativity(l) \\\\\n\\end{aligned}\n\\] 假设只访问A/B矩阵, 考虑在l层级的j循环, A矩阵并没有被j索引, 但是在k循环中多次访问A行, 此时A会停留在cache中, 而B矩阵的行j循环才会被访问, 因此B矩阵在cache中是流式传输的.\npacking 会增加额外的数据移动, 为了减少packing带来的额外时间. 比如要复用pack过的数据, 但是由于cache容量的限制, 大部分情况是没办法存储整个的buffer的. 假设A时需要pack的tneosr, 设\\(IS\\)表示整个迭代空间, 令\\(IS_A\\)表示参与访问A的循环子集, 设packing在最后一层cache \\(ll\\), 那么packing的cost为包含从主存加载以及存储数据到\\(ll\\)层上:\n\\[\n\\begin{aligned}\nPackCost^{A, buf\\in Mem}_{mem \\rightarrow l3} = \\prod_{idx \\in IS_A} idx\n\\end{aligned}\n\\]\n假设l3级别的tiling循环为\\(i_1^{L3},i_2^{L3},\\ldots,i_l^{L3}\\), 假设只有\\(i_2^{L3}, i_l^{L3}\\)是A的reuse index,那么意味着\\(i_2^{L3}, i_l^{L3} \\notin IS_A\\). 假设packed A在level 3进行构造, 那么代码可能类似这样: \\[\n\\begin{aligned}\nfor\\ loop&\\  i_1^{L3}\\\\\nfor\\ &loop\\  i_2^{L3} \\\\\n& \\ldots\\\\\n&for\\ loop\\  i_l^{L3}\\\\\n& \\text{Packing buffer resides here;}\n\\end{aligned}\n\\]\n注意这里packed A将会在\\(i_2^{L3}\\)的循环中被填充满, 即使\\(i_2^{L3}\\)是他的reuse index, 这意味着对于给定tenors的内部cache packing的总数据移动是tensor size和packing cache level以上的所有reuse loops的乘积. 公式化的描述如下:\n\\[\n\\begin{gathered}\nR D X^{L 3}=\\left\\{i_g^{L 3} \\mid i_g \\notin I S_A \\wedge\\left(\\exists i_h \\in I S_A\\right)\\left[i_g^{L 3}&gt;i_h^{L 3}\\right]\\right\\} \\\\\n\\text { PackCost } t_{m e m \\rightarrow L b}^{A, b u f \\in L 3}=\\prod_{i d x \\in I S_A} i d x * \\prod_{r d x \\in R D X^{L 3}} \\operatorname{NIter}(r d x)\n\\end{gathered}\n\\]\n首先令\\(i^{L3}_p&gt; i^{L3}_q\\)表示在L3内存层级中\\(i^{L3}_p\\)循环在\\(i^{L3}_q\\)之上. 再令\\(\\operatorname{NIter}(i^{L3}_p)\\)表示循环的迭代次数, 令\\(Tile(i^{L3}_p )\\)表示索引\\(p\\)的tile大小, \\(N_p\\)表示索引\\(p\\)的全局大小(我理解就是这个tensor在\\(p\\)维度的大小). 然后\\(RDX^{L3}\\)表示在与A无关的循环迭代中, 存在的迭代\\(i_g^{L 3}\\)级别高于\\(i_h^{L 3}\\), 将这些迭代的总次数累积起来与参与A的循环\\(idx\\)进行乘积.\n对于任意内存层级的pack cost可以通过如下公式计算:\n\\[\n\\begin{aligned}\n& R D X^{L_c}=\\left\\{i_g^{L_c} \\mid i_g \\notin I S_A \\wedge\\left(\\exists i_h \\in I S_A\\right)\\left[i_g^{L_c}&gt;i_h^{L_c}\\right]\\right\\} \\\\\n& \\text { PackCost } \\operatorname{mem}_{m \\rightarrow L_c}^{A, b u f \\in L_c}=\\prod_{i d x \\in I S_A} i d x * \\prod_{r d x \\in R D X^{L_c}} \\text { NIter }(r d x) \\\\\n& =\\prod_{i d x \\in I S_A} i d x * \\prod_{i_p \\in R D X^{L_c}}\\left(N_p / \\text { Tile }\\left(i_p^{L_c}\\right)\\right) \\\\\n&\n\\end{aligned}\n\\]\n\n\n6. 疑问\n\n我在思考他这里的DM和DF和显式的指定sub tensor放置在哪个循环下是否有共同性.\n他这里的group_outer(l)的函数是一个离散函数, 但是求解器只能支持连续的变量, 现在问题转化为如何通过连续的变量构造出分段函数? 这个方法实际上和对于多层memory的循环加在哪个位置息息相关.\n在约束编程中通过什么方式控制几个变量属于一个区间?\n怎么样进行多阶段求解?\n他这里感觉没有考虑在l1上cache packed buffer, 实际上可以开很大一块buffer, 然后在l1的循环中每次load并pack一小块, 然后在l1中缓存起来, 这样切换m/n的时候可以尽量复用."
  },
  {
    "objectID": "posts/mult-cuda.html",
    "href": "posts/mult-cuda.html",
    "title": "Ubuntu多cuda版本控制",
    "section": "",
    "text": "为了学习CenterNet，配置环境弄了半天。。由于我是主用tensorflow的，pytorch搞不来，只能按他的步骤来。他的环境比较老，是cuda 9.0 cudnn 7.1的，然而我早就在用cuda 10.1 cudnn 7.5了，所以我还得安装这个版本的cuda。\n下面我就说下安装多个版本的cuda的注意点。\n\n\n安装cuda 9.0\n下载好了之后执行(因为我是18.04 所以要加override避免gcc版本不匹配的无法安装问题)：\nsudo sh cuda_9.0.176_384.81_linux.run --override\n记得安装过程中下面两点要选no：\n\nInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 375.26?\nDo you want to install a symbolic link at /usr/local/cuda?\n\n\n\n安装cudnn\n下载好了之后：\ntar -xvf cudnn-9.0-linux-x64-v7.tgz\nsudo cp cuda/include/cudnn.h /usr/local/cuda-9.0/include/\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda-9.0/lib64/\nsudo chmod a+r /usr/local/cuda-9.0/include/cudnn.h\nsudo chmod a+r /usr/local/cuda-9.0/lib64/libcudnn*\n\n\n修改环境变量\n改为如下：\nexport CUDA_HOME=/usr/local/cuda\nexport PATH=$PATH:$CUDA_HOME/bin\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\n\n设置版本切换器：\nsudo update-alternatives --install /usr/local/cuda cuda /usr/local/cuda-9.0 40\nsudo update-alternatives --install /usr/local/cuda cuda /usr/local/cuda-10.0 50 \n然后输入sudo update-alternatives --config cuda即可选择版本：\nThere are 2 choices for the alternative cuda (providing /usr/local/cuda).\n\n  Selection    Path                  Priority   Status\n------------------------------------------------------------\n* 0            /usr/local/cuda-10.0   50        auto mode\n  1            /usr/local/cuda-10.0   50        manual mode\n  2            /usr/local/cuda-9.0    40        manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number:"
  },
  {
    "objectID": "posts/nand2tetris-week1.html",
    "href": "posts/nand2tetris-week1.html",
    "title": "Nand2Tetris week1",
    "section": "",
    "text": "准备系统学习计算机体系结构，网上的课程还是挺多的，不过感觉coursera上这门从与非门到俄罗斯方块感觉好一些，虽然学校排名不高，但基于项目导向会比较容易吸收一点。\n其他的课我找到有，有兴趣的同学也可以看看： 1. 伯克利计算机体系结构cs152"
  },
  {
    "objectID": "posts/nand2tetris-week1.html#交换律",
    "href": "posts/nand2tetris-week1.html#交换律",
    "title": "Nand2Tetris week1",
    "section": "交换律",
    "text": "交换律\n\\[\n\\begin{aligned}\n  x \\land y = y \\land x \\\\\n  x \\lor y = y \\lor x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/nand2tetris-week1.html#结合律",
    "href": "posts/nand2tetris-week1.html#结合律",
    "title": "Nand2Tetris week1",
    "section": "结合律",
    "text": "结合律\n\\[\n\\begin{aligned}\n  x \\land (y \\land z) = (x \\land y) \\land z \\\\\n  x \\lor (y \\lor z) = (x \\lor y) \\lor z \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/nand2tetris-week1.html#分配律",
    "href": "posts/nand2tetris-week1.html#分配律",
    "title": "Nand2Tetris week1",
    "section": "分配律",
    "text": "分配律\n\\[\n\\begin{aligned}\n  x \\land (y \\lor z) = (x \\land y) \\lor (x \\land z) \\\\\n  x \\lor (y \\land z) = (x \\lor y) \\land (x \\lor z)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/nand2tetris-week1.html#摩根定律",
    "href": "posts/nand2tetris-week1.html#摩根定律",
    "title": "Nand2Tetris week1",
    "section": "摩根定律",
    "text": "摩根定律\n\\[\n\\begin{aligned}\n  \\lnot ( x \\land y ) = \\lnot x \\lor \\lnot y \\\\\n  \\lnot ( x \\lor y ) = \\lnot x \\lor \\lnot y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/nand2tetris-week1.html#example",
    "href": "posts/nand2tetris-week1.html#example",
    "title": "Nand2Tetris week1",
    "section": "example",
    "text": "example\n\\[\n\\begin{aligned}\n  & \\lnot (\\lnot (x) \\land \\lnot(x \\lor y)) \\\\\n  = &  \\lnot (\\lnot (x) \\land (\\lnot(x)  \\land \\lnot (y))) \\\\\n  = & \\lnot((\\lnot (x) \\land \\lnot(x))  \\land \\lnot (y))) \\\\\n  = & \\lnot(\\lnot (x) \\land \\lnot (y))) \\\\\n  = & \\lnot(\\lnot (x \\lor y)) \\\\\n  = & (x \\lor y)\n\\end{aligned}\n\\]\n或者可以用真值表的方法得到化简的结果。"
  },
  {
    "objectID": "posts/nand2tetris-week1.html#从真值表推导出公式",
    "href": "posts/nand2tetris-week1.html#从真值表推导出公式",
    "title": "Nand2Tetris week1",
    "section": "从真值表推导出公式",
    "text": "从真值表推导出公式\n\n\n\n\n\n\n\n\n\n\n\n\nx\ny\nz\nf\n$x y z $\n$x y z $\n$x y z $\n\n\n\n\n0\n0\n0\n1\n1\n0\n0\n\n\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n1\n0\n1\n0\n\n\n0\n1\n1\n0\n0\n0\n0\n\n\n1\n0\n0\n1\n0\n0\n1\n\n\n1\n0\n1\n0\n0\n0\n0\n\n\n1\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n0\n0\n\n\n\n最后把三个分步骤的都用或进行连接就得到了一个公式。"
  },
  {
    "objectID": "posts/nand2tetris-week1.html#利用nand构建任意操作",
    "href": "posts/nand2tetris-week1.html#利用nand构建任意操作",
    "title": "Nand2Tetris week1",
    "section": "利用NAND构建任意操作",
    "text": "利用NAND构建任意操作\n\nNAND = (not (x and y))\n\n\n\nx\ny\nNAND\n\n\n\n\n0\n0\n1\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n\n\\(\\lnot x = x \\text{ nand } x\\)\n\\(x \\land y = \\lnot(x \\text{ nand } y)\\)\n\\(x \\lor y = \\lnot x \\text{ nand } \\lnot y\\)"
  },
  {
    "objectID": "posts/nand2tetris-week3.html",
    "href": "posts/nand2tetris-week3.html",
    "title": "Nand2Tetris week3",
    "section": "",
    "text": "第三周，目标是构建main memory unit，也就是ram。而内存逻辑需要基于时钟循序，所以与之前的逻辑门电路不同。\n\n\nSequential Logic\n\n使用相同的硬件做同样的事情\n需要记录历史状态\n\n利用一个波形去记录时间是非常直观的想法。\n\n所以序列函数的输入是上个时间步的结果 \\[\n\\begin{aligned}\n  out[t]=function(in[t-1])\n\\end{aligned}\n\\]\n\n\nFlip Flop\n我们需要一个硬件去记录上一个时间步的状态。这个硬件被称为D Flip Flop,DFF的公式为out[t]=in[t-1]，他仅仅就是单纯的保留状态，然后需要利用DFF来构建出一个1bit的寄存器:\n\nBit的逻辑也比较简单，通过上一时刻的load与in的状态来决定当前时刻的输出：\nif (load(t-1)){\n    out = in(t-1);\n}else{\n    out = out(t-1);\n}\n不过这门课程是没有教如何从NAND去构建D Flip Flop门。\n\n\nMemory Units\n这一课主要介绍的RAM的实现。\n首先将1bit寄存器进行泛化可以得到多位的寄存器，可以把多个bit看作一个word，此课程主要讨论16bit的cpu构建。\n\n\n寄存器的读取\n\n读取就直接检查当前时间节点的输出。\n\n寄存器的写入\n\nset in = v;\nset load = 1;\n接下来寄存器内部就存储了v，然后下一个周期的输出即为v。\n\nRAM的读取\n\nRAM就是由一系列的寄存器构成的，通过一个address去索引对应位置的寄存器，然后进行寄存器读写操作。\n\n\n\nCounters\nCounters就是一种硬件，他支持重置为0，自增以及随机设置值。实际中我们把他当作PC来使用，用于读取代码并执行。\n\n\nProject\n// This file is part of www.nand2tetris.org\n// and the book \"The Elements of Computing Systems\"\n// by Nisan and Schocken, MIT Press.\n// File name: projects/03/a/Bit.hdl\n\n/**\n * 1-bit register:\n * If load[t] == 1 then out[t+1] = in[t]\n *                 else out does not change (out[t+1] = out[t])\n */\n\nCHIP Bit {\n    IN in, load;\n    OUT out;\n\n    PARTS:\n    // Put your code here:\n    Mux(a=gayout,b=in,sel=load,out=a);\n    DFF(in=a,out=out,out=gayout);\n}\n// This file is part of www.nand2tetris.org\n// and the book \"The Elements of Computing Systems\"\n// by Nisan and Schocken, MIT Press.\n// File name: projects/03/a/PC.hdl\n\n/**\n * A 16-bit counter with load and reset control bits.\n * if      (reset[t] == 1) out[t+1] = 0\n * else if (load[t] == 1)  out[t+1] = in[t]\n * else if (inc[t] == 1)   out[t+1] = out[t] + 1  (integer addition)\n * else                    out[t+1] = out[t]\n */\n\nCHIP PC {\n    IN in[16],load,inc,reset;\n    OUT out[16];\n\n    PARTS:\n    // Put your code here:\n    Inc16(in=gayout, out=add);\n    Mux16(a=gayout, b=add, sel=inc, out=incout);\n    Mux16(a=incout, b=in, sel=load, out=loadout);\n    Mux16(a=loadout, b=false, sel=reset, out=resetout);\n    Register(in=resetout, load=true, out=out, out=gayout);\n}\n// This file is part of www.nand2tetris.org\n// and the book \"The Elements of Computing Systems\"\n// by Nisan and Schocken, MIT Press.\n// File name: projects/03/a/RAM64.hdl\n\n/**\n * Memory of 64 registers, each 16 bit-wide. Out holds the value\n * stored at the memory location specified by address. If load==1, then \n * the in value is loaded into the memory location specified by address \n * (the loaded value will be emitted to out from the next time step onward).\n */\n\nCHIP RAM64 {\n    IN in[16], load, address[6];\n    OUT out[16];\n\n    PARTS:\n    // Put your code here:\n    DMux8Way(in=load, sel=address[3..5], a=load0, b=load1, c=load2, d=load3, e=load4, f=load5, g=load6, h=load7);\n    RAM8(in=in, load=load0, address=address[0..2], out=out0);\n    RAM8(in=in, load=load1, address=address[0..2], out=out1);\n    RAM8(in=in, load=load2, address=address[0..2], out=out2);\n    RAM8(in=in, load=load3, address=address[0..2], out=out3);\n    RAM8(in=in, load=load4, address=address[0..2], out=out4);\n    RAM8(in=in, load=load5, address=address[0..2], out=out5);\n    RAM8(in=in, load=load6, address=address[0..2], out=out6);\n    RAM8(in=in, load=load7, address=address[0..2], out=out7);\n    Mux8Way16(a=out0, b=out1, c=out2, d=out3, e=out4, f=out5, g=out6, h=out7, sel=address[3..5], out=out);\n    \n}\n\n// This file is part of www.nand2tetris.org\n// and the book \"The Elements of Computing Systems\"\n// by Nisan and Schocken, MIT Press.\n// File name: projects/03/a/RAM8.hdl\n\n/**\n * Memory of 8 registers, each 16 bit-wide. Out holds the value\n * stored at the memory location specified by address. If load==1, then \n * the in value is loaded into the memory location specified by address \n * (the loaded value will be emitted to out from the next time step onward).\n */\n\nCHIP RAM8 {\n    IN in[16], load, address[3];\n    OUT out[16];\n\n    PARTS:\n    // Put your code here:\n    DMux8Way(in=load, sel=address, a=load0, b=load1, c=load2, d=load3, e=load4, f=load5, g=load6, h=load7);\n    Register(in=in, load=load0, out=out0);\n    Register(in=in, load=load1, out=out1);\n    Register(in=in, load=load2, out=out2);\n    Register(in=in, load=load3, out=out3);\n    Register(in=in, load=load4, out=out4);\n    Register(in=in, load=load5, out=out5);\n    Register(in=in, load=load6, out=out6);\n    Register(in=in, load=load7, out=out7);\n    Mux8Way16(a=out0, b=out1, c=out2, d=out3, e=out4, f=out5, g=out6, h=out7, sel=address, out=out);\n}\n\n// This file is part of www.nand2tetris.org\n// and the book \"The Elements of Computing Systems\"\n// by Nisan and Schocken, MIT Press.\n// File name: projects/03/a/Register.hdl\n\n/**\n * 16-bit register:\n * If load[t] == 1 then out[t+1] = in[t]\n * else out does not change\n */\n\nCHIP Register {\n    IN in[16], load;\n    OUT out[16];\n\n    PARTS:\n    // Put your code here:\n    Bit(in=in[0], load=load, out=out[0]);\n    Bit(in=in[1], load=load, out=out[1]);\n    Bit(in=in[2], load=load, out=out[2]);\n    Bit(in=in[3], load=load, out=out[3]);\n    Bit(in=in[4], load=load, out=out[4]);\n    Bit(in=in[5], load=load, out=out[5]);\n    Bit(in=in[6], load=load, out=out[6]);\n    Bit(in=in[7], load=load, out=out[7]);\n    Bit(in=in[8], load=load, out=out[8]);\n    Bit(in=in[9], load=load, out=out[9]);\n    Bit(in=in[10], load=load, out=out[10]);\n    Bit(in=in[11], load=load, out=out[11]);\n    Bit(in=in[12], load=load, out=out[12]);\n    Bit(in=in[13], load=load, out=out[13]);\n    Bit(in=in[14], load=load, out=out[14]);\n    Bit(in=in[15], load=load, out=out[15]);\n}"
  },
  {
    "objectID": "posts/ncnn-learn.html",
    "href": "posts/ncnn-learn.html",
    "title": "ncnn学习",
    "section": "",
    "text": "对ncnn学习的一些汇总。"
  },
  {
    "objectID": "posts/ncnn-learn.html#块对齐-block-aligin",
    "href": "posts/ncnn-learn.html#块对齐-block-aligin",
    "title": "ncnn学习",
    "section": "块对齐 (block aligin)",
    "text": "块对齐 (block aligin)\n这是nihui对于mat内存排布的说明。\nmat shape w=3 h=2 c=4\n\ninternal memory layout\n[(a00 a01 a02) (a10 a11 a12) pad pad]\n[(b00 b01 b02) (b10 b11 b12) pad pad]\n[(c00 c01 c02) (c10 c11 c12) pad pad]\n[(d00 d01 d02) (d10 d11 d12) pad pad]\n\neach channel is 16byte aligned, \npadding values may be filled in channel gaps\n\nmat.data -&gt; address of a00\nmat.row(1) -&gt; address of a10\nmat.channel(0).row(1) -&gt; address of a10\nmat.channel(1).row(1) -&gt; address of b10\n\nby channel alignSize\nncnn通常把以单个通道的图像(h*w)进行读取，然后进行一些卷积操作，所以要by channel的对数据进行对齐，考虑到对于不同的元素有不同的elemsize,同时在分配时还需要对内存块大小进行对齐，为了快速的对hw的内存进行读写，他这里默认分配16bit的倍数。\n    // element size in bytes\n    // 4 = float32/int32\n    // 2 = float16\n    // 1 = int8/uint8\n    // 0 = empty\n    size_t elemsize;\n比如我们申请float矩阵w=3,h=9,c=4，那么每一个channel的内存块本来应该是3*9=27 byte = 27*elemsize = 108 bit，然而108不是16的倍数，所以用alignSize计算到最小16的倍数为112，然后再除elemsize得到cstep为28。\n    cstep = alignSize((size_t)w * h * elemsize, 16) / elemsize;\n\n\nwhole alignSize\n然后根据上述思路，整体的大小是以4倍大小分配的。\nsize_t totalsize = alignSize(total() * elemsize, 4);"
  },
  {
    "objectID": "posts/ncnn-learn.html#内存申请-malloc",
    "href": "posts/ncnn-learn.html#内存申请-malloc",
    "title": "ncnn学习",
    "section": "内存申请 (Malloc)",
    "text": "内存申请 (Malloc)\n同时申请到的内存位置也需要对齐在内存上，便于我们的cpu整块读取，下面是整体的函数，我这里执行的是posix_memalign，不过还是有必要讲讲具体思路。\n#if __AVX__\n// the alignment of all the allocated buffers\n#define MALLOC_ALIGN 32\n#else\n// the alignment of all the allocated buffers\n#define MALLOC_ALIGN 16\n#endif\n\nstatic inline void* fastMalloc(size_t size)\n{\n#if _MSC_VER\n    return _aligned_malloc(size, MALLOC_ALIGN);\n#elif (defined(__unix__) || defined(__APPLE__)) && _POSIX_C_SOURCE &gt;= 200112L || (__ANDROID__ && __ANDROID_API__ &gt;= 17)\n    void* ptr = 0;\n    if (posix_memalign(&ptr, MALLOC_ALIGN, size))\n        ptr = 0;\n    return ptr;\n#elif __ANDROID__ && __ANDROID_API__ &lt; 17\n    return memalign(MALLOC_ALIGN, size);\n#else\n    unsigned char* udata = (unsigned char*)malloc(size + sizeof(void*) + MALLOC_ALIGN);\n    if (!udata)\n        return 0;\n    unsigned char** adata = alignPtr((unsigned char**)udata + 1, MALLOC_ALIGN);\n    adata[-1] = udata;\n    return adata;\n#endif\n}\n\nfastMalloc\n\nmalloc\n假设我是内存对齐MALLOC_ALIGN是32位。\nunsigned char* udata = (unsigned char*)malloc(size + sizeof(void*) + MALLOC_ALIGN);\n这里加上sizeof(void*)是为了用来保存原始malloc出来的数据空间，然后加上MALLOC_ALIGN的大小，因为我们要进行多大的对齐，我们需要偏移的大小总是在0~MALLOC_ALIGN中，所以加上MALLOC_ALIGN即可。\n如果我的size是112，那么实际申请的大小是112+8+32=152，假设我这里申请到的内存为0x555555b1cb30。\n\n\nalign ptr\n接下来我们要对刚刚申请的udata进行一系列操作，首要的事情就是要把起步的内存块进行一个内存对齐，我们要对一个指针操作，那么需要先转成指针的指针，并且需要考虑到对内存进行偏移之后，我们需要保存原来的block的起始地址用于释放内存，否则就会出现问题，ncnn的思路就是原来申请的内存块的头部存放着原始地址，后面一块数据才是实际使用的。\n所以我们会跳过一个小的内存开始对齐，其中对齐的方法也是和找到MALLOC_ALIGN：\nunsigned char **adata = alignPtr((unsigned char **)udata + 1, MALLOC_ALIGN);\n然后我们返回对齐的指针adata，然后把adata前面一个地址空间保存raw的地址，最终的内存分布如下图所示：\n             align with MALLOC_ALIGN\n                0x40\n                 |\n  0x30    0x38   V       0x44     0x48    0x4C      0x50\n     |------|----|--------|--------|--------|--------|\n    head     head  data1     data2    data3    data4\n             addr\n\n\n\nNCNN x86 cpu加速\n想要使用ncnn的一些加速方法，需要从内存管理就开始适配，比如我想给定输入大小进行malloc，ncnn底层就会自动帮我padding到4的倍数，这就需要十分注意。所以这里显示的h，w是正确的，但是cstep并不是6。\nTEST(cpp_lang, ncnn_mat_create_shape)\n{\n    nncase::runtime_shape_t in_shape { 2, 3, 4 };\n    Mat m((int)in_shape[0], (int)in_shape[1], (int)in_shape[2]);\n    cout &lt;&lt; m.cstep &lt;&lt; endl; // 8\n    cout &lt;&lt; m.w  &lt;&lt; endl; // 2\n    cout &lt;&lt; m.h  &lt;&lt; endl; // 3\n    cout &lt;&lt; in_shape[0] * in_shape[1] &lt;&lt; endl; // 6\n}\n\nelempack\n首先获得输入的elemsize和elempack，然后默认输出的out_elempack=1\n    int w = bottom_blob.w;\n    int h = bottom_blob.h;\n    int channels = bottom_blob.c;\n    size_t elemsize = bottom_blob.elemsize;\n    int elempack = bottom_blob.elempack;\n\n    const int kernel_extent_w = dilation_w * (kernel_w - 1) + 1;\n    const int kernel_extent_h = dilation_h * (kernel_h - 1) + 1;\n\n    Mat bottom_blob_bordered;\n    make_padding(bottom_blob, bottom_blob_bordered, opt);\n    if (bottom_blob_bordered.empty())\n        return -100;\n\n    w = bottom_blob_bordered.w;\n    h = bottom_blob_bordered.h;\n\n    int outw = (w - kernel_extent_w) / stride_w + 1;\n    int outh = (h - kernel_extent_h) / stride_h + 1;\n    int out_elempack = 1;\n根据平台特性设定out_elempack:\n#if __SSE2__\n    if (opt.use_packing_layout)\n    {\n#if __AVX__\n        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4 : 1; // 如果是AVX，那么256bit一组，所以elempack设置为8\n#else\n        out_elempack = num_output % 4 == 0 ? 4 : 1; \n        // 如果是SSE，那么128bit一组，所以elempack设置为4\n#endif\n    }\n#endif // __SSE2__\n计算输出elemsize,假设原始输入c是3，他的elemsize是4，输入数据是3*4=12， out channel是64，假设elemsize是4，输出数据是64*4=256 , 现在输出要8个一组，所以256/8=32 所以输出的elemsize是32。最后输出top blob申请的内存就是outw,outh,channel=8 (64/8),elemsize=32 (4*8)\n所以ncnn这都是channel通道上的packing，所以对于channel数大的情况下，卷积速度就快。\n    size_t out_elemsize = elemsize / elempack * out_elempack; \n\n    top_blob.create(outw, outh, num_output / out_elempack, out_elemsize, out_elempack, opt.blob_allocator);\n    if (top_blob.empty())\n        return -100;\n\n\n卷积执行操作\nx86的cpu优化主要还是看packing的，所以他的卷积函数选择都是看输入packing大小和输出packing大小，主要我看了一下就这一些选项，其中的卷积函数都是类似的。\nif (elempack == 8 && out_elempack == 8)\nif (elempack == 1 && out_elempack == 8)\nif (elempack == 4 && out_elempack == 8)\nif (elempack == 8 && out_elempack == 1)\nif (elempack == 8 && out_elempack == 4)\n同时这里卷积的输出后，他可能是packing的，所以ncnn还提供了convert_packing函数对pack的mat进行转换，不过他只支持output与pack的值为倍数关系时才能成功转换,这里进行转换之后原来的内存块padding的位置应该是被填充了，然后尾部会多余一些值。\n                                            h*w\n                                    w        w       w\n                                c1 [ 0  1] [ 2  3] [ 4  5] [ p  p]\n                                c2 [ 6  7] [ 8  9] [10 11] [ p  p]\n                                c3 [12 13] [14 15] [16 17] [ p  p]\n                                c4 [18 19] [20 21] [22 23] [ p  p]\n                                            |\n                                            | convert_packing\n                                            v\n                                           h*w\n                w                           w                             w\nc1 [ (0 6 12 18) (1 7 13 19) ] [ (2 8 14 20)  (3 9 15 21) ] [ (4 10 16 22)  (5 11 17 23)] [o o o o o o o o]"
  },
  {
    "objectID": "posts/np-where.html",
    "href": "posts/np-where.html",
    "title": "python 多维数组指定区域中寻找元素索引",
    "section": "",
    "text": "今天想完成一个功能,需要在一个多维数组中的指定区域找到对应元素在整个数组中的索引,因为这个问题描述起来不方便,找了半天也没有找到好的答案.因此就自己尝试了一下np.where果然可以,但是网上的一些例子中都没提到这个的用法,所以记录一下."
  },
  {
    "objectID": "posts/np-where.html#code",
    "href": "posts/np-where.html#code",
    "title": "python 多维数组指定区域中寻找元素索引",
    "section": "CODE",
    "text": "CODE\nimport numpy as np\n\n\nif __name__ == \"__main__\":\n    # 构造数组\n    a = np.zeros((7, 10, 5, 25))\n    # 首先自己设置几个值\n    dim1 = np.random.randint(0, high=7, size=3)\n    dim2 = np.random.randint(0, high=10, size=3)\n    dim3 = np.random.randint(0, high=5, size=3)\n    a[dim1, dim2, dim3, 4] = 1\n    # 我需要在最后一维的第5列找到大于0.7的元素的索引\n    idex1, idex2, idex3 = np.where(a[..., 4] &gt; .7)\n    print('dim1:',dim1, idex1)\n    print('dim2:',dim2, idex2)\n    print('dim3:',dim3, idex3)"
  },
  {
    "objectID": "posts/np-where.html#run",
    "href": "posts/np-where.html#run",
    "title": "python 多维数组指定区域中寻找元素索引",
    "section": "RUN",
    "text": "RUN\ndim1: [6 1 6] [1 6 6]\ndim2: [1 3 1] [3 1 1]\ndim3: [1 4 4] [4 1 4]"
  },
  {
    "objectID": "posts/numpy-slice-range.html",
    "href": "posts/numpy-slice-range.html",
    "title": "numpy中动态范围切片",
    "section": "",
    "text": "今天想把两个不同的形状的数组进行赋值,因为数组形状是动态的,所以要想一个办法进行动态的范围切片.\n\n\n解决方案\n两个数组:\nA = [[3,3,32,3],[32]]\nB = [[3,3,64,3],[64]]\n赋值 B -&gt; A\nimport numpy as np\nA = np.array([np.random.rand(3, 3, 32, 3), np.random.rand(32)])\nB = np.array([np.random.rand(3, 3, 64, 3), np.random.rand(64)])\nfor i in range(len(A)):\n    A[i] = B[i][[slice(0, s) for s in A[i].shape]]\n\nassert np.array_equal(A[0], B[0][:, :, :32, :])\nassert np.array_equal(A[1], B[1][:32])"
  },
  {
    "objectID": "posts/oledfix.html",
    "href": "posts/oledfix.html",
    "title": "OLED错误修复",
    "section": "",
    "text": "这几天画的板子，画的是0.91寸的oled裸屏。但是我写好了驱动程序，对ssd1306写入是没有问题的，但是我的屏幕亮不起来。\n\n\n发现问题\n我对比了中景园电子的原理图，我感觉我自己很难受。 看下面的图：\n\n我的原理图：\n\n\n\n解决问题\n我看了这个屏幕的datasheet发现。这个屏幕的vcc引脚是使能作用，不接或者接地是使能，接高不使能。 所以我把将我的3.3v线割掉了，就成功解决了问题。"
  },
  {
    "objectID": "posts/ownCloud.html",
    "href": "posts/ownCloud.html",
    "title": "ubuntu18.04安装owncloud",
    "section": "",
    "text": "最近实验室闲置了一台主机，就将这个主机搭建成一个公有云给大家使用。 系统版本是ubuntu server。\n\n\n安装Apache\n首先安装apache以供owncloud使用：\nsudo apt install apache2\n需要禁用apache目录列表\nsudo a2dismod autoindex\n开启额外模块\nsudo a2enmod rewrite\nsudo a2enmod headers\nsudo a2enmod env\nsudo a2enmod dir\nsudo a2enmod mime\n重启apache\nsudo systemctl restart apache2\n\n\n安装MariaDB Server\nsudo apt-get install mariadb-server mariadb-client\n安装完成后添加密码\nsudo mysql_secure_installation\n执行后设置密码、移除匿名用户、不允许root登录、删除测试数据库。 接下来登录MariaDB并创建数据库\nsudo mysql -u root -p\n下面的命令是建立数据库，其中的用户名和密码需要替换成自己的。\nCREATE DATABASE owncloud;\nCREATE USER 'oc_user'@'localhost' IDENTIFIED BY 'PASSWORD';\nGRANT ALL ON owncloud.* TO 'oc_user'@'localhost' IDENTIFIED BY 'PASSWORD' WITH GRANT OPTION; \nFLUSH PRIVILEGES;\nEXIT;\n\n\n安装php\n现在owncloud只支持php7.1：\nsudo apt-get install software-properties-common\nsudo add-apt-repository ppa:ondrej/php\nsudo apt update\nsudo apt install php7.1\n接着安装php模块：\nsudo apt-get install php7.1-cli php7.1-common php7.1-mbstring php7.1-gd php7.1-intl php7.1-xml php7.1-mysql php7.1-zip php7.1-curl php7.1-xmlrpc\n安装完成后配置一下：\nsudo vi /etc/php/7.1/apache2/php.ini\n修改以下内容\nfile_uploads = On\nallow_url_fopen = On\nmemory_limit = 256M\nupload_max_file_size = 100M\n重启apache\nsudo systemctl restart apache2\n\n\n下载owncloud\ncd /tmp \nwget https://download.owncloud.org/community/owncloud-10.0.3.zip\n解压并移动文件：\nunzip owncloud-10.0.3.zip\nsudo mv owncloud /var/www/html/owncloud/\n\n\n设置目录和权限\n为了保证owncloud工作，需要设置操作权限：\nsudo chown -R www-data:www-data /var/www/html/owncloud/\nsudo chmod -R 755 /var/www/html/owncloud/\n\n\n完成安装\n需要登录到对应的网址(http://ipadress/owncloud)进行最后的设置： 输入你想要的账户名与密码，以及配置数据库的名字。 \n\n\n完成后的界面"
  },
  {
    "objectID": "posts/pb-to-pkl.html",
    "href": "posts/pb-to-pkl.html",
    "title": "Tensorflow加载pb文件继续训练",
    "section": "",
    "text": "Tensorflow中模型即代码\n上面这句话说的很对,当我们只有一个预训练好的pb文件,我们如何加载这个模型继续训练呢?今天就来解决这个问题."
  },
  {
    "objectID": "posts/pb-to-pkl.html#加载pb文件",
    "href": "posts/pb-to-pkl.html#加载pb文件",
    "title": "Tensorflow加载pb文件继续训练",
    "section": "1. 加载pb文件",
    "text": "1. 加载pb文件\n我们得到了一个.pb文件,不论是提取他的参数还是用他进行推理,都得加载这个文件.请看下面几行代码:\ndef load_model(model, input_map=None):\n    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n    #  or if it is a protobuf file with a frozen graph\n    model_exp = os.path.expanduser(model)\n    if (os.path.isfile(model_exp)):\n        print('Model filename: %s' % model_exp)\n        with gfile.GFile(model_exp, 'rb') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def, input_map=input_map, name='')\n    else:\n        print('Model directory: %s' % model_exp)\n        meta_file, ckpt_file = get_model_filenames(model_exp)\n\n        print('Metagraph file: %s' % meta_file)\n        print('Checkpoint file: %s' % ckpt_file)\n\n        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n\n讲解\n这里是分两种情况,我们就看第一种:直接使用tf.gfile.GFile打开此文件,然后获得当前文件的图定义,读入数据,最后将此图导入."
  },
  {
    "objectID": "posts/pb-to-pkl.html#获得操作",
    "href": "posts/pb-to-pkl.html#获得操作",
    "title": "Tensorflow加载pb文件继续训练",
    "section": "2. 获得操作",
    "text": "2. 获得操作\n一般我们加载了图之后,都是去获得他的占位符去进行输入,然后输出.为了得到所有的权重,我们使用g.get_operations()获得所有的操作节点.\nimport tensorflow as tf\nfrom mobilenetv1.base_func import *\n\n\nif __name__ == \"__main__\":\n    g = tf.get_default_graph()\n    sess = tf.Session()\n    load_model('pretrained/mobilenetv1_1.0.pb')\n    g.get_operations()\n注意:\n导入pb文件之后使用tf.global_variables()等获取变量的方式都是无效的,获得的都是空值.如下所示:\nIn [6]:     tf.global_variables()\nOut[6]: []\n\nIn [7]:     tf.trainable_variables()\nOut[7]: []"
  },
  {
    "objectID": "posts/pb-to-pkl.html#获得tensor",
    "href": "posts/pb-to-pkl.html#获得tensor",
    "title": "Tensorflow加载pb文件继续训练",
    "section": "3. 获得tensor",
    "text": "3. 获得tensor\n当我们有了操作列表之后如何进行读取变量呢?让我们先看看操作列表中的数据:\nIn [11]: optlist\nOut[11]:\n[&lt;tf.Operation 'inputs' type=Placeholder&gt;,\n &lt;tf.Operation 'MobileNetV1/SpaceToBatchND/block_shape' type=Const&gt;,\n &lt;tf.Operation 'MobileNetV1/SpaceToBatchND/paddings' type=Const&gt;,\n &lt;tf.Operation 'MobileNetV1/SpaceToBatchND' type=SpaceToBatchND&gt;,\n &lt;tf.Operation 'MobileNetV1/Conv2d_0_3x3/weights' type=Const&gt;,\n &lt;tf.Operation 'MobileNetV1/Conv2d_0_3x3/weights/read' type=Identity&gt;,\n &lt;tf.Operation 'MobileNetV1/Conv2d_0_3x3/Conv2D' type=Conv2D&gt;,\n &lt;tf.Operation 'MobileNetV1/Conv2d_0_3x3/BatchNorm/beta' type=Const&gt;,\n &lt;tf.Operation 'MobileNetV1/Conv2d_0_3x3/BatchNorm/beta/read' type=Identity&gt;\n我们可以看到这里的这些不是变量,并且种类烦杂,不过我直接说明xxxx/read等操作就是读取预存的权重的操作.因此我们可以直接把这些操作过滤出来.\ndef get_vars_from_optlist(optlist: list)-&gt;list:\n    \"\"\" 从optlist获得所有的变量节点 \"\"\"\n    varlist = [node for node in optlist if '/read' in node.name]\n    return varlist\n现在我们有个对应的读取变量操作列表,但是要读取变量还是要进行转化,因为varlist只是一个操作,还没有变成可运行的tensor,所以我只要在操作名后面加上:0,同时get_tensor_by_name()即可得到对应的tensor\ndef convert_vars_to_tensor(g, varlist: list)-&gt;list:\n    \"\"\" 把varlist中的操作转变为可运行的tensor \"\"\"\n    tensorlist = []\n    for var in varlist:\n        tensorlist.append(g.get_tensor_by_name(var.name+':0'))\n    return tensorlist"
  },
  {
    "objectID": "posts/pb-to-pkl.html#读取变量",
    "href": "posts/pb-to-pkl.html#读取变量",
    "title": "Tensorflow加载pb文件继续训练",
    "section": "4. 读取变量",
    "text": "4. 读取变量\n有了tensorlist,我们可以来读取变量了.为了restore的方便,我将他保存成字典的形式,并且修改每一个key都与原图中的变量名相同,这样restore的时候直接判断名字是否相同即可.\n# 将所有变量存入字典\nvardict = {}\nfor v in tensorlist:\n    vardict[v.name.replace('/read', '')] = sess.run(v)"
  },
  {
    "objectID": "posts/pb-to-pkl.html#保存字典",
    "href": "posts/pb-to-pkl.html#保存字典",
    "title": "Tensorflow加载pb文件继续训练",
    "section": "5. 保存字典",
    "text": "5. 保存字典\n使用下面的这个函数保存我们的vardict\ndef save_pkl(obj, name):\n    with open(name, 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
  },
  {
    "objectID": "posts/pb-to-pkl.html#恢复权重",
    "href": "posts/pb-to-pkl.html#恢复权重",
    "title": "Tensorflow加载pb文件继续训练",
    "section": "6. 恢复权重",
    "text": "6. 恢复权重\n注意: 要恢复权重,你必须得有原图的定义,否则你必须重新写一个.\n\n首先定义一个原图,接下来需要回复权重.\n搜集此图中所有可训练的变量(我这里用except_last控制是否加载最后一层权重)\n加载之前保存的字典文件\n使用tf.assign(),将modelvarlist与pre_weight_dict中名字相同的变量进行赋值,存入optlist\n使用sess.run(optlist),进行赋值操作\n大功告成~\n\ndef restore_form_pkl(sess: tf.Session(), pklpath: str, except_last=True):\n    \"\"\" restore the pre-train weight form the .pkl file\n\n    Parameters\n    ----------\n    sess : tf.Session\n        sess\n    pklpath : str\n        .pkl file path\n    except_last : bool, optional\n        whether load the last layer weight, when you custom the net shouldn't load \n        the layer name scope is 'MobileNetV1/Bottleneck2'\n        (the default is True, which not load the last layer weight)\n    \"\"\"\n    # tf.global_variables() == tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    # filter the last layer weight\n    modelvarlist = [var for var in tf.trainable_variables(scope='MobileNetV1') if not (except_last and 'MobileNetV1/Bottleneck2' in var.name)]\n    pre_weight_dict = load_pkl(pklpath)\n\n    # make sure the number equal\n    var_num = len(modelvarlist)\n\n    # save the opt to list\n    opt_list = []\n    for newv in modelvarlist:\n        for k, oldv in pre_weight_dict.items():\n            if k == newv.name:\n                opt_list.append(tf.assign(newv, oldv))\n\n    # make sure the number equal\n    assert len(opt_list) == var_num\n    # run the assign\n    sess.run(opt_list)"
  },
  {
    "objectID": "posts/piecewise-regression.html",
    "href": "posts/piecewise-regression.html",
    "title": "piecewise regression",
    "section": "",
    "text": "分段线性拟合算法,即将一段y关于x的函数进行分段,每个区间用一个更加简单的函数进行拟合.\n\n分段线性拟合在npu中是很常见的,主要是因为近年来设计的各种激活函数都是各种函数的组合形式,或者说大多类似relu的派生,这些激活函数的在数值是很容易被分段拆分为多个小段的,然后利用硬件加速对多个段进行执行,下面就是一个真实的模型在量化时输出的范围:\n\n可以看到他特别类似Swish激活,虽然网络中没有使用这个激活方式,但是量化后还是统计出这样的激活,可能也表明了Swish激活是有一定道理的.\n代码:\n# 先删除nan的数据\narr = np.array([a for a in arr if not np.isnan(a[1])])\n\n# 1 首先分n-1个段,构造出 kx+b 的标准形式\nx = np.concatenate([arr[:-1, 0:1], arr[1:, 0:1]], 1)\nk = (arr[1:, 1:2] - arr[:-1, 1:2]) / (arr[1:, 0:1] - arr[:-1, 0:1])\nb = arr[:-1, 1:2]\n\n\nwhile len(x) != 15:\n    # 找到当前最小的差值位置\n    i = np.argmin(np.abs(k[:-1] - k[1:]))\n    # 合并他和下一项\n    y0 = x[i, 0] * k[i] + b[i]\n    y1 = x[i + 1, 1] * k[i + 1] + b[i + 1]\n    k[i] = (y1 - y0) / (x[i + 1, 1] - x[i, 0])\n    x[i, 1] = x[i + 1, 1]\n    x = np.delete(x, i + 1, axis=0)\n    k = np.delete(k, i + 1)\n    b = np.delete(b, i + 1)"
  },
  {
    "objectID": "posts/polyhedral-learn.html",
    "href": "posts/polyhedral-learn.html",
    "title": "polyhedral入门学习",
    "section": "",
    "text": "学习关于polyhedral的基础概念."
  },
  {
    "objectID": "posts/polyhedral-learn.html#reversing-a-loop-nest",
    "href": "posts/polyhedral-learn.html#reversing-a-loop-nest",
    "title": "polyhedral入门学习",
    "section": "reversing a loop nest",
    "text": "reversing a loop nest\nfor i in range(4):\n  a[i] = a[i] + 1\n# a[0] = a[0] + 1\n# a[1] = a[1] + 1\n# a[2] = a[2] + 1\n# a[3] = a[3] + 1\n如何在不改变边界的情况下修改循环的顺序,我们可以通过修改索引值的方式调整循环的顺序, 定义一个变换函数把索引进行线性映射.\nT = lambda i: 4 - 1 - i\nfor i in range(4):\n  a[T(i)] = a[T(i)] + 1\n# a[3] = a[3] + 1\n# a[2] = a[2] + 1\n# a[1] = a[1] + 1\n# a[0] = a[0] + 1\n我们可以把这个变换函数写成线性函数的形式\n# T = a*x + b\nT = lambda x: -x + 3"
  },
  {
    "objectID": "posts/polyhedral-learn.html#can-we-reverse-this-loop",
    "href": "posts/polyhedral-learn.html#can-we-reverse-this-loop",
    "title": "polyhedral入门学习",
    "section": "can we reverse this loop?",
    "text": "can we reverse this loop?\n我们可以反转这个循环到另一个吗? 显然不可以,因为他们的改变顺序后,代码的实际执行的结果变了.\nfor i in range(1,4):\n  x[i] = x[i-1]\n#     |\n#     V\nfor i in range(3,0,-1):\n  x[i] = x[i-1]\n\n多面体编译的思想即: 1. 取现有的loop nest 2. 决定需要reorder的迭代位置 3. 提出一个线性函数进行映射到你需要的循环顺序 4. 映射并得到一个新的loop nest"
  },
  {
    "objectID": "posts/polyhedral-learn.html#integer-linear-programming-ilp",
    "href": "posts/polyhedral-learn.html#integer-linear-programming-ilp",
    "title": "polyhedral入门学习",
    "section": "Integer Linear Programming (ILP)",
    "text": "Integer Linear Programming (ILP)\n整数线性规划问题就是专门解决线性变换下的不等式求解问题的.比如我们需要求解(中间不能出现非线性函数,出现两个变量互相作用,):\n\\[\n\\begin{aligned}\n  3 x + 4y +7 \\geq 0 \\\\\n   -3 x - 3 \\leq 0\n\\end{aligned}\n\\]\n这些问题其实是NP-Hard问题,并且有可能同时需要解决上百个变量的情况"
  },
  {
    "objectID": "posts/polyhedral-learn.html#lexicographic-order",
    "href": "posts/polyhedral-learn.html#lexicographic-order",
    "title": "polyhedral入门学习",
    "section": "Lexicographic Order",
    "text": "Lexicographic Order\n词典顺序定义成一种\\(&gt;&gt;\\)或者\\(&lt;&lt;\\)的形式, 他的机制类似于: \\[\n\\begin{aligned}\n[a,b] &gt;&gt; [c,d]  == \\left(a&gt;c\\ ||\\ (a==c \\ \\&\\&\\ b &gt; d)\\right)\n\\end{aligned}\n\\]\n同时要检查他的正确性, 我们需要利用ILP solver多重递归的解析每个分支的有效性."
  },
  {
    "objectID": "posts/polyhedral-learn.html#多个维度下的线性变换问题",
    "href": "posts/polyhedral-learn.html#多个维度下的线性变换问题",
    "title": "polyhedral入门学习",
    "section": "多个维度下的线性变换问题",
    "text": "多个维度下的线性变换问题\nfor i in range(1,4):\n  for j in range(1,3):\n      A[i][j] = A[i-1][j+1]\n此时如果我们交换i和j的循环顺序."
  },
  {
    "objectID": "posts/ppcg-learn.html",
    "href": "posts/ppcg-learn.html",
    "title": "ppcg 学习",
    "section": "",
    "text": "学习一个ppcg的整体流程与细节.\n\n\nppcg启动\nppcg编译好之后生成的ppcg并不是elf文件, 而是一个shell脚本, 实际上需要执行./libs/ppcg.\n\n\nppcg代码转换\n\n转换的核心逻辑是callback, 注册在clang的parser中, 在parser完成后自动进行转换.\n\n\n\nisl打印调试\n\nisl是通过宏构造了一堆isl_${type}_dump和isl_${type}_to_str的函数. 比如isl_schedule_dump/isl_union_set_dump/isl_set_dump等等."
  },
  {
    "objectID": "posts/process-bar.html",
    "href": "posts/process-bar.html",
    "title": "自己实现的进度条库",
    "section": "",
    "text": "为了使程序变得美观大方,所以我写了一个命令行进度条的库,用c语言写的.下面介绍一下库的用法.\n\n\n源代码\nprobar.c:\n/*\n * @Author: Zheng Qihang\n * @Date: 2018-11-16 17:06:20\n * @Last Modified by: Zheng Qihang\n * @Last Modified time: 2018-11-16 17:10:58\n */\n#include &lt;stdarg.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;unistd.h&gt;\n\nstruct PrograssBar_S {\n    int len;\n    int delta;\n    int cnt;\n    char background;\n    char foreground;\n    char label[200];\n    char *pbar;\n};\n/* @brief 全局变量用于保存参数\n *\n * */\nstatic struct PrograssBar_S g_probar= {};\n/* @brief 初始化\n * @ len          : 进度条长度\n * @ delta        : 进度条步进长度\n * @ background   : 进度条背景\n * @ foreground   : 进度条前景\n * @ label        : 进度条左侧图形  不可超过200!\n * */\nvoid probar_init(int len, int delta, char backgroud, char foreground,\n                 const char *label) {\n    g_probar.cnt= 0;\n    g_probar.len= len;\n    g_probar.delta= delta;\n    g_probar.background= backgroud;\n    g_probar.foreground= foreground;\n    char *pg_label= g_probar.label;\n    for (const char *p= label; *p != '\\0'; ++p) { *pg_label++= *p; }\n    *pg_label= '\\0';\n    g_probar.pbar= (char *)malloc(sizeof(char) * g_probar.len);\n    memset((void *)g_probar.pbar, 0x20, g_probar.len);\n    printf(\"%s  [%-*s]   [%05.2f%%]   %s\\r\", g_probar.label, g_probar.len,\n           g_probar.pbar, g_probar.cnt * (100 / (float)g_probar.len), \"\");\n    fflush(stdout);\n    g_probar.cnt++;\n}\n\n/* @brief 绘制进度条\n * @    show_str  : 用于描述进度\n *      最后一个字符传入后缀\\r使进度条消失,\\n使进度条保留\n * */\nvoid probar_show(const char *show_str) {\n    if (g_probar.cnt == g_probar.len) { /* end */\n        printf(\"%s  [%-*s]   [%05.2f%%]   %s\", g_probar.label, g_probar.len,\n               g_probar.pbar, g_probar.cnt * (100 / (float)g_probar.len), show_str);\n        g_probar.cnt++;\n        return;\n    } else if (g_probar.cnt &gt; g_probar.len) {\n        return;\n    }\n    printf(\"%*c\\r\", 80, ' ');\n    fflush(stdout);\n    printf(\"%s  [%-*s]   [%05.2f%%]   %s\\r\", g_probar.label, g_probar.len,\n           g_probar.pbar, g_probar.cnt * (100 / (float)g_probar.len), show_str);\n    fflush(stdout);\n    if (g_probar.cnt &gt; 0) { g_probar.pbar[g_probar.cnt - 1]= g_probar.background; }\n    g_probar.pbar[g_probar.cnt]= g_probar.foreground;\n    g_probar.pbar[g_probar.cnt + 1]= '\\0';\n    g_probar.cnt++;\n}\n\n/* @brief 清空变量\n *\n * */\nvoid probar_delete() {\n    g_probar.cnt= 0;\n    g_probar.len= 0;\n    g_probar.delta= 0;\n    g_probar.background= '=';\n    g_probar.foreground= '&gt;';\n    free(g_probar.pbar);\n    printf(\"%*c\\r\", 80, ' ');\n    fflush(stdout);\n}\nprobar.h:\n/*\n * @Author: Zheng Qihang \n * @Date: 2018-11-16 17:06:22 \n * @Last Modified by:   Zheng Qihang \n * @Last Modified time: 2018-11-16 17:06:22 \n */\n#ifndef __PROBAR_H\n#define __PROBAR_H\n\nextern void probar_init(int len, int delta, char backgroud, char foreground,\n                        const char *label);\nextern void probar_delete();\nextern void probar_show(const char *show_str);\n\n#endif\n\n\n用法\nmain.c\n#include \"probar.h\"\n#include &lt;unistd.h&gt;\nint main(int argc, char const *argv[]) {\n    /* 传入 总长度 步进长度 背景字符 前景字符 显示字 即可 */\n    probar_init(12, 1, '=', '&gt;', \"\\e[0;33m[  WAIT  ] \\e[0m\");\n    for (int i= 0; i &lt; 12; ++i) {\n        /* 最后一个字符传入后缀\\r使进度条消失 */\n        char *s[12]= {\"这\", \"是\", \"进\", \"度\", \"条\", \"库\",\n                      \"的\", \"简\", \"单\", \"示\", \"例\", \"!\\r\"};\n        probar_show(s[i]);\n        usleep(1000 * 300);\n    }\n    probar_delete();\n\n    probar_init(12, 1, '=', '&gt;', \"\\e[0;33m[  WAIT  ] \\e[0m\");\n    for (int i= 0; i &lt; 12; ++i) {\n        /* 最后一个字符传入后缀\\n使进度条保留 */\n        char *s[12]= {\"这\", \"是\", \"进\", \"度\", \"条\", \"库\",\n                      \"的\", \"简\", \"单\", \"示\", \"例\", \"!\\n\"};\n        probar_show(s[i]);\n        usleep(1000 * 300);\n    }\n    probar_delete();\n    return 0;\n}\n\n\n编译运行\n➜  Cpp_study gcc probar.c test.c\n➜  Cpp_study ./a.out -s\n效果如下:"
  },
  {
    "objectID": "posts/pulseaudio-compile.html",
    "href": "posts/pulseaudio-compile.html",
    "title": "pulseaudio交叉编译",
    "section": "",
    "text": "交叉编译pulseaudio到树莓派中并调用相应api.但是要交叉编译一个pulseaudio需要先交叉编译十几个依赖,这个是非常麻烦的事情.所以我找了一个简单的方法.\n\n\n编译\n\n下载buildroot\n\n我下载的是最新的稳定版\n\n配置\n\n为树莓派配置:\ncd buildroot-2018.11.3\nmake raspberrypi3_defconfig\n然后配置外部交叉编译链,参考https://blog.csdn.net/flfihpv259/article/details/51970370 Toolchain path不需要是bin目录的上一层. 记得Toolchain prefix改成${ARCH}-linux-gnueabihf\n然后sudo make\n\n编译\n\n当buildroot编译完成之后,在output目录下面的host文件中即包含一整套的交叉编译链,这个真的太好了~\n我将/home/zqh/Documents/buildroot-2018.11.3/output/host/bin添加到path,然后使用如下命令去编译pulseaudio的例子即可:\narm-linux-gnueabihf-gcc testrecord.c -lpulse -lpulse-simple\n大功告成~"
  },
  {
    "objectID": "posts/python-trick.html",
    "href": "posts/python-trick.html",
    "title": "一些python中的小坑",
    "section": "",
    "text": "最近在重度使用 python , 记录一下一些进阶使用时的问题.\n\n\n小心输入参数为可能为生成器\n给一个例子如下,我们的 node 需要接受一个 tuple 参数\nclass Node():\n  def __init__(self, op: str, args: tuple) -&gt; None:\n    self.op = op\n    self.args = args\n\n  def __repr__(self) -&gt; str:\n    if self.args:\n      return '{' + f\" {self.op}({','.join(str(arg) for arg in self.args)}) \" + '}'\n    else:\n      return f' {self.op}() '\n但是我们可能在构造的时候没有注意到实际传入的变量类型是什么,当我们调用 node 第一次与第二次时,可以观察到出现了不同的结果:\nnode = Node('a', (Node('a' + str(i), ()) for i in range(10)))\nnode\n# { a( a0() , a1() , a2() , a3() , a4() , a5() , a6() , a7() , a8() , a9() ) }\n\nnode\n# { a() }\n这其实就是 python3 里面的一个 feature 那就是列表推导默认得到生成器, 结果我一不小心踩进去找了半天 bug. 所以我们在构造节点的时候需要这样:\nnode = Node('a', tuple(Node('a' + str(i), ()) for i in range(10)))\n\n\n__new__ 的时候默认参数的问题\n比如我们这样定义一个类,但是用默认参数初始化(无参数初始化)就会报错, 比如得在 overwrite 的 __new__ 那边加上默认参数才可以,我属实是没明白.\nclass Base():\n  def __new__(cls, name, attr):\n    obj = super().__new__(cls)\n    obj.name = name\n    obj.attr = attr\n    return obj\n\n  def __init__(self, value = 0) -&gt; None:\n    self.value = value\n\n  def __repr__(self) -&gt; str:\n    return f'{self.name} {self.attr}: {self.value}'\n\nclass Var(Base):\n  def __new__(cls, value: int): # &lt;- 必须在这里也加上默认参数\n    return super().__new__(cls, 'var', 'constant')\n\n\n__future__.annotations与type hint反射冲突\n通常情况下__future__.annotations是用来解决当前type hint的类型还没有被声明就被使用的情况,但是如果像我这样拿type hint来做自省,那么就会遇到问题,也就是自省得到的结果从一个type被转变为了一个str,就做不了更多的事情了.\n# from __future__ import annotations\nfrom enum import Enum\n\n\nclass color(Enum):\n  r = 0x01\n  g = 0x02\n  b = 0x03\n\n\nclass A():\n  c: color.r\n\nprint(A.__annotations__)\n输出\n# 开启__future__.annotations\n{'c': 'color.r'}\n# 关闭__future__.annotations\n{'c': &lt;color.r: 1&gt;}\n\n\npython与cpp交互时需要注意对象的生命周期\n我们底层在cpp中实现了一个解释器,这个解释器的 pc 指向了一个指令bytes,但是如果在python中以这种形式调用就会出错:\ninterp =  interpreter()\ninterp.load(open('xx','rb').read())\ninterp.run()\n而且报错的行为还是非常诡异的,不开启python的 debug 时候,直接报错 core dump, 如果开启 debug 的时候, 解释器pc读到的指令全部都是0x00.\n实际上问题就是python会自动把我们读出来的指令流回收,因为他认为退出了load的函数域就没有人在使用了,但是实际上我们的指针还指向那块内存. 同时我估计 python debug 的时候分配的内存区域和正常执行时不一样,所以debug状态指针不会报越界错误. 目前的解决方案就是在pybind11外面再用继承的方式重新实现一些类方法,手动保存好程序数据.\n\n\nnamedtuple默认参数为list时\n代码如下, 如果是默认参数为list的NamedTuple,在新创建对象时那两个 list 还是指向同一个 list, 就会导致意外的问题. 其实主要是NamedTuple是通过metaclass构造的.我们没法重写new方法,这个太蛋疼了.\nfrom typing import NamedTuple\n\nclass T(NamedTuple):\n  l1 = []\n  l2 = []\nt1 = T()\nt1.l1.append(1)\nt1.l2.append(2)\n\nt2 = T()\nt2.l1 # [1]\nt2.l2 # [2]\n\n\nproduct 的实现\nfi = [i for i in range(2)]\nfj = [j for j in range(3)]\nfk = [k for k in range(4)]\n\nsequences = [tuple(pool) for pool in [fi, fj, fk]]\naccumulator = [[]]\nfor sequence in sequences:\n  temp = []\n  for acc in accumulator:\n    for item in sequence:\n      temp.append(acc + [item])\n  print(temp)\n  accumulator = temp\n# step 1: [[0], [1]]\n# step 2: [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2]]\n# step 3: [[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 0, 3], [0, 1, 0], [0, 1, 1], [0, 1, 2], [0, 1, 3], [0, 2, 0], [0, 2, 1], [0, 2, 2], [0, 2, 3], [1, 0, 0], [1, 0, 1], [1, 0, 2], [1, 0, 3], [1, 1, 0], [1, 1, 1], [1, 1, 2], [1, 1, 3], [1, 2, 0], [1, 2, 1], [1, 2, 2], [1, 2, 3]]\n\n\nsplit combine 实现\nsequences = [tuple(pool) for pool in [fi, fj, fk]]\naccumulator = [[]]\nfor sequence in sequences:\n  temp = []\n  init = True\n  for item in sequence:\n    if init:\n      for acc in accumulator:\n        temp.append([item] + acc)\n      init = False\n    else:\n      temp.append([item] + accumulator[-1])\n  print(temp)\n  accumulator = temp\n# step 1. [[0], [1], [2], [3], [4]]\n# step 1. [[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [1, 4], [2, 4]]\n# step 1. [[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 0, 3], [0, 0, 4], [0, 1, 4], [0, 2, 4], [1, 2, 4], [2, 2, 4], [3, 2, 4]]\n\n\n打包相关\n主要还是使用setup.py, 在里面可以继承他原本的cmd类来执行自己的命令，主要就是他默认的命令是有一些执行顺序的。\npython setup.py \nStandard commands:\n  build             build everything needed to install\n  build_py          \"build\" pure Python modules (copy to build directory)\n  build_ext         build C/C++ extensions (compile/link to build directory)\n  build_clib        build C/C++ libraries used by Python extensions\n  build_scripts     \"build\" scripts (copy and fixup #! line)\n  clean             (no description available)\n  install           install everything from build directory\n  install_lib       install all Python modules (extensions and pure Python)\n  install_headers   install C/C++ header files\n  install_scripts   install scripts (Python or otherwise)\n  install_data      install data files\n  sdist             create a source distribution (tarball, zip file, etc.)\n  bdist             create a built (binary) distribution\n  bdist_dumb        create a \"dumb\" built distribution\n  bdist_rpm         create an RPM distribution\n  check             perform some checks on the package\n\nExtra commands:     # 这里的命令是用户在setup.py中自定义的部分\n  install_ext       install all Python modules (extensions and pure Python)\n  develop           (no description available)\n  pytests           (no description available)\n  ctests            (no description available)\n  bdist_wheel       create a wheel distribution\n  alias             define a shortcut to invoke one or more commands\n  bdist_egg         create an \"egg\" distribution\n  dist_info         DO NOT CALL DIRECTLY, INTERNAL ONLY: create .dist-info directory\n  easy_install      (no description available)\n  editable_wheel    DO NOT CALL DIRECTLY, INTERNAL ONLY: create PEP 660 editable wheel\n  egg_info          create a distribution's .egg-info directory\n  install_egg_info  Install an .egg-info directory for the package\n  rotate            delete older distributions, keeping N newest files\n  saveopts          save supplied options to setup.cfg or other config file\n  setopt            set an option in setup.cfg or another config file\n  isort             Run isort on modules registered in setuptools\npip install -e .对应的其实是setup.py中的develop命令，"
  },
  {
    "objectID": "posts/pythonnet.html",
    "href": "posts/pythonnet.html",
    "title": "Pythonnet踩坑",
    "section": "",
    "text": "关于python和dotnet互相调用时遇到的一些问题"
  },
  {
    "objectID": "posts/pythonnet.html#pkg-config-失败",
    "href": "posts/pythonnet.html#pkg-config-失败",
    "title": "Pythonnet踩坑",
    "section": "1. pkg-config 失败",
    "text": "1. pkg-config 失败\n报错没有pkg-config,我这里是mac,所以执行:\nbrew install pkg-config\n然后接着报错:\n❯ pkg-config --libs mono-2\nPackage mono-2 was not found in the pkg-config search path.\nPerhaps you should add the directory containing `mono-2.pc'\nto the PKG_CONFIG_PATH environment variable\nNo package 'mono-2' found\n需要添加环境变量PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/usr/lib/pkgconfig:/Library/Frameworks/Mono.framework/Versions/6.12.0/lib/pkgconfig:$PKG_CONFIG_PATH,添加之后能找到正确路径."
  },
  {
    "objectID": "posts/pythonnet.html#clang-编译问题",
    "href": "posts/pythonnet.html#clang-编译问题",
    "title": "Pythonnet踩坑",
    "section": "2. clang 编译问题",
    "text": "2. clang 编译问题\n安装时报错如下\n  clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/lisa/mambaforge/include -arch arm64 -fPIC -O2 -isystem /Users/lisa/mambaforge/include -arch arm64 -I/Users/lisa/mambaforge/include/python3.9 -c src/monoclr/clrmod.c -o build/temp.macosx-11.0-arm64-3.9/src/monoclr/clrmod.o -D_THREAD_SAFE -I/Library/Frameworks/Mono.framework/Versions/6.12.0/lib/pkgconfig/../../include/mono-2.0\n  In file included from src/monoclr/clrmod.c:1:\n  In file included from src/monoclr/pynetclr.h:4:\n  /Users/lisa/mambaforge/include/python3.9/Python.h:25:10: fatal error: 'stdio.h' file not found\n  #include &lt;stdio.h&gt;\n           ^~~~~~~~~\n  1 error generated.\n  error: command '/usr/local/bin/clang' failed with exit code 1\n  ----------------------------------------\n  ERROR: Failed building wheel for pythonnet\n发现是我用的clang是自己从llvm编译的,然后标准的clang就有这个问题.我删除之后重新安装了apple-clang就可以编译了."
  },
  {
    "objectID": "posts/pythonnet.html#import-clr-失败",
    "href": "posts/pythonnet.html#import-clr-失败",
    "title": "Pythonnet踩坑",
    "section": "3. import clr 失败",
    "text": "3. import clr 失败\nImportError                               Traceback (most recent call last)\n~/Documents/nncase/tests/importer/tflite/basic/test_batch_to_space.py in &lt;module&gt;\n----&gt; 16 import clr\n      17 clr.AddReference(\"Nncase.Importer\")\n\nImportError: dlopen(/Users/lisa/mambaforge/lib/python3.9/site-packages/clr.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_mono_assembly_get_image'\n发现他对于除了win32的平台,全部都是用mono,但是我明明就是有dotnetcore的…. 然后发现他的思路是除了win32,先全部用mono去编译,编译好了再自己选择用什么runtime\nfrom clr_loader import get_coreclr\nrt = get_coreclr(\"/Users/lisa/Documents/nncase/runtimeconfig.json\")\nfrom pythonnet import set_runtime\nset_runtime(rt)\nimport clr\n然后我加载.netruntime的lib的时候又遇到一个问题,我用的是x64的.net,然后直接在m1上转译的运行没问题,但是一段编译好的arm代码要去加载那个x64的lib肯定是不行的, 所以我还得装.net6.\n'/usr/local/share/dotnet/host/fxr/5.0.11/libhostfxr.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')),"
  },
  {
    "objectID": "posts/pythonnet.html#从master分支安装",
    "href": "posts/pythonnet.html#从master分支安装",
    "title": "Pythonnet踩坑",
    "section": "4. 从master分支安装",
    "text": "4. 从master分支安装\n发现上一个release其实并不支持dotnet core, 现在尝试从master分支安装.直接拉最新的代码之后:\npip install -e .\n然后运行:\n❯ python\nPython 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:00:30) \n[Clang 11.0.1 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import clr\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/lisa/Documents/pythonnet/clr.py\", line 6, in &lt;module&gt;\n    load()\n  File \"/Users/lisa/Documents/pythonnet/pythonnet/__init__.py\", line 36, in load\n    set_default_runtime()\n  File \"/Users/lisa/Documents/pythonnet/pythonnet/__init__.py\", line 22, in set_default_runtime\n    set_runtime(clr_loader.get_mono())\n  File \"/Users/lisa/mambaforge/lib/python3.9/site-packages/clr_loader/__init__.py\", line 21, in get_mono\n    impl = Mono(\n  File \"/Users/lisa/mambaforge/lib/python3.9/site-packages/clr_loader/mono.py\", line 25, in __init__\n    initialize(\n  File \"/Users/lisa/mambaforge/lib/python3.9/site-packages/clr_loader/mono.py\", line 103, in initialize\n    _MONO = load_mono(libmono)\n  File \"/Users/lisa/mambaforge/lib/python3.9/site-packages/clr_loader/ffi/__init__.py\", line 38, in load_mono\n    return ffi.dlopen(path, ffi.RTLD_GLOBAL)\n  File \"/Users/lisa/mambaforge/lib/python3.9/site-packages/cffi/api.py\", line 150, in dlopen\n    lib, function_cache = _make_ffi_library(self, name, flags)\n  File \"/Users/lisa/mambaforge/lib/python3.9/site-packages/cffi/api.py\", line 832, in _make_ffi_library\n    backendlib = _load_backend_lib(backend, libname, flags)\n  File \"/Users/lisa/mambaforge/lib/python3.9/site-packages/cffi/api.py\", line 827, in _load_backend_lib\n    raise OSError(msg)\nOSError: cannot load library '/Library/Frameworks/Mono.framework/Versions/Current/lib/libmonosgen-2.0.dylib': dlopen(/Library/Frameworks/Mono.framework/Versions/Current/lib/libmonosgen-2.0.dylib, 0x000A): tried: '/Users/lisa/Documents/nncase/out/build/debug/lib/libmonosgen-2.0.dylib' (no such file), '/Users/lisa/Documents/k510-gnne-compiler/out/build/debug/lib/libmonosgen-2.0.dylib' (no such file), '/libmonosgen-2.0.dylib' (no such file), '/Library/Frameworks/Mono.framework/Versions/Current/lib/libmonosgen-2.0.dylib' (fat file, but missing compatible architecture (have 'i386,x86_64', need 'arm64e')), '/usr/local/lib/libmonosgen-2.0.dylib' (no such file), '/usr/lib/libmonosgen-2.0.dylib' (no such file), '/Users/lisa/Documents/nncase/out/build/debug/lib/libmonosgen-2.0.1.dylib' (no such file), '/Users/lisa/Documents/k510-gnne-compiler/out/build/debug/lib/libmonosgen-2.0.1.dylib' (no such file), '/libmonosgen-2.0.1.dylib' (no such file), '/Library/Frameworks/Mono.framework/Versions/6.12.0/lib/libmonosgen-2.0.1.dylib' (fat file, but missing compatible architecture (have 'i386,x86_64', need 'arm64e')), '/usr/local/lib/libmonosgen-2.0.1.dylib' (no such file), '/usr/lib/libmonosgen-2.0.1.dylib' (no such file).  Additionally, ctypes.util.find_library() did not manage to locate a library called '/Library/Frameworks/Mono.framework/Versions/Current/lib/libmonosgen-2.0.dylib'\n发现是mono的lib也是转译的,需要加在arm64的dotnet core. 用上面的切换运行时的方法就可以成功加载了."
  },
  {
    "objectID": "posts/pythonnet.html#python-调用-.net",
    "href": "posts/pythonnet.html#python-调用-.net",
    "title": "Pythonnet踩坑",
    "section": "5. python 调用 .net",
    "text": "5. python 调用 .net\n我发现他这里有点问题,如果只添加Nncase.Importer会提示找不到Nncase的namespace, 需要添加Nncase.Core才可以. 并且好像是不支持嵌套的 namespace导入.反正我没法直接导入Nncase.Importer.TFlite, 并且python调用.net不太好调试, 所以还是放弃了.\nimport clr\nimport sys\n\nclr.AddReference(\n    \"/Users/lisa/Documents/nncase/src/Nncase.Core/bin/Debug/net6.0/Nncase.Core.dll\")\nclr.AddReference(\n    \"/Users/lisa/Documents/nncase/src/Nncase.Importer/bin/Debug/net6.0/Nncase.Importer.dll\")\n\nfrom System import String\nfrom System.Collections import *\nfrom Nncase import Importers"
  },
  {
    "objectID": "posts/pythonnet.html#net-调用-python",
    "href": "posts/pythonnet.html#net-调用-python",
    "title": "Pythonnet踩坑",
    "section": "6. .net 调用 python",
    "text": "6. .net 调用 python\n这里就是需要提供一个环境变量或者手动指定pythonlib的路径比较麻烦, 其他还好.\nusing Xunit;\nusing Nncase;\nusing Nncase.IR;\nusing System.Numerics.Tensors;\nusing System.Collections.Generic;\nusing Python.Runtime;\nusing System;\n\nnamespace Nncase.Tests.Importer\n{\n\n    public class TestPython\n    {\n        [Fact]\n        public void TestNumpy()\n        {\n            Runtime.PythonDLL = \"/Users/lisa/mambaforge/lib/libpython3.9.dylib\";\n            using (Py.GIL())\n            {\n                dynamic np = Py.Import(\"numpy\");\n                Console.WriteLine(np.cos(np.pi * 2));\n\n                dynamic sin = np.sin;\n                Console.WriteLine(sin(5));\n\n                double c = (double)(np.cos(5) + sin(5));\n                Console.WriteLine(c);\n\n                dynamic a = np.array(new List&lt;float&gt; { 1, 2, 3 });\n                Console.WriteLine(a.dtype);\n\n                dynamic b = np.array(new List&lt;float&gt; { 6, 5, 4 }, dtype: np.int32);\n                Console.WriteLine(b.dtype);\n\n                Console.WriteLine(a * b);\n            }\n        }\n    }\n\n\n}"
  },
  {
    "objectID": "posts/pytorch-lighting.html",
    "href": "posts/pytorch-lighting.html",
    "title": "pytorch-lighting隐藏的坑",
    "section": "",
    "text": "最近发现pytorch-lighting比较好用，比我在tensorflow里面自己写的那个好，不过因为他的结构嵌套的比较深，用起来还是会踩坑。这里来记录一下。"
  },
  {
    "objectID": "posts/pytorch-lighting.html#使用pretrain-model提取特征",
    "href": "posts/pytorch-lighting.html#使用pretrain-model提取特征",
    "title": "pytorch-lighting隐藏的坑",
    "section": "使用pretrain model提取特征。",
    "text": "使用pretrain model提取特征。\n官方实例如下\nimport torchvision.models as models\n\nclass ImagenetTransferLearning(LightningModule):\n    def __init__(self):\n        # init a pretrained resnet\n        num_target_classes = 10\n        self.feature_extractor = models.resnet50(pretrained=True)\n        self.feature_extractor.eval()\n\n        # use the pretrained model to classify cifar-10 (10 image classes)\n        self.classifier = nn.Linear(2048, num_target_classes)\n\n    def forward(self, x):\n        representations = self.feature_extractor(x)\n        x = self.classifier(representations)\n        ...\n但是很不幸，在train_step中self.feature_extractor被调用的时候他的状态还是train的，并且由于每次模型将会被调用self.train被重置状态，因此如果需要一个完全固定的预训练模型需要这样，用下面这个类作为基类比较好：\nclass PretrainNet(pl.LightningModule):\n  def train(self, mode: bool):\n    return super().train(False)\n\n  def state_dict(self, destination, prefix, keep_vars):\n    destination = OrderedDict()\n    destination._metadata = OrderedDict()\n    return destination\n\n  def setup(self, device: torch.device):\n    self.freeze()\n我属实被他这个坑了，训练的GAN一直不起作用，因为预训练模型的bn层参数会逐渐被改变，导致越训练模型输出越趋于同一个值。"
  },
  {
    "objectID": "posts/rasp-config.html",
    "href": "posts/rasp-config.html",
    "title": "树莓派修改配置使能串口登陆",
    "section": "",
    "text": "树莓派真的太不人性化了，新的系统永远都是默认不开启串口登陆以及ssh-server的，每次接个显示器不是很累吗？这次我先配好一个，记录一下。\n\n现在貌似最新的系统叫buster了。\n\ncmdline.txt\n\ndwc_otg.lpm_enable=0 console=serial0,115200 console=tty1 root=PARTUUID=b5a65753-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-console\n\nconfig.txt\n\n# For more options and information see\n# http://rpf.io/configtxt\n# Some settings may impact device functionality. See link above for details\n\n# uncomment if you get no picture on HDMI for a default \"safe\" mode\n#hdmi_safe=1\n\n# uncomment this if your display has a black border of unused pixels visible\n# and your display can output without overscan\n#disable_overscan=1\n\n# uncomment the following to adjust overscan. Use positive numbers if console\n# goes off screen, and negative if there is too much border\n#overscan_left=16\n#overscan_right=16\n#overscan_top=16\n#overscan_bottom=16\n\n# uncomment to force a console size. By default it will be display's size minus\n# overscan.\n#framebuffer_width=1280\n#framebuffer_height=720\n\n# uncomment if hdmi display is not detected and composite is being output\n#hdmi_force_hotplug=1\n\n# uncomment to force a specific HDMI mode (this will force VGA)\n#hdmi_group=1\n#hdmi_mode=1\n\n# uncomment to force a HDMI mode rather than DVI. This can make audio work in\n# DMT (computer monitor) modes\n#hdmi_drive=2\n\n# uncomment to increase signal to HDMI, if you have interference, blanking, or\n# no display\n#config_hdmi_boost=4\n\n# uncomment for composite PAL\n#sdtv_mode=2\n\n#uncomment to overclock the arm. 700 MHz is the default.\n#arm_freq=800\n\n# Uncomment some or all of these to enable the optional hardware interfaces\n#dtparam=i2c_arm=on\n#dtparam=i2s=on\n#dtparam=spi=on\n\n# Uncomment this to enable the lirc-rpi module\n#dtoverlay=lirc-rpi\n\n# Additional overlays and parameters are documented /boot/overlays/README\n\n# Enable audio (loads snd_bcm2835)\ndtparam=audio=on\n\n[pi4]\n# Enable DRM VC4 V3D driver on top of the dispmanx display stack\ndtoverlay=vc4-fkms-v3d\nmax_framebuffers=2\n\n[all]\n#dtoverlay=vc4-fkms-v3d\nenable_uart=1"
  },
  {
    "objectID": "posts/retinaface.html",
    "href": "posts/retinaface.html",
    "title": "retinaface总结",
    "section": "",
    "text": "本次主要总结一下retinaface和Ultra-Light-Fast-Generic-Face-Detector-1MB。\n  实际上retinaface和Ultra-Light-Fast-Generic-Face-Detector-1MB的思路都是基于SSD的，本来我做yolo之后准备学习一下SSD的，做完这两个模型也算是学习到了。由于我目前不开源基于tensorflow的训练代码，下面的代码大家仅供参考～\n\n\n网络设计\n  骨干网络实际上随便来，主要就是SSD预测层和YOLO不太一样。\n\nYOLO的预测层\n只使用一个Conv2D 1×1得到anchor_num * (class_num + 5)的输出。\nSSD的预测层\n如果只有bbox输出和class输出，那么使用两个Conv2D 1×1分别得到anchor_num * 4和anchor_num * class_num。不过这样卷积的参数实际上相同的，谁好谁坏得实验下才能知道了。\n\n  这里的标签与YOLO中不太一样，YOLO的标签制作可以看这篇文章。前面的图像增强啥我就不讲了，不过我之前在YOLO里面用了多尺度训练，但是实际上用动态图像裁剪缩放也可以得到相同的效果，所以这里我就没有再用多尺度训练了。\n\n\nanchor生成\n  这个anchor生成和YOLO中的anchor生成方式类似，只不过这里生成的anchor还需要加上中心点的位置，grid的位置加0.5然后乘上比例：\ncx = (j + 0.5) * steps[k] / image_size[1]\ncy = (i + 0.5) * steps[k] / image_size[0]\nanchor的宽度则是当前feature map指定的anchor宽度除以总宽度\ns_kx = min_size[1] / in_hw[1]\ns_ky = min_size[0] / in_hw[0]\n然后把几个feature map上生成的anchor全部连接起来：\noutput = np.concatenate([\n    np.reshape(anchors[0], (-1, 4)),\n    np.reshape(anchors[1], (-1, 4)),\n    np.reshape(anchors[2], (-1, 4))], 0)\n我觉得这样比YOLO中的方式好，YOLO中计算loss的时候需要计算全体的gt，但是算的时候是分开算的，比较蛋疼。\n\n\n标签制作\n\n首先计算gt与anchor的iou\noverlaps = tf_bbox_iou(bbox, self.corner_anchors)\n找到大于阈值的的anchor位置，这个阈值可以设置的小一点，不过太小也不好，那样label中的匹配anchor数量就会太多\nbest_prior_overlap = tf.reduce_max(overlaps, 1)\nbest_prior_idx = tf.argmax(overlaps, 1, tf.int32)\nbest_prior_idx_filter = tf.boolean_mask(best_prior_idx, valid_gt_idx, axis=0)\n找到匹配的anchor的最优gt，并为每个anchor的位置分配给其最匹配的gt\nbest_truth_overlap = tf.reduce_max(overlaps, 0)\nbest_truth_idx = tf.argmax(overlaps, 0, tf.int32)\nbest_truth_overlap = tf.tensor_scatter_nd_update(\n    best_truth_overlap, best_prior_idx_filter[:, None],\n    tf.ones_like(best_prior_idx_filter, tf.float32) * 2.)\nbest_truth_idx = tf.tensor_scatter_nd_update(\n    best_truth_idx, best_prior_idx[:, None],\n    tf.range(tf.size(best_prior_idx), dtype=tf.int32))\n\nmatches = tf.gather(bbox, best_truth_idx)\n设置每个anchor位置对应的置信度，如果其iou score小于阈值则设置为0\n编码生成bbox label，将gt中心坐标转换为相对anchor中心的偏移，且尺度除以移除以(方差*gt宽度)，gt的宽高除anchor宽高并进行对数化再除方差\nlabel_loc = tf_encode_bbox(matches, self.anchors, self.variances)\n编码生成landmark label，计算gt中心坐标对应anchor的中心偏移且尺度除以移除以(方差*anchor宽度)\nmatches_landm = tf.gather(landm, best_truth_idx)\nlabel_landm = tf_encode_landm(matches_landm, self.anchors, self.variances)\n生成calss label，将无效的anchor位置概率设置为0\npython label_conf = tf.gather(clses, best_truth_idx) # filter gt and anchor overlap less than pos_thresh, set as background label_conf = tf.where(best_truth_overlap[:, None] &lt; self.pos_thresh,                           tf.zeros_like(label_conf), label_conf)\n\n\n\n损失计算\n\n直接根据calss label得到mask\nbc_num = tf.shape(y_pred)[0]\nloc_data, landm_data, conf_data = tf.split(y_pred, [4, 10, 2], -1)\nloc_t, landm_t, conf_t = tf.split(y_true, [4, 10, 1], -1)\n# landmark loss\npos_landm_mask = tf.greater(conf_t, 0.)  # get valid landmark num\n根据mask得到有效的landmark label，直接使用smooth_l1_loss\nnum_pos_landm = tf.maximum(tf.reduce_sum(tf.cast(pos_landm_mask, tf.float32)), 1)  # sum pos landmark num\npos_landm_mask = tf.tile(pos_landm_mask, [1, 1, 10])  # 10, 16800, 10\n# filter valid lanmark\nlandm_p = tf.reshape(tf.boolean_mask(landm_data, pos_landm_mask), (-1, 10))\nlandm_t = tf.reshape(tf.boolean_mask(landm_t, pos_landm_mask), (-1, 10))\nloss_landm = tf.reduce_sum(huber_loss(landm_t, landm_p))\n根据mask得到有效的bbox label，直接使用smooth_l1_loss\npos_loc_mask = tf.tile(pos_conf_mask, [1, 1, 4])\nloc_p = tf.reshape(tf.boolean_mask(loc_data, pos_loc_mask), (-1, 4))  # 792,4\nloc_t = tf.reshape(tf.boolean_mask(loc_t, pos_loc_mask), (-1, 4))\nloss_loc = tf.reduce_sum(huber_loss(loc_p, loc_t))\n利用logsumexp将预测出的分类概率求和，得到所有类别概率之和并减去当前这个anchor所负责的类别的概率。\n# Compute max conf across batch for hard negative mining\nbatch_conf = tf.reshape(conf_data, (-1, 2))  # 10,16800,2 -&gt; 10*16800,2\nloss_conf = (tf.reduce_logsumexp(batch_conf, 1, True) -\n             tf.gather_nd(batch_conf,\n                          tf.concat([tf.range(tf.shape(batch_conf)[0])[:, None],\n                                     tf.reshape(conf_t, (-1, 1))], 1))[:, None])\n难例挖掘，根据mask得到所有的负样本概率，进行排序并选择合适的负样本数量。\nloss_conf = loss_conf * tf.reshape(tf.cast(tf.logical_not(pos_conf_mask), tf.float32), (-1, 1))\nloss_conf = tf.reshape(loss_conf, (bc_num, -1))\nidx_rank = tf.argsort(tf.argsort(loss_conf, 1, direction='DESCENDING'), 1)\n\nnum_pos_conf = tf.reduce_sum(tf.cast(pos_conf_mask, tf.float32), 1)\nnum_neg_conf = tf.minimum(lsfn.negpos_ratio * num_pos_conf,\n                          tf.cast(tf.shape(pos_conf_mask)[1], tf.float32) - 1.)\nneg_conf_mask = tf.less(tf.cast(idx_rank, tf.float32),\n                        tf.tile(num_neg_conf, [1, tf.shape(pos_conf_mask)[1]]))[..., None]\n根据正样本的位置和难例挖掘负样本位置，得到需要计算概率值的位置，对这些位置计算其交叉熵。\n# calc pos , neg confidence loss\npos_idx = tf.tile(pos_conf_mask, [1, 1, 2])\nneg_idx = tf.tile(neg_conf_mask, [1, 1, 2])\n\nconf_p = tf.reshape(tf.boolean_mask(\n    conf_data,\n    tf.equal(tf.logical_or(pos_idx, neg_idx), True)), (-1, 2))\nconf_t = tf.boolean_mask(conf_t, tf.equal(tf.logical_or(pos_conf_mask, neg_conf_mask), True))\n\nloss_conf = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(conf_t, conf_p))\n\n\n\n推理\n推理好像没啥好说的\n\n得到输出概率，根据概率过滤得bbox和landmark\n\"\"\" softmax class\"\"\"\nclses = softmax(clses, -1)\nscore = clses[:, 1]\n\"\"\" decode \"\"\"\nbbox = decode_bbox(bbox, h.anchors.numpy(), h.variances.numpy())\nbbox = bbox * np.tile(h.org_in_hw[::-1], [2])\n\"\"\" landmark \"\"\"\nlandm = decode_landm(landm, h.anchors.numpy(), h.variances.numpy())\nlandm = landm * np.tile(h.org_in_hw[::-1], [5])\n\"\"\" filter low score \"\"\"\ninds = np.where(score &gt; obj_thresh)[0]\nbbox = bbox[inds]\nlandm = landm[inds]\nscore = score[inds] \n解码bbox和landmark，然后nms就完事了\n\"\"\" keep top-k before NMS \"\"\"\norder = np.argsort(score)[::-1]\nbbox = bbox[order]\nlandm = landm[order]\nscore = score[order]\n\"\"\" do nms \"\"\"\nkeep = nms_oneclass(bbox, score, nms_thresh)\nbbox = bbox[keep]\nlandm = landm[keep]\nscore = score[keep]\n\"\"\" reverse img \"\"\"\nbbox, landm = reverse_ann(bbox, landm, h.org_in_hw, np.array(orig_hw))\nresults.append([bbox, landm, score])\n\n\n\n总结\n\n总的来说SSD和YOLO的思想都很好，像YOLO中就没有难例挖掘的东西，因为他把所有负样本都计算损失了～\n不过我觉得YOLO中聚类anchor的方式放到SSD里面绝对是有效的，因为原本的SSD的anchor都是方形的，这样肯定效果没有特定比例的anchor效果好，并且我用聚类生成anchor之后，模型收敛明显加快了。\n还有就是SSD的模型没有上采样部分，这样速度虽然快，但是感受野就没法共享了，现在增加SSD模型的感受野的方式可以在模型中添加FPN、SSH、RFB模块。\n最后我最近还是对DIOU很感兴趣的，希望有空可以加上去试试效果。接下来一段时间得搞语音的东西发论文去了。"
  },
  {
    "objectID": "posts/scalar-isa-compile.html",
    "href": "posts/scalar-isa-compile.html",
    "title": "标量指令集编译器简易实现",
    "section": "",
    "text": "之前没有接触过标量isa的编译器该怎么写,所以需要学习一下. 主要参考自RednaxelaFX的寄存器分配问题 以及chibicc简易c编译器."
  },
  {
    "objectID": "posts/scalar-isa-compile.html#函数调用帧栈指针行为",
    "href": "posts/scalar-isa-compile.html#函数调用帧栈指针行为",
    "title": "标量指令集编译器简易实现",
    "section": "函数调用帧栈指针行为",
    "text": "函数调用帧栈指针行为\n#include \"test.h\"\n\nint foo(int a,int b){\n  return a + b;\n}\n\nvoid main(){\n  foo(1,2);\n}\n\n1. 在main函数中\n这里先把imm加载,然后push到栈上,然后再pop到两个寄存器上.开始调用foo\n0040117D: 48 C7 C0 02 00 00 00       movq   $0x2, %rax \n00401184: 50                         pushq  %rax\n00401185: 48 C7 C0 01 00 00 00       movq   $0x1, %rax\n0040118C: 50                         pushq  %rax\n0040118D: 48 8D 05 1F 00 00 00       leaq   0x1f(%rip), %rax  ; foo \n00401194: 5F                         popq   %rdi\n00401195: 5E                         popq   %rsi\n00401196: 49 89 C2                   movq   %rax, %r10\n00401199: 48 C7 C0 00 00 00 00       movq   $0x0, %rax\n004011A0: 41 FF D2                   callq  *%r10 # 此时rsp = 0xbc10, rbp = 0xbca0\n\nstep 1 调用前\n%rbp -&gt; | xxx             |  high \n        | xxx             |   ^\n%rsp -&gt; | main 函数最后参数 |   |\n        | empty           |   |\n        | empty           |   |\n        | empty           |   |\n        | empty           |  low\n\n\nstep 2 开始调用\ncallq  *%r10后的结果如下: 因为call会把call下一条指令的地址压到栈上作为return要用的address.\n%rbp -&gt; | xxx             |  high \n        | xxx             |   ^\n        | main 函数最后参数 |   |\n%rsp -&gt; | return address  |   |\n        | empty           |   |\n        | empty           |   |\n        | empty           |  low\n\n\n\n2. 在foo函数中\n004011B3: 55                         pushq  %rbp # 保存之前的rbp之后, rsp = 0xbc08\n004011B4: 48 89 E5                   movq   %rsp, %rbp\n004011B7: 48 83 EC 10                subq   $0x10, %rsp\n004011BB: 48 89 65 F8                movq   %rsp, -0x8(%rbp)\n004011BF: 89 7D F4                   movl   %edi, -0xc(%rbp)\n004011C2: 89 75 F0                   movl   %esi, -0x10(%rbp)\n004011C5: 48 8D 45 F0                leaq   -0x10(%rbp), %rax\n004011C9: 48 63 00                   movslq (%rax), %rax\n004011CC: 50                         pushq  %rax\n004011CD: 48 8D 45 F4                leaq   -0xc(%rbp), %rax\n004011D1: 48 63 00                   movslq (%rax), %rax\n004011D4: 5F                         popq   %rdi\n004011D5: 01 F8                      addl   %edi, %eax\n004011D7: EB 00                      jmp    0x4011d9\n004011D9: 48 89 EC                   movq   %rbp, %rsp\n004011DC: 5D                         popq   %rbp\n004011DD: C3                         retq   \n\nstep 1\npush %rbq\n⚠️ rsp的push是先递减然后修改对应的值!\n%rbp -&gt; | xxx             |  high \n        | xxx             |   ^\n        | xxx             |   |\n        | return address  |   |\n%rsp -&gt; | old rbp         |   |\n        | empty           |   |\n        | empty           |  low\n\n\nstep 2\nmovq   %rsp, %rbp\n              | xxx             |  high \n              | xxx             |   ^\n              | xxx             |   |\n              | return address  |   |\n%rbp, %rsp -&gt; | old rbp         |   |\n              | empty           |   |\n              | empty           |  low\n\n\nstep 3\nsubq   $0x10, %rsp\n              | xxx             |  high &lt;-┐\n              | xxx             |   ^     |\n              | xxx             |   |     |\n              | return address  |   |     |\n%rbp       -&gt; | old rbp         |   | ----┘\n              | empty           |   |\n              | empty           |   |\n              | empty           |   |\n%rsp       -&gt; | empty           |  low\n\n\nstep 4\n从寄存器中把参数写入内存. 他这里还有存了一个rsp,可能是有别的用途.?\nmovq   %rsp, -0x8(%rbp)\nmovl   %edi, -0xc(%rbp)\nmovl   %esi, -0x10(%rbp)\n              | xxx             |  high &lt;---┐\n              | xxx             |   ^       |\n              | xxx             |   |       |\n              | return address  |   |       |\n%rbp       -&gt; | old rbp         |   | ------┘\n              | empty           |   |\n              | old rsp         |   | ------┐\n              | 1  (arg 0)      |   |       ⏐\n%rsp       -&gt; | 2  (arg 1)      |  low &lt;----┘\n\n\nstep 5\naddl   %edi, %eax这里把计算结果存入eax,eax是rax的一半.\n\n\nstep 6\n004011D7: EB 00                      jmp    0x4011d9\n004011D9: 48 89 EC                   movq   %rbp, %rsp\n返回时, 先jmp到return的位置, 然后rsp指向当前帧顶部:\n              | xxx             |  high &lt;---┐\n              | xxx             |   ^       |\n              | xxx             |   |       |\n              | return address  |   |       |\n%rbp %rsp  -&gt; | old rbp         |   | ------┘\n              | empty           |   |\n              | old rsp         |   | ------┐\n              | 1  (arg 0)      |   |       ⏐\n              | 2  (arg 1)      |  low &lt;----┘\n\n\nstep 7\n004011DC: 5D                         popq   %rbp\n接下来恢复rbp到上一帧的栈顶, 此时rsp指向返回地址.\n%rbp  -&gt;    | xxx             |  high &lt;---┐\n            | xxx             |   ^       |\n            | xxx             |   |       |\n%rsp  -&gt;    | return address  |   |       |\n            | old rbp         |   | ------┘\n            | empty           |   |\n            | old rsp         |   | ------┐\n            | 1  (arg 0)      |   |       ⏐\n            | 2  (arg 1)      |  low &lt;----┘\n\n\nstep 8\n004011DD: C3                         retq \nreturn实际上是先推出rsp的中的值,然后根据此地址进行跳转.这里的return address就是之前call的下一条指令.\n%rbp  -&gt;    | xxx             |  high &lt;---┐\n            | xxx             |   ^       |\n%rsp  -&gt;    | xxx             |   |       |\n            | return address  |   |       |\n            | old rbp         |   | ------┘\n            | empty           |   |\n            | old rsp         |   | ------┐\n            | 1  (arg 0)      |   |       ⏐\n            | 2  (arg 1)      |  low &lt;----┘"
  },
  {
    "objectID": "posts/scalar-isa-compile.html#函数通过栈传参行为分析",
    "href": "posts/scalar-isa-compile.html#函数通过栈传参行为分析",
    "title": "标量指令集编译器简易实现",
    "section": "函数通过栈传参行为分析",
    "text": "函数通过栈传参行为分析\n在x86中,通常通过6个寄存器进行int类型参数传递,分别是rdi, rsi, rdx, rcx, r8, r9. 如果是浮点类型的参数,利用的是8个浮点寄存器.当参数为大的结构体/联合体,或者参数个数超过寄存器能容纳的数量时, 将通过栈传递参数.\n栈传递参数是在caller中进行的, 将函数参数从右到左的压到栈上(便于支持变长参数):\n%rbp -&gt; | xxx             |  high \n        | xxx             |   ^\n        | callee arg 9    |   |\n        | callee arg 8    |   |\n%rsp -&gt; | callee arg 7    |   |\n        | empty           |   |\n        | empty           |   |\n        | empty           |   |\n        | empty           |  low\n压完栈之后进入函数中后,帧栈位置如下:\n%rbp -&gt; | xxx             |  high &lt;---┐\n        | xxx             |   ^       |\n        | callee arg 9    |   |       |\n        | callee arg 8    |   |       |\n        | callee arg 7    |   |       |\n        | return address  |   |       |\n%rsp -&gt; | old rbp         |   | ------┘\n        | empty           |   |\n        | empty           |  low\n在代码生成前我们就需要确定所有的参数是通过寄存器传递还是栈传递,因此在子函数中获取local var只需要给出之前分配变量位置时设定的偏移即可.\n同时要注意,结构体的压栈顺序也是倒序的,例如结构体如下:\ntypedef struct\n{\n    int n;\n    int c;\n    int h;\n    int w;\n} shape_t;\n\ntypedef struct\n{\n    shape_t shape;\n    unsigned int addr;\n} buffer_t;\n压栈的时候是先把栈向下到对应位置,然后向上copy, 最终的数据摆放应该是如下的:\n%rbp -&gt; | xxx             |  high &lt;---┐\n        | xxx             |   ^       |\n        | buffer.addr     |   |       |\n        | buffer.shape.w  |   |       |\n        | buffer.shape.h  |   |       |\n        | buffer.shape.c  |   |       |\n        | buffer.shape.n  |   |       |\n        | return address  |   |       |\n%rsp -&gt; | old rbp         |   | ------┘\n        | empty           |   |\n        | empty           |  low"
  },
  {
    "objectID": "posts/scalar-isa-compile.html#函数调用相对地址计算",
    "href": "posts/scalar-isa-compile.html#函数调用相对地址计算",
    "title": "标量指令集编译器简易实现",
    "section": "函数调用相对地址计算",
    "text": "函数调用相对地址计算\n我才发现在调用函数的时候是通过%rip寄存器去寻址的,给出如下函数:\nint foo(int a) { return a + 1; }\nint foo2(int a) { return foo(a) + 2; }\n\nint main() {\n  int b = 1;\n  foo2(b);\n  return 0;\n}\n编译结果: 注意到下面调用函数时使用了lea foo2(%rip), %rax来获得对应的地址. 然后我查看了%rip的作用是:\n\nThe role of the %rip register The %rip register on x86-64 is a special-purpose register that always holds the memory address of the next instruction to execute in the program’s code segment. The processor increments %rip automatically after each instruction, and control flow instructions like branches set the value of %rip to change the next instruction. Perhaps surprisingly, %rip also shows up when an assembly program refers to a global variable. See the sidebar under “Addressing modes” below to understand how %rip-relative addressing works.\n\n也就是他指向了下一个指令的地址.\nmain:\n  . # 忽略一些指令\n  .\n  .\n  mov $4, %rcx\n  lea -4(%rbp), %rdi\n  mov $0, %al\n  rep stosb\n  lea -4(%rbp), %rax\n  push %rax\n  mov $1, %rax\n  pop %rdi\n  mov %eax, (%rdi)\n  lea -4(%rbp), %rax\n  movsxd (%rax), %rax\n  push %rax\n  lea foo2(%rip), %rax\n  pop %rdi\n  mov %rax, %r10\n  mov $0, %rax\n  call *%r10\n  add $0, %rsp\n  mov $0, %rax\n  jmp .L.return.main\n  mov $0, %rax\n.L.return.main:\n  mov %rbp, %rsp\n  pop %rbp\n  ret\n  .globl foo2\n  .text\n  .type foo2, @function\nfoo2:\n  push %rbp\n  mov %rsp, %rbp\n  sub $16, %rsp\n  mov %rsp, -8(%rbp)\n  mov %edi, -12(%rbp)\n  mov $2, %rax\n  push %rax\n  sub $8, %rsp\n  lea -12(%rbp), %rax\n  movsxd (%rax), %rax\n  push %rax\n  lea foo(%rip), %rax\n  pop %rdi\n  mov %rax, %r10\n  mov $0, %rax\n  call *%r10\n  add $8, %rsp\n  pop %rdi\n  add %edi, %eax\n  jmp .L.return.foo2\n.L.return.foo2:\n  mov %rbp, %rsp\n  pop %rbp\n  ret\n  .globl foo\n  .text\n  .type foo, @function\nfoo:\n  push %rbp\n  mov %rsp, %rbp\n  sub $16, %rsp\n  mov %rsp, -8(%rbp)\n  mov %edi, -12(%rbp)\n  mov $1, %rax\n  push %rax\n  lea -12(%rbp), %rax\n  movsxd (%rax), %rax\n  pop %rdi\n  add %edi, %eax\n  jmp .L.return.foo\n.L.return.foo:\n  mov %rbp, %rsp\n  pop %rbp\n  ret\n接下来我再用gnu as进行汇编得到:\nmain:\n push   rbp\n mov    rbp,rsp\n sub    rsp,0xa0\n mov    QWORD PTR [rbp-0x10],rsp\n mov    DWORD PTR [rbp-0xa0],0x0\n mov    DWORD PTR [rbp-0x9c],0x30\n mov    QWORD PTR [rbp-0x98],rbp\n add    QWORD PTR [rbp-0x98],0x10\n mov    QWORD PTR [rbp-0x90],rbp\n add    QWORD PTR [rbp-0x90],0xffffffffffffff78\n mov    QWORD PTR [rbp-0x88],rdi\n mov    QWORD PTR [rbp-0x80],rsi\n mov    QWORD PTR [rbp-0x78],rdx\n mov    QWORD PTR [rbp-0x70],rcx\n mov    QWORD PTR [rbp-0x68],r8\n mov    QWORD PTR [rbp-0x60],r9\n movsd  QWORD PTR [rbp-0x58],xmm0\n movsd  QWORD PTR [rbp-0x50],xmm1\n movsd  QWORD PTR [rbp-0x48],xmm2\n movsd  QWORD PTR [rbp-0x40],xmm3\n movsd  QWORD PTR [rbp-0x38],xmm4\n movsd  QWORD PTR [rbp-0x30],xmm5\n movsd  QWORD PTR [rbp-0x28],xmm6\n movsd  QWORD PTR [rbp-0x20],xmm7\n mov    rcx,0x4\n lea    rdi,[rbp-0x4]\n mov    al,0x0\n rep stos BYTE PTR es:[rdi],al\n lea    rax,[rbp-0x4]\n push   rax\n mov    rax,0x1\n pop    rdi\n mov    DWORD PTR [rdi],eax\n lea    rax,[rbp-0x4]\n movsxd rax,DWORD PTR [rax]\n push   rax\n lea    rax,[rip+0x0]        # b4 &lt;main+0xb4&gt;\n pop    rdi\n mov    r10,rax\n mov    rax,0x0\n call   r10\n add    rsp,0x0\n mov    rax,0x0\n jmp    d6 &lt;main+0xd6&gt;\n mov    rax,0x0\n mov    rsp,rbp\n pop    rbp\n ret    \nfoo2:\n push   rbp\n mov    rbp,rsp\n sub    rsp,0x10\n mov    QWORD PTR [rbp-0x8],rsp\n mov    DWORD PTR [rbp-0xc],edi\n mov    rax,0x2\n push   rax\n sub    rsp,0x8\n lea    rax,[rbp-0xc]\n movsxd rax,DWORD PTR [rax]\n push   rax\n lea    rax,[rip+0x0]        # 105 &lt;foo2+0x2a&gt;\n pop    rdi\n mov    r10,rax\n mov    rax,0x0\n call   r10\n add    rsp,0x8\n pop    rdi\n add    eax,edi\n jmp    11c &lt;foo2+0x41&gt;\n mov    rsp,rbp\n pop    rbp\n ret    \nfoo:\n push   rbp\n mov    rbp,rsp\n sub    rsp,0x10\n mov    QWORD PTR [rbp-0x8],rsp\n mov    DWORD PTR [rbp-0xc],edi\n mov    rax,0x1\n push   rax\n lea    rax,[rbp-0xc]\n movsxd rax,DWORD PTR [rax]\n pop    rdi\n add    eax,edi\n jmp    144 &lt;foo+0x23&gt;\n mov    rsp,rbp\n pop    rbp\n ret    \n上面有个很奇怪的地方,lea不是应该得到的是foo的地址, 他这里的注释的解释如下:\nlea    rax,[rip+0x0]        # b4 &lt;main+0xb4&gt; , b4 是下一个指令的地址, &lt;main + 0xb4&gt;就是main为0,加上偏移b4\n\nlea    rax,[rip+0x0]        # 105 &lt;foo2+0x2a&gt;, 105 是下一个指令的地址, &lt;foo2 + 0x2a&gt;就是foo2为0xdb,加上偏移2a"
  },
  {
    "objectID": "posts/skimage-mem-leak.html",
    "href": "posts/skimage-mem-leak.html",
    "title": "skimage中resize内存泄漏",
    "section": "",
    "text": "我今天运行个模型,跑着跑着内存就泄漏了,我很奇怪,然后用memory_profiler分析了下内存泄漏的点.发现是skimage的resize中出现了泄漏."
  },
  {
    "objectID": "posts/skimage-mem-leak.html#原本的代码",
    "href": "posts/skimage-mem-leak.html#原本的代码",
    "title": "skimage中resize内存泄漏",
    "section": "原本的代码:",
    "text": "原本的代码:\nimg = resize(img, [int(img.shape[0] * resize_factor),\n           int(img.shape[1] * resize_factor)],\n     preserve_range=True).astype(np.uint8)\n然后分析内存的时候得到如下:\nLine #    Mem usage    Increment   Line Contents\n================================================\n130 871.9648 MiB   0.0000 MiB           img = img_as_ubyte(resize(img, [int(img.shape[0] * resize_factor),\n131 924.0703 MiB  52.1055 MiB                                           int(img.shape[1] * resize_factor)]))\n\n\n130 924.0703 MiB   0.0000 MiB           img = img_as_ubyte(resize(img, [int(img.shape[0] * resize_factor),\n131 946.7148 MiB  22.6445 MiB                                           int(img.shape[1] * resize_factor)]))\n\n130 875.6328 MiB   0.0000 MiB           img = img_as_ubyte(resize(img, [int(img.shape[0] * resize_factor),\n131 932.0742 MiB  56.4414 MiB                                           int(img.shape[1] * resize_factor)]))"
  },
  {
    "objectID": "posts/skimage-mem-leak.html#修改后代码",
    "href": "posts/skimage-mem-leak.html#修改后代码",
    "title": "skimage中resize内存泄漏",
    "section": "修改后代码",
    "text": "修改后代码\n我参考了网上的人,修改之后发现还是不行\nimg = img_as_ubyte(resize(img, [int(img.shape[0] * resize_factor),\n                                int(img.shape[1] * resize_factor)]))\nLine #    Mem usage    Increment   Line Contents\n================================================\n132 795.3477 MiB   0.0000 MiB           img = resize(img, [int(img.shape[0] * resize_factor),\n133 795.3477 MiB   0.0000 MiB                              int(img.shape[1] * resize_factor)],\n134 807.4141 MiB  12.0664 MiB                        preserve_range=True).astype(np.uint8)\n\n\n132 807.4141 MiB   0.0000 MiB           img = resize(img, [int(img.shape[0] * resize_factor),\n133 807.4141 MiB   0.0000 MiB                              int(img.shape[1] * resize_factor)],\n134 823.9102 MiB  16.4961 MiB                        preserve_range=True).astype(np.uint8)\n\n132 824.1680 MiB   0.0000 MiB           img = resize(img, [int(img.shape[0] * resize_factor),\n133 824.1680 MiB   0.0000 MiB                              int(img.shape[1] * resize_factor)],\n134 824.1758 MiB   0.0078 MiB                        preserve_range=True).astype(np.uint8)"
  },
  {
    "objectID": "posts/skimage-mem-leak.html#只测试resize",
    "href": "posts/skimage-mem-leak.html#只测试resize",
    "title": "skimage中resize内存泄漏",
    "section": "只测试resize",
    "text": "只测试resize\n之前的分析文件:\nLine #    Mem usage    Increment   Line Contents\n================================================\n282 880.1875 MiB   0.0000 MiB           if boxes is None:\n283 895.7812 MiB  15.5938 MiB               self._resize_neg_img(im_in, img)\n284                                     else:\n285                                         boxes = self._resize_pos_img(im_in, img, boxes)\n\n\n282 840.2148 MiB   0.0000 MiB           if boxes is None:\n283 889.9688 MiB  49.7539 MiB               self._resize_neg_img(im_in, img)\n284                                     else:\n285                                         boxes = self._resize_pos_img(im_in, img, boxes)\n\n282 889.9688 MiB   0.0000 MiB           if boxes is None:\n283 890.1172 MiB   0.1484 MiB               self._resize_neg_img(im_in, img)\n284                                     else:\n285                                         boxes = self._resize_pos_img(im_in, img, boxes)\n我发现都是_resize_neg_img函数出的问题,然后就只测试这个函数看看有没有问题,发现好像并没有内存泄漏.\n\n我发现内存泄漏还是得在tf.data对象运行起来之后才能发现的.所以我得继续测试.\n这里用了那个line_profiler测试了一下代码执行时间.发现了一个大问题:\nimg = img_as_ubyte(rescale(img, resize_factor, multichannel=True))  # 107486.0ms\nimg = resize(img, None, fx=resize_factor, fy=resize_factor)  # 849.4ms\n!用skimage和opencv的resize速度居然差10倍以上."
  },
  {
    "objectID": "posts/skimage-mem-leak.html#修改读取方式后",
    "href": "posts/skimage-mem-leak.html#修改读取方式后",
    "title": "skimage中resize内存泄漏",
    "section": "修改读取方式后",
    "text": "修改读取方式后\n最后我虽然没有找到哪里出现了问题,但是我首先修改了图像处理函数都使用opencv完成.\n第二,我看了一下tfrecord的制作方式,将数据转换为了tfrecord的存储方式. 制作时,我们可以选择将图像解码前的buf序列化,也可以先读取出来然后用tf.io.serialize_tensor()的方式进行序列化,后者所存储的tfrecord会大10倍以上.我经过测试之后发现,直接存未序列化的图像数据比较好,因为读取的时候对tensor反序列化和解码图像速度差不太多.\ndef make_example(img_string: str, label: int, bbox_string: str):\n    \"\"\" make example \"\"\"\n    feature = {\n        'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_string])),\n        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n        'bbox': tf.train.Feature(bytes_list=tf.train.BytesList(value=[bbox_string])),\n    }\n\n    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n    \nwith tf.io.TFRecordWriter(str(record_file)) as writer:\n    for idx in tqdm(idx_list, total=len(idx_list)):\n        im_buf, label, bboxes = data[idx]\n        bboxes = tf.io.serialize_tensor(bboxes).numpy()\n        serialized_example = make_example(im_buf.tostring(), label, bboxes)\n        writer.write(serialized_example)\n读取代码实例,tfrecord总体来说还是比较简单的,唯一的不方便就是我们调试的时候不方便,我之前都是从列表里面读取图像的,那样调试的时候直接执行就能很快找到问题,现在需要先启动一个小的dataset对象才可以进行图像读取,比较难受.综合考虑还是性价比较高,训练的速度有明显提升:\ndef build_datapipe(self, pos_tfrecord: tf.Tensor, neg_tfrecord: tf.Tensor,\n                   batch_size: int, rand_seed: int, is_augment: bool,\n                   is_normlize: bool, is_training: bool) -&gt; tf.data.Dataset:\n\n    def _wapper(raw_img: np.ndarray, ann: np.ndarray, is_augment: bool) -&gt; [np.ndarray, tuple]:\n        \"\"\" wapper for process image and ann to label \"\"\"\n        raw_img, ann = self.process_img(raw_img, ann, is_augment, True, False)\n        labels = self.ann_to_label(ann)\n        return (raw_img, *labels)\n\n    def _parser(stream: bytes):\n        example = tf.io.parse_single_example(stream, {\n            'img_raw': tf.io.FixedLenFeature([], tf.string),\n            'label': tf.io.FixedLenFeature([], tf.int64),\n            'bbox': tf.io.FixedLenFeature([], tf.string),\n        })  # type:dict\n\n        raw_img = tf.image.decode_image(example['img_raw'], channels=3)\n        label = example['label']\n        bbox = tf.io.parse_tensor(example['bbox'], tf.float32)\n\n        # load image -&gt; resize image -&gt; image augmenter -&gt; make labels\n        raw_img, *labels = tf.numpy_function(\n            _wapper, [raw_img, bbox, is_augment],\n            [tf.uint8] + [tf.float32] * self.scale_num, name='process_img')\n\n        # normlize image\n        if is_normlize:\n            img = self.normlize_img(raw_img)\n        else:\n            img = tf.cast(raw_img, tf.float32)\n\n        for i, v in enumerate(self.featuremap_size):\n            labels[i].set_shape((v, v, self.out_channels + 2))\n        img.set_shape((self.in_hw[0], self.in_hw[1], 3))\n\n        return img, tuple(labels)\n\n    if is_training:\n        pos_ds = (tf.data.TFRecordDataset(pos_tfrecord, buffer_size=100,\n                                          num_parallel_reads=6).\n                  shuffle(batch_size * 200, rand_seed).repeat().map(_parser))\n        neg_ds = (tf.data.TFRecordDataset(neg_tfrecord, buffer_size=100,\n                                          num_parallel_reads=6).\n                  shuffle(batch_size * 200, rand_seed).repeat().map(_parser))\n        ds = (tf.data.experimental.sample_from_datasets(\n            [pos_ds, neg_ds], [1 - self.neg_sample_ratio,\n                               self.neg_sample_ratio]).\n            batch(batch_size, True).prefetch(-1))\n    else:\n        raise NotImplementedError('No support to test eval')\n\n    return ds"
  },
  {
    "objectID": "posts/skimage-mem-leak.html#进一步定位问题",
    "href": "posts/skimage-mem-leak.html#进一步定位问题",
    "title": "skimage中resize内存泄漏",
    "section": "进一步定位问题",
    "text": "进一步定位问题\n现在我发现虽然用了tfrecord+tf.data的方式,但还是有内存泄漏的问题,不过这次更加进一步的定位到了问题:\n测试代码如下,我检测fn是否报错.\n@profile(stream=open('tmp/process.log', 'w'), precision=4)\ndef fn(sess, h: LFFDHelper, imgs, bboxs):\n    img, ann = sess.run([imgs, bboxs])\n    img, ann = h.resize_img(img, ann)\n    img, ann = h.data_augmenter(img, ann)\n    label = h.ann_to_label(ann)\n    return img, label\n\n\ndef _parser(stream: bytes):\n    example = tf.io.parse_single_example(stream, {\n        'img_raw': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n        'bbox': tf.io.FixedLenFeature([], tf.string),\n    })  # type:dict\n    raw_img = tf.image.decode_image(example['img_raw'], channels=3)\n    label = example['label']\n    # bbox = tf.io.parse_tensor(example['bbox'], tf.float32)\n    bbox = 0.\n    return raw_img, bbox\n\n\ndef test_no_dataset_memory_leak():\n    \"\"\" 测试以非dataset的方式运行时内存泄漏问题\n    ! 1.发现问题出现在tf.data读取tfrecord的地方\n     \"\"\"\n    neg_resize_factor = np.array([0.5, 3.5])\n    in_hw = np.array([640, 640])\n    featuremap_size = np.array([159, 79, 39, 19, 9])\n    h = LFFDHelper('data/lffd_img_ann.npy',\n                   featuremap_size, in_hw, neg_resize_factor, 0.2, 0.1)\n\n    batch_size = 16\n    pos_ds = (tf.data.Dataset.list_files(h.train_pos, True).\n              interleave(tf.data.TFRecordDataset, len(h.train_pos), 1, 4).\n              shuffle(batch_size * 500).repeat().map(_parser))\n    neg_ds = (tf.data.Dataset.list_files(h.train_neg, True).\n              interleave(tf.data.TFRecordDataset, len(h.train_neg), 1, 4).\n              shuffle(batch_size * 500).repeat().map(_parser))\n    ds = (tf.data.experimental.sample_from_datasets(\n        [pos_ds, neg_ds],\n        [1 - h.neg_sample_ratio, h.neg_sample_ratio]).prefetch(-1))\n\n    iters = ds._make_one_shot_iterator()\n    imgs, bboxs = iters.get_next()\n    sess = tf.Session()\n    for i in range(5000):\n        fn(sess, h, imgs, bboxs)\n\n\ntest_no_dataset_memory_leak()\nlog文件如下,可以发现正常的一个读取tfrecord的流程就会出问题:\nFilename: devlop/dev_lffd.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n  1156 2506.0898 MiB 2506.0898 MiB   @profile(stream=open('tmp/process.log', 'w'), precision=4)\n  1157                             def fn(sess, h: LFFDHelper, imgs, bboxs):\n  1158 2506.3477 MiB   0.2578 MiB       img, ann = sess.run([imgs, bboxs])\n  1159 2506.3477 MiB   0.0000 MiB       img, ann = h.resize_img(img, ann)\n  1160 2506.3477 MiB   0.0000 MiB       img, ann = h.data_augmenter(img, ann)\n  1161 2506.3477 MiB   0.0000 MiB       label = h.ann_to_label(ann)\n  1162 2506.3477 MiB   0.0000 MiB       return img, label\n\n\nFilename: devlop/dev_lffd.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n  1156 2506.3477 MiB 2506.3477 MiB   @profile(stream=open('tmp/process.log', 'w'), precision=4)\n  1157                             def fn(sess, h: LFFDHelper, imgs, bboxs):\n  1158 2506.6055 MiB   0.2578 MiB       img, ann = sess.run([imgs, bboxs])\n  1159 2506.6055 MiB   0.0000 MiB       img, ann = h.resize_img(img, ann)\n  1160 2506.6055 MiB   0.0000 MiB       img, ann = h.data_augmenter(img, ann)\n  1161 2506.6055 MiB   0.0000 MiB       label = h.ann_to_label(ann)\n  1162 2506.6055 MiB   0.0000 MiB       return img, label\n\n\nFilename: devlop/dev_lffd.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n  1156 2506.6055 MiB 2506.6055 MiB   @profile(stream=open('tmp/process.log', 'w'), precision=4)\n  1157                             def fn(sess, h: LFFDHelper, imgs, bboxs):\n  1158 2506.8633 MiB   0.2578 MiB       img, ann = sess.run([imgs, bboxs])\n  1159 2506.8633 MiB   0.0000 MiB       img, ann = h.resize_img(img, ann)\n  1160 2506.8633 MiB   0.0000 MiB       img, ann = h.data_augmenter(img, ann)\n  1161 2506.8633 MiB   0.0000 MiB       label = h.ann_to_label(ann)\n  1162 2506.8633 MiB   0.0000 MiB       return img, label"
  },
  {
    "objectID": "posts/skimage-mem-leak.html#检查下别的tf.data输入是否会出现内存泄漏问题",
    "href": "posts/skimage-mem-leak.html#检查下别的tf.data输入是否会出现内存泄漏问题",
    "title": "skimage中resize内存泄漏",
    "section": "检查下别的tf.data输入是否会出现内存泄漏问题",
    "text": "检查下别的tf.data输入是否会出现内存泄漏问题\n我做了6次实验如下:\n\n去除prefetch依旧内存泄漏\n两个dataset对象共用一个map函数依旧内存泄漏\n只读取一个pos dataset依旧内存泄漏\n不使用交错读取就没有问题了\n不会内存泄漏的构建过程如下:\ndef test_sample_dataset_memory_leak():\n    neg_resize_factor = np.array([0.5, 3.5])\n    in_hw = np.array([640, 640])\n    featuremap_size = np.array([159, 79, 39, 19, 9])\n    h = LFFDHelper('data/lffd_img_ann.npy',\n                   featuremap_size, in_hw, neg_resize_factor, 0.2, 0.1)\n\n    batch_size = 16\n    pos_ds = (tf.data.TFRecordDataset(h.train_pos, buffer_size=100, num_parallel_reads=4).\n              shuffle(batch_size * 500).\n              repeat().\n              map(_parser))\n    neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=100, num_parallel_reads=4).\n              shuffle(batch_size * 500).\n              repeat().\n              map(_parser))\n    ds = (tf.data.experimental.sample_from_datasets(\n        [pos_ds, neg_ds],\n        [1 - h.neg_sample_ratio, h.neg_sample_ratio]))\n\n    iters = ds._make_one_shot_iterator()\n    imgs, bboxs = iters.get_next()\n    sess = tf.Session()\n    for i in range(5000):\n        fn(sess, h, imgs, bboxs)"
  },
  {
    "objectID": "posts/skimage-mem-leak.html#新的问题",
    "href": "posts/skimage-mem-leak.html#新的问题",
    "title": "skimage中resize内存泄漏",
    "section": "新的问题",
    "text": "新的问题\n我之所以需要使用交错读取,是为了保证对多个tfrecord的随机性,如果不能交错读取…那我不能随机训练起来不是很蛋疼么?我得看看非交错读取和交错读取的读取随机性.\n\n首先生成一个总长10000的tfrecord文件.\ndef test_make_range_tfrecord():\n    \"\"\" 生成按顺序的tfrecord,用于检查tfrecord生成的随机性 \"\"\"\n    a = np.arange(10000)\n    for i in range(0, len(a), 2000):\n        with tf.io.TFRecordWriter(f'tmp/{i}.tfrecords') as writer:\n            for num in a[i:i + 2000]:\n                feature = {\n                    'var': tf.train.Feature(int64_list=tf.train.Int64List(value=[num])),\n                }\n                serialized_example = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n\n                writer.write(serialized_example)\n测试无交错的读取tfrecord.\ndef test_read_range_tfrecord_no_interleave():\n    \"\"\" 测试无交错的读取顺序tfrecord dataset \"\"\"\n    l = ['tmp/0.tfrecords', 'tmp/2000.tfrecords',\n        'tmp/4000.tfrecords', 'tmp/6000.tfrecords',\n        'tmp/8000.tfrecords']\n\n    def _par(stream: bytes):\n        example = tf.io.parse_single_example(stream, {\n            'var': tf.io.FixedLenFeature([], tf.int64),\n        })  # type:dict\n        var = example['var']\n        # bbox = tf.io.parse_tensor(example['bbox'], tf.float32)\n        return var\n\n    # @profile(stream=open('tmp/_fn.log', 'w'), precision=4)\n    def _fn(sess, next_num):\n        num = sess.run([next_num])\n        return num[0]\n\n    ds = (tf.data.TFRecordDataset(l, buffer_size=100, num_parallel_reads=4).\n        #   shuffle(16 * 500).\n        repeat().map(_par))\n\n    iters = ds._make_one_shot_iterator()\n    next_num = iters.get_next()\n    sess = tf.Session()\n    arr = np.zeros((10000))\n    f = open('tmp/_fn.log', 'w')\n    for i in range(10000):\n        arr[i] = _fn(sess, next_num)\n        print(arr[i], file=f)\n    plt.plot(arr)\n    plt.show()\n输出结果,可以发现采样现场为4,就是从4个tfrecord里面按顺序取: sh  0.0  2000.0  4000.0  6000.0  1.0  2001.0  4001.0  6001.0  2.0  2002.0  4002.0  6002.0  3.0  2003.0  4003.0  6003.0  4.0 但采样到后面就会出现比较单一的情况:  然后并没有出现内存泄漏的问题: \n测试有交错的读取tfrecord.\ndef test_read_range_tfrecord_has_interleave():\n  \"\"\" 测试使用交错的读取tfrecord dataset \"\"\"\n  l = ['tmp/0.tfrecords', 'tmp/2000.tfrecords',\n      'tmp/4000.tfrecords', 'tmp/6000.tfrecords',\n      'tmp/8000.tfrecords']\n\n  def _par(stream: bytes):\n      example = tf.io.parse_single_example(stream, {\n          'var': tf.io.FixedLenFeature([], tf.int64),\n      })  # type:dict\n      var = example['var']\n      # bbox = tf.io.parse_tensor(example['bbox'], tf.float32)\n      return var\n\n  # @profile(stream=open('tmp/_fn.log', 'w'), precision=4)\n  def _fn(sess, next_num):\n      num = sess.run([next_num])\n      return num[0]\n\n  ds = (tf.data.Dataset.list_files(l, True).\n      interleave(tf.data.TFRecordDataset, len(l), 1, 4).\n      repeat().map(_par))\n\n  iters = ds._make_one_shot_iterator()\n  next_num = iters.get_next()\n  sess = tf.Session()\n  arr = np.zeros((10000))\n  f = open('tmp/_fn.log', 'w')\n  for i in range(10000):\n      arr[i] = (_fn(sess, next_num))\n      print(arr[i], file=f)\n  plt.plot(arr)\n  plt.show()\n我发现这个交错4线程读取,他的预期行为和直接4线程读取tfrecords也是一样的:\n2000.0\n0.0\n8000.0\n4000.0\n6000.0\n2001.0\n1.0\n8001.0\n4001.0\n6001.0\n2002.0\n2.0\n8002.0\n4002.0\n6002.0\n2003.0\n3.0\n8003.0\n4003.0\n6003.0\n2004.0\n4.0\n这个采样到最后就不会出现单一数值的情况:  但是我发现也没有出现内存泄漏的情况!!我心累了,看来之前的内存泄漏问题原因还不在这里."
  },
  {
    "objectID": "posts/skimage-mem-leak.html#进一步检查是不是tf.example的格式导致的.",
    "href": "posts/skimage-mem-leak.html#进一步检查是不是tf.example的格式导致的.",
    "title": "skimage中resize内存泄漏",
    "section": "进一步检查是不是tf.example的格式导致的.",
    "text": "进一步检查是不是tf.example的格式导致的.\n我忽然发现前面定位问题还没定位到,最后一个我认为没有内存泄漏的方式在我运行时间够长之后还是有内存存在的.这个tf.example我尝试了,并不影响.继续尝试:\n\n之前使用非交错读取: python      pos_ds = (tf.data.TFRecordDataset(h.train_pos, buffer_size=100, num_parallel_reads=4).                shuffle(batch_size * 500).                repeat())      neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=100, num_parallel_reads=4).                shuffle(batch_size * 500).                repeat())      ds = (tf.data.experimental.sample_from_datasets(          [pos_ds, neg_ds],          [1 - h.neg_sample_ratio, h.neg_sample_ratio]).map(_parser))\n但是运行时间长了之后: \n单个数据集非交错读取\npos_ds = (tf.data.TFRecordDataset(h.train_pos, buffer_size=100, num_parallel_reads=4).\n          shuffle(batch_size * 500).\n          repeat())\n# neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=100, num_parallel_reads=4).\n#           shuffle(batch_size * 500).\n#           repeat())\n# ds = (tf.data.experimental.sample_from_datasets(\n#     [pos_ds, neg_ds],\n#     [1 - h.neg_sample_ratio, h.neg_sample_ratio]).map(_parser))\nds = pos_ds.map(_parser)\n运行,感觉好像还是有点问题,我人都晕了…: \n单个数据集交错读取\npos_ds = (tf.data.Dataset.list_files(h.train_pos, True).\n          interleave(tf.data.TFRecordDataset, len(h.train_pos), 1, 4).\n          shuffle(batch_size * 500).repeat())\n# neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=100, num_parallel_reads=4).\n#           shuffle(batch_size * 500).\n#           repeat())\n# ds = (tf.data.experimental.sample_from_datasets(\n#     [pos_ds, neg_ds],\n#     [1 - h.neg_sample_ratio, h.neg_sample_ratio]).map(_parser))\nds = pos_ds.map(_parser)\n运行结果,这个占内存更大并且持续升高比例大: \n双数据集无交错采样读取:\npos_ds = (tf.data.TFRecordDataset(h.train_pos, buffer_size=100, num_parallel_reads=8).\n          shuffle(batch_size * 500).repeat())\nneg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=100, num_parallel_reads=8).\n          shuffle(batch_size * 500).repeat())\nds = (tf.data.experimental.sample_from_datasets(\n    [pos_ds, neg_ds],\n    [1 - h.neg_sample_ratio, h.neg_sample_ratio]).map(_parser))\n还是在增加,我头大…为什么简单的数据集就不会有这个情况."
  },
  {
    "objectID": "posts/sort.html",
    "href": "posts/sort.html",
    "title": "排序算法小集",
    "section": "",
    "text": "直接上程序"
  },
  {
    "objectID": "posts/sort.html#main.cpp",
    "href": "posts/sort.html#main.cpp",
    "title": "排序算法小集",
    "section": "main.cpp",
    "text": "main.cpp\n#include &lt;cstdint&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstring&gt;\n#include &lt;ctime&gt;\n\nint TEST1[800]= {\n    985, 827, 960, 29,  410, 360, 931, 831, 845, 668, 199, 766, 72,  604, 470,\n    877, 855, 228, 614, 507, 627, 388, 46,  949, 911, 519, 479, 636, 150, 195,\n    189, 135, 374, 501, 165, 785, 213, 448, 616, 59,  468, 815, 177, 541, 419,\n    648, 770, 627, 228, 736, 486, 207, 124, 532, 157, 388, 404, 988, 376, 554,\n    184, 565, 41,  558, 419, 558, 695, 632, 359, 311, 43,  827, 479, 221, 368,\n    250, 221, 138, 877, 449, 875, 715, 9,   351, 248, 518, 91,  652, 506, 468,\n    558, 42,  385, 599, 953, 804, 158, 648, 789, 517, 312, 832, 696, 143, 405,\n    417, 393, 626, 907, 623, 428, 134, 338, 437, 486, 938, 307, 577, 942, 165,\n    397, 500, 208, 783, 452, 513, 939, 962, 513, 80,  831, 177, 913, 527, 320,\n    318, 944, 66,  297, 852, 41,  725, 986, 731, 514, 824, 670, 173, 754, 612,\n    338, 151, 113, 898, 286, 917, 411, 226, 879, 277, 306, 710, 454, 571, 237,\n    127, 242, 182, 545, 539, 386, 586, 616, 372, 317, 482, 197, 339, 655, 951,\n    952, 993, 454, 417, 244, 741, 334, 7,   967, 213, 284, 625, 275, 91,  549,\n    512, 570, 791, 46,  115, 682, 432, 53,  298, 157, 370, 132, 706, 710, 139,\n    657, 14,  484, 463, 431, 728, 204, 117, 736, 523, 682, 372, 149, 957, 463,\n    50,  469, 385, 841, 868, 500, 523, 652, 553, 173, 809, 276, 657, 515, 338,\n    796, 524, 704, 280, 988, 135, 361, 544, 604, 97,  68,  286, 821, 569, 243,\n    285, 619, 64,  670, 812, 284, 523, 335, 937, 428, 860, 98,  56,  517, 614,\n    746, 665, 490, 450, 297, 830, 585, 658, 375, 541, 107, 443, 827, 929, 364,\n    422, 566, 983, 839, 588, 795, 475, 463, 482, 412, 892, 342, 863, 300, 211,\n    829, 47,  876, 319, 497, 173, 150, 435, 832, 877, 976, 291, 672, 156, 220,\n    36,  930, 138, 19,  769, 727, 166, 597, 190, 648, 361, 434, 342, 224, 735,\n    553, 53,  782, 781, 725, 279, 954, 875, 714, 138, 104, 691, 430, 776, 199,\n    2,   812, 129, 493, 183, 251, 220, 349, 848, 762, 349, 209, 197, 691, 434,\n    284, 596, 839, 66,  377, 564, 697, 331, 791, 764, 822, 895, 807, 604, 671,\n    6,   958, 483, 135, 451, 18,  386, 671, 367, 234, 786, 68,  796, 335, 759,\n    230, 619, 355, 69,  37,  84,  634, 86,  416, 425, 850, 590, 673, 657, 194,\n    344, 663, 152, 180, 151, 604, 198, 537, 627, 918, 124, 765, 986, 920, 100,\n    98,  502, 71,  805, 923, 460, 890, 909, 547, 658, 687, 397, 248, 360, 407,\n    794, 56,  422, 946, 236, 573, 902, 787, 463, 882, 57,  939, 647, 395, 859,\n    100, 493, 361, 171, 299, 636, 632, 541, 546, 179, 199, 233, 928, 447, 593,\n    687, 241, 649, 110, 539, 238, 35,  794, 377, 498, 676, 434, 437, 675, 181,\n    648, 775, 675, 361, 947, 326, 998, 931, 867, 896, 462, 66,  129, 742, 865,\n    74,  430, 458, 75,  540, 349, 665, 927, 143, 42,  778, 171, 828, 567, 847,\n    10,  216, 622, 37,  929, 921, 715, 279, 852, 582, 175, 666, 0,   656, 409,\n    217, 730, 839, 27,  158, 731, 376, 823, 658, 872, 866, 436, 395, 46,  356,\n    242, 408, 924, 865, 445, 853, 138, 160, 485, 991, 94,  660, 657, 446, 317,\n    418, 15,  399, 257, 42,  557, 988, 771, 381, 999, 995, 599, 787, 390, 645,\n    143, 633, 406, 67,  850, 851, 273, 988, 364, 758, 331, 810, 770, 341, 609,\n    87,  759, 624, 487, 369, 667, 396, 357, 790, 777, 708, 785, 728, 496, 527,\n    726, 639, 512, 132, 59,  362, 983, 684, 703, 699, 442, 386, 862, 212, 79,\n    471, 652, 191, 95,  491, 560, 114, 887, 269, 256, 17,  978, 393, 745, 474,\n    921, 471, 465, 433, 603, 524, 148, 939, 208, 203, 990, 2,   589, 852, 567,\n    669, 675, 219, 212, 123, 710, 772, 589, 949, 41,  846, 966, 19,  239, 712,\n    493, 512, 535, 311, 946, 139, 835, 446, 430, 396, 649, 420, 750, 590, 625,\n    317, 611, 300, 536, 823, 423, 598, 595, 13,  548, 989, 211, 866, 8,   450,\n    930, 854, 315, 466, 165, 261, 605, 352, 59,  35,  100, 708, 807, 851, 298,\n    432, 168, 910, 733, 57,  85,  508, 7,   681, 873, 907, 22,  84,  774, 30,\n    535, 704, 236, 202, 170, 753, 463, 127, 458, 874, 514, 558, 934, 322, 761,\n    232, 754, 282, 494, 839, 691, 580, 348, 698, 613, 221, 958, 635, 658, 84,\n    17,  545, 140, 606, 747, 663, 711, 562, 790, 169, 436, 657, 80,  370, 979,\n    193, 954, 85,  475, 801, 925, 518, 381, 625, 569, 346, 198, 527, 333, 856,\n    611, 350, 401, 751, 308, 148, 414, 20,  710, 557, 541, 498, 214, 973, 868,\n    545, 519, 823, 630, 346};\n\n/**\n * @brief  冒泡法与插入法 交换次数相同 并且与数组中逆序数个数相同！\n *\n *      定理：任意N个不同元素组成的序列平均具有N(N-1)/4个逆序对\n *      定理：任何以交换相邻元素的排序算法，其平均时间复杂度为Ω(N^2)\n **/\nvoid BubbleSort(int *array, int length) {\n    int temp;\n    for (int i= length; i &gt; 0; i--) {\n        for (int j= 0; j &lt; (i - 1); j++) {\n            if (array[j] &gt; array[j + 1]) {\n                temp        = array[j];\n                array[j]    = array[j + 1];\n                array[j + 1]= temp;\n            }\n        }\n    }\n}\n\nvoid InsertionSort(int *array, int length) {\n    int Tmp, i;\n    for (int p= 1; p &lt; length; p++) {\n        Tmp= array[p]; /* 取值 */\n        for (i= p; i &gt; 0 && (array[i - 1] &gt; Tmp); i--) {\n            array[i]= array[i - 1]; /* 向后移动 */\n        }\n        array[i]= Tmp; /* 插入元素 */\n    }\n}\n\n/**\n * @brief 希尔排序\n *        此方法是去间隔进行插入排序。\n *        可行性在于下一次增量排序不会影响上一次增量排序\n **/\nvoid ShellSort(int *array, int length) {\n    int Tmp, i;\n    for (int j= length / 2; j &gt; 0; j/= 2) {\n        for (int p= j; p &lt; length; p++) {\n            Tmp= array[p]; /* 取值 */\n            for (i= p; i &gt;= j && (array[i - j] &gt; Tmp); i-= j) {\n                array[i]= array[i - j]; /* 向后移动 */\n            }\n            array[i]= Tmp; /* 插入元素 */\n        }\n    }\n}\n\n/**\n * @brief 交换元素\n **/\nvoid Swap(int *a, int *b) {\n    int tmp= *b;\n    *b     = *a;\n    *a     = tmp;\n}\n/**\n * @brief 将大的元素下沉\n **/\nvoid PercDown(int *array, int index, int length) {\n    int temp= array[index];\n    for (int k= index * 2 + 1; k &lt; length; k= k * 2 + 1) {\n        //如果右边值大于左边值，指向右边\n        if (k + 1 &lt; length && array[k] &lt; array[k + 1]) {\n            k++;\n        }\n        //如果子节点大于父节点，将子节点值赋给父节点,并以新的子节点作为父节点（不用进行交换）\n        if (array[k] &gt; temp) {\n            array[index]= array[k];\n            index       = k;\n        } else\n            break;\n    }\n    // put the value in the final position\n    array[index]= temp;\n}\n/**\n * @brief 堆排序\n *        堆排序是选择排序的改进\n *        构建成最大堆，再调整顺序\n **/\nvoid HeapSort(int *array, int length) {\n    /* 构建最大堆--数组形式 */\n    for (int i= length / 2; i &gt;= 0; i--) {\n        PercDown(array, i, length);\n    }\n    for (int i= length - 1; i &gt; 0; i--) {\n        /* 移除最大元素 */\n        Swap(&array[0], &array[i]);\n        /* 以根节点为0下沉，且数组长度逐渐缩短 */\n        PercDown(array, 0, i);\n    }\n}\n\nint Median3(int *array, int left, int right) {\n    int center= (left + right) / 2;\n    if (array[left] &gt; array[center]) {\n        Swap(&array[left], &array[center]);\n    }\n    if (array[left] &gt; array[right]) {\n        Swap(&array[left], &array[right]);\n    }\n    if (array[center] &gt; array[right]) {\n        Swap(&array[center], &array[right]);\n    }\n    Swap(&array[center], &array[right - 1]);\n    return array[right - 1];\n}\nvoid QuickSort(int *array, int left, int right) {\n\n    if (30 &lt;= right - left) {\n        int pivot= Median3(array, left, right);\n        int i    = left;\n        int j    = right - 1;\n        while (1) {\n            while (array[++i] &lt; pivot) {\n            }\n            while (array[--j] &gt; pivot) {\n            }\n            if (i &lt; j) {\n                Swap(&array[i], &array[j]);\n            } else {\n                break;\n            }\n        }\n        Swap(&array[i], &array[right - 1]);\n        QuickSort(array, left, i - 1);\n        QuickSort(array, i + 1, right);\n    } else {\n        InsertionSort(array + left, right - left + 1);\n    }\n}\n\n/**\n * @brief 快速排序\n **/\nvoid QuickSort(int *array, int length) {\n    QuickSort(array, 0, length - 1);\n}\n\n\nint main(int argc, char const *argv[]) {\n    struct timespec tpstart;\n    struct timespec tpend;\n    int *A= (int *)malloc(sizeof(TEST1));\n    int *B= (int *)malloc(sizeof(TEST1));\n    int *C= (int *)malloc(sizeof(TEST1));\n    int *D= (int *)malloc(sizeof(TEST1));\n    int *E= (int *)malloc(sizeof(TEST1));\n\n    memcpy(A, TEST1, sizeof(TEST1));\n    memcpy(B, TEST1, sizeof(TEST1));\n    memcpy(C, TEST1, sizeof(TEST1));\n    memcpy(D, TEST1, sizeof(TEST1));\n    memcpy(E, TEST1, sizeof(TEST1));\n    clock_gettime(CLOCK_MONOTONIC, &tpstart);\n    BubbleSort(A, 800);\n    clock_gettime(CLOCK_MONOTONIC, &tpend);\n    printf(\"冒泡排序：%ld us\\n\", (tpend.tv_nsec - tpstart.tv_nsec) / 1000);\n\n    clock_gettime(CLOCK_MONOTONIC, &tpstart);\n    InsertionSort(B, 800);\n    clock_gettime(CLOCK_MONOTONIC, &tpend);\n    printf(\"插入排序：%ld us\\n\", (tpend.tv_nsec - tpstart.tv_nsec) / 1000);\n\n    clock_gettime(CLOCK_MONOTONIC, &tpstart);\n    ShellSort(C, 800);\n    clock_gettime(CLOCK_MONOTONIC, &tpend);\n    printf(\"希尔排序：%ld us\\n\", (tpend.tv_nsec - tpstart.tv_nsec) / 1000);\n\n    clock_gettime(CLOCK_MONOTONIC, &tpstart);\n    HeapSort(D, 800);\n    clock_gettime(CLOCK_MONOTONIC, &tpend);\n    printf(\"堆排序：%ld us\\n\", (tpend.tv_nsec - tpstart.tv_nsec) / 1000);\n\n    clock_gettime(CLOCK_MONOTONIC, &tpstart);\n    QuickSort(E, 800);\n    clock_gettime(CLOCK_MONOTONIC, &tpend);\n    printf(\"快速排序：%ld us\\n\", (tpend.tv_nsec - tpstart.tv_nsec) / 1000);\n\n    free(A);\n    free(B);\n    free(C);\n    free(D);\n    free(E);\n    return 0;\n}"
  },
  {
    "objectID": "posts/sort.html#执行",
    "href": "posts/sort.html#执行",
    "title": "排序算法小集",
    "section": "执行",
    "text": "执行\n➜  sort clang++ -g ./main.cpp && ./a.out\n冒泡排序：863 us\n插入排序：336 us\n希尔排序：84 us\n堆排序：76 us\n快速排序：54 us"
  },
  {
    "objectID": "posts/spidev.html",
    "href": "posts/spidev.html",
    "title": "OrangPi开启spi-dev",
    "section": "",
    "text": "我想在OrangePi Zero中打开Spidev，编写一些应用层的驱动。\n\n\n配置armbian-config\n我首先打开armbian-config，进入hardware使能spidev以及spidev-add-cs1并重启。\n\n\n发现问题\n接着我发现在/dev/目录下没有spidev生成。我查看了许多。 发现他们的系统在armbian-config中使能了，就会去加载对应的dtbo。 我做如下查看:\nroot@pi:~# vi /boot/dtb-4.17.11-sunxi64/allwinner/overlay/sun50i-h5-spi-spidev.dtbo \nÐ^Mþí^@^@^C^L^@^@^@8^@^@^B¬^@^@^@(^@^@^@^Q^@^@^@^P^@^@^@^@^@^@^@`^@^@^Bt^@^@^@^@\n^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^A^@^@^@^@^@^@^@^C^@^@^@^T^@^@^@^@allwinner,sun50i\n-h5^@^@^@^@^Afragment@0^@^@^@^@^@^C^@^@^@       ^@^@^@^K/aliases^@^@^@^@^@^@^@^A\n__overlay__^@^@^@^@^C^@^@^@^Q^@^@^@^W/soc/spi@1c68000^@^@^@^@^@^@^@^C^@^@^@^Q^@^\n@^@^\\/soc/spi@1c69000^@^@^@^@^@^@^@^B^@^@^@^B^@^@^@^Afragment@1^@^@^@^@^@^C^@^@^\n@^D^@^@^@!ÿÿÿÿ^@^@^@^A__overlay__^@^@^@^@^C^@^@^@^D^@^@^@(^@^@^@^A^@^@^@^C^@^@^@\n^D^@^@^@7^@^@^@^@^@^@^@^Aspidev^@^@^@^@^@^C^@^@^@^G^@^@^@^@spidev^@^@^@^@^@^C^@^\n@^@     ^@^@^@Cdisabled^@^@^@^@^@^@^@^C^@^@^@^D^@^@^@J^@^@^@^@^@^@^@^C^@^@^@^D^@\n^@^@N^@^OB@^@^@^@^B^@^@^@^B^@^@^@^B^@^@^@^Afragment@2^@^@^@^@^@^C^@^@^@^D^@^@^@!\nÿÿÿÿ^@^@^@^A__overlay__^@^@^@^@^C^@^@^@^D^@^@^@(^@^@^@^A^@^@^@^C^@^@^@^D^@^@^@7^\n@^@^@^@^@^@^@^Aspidev^@^@^@^@^@^C^@^@^@^G^@^@^@^@spidev^@^@^@^@^@^C^@^@^@\n^@^@^@Cdisabled^@^@^@^@^@^@^@^C^@^@^@^D^@^@^@J^@^@^@^@^@^@^@^C^@^@^@^D^@^@^@N^@^\nOB@^@^@^@^B^@^@^@^B^@^@^@^B^@^@^@^A__fixups__^@^@^@^@^@^C^@^@^@^U^@^@^@^W/fragme\nnt@1:target:0^@^@^@^@^@^@^@^C^@^@^@^U^@^@^@^\\/fragment@2:target:0^@^@^@^@^@^@^@^\nB^@^@^@^B^@^@^@ compatible^@target-path^@spi0^@spi1^@target^@#address-cells^@#si\nze-cells^@status^@reg^@spi-max-frequency^@\n这里发现这里spidev跟着的状态是disable的。然后我又查看了他的源码： /linux-4.17.y/arch/arm64/boot/dts/allwinner/overlay/sun50i-h5-spi-spidev.dts\n/dts-v1/;\n/plugin/;\n\n/ {\n    compatible = \"allwinner,sun50i-h5\";\n\n    fragment@0 {\n        target-path = \"/aliases\";\n        __overlay__ {\n            spi0 = \"/soc/spi@1c68000\";\n            spi1 = \"/soc/spi@1c69000\";\n        };\n    };\n\n    fragment@1 {\n        target = &lt;&spi0&gt;;\n        __overlay__ {\n            #address-cells = &lt;1&gt;;\n            #size-cells = &lt;0&gt;;\n            spidev {\n                compatible = \"spidev\";\n                status = \"disabled\";\n                reg = &lt;0&gt;;\n                spi-max-frequency = &lt;1000000&gt;;\n            };\n        };\n    };\n\n    fragment@2 {\n        target = &lt;&spi1&gt;;\n        __overlay__ {\n            #address-cells = &lt;1&gt;;\n            #size-cells = &lt;0&gt;;\n            spidev {\n                compatible = \"spidev\";\n                status = \"disabled\";\n                reg = &lt;0&gt;;\n                spi-max-frequency = &lt;1000000&gt;;\n            };\n        };\n    };\n};\n我就开始认为没有打开使能才会没有产生spidev。但是在我又询问了论坛的人后才知道。\n\n\n修改armbianEnv.txt\n他们的手册中写到：\nparam_* - overlay parameters\n需要添加参数。。\n我看了一会知道了他们的启动运行流程：\n启动-&gt;读取armbianEnv.txt-&gt;加载overlay参数对应的dtbo-&gt;继续读取armbianEnv.txt的参数项-&gt;根据参数来运行sun50i-h5-fixup.scr-&gt;将对应的外设使能\n所以现在需要添加：这个两个参数使能spidev。\nparam_spidev_spi_bus=1 param_spidev_spi_cs=1\n\n\n完成\n最终可以看到：\nroot@pi:~# ls /dev/s\nshm/       snd/       spidev1.1  stderr     stdin      stdout \n\n\n测试spi\n我们使用linux中自带的spidev_test.c进行测试。 进入/linux-4.17.y/tools/spi中交叉编译：\n➜  ~ cd /home/zqh/sources/linux-mainline/linux-4.17.y/tools/spi \n➜  spi git:(84d52eb0) ✗ aarch64-linux-gnu-gcc -o spidev_test spidev_test.c -lpthread -static\n➜  spi git:(84d52eb0) ✗ ls\ninclude  Makefile  spidev_fdx.c  spidev_test  spidev_test.c  spidev_test-in.o\n将spidev_test拷贝入开发板中，并且短接MISO和MOSI。运行测试程序（记得加-v选项）。\nroot@pi:~# ./spidev_test -D /dev/spidev1.0 -v\nspi mode: 0x0\nbits per word: 8\nmax speed: 500000 Hz (500 KHz)\nTX | FF FF FF FF FF FF 40 00 00 00 00 95 FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF F0 0D  | ......@....�..................�.\nRX | FF FF FF FF FF FF 40 00 00 00 00 95 FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF F0 0D  | ......@....�..................�.\nroot@pi:~# ./spidev_test -D /dev/spidev1.0 -v -p helloworld\nspi mode: 0x0\nbits per word: 8\nmax speed: 500000 Hz (500 KHz)\nTX | 68 65 6C 6C 6F 77 6F 72 6C 64 __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __  | helloworld\nRX | 68 65 6C 6C 6F 77 6F 72 6C 64 __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __  | helloworld"
  },
  {
    "objectID": "posts/ssl-ict.html",
    "href": "posts/ssl-ict.html",
    "title": "半监督学习：Interpolation Consistency Training",
    "section": "",
    "text": "第六个算法Interpolation Consistency Training forSemi-Supervised Learning，这个算法是利用mixup提出了一种简单的一致性正则化方法。"
  },
  {
    "objectID": "posts/ssl-ict.html#插值一致性训练",
    "href": "posts/ssl-ict.html#插值一致性训练",
    "title": "半监督学习：Interpolation Consistency Training",
    "section": "插值一致性训练",
    "text": "插值一致性训练\n首先给出mixup的公式： \\[\n\\begin{align}\n\\operatorname{Mix}_{\\lambda}(a, b)=\\lambda \\cdot a+(1-\\lambda) \\cdot b\n\\end{align}\n\\]\n插值一致性训练(ICT)训练分类器\\(f_\\theta\\)以在未标记点的插值中提供一致的预测: \\[\nf_{\\theta}(\\operatorname{Mix}_{\\lambda}(u_j, u_k))\\approx \\operatorname{Mix}_{\\lambda}(f_{\\theta'}(u_j),f_{\\theta'} (u_k))\n\\]\n其中\\(\\theta'\\)是\\(\\theta\\)的滑动平均。为什么在未标记样本之间进行插值可以为半监督训练提供良好的一致性扰动？首先，应用一致性正则化的最有用样本是决策边界附近的样本。在这样的靠近边距未标记样本上添加一个小扰动\\(\\delta\\)可能把\\(u_j+\\delta\\)推入决策边界的另一侧，将是一个不错的位置。\n\n回到低边距的无标签数据\\(u_j\\)，如何找到一个合适的扰动\\(\\delta\\)将\\(u_i\\)与\\(u_j+\\delta\\)尽量分布在决策面相对两侧？其中随机扰动是一种无效策略，因为接近决策边距方向的子集仅仅占作为空间的一小部分。取而代之，考虑向第二个随机选择的无标签样本进行插值\\(u_j+\\delta=\\operatorname{Mix}_\\lambda(u_j,u_k)\\)，然后两个无标签样本\\(u_j,u_k\\)有三种情况：\n\n位于同一集群中\n位于不同的集群中，但属于同一类别\n位于不同的集群上，属于不同的类别\n\n由聚类假设，则(1)的概率随类别数的增加而降低;如果假设每个类别的聚类数是平衡的，则(2)的概率较低;最后，(3)的概率为最高的。然后，假设\\(u_j,u_k\\)中的一个位于决策边界附近(它是执行一致性的一个很好的候选者)，则由于(3的概率很高)插值\\(u_k\\)有可能朝着低密度区域插值，其次是另一类的聚类。由于这是移动决策的好方向，因此插值对于基于一致性的正则化是一个很好的扰动。\n到目前为止，我们的论证认为，随机未标记样本之间的插值可能会落在低密度区域中。因此，此类插值是可以应用基于一致性的正则化的良好位置。但是，我们应该如何标记这些插值呢？与单个未标记的样本\\(u_j\\)的随机或对抗性扰动不同，我们的方案涉及两个未标记的样本\\(u_j,u_k\\)。直观地讲，我们希望将决策边界尽可能地远离类边界，因为众所周知，具有较大余量的决策边界可以更好地泛化(A framework for structural riskminimisation),在有监督的学习环境中，一种实现大幅度决策边界的方法是mixup。在mixup中，通过强制预测模型在样本之间线性变化，将决策边界推离类边界。在这里，我们通过训练模式将mixup扩展到半监督学习，通过训练模型\\(f_\\theta\\)预测样本点\\(\\operatorname{Mix}_\\lambda(u_j,u_k)\\)的fake label\\(\\operatorname{Mix}_\\lambda(f_\\theta(u_j),f_\\theta(u_k))\\)。为了遵守更保守的一致正则化，鼓励模型\\(f_\\theta\\)预测样本点\\(\\operatorname{Mix}_\\lambda(u_j,u_k)\\)的fake label的\\(\\operatorname{Mix}_\\lambda(f_{\\theta'}(u_j),f_{\\theta'}(u_k))\\)，其中\\(\\theta'\\)是\\(\\theta\\)的滑动平滑，就如同mean teacher一样。\n总之ICT可以描述为以下公式： \\[\n\\begin{align}\n\\mathcal{L}_{U S}=\\underset{u_{j}, u_{k} \\sim P(X)}{\\mathbb{E}} \\underset{\\lambda \\sim \\operatorname{Beta}(\\alpha, \\alpha)}{\\mathbb{E}} \\ell\\left(f_{\\theta}\\left(\\operatorname{Mix}_{\\lambda}\\left(u_{j}, u_{k}\\right)\\right), \\operatorname{Mix}_{\\lambda}\\left(f_{\\theta^{\\prime}}\\left(u_{j}\\right), f_{\\theta^{\\prime}}\\left(u_{k}\\right)\\right)\\right)\n\\end{align}\n\\]\n\n其本质是结合了mixup结合了mean teacher。"
  },
  {
    "objectID": "posts/ssl-mean-teacher.html",
    "href": "posts/ssl-mean-teacher.html",
    "title": "半监督学习：mean teacher",
    "section": "",
    "text": "第三个算法mean teacher，此算法是对Π model的升级。"
  },
  {
    "objectID": "posts/ssl-mean-teacher.html#mean-teacher",
    "href": "posts/ssl-mean-teacher.html#mean-teacher",
    "title": "半监督学习：mean teacher",
    "section": "mean teacher",
    "text": "mean teacher\n为了克服Temporal Ensembling的局限性，建议对模型权重取平均而不是预测结果。由于teacher模型是连续的student模型权重平均值，因此将其称为mean teacher方法(图2)。 在训练步骤的上平均模型权重往往会产生比直接使用最终权重更准确的模型。我们可以在培训过程中利用这一优势来构建更好的目标。与使用学生模型共享权重不同，教师模型使用学生模型的EMA权重。现在，它可以在每个步骤之后而不是每个时期都汇总信息。另外，由于权重平均值改善了所有层的输出，而不仅仅是顶部输出，因此目标模型具有更好的中间表示。这些方面在时间合计方面具有两个实践优势：首先，目标标签更准确可导致学生模型与教师模型之间的反馈回路更快，从而提高测试准确性。其次，该方法可扩展到大型数据集和在线学习。\n\n好了到这里其实应该清晰了，mean teacher对于Temporal Ensembling的实际改进其实就在与teacher模型的权重更新方式，使用的是student模型权重的滑动平均，而student模型实际上和Π model相同。"
  },
  {
    "objectID": "posts/ssl-mixup.html",
    "href": "posts/ssl-mixup.html",
    "title": "半监督学习：mixup",
    "section": "",
    "text": "第五个算法mixup: BEYOND EMPIRICAL RISK MINIMIZATION，这个算法应该很多人都听过或者用过，它十分简单但又十分有效，是之后大部分半监督论文都用到的技巧。"
  },
  {
    "objectID": "posts/ssl-mixup.html#从经验风险最小化到mixup",
    "href": "posts/ssl-mixup.html#从经验风险最小化到mixup",
    "title": "半监督学习：mixup",
    "section": "从经验风险最小化到mixup",
    "text": "从经验风险最小化到mixup\n在监督学习中，我们的目标是找到一个函数\\(f\\in\\mathcal{F}\\)，该函数描述遵循联合分布\\(P(X,Y)\\)的随机特征向量\\(X\\)和随机目标向量\\(Y\\)之间的关系。为此，我们首先定义一个损失函数，该函数对预测\\(f(x)\\)和实际目标\\(y\\)之间的差异进行惩罚，例如\\((x,y)\\sim p\\)。然后，我们将数据分布\\(P\\)上损失函数\\(\\ell\\)的平均值最小化，也称为预期风险： \\[\n\\begin{align}\nR(f)=\\int \\ell(f(x), y) \\mathrm{d} P(x, y)\n\\end{align}\n\\]\n不幸的是，在大多数实际情况下，分布\\(P\\)是未知的。相反，我们通常可以加载一组训练数据\\(\\mathcal{D}={(x_i,y_i)}_{i=1}^n\\)，其中\\((x_i,y_i)\\sim P\\),\\(i=1,\\ldots,n\\)。使用训练数据\\(\\mathcal{D}\\)，我们可以根据经验分布来近似： \\[\n\\begin{align}\nP_{\\delta}(x, y)=\\frac{1}{n} \\sum_{i=1}^{n} \\delta\\left(x=x_{i}, y=y_{i}\\right)\n\\end{align}\n\\]\n其中\\(\\delta\\left(x=x_{i}, y=y_{i}\\right)\\)是\\((x_{i}, y_{i})\\)的狄克拉质量中心。使用经验分布\\(P_\\delta\\)，我们现在可以通过经验风险来近似预期风险:\n\\[\n\\begin{align}\nR_{\\delta}(f)=\\int \\ell(f(x), y) \\mathrm{d} P_{\\delta}(x, y)=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(f\\left(x_{i}\\right), y_{i}\\right)\n\\end{align}\\tag{1}\n\\]\n通过最小化学习函数\\(1\\)被称为经验风险最小化(ERM)原理(Vapnik，1998年)。尽管计算效率很高，但经验风险\\(1\\)仅在一组有限的n个示例中监控行为偏离。当考虑具有大量参数的功能时(例如大型神经网络)，最小化\\(1\\)的一种简单方法是记忆训练数据(Zhang et al。，2017)。记忆反过来会导致训练数据之外的不良行为(Szegedy等，2014)。\n但是，朴素估计\\(P_\\delta\\)是逼近真实分布\\(P\\)的许多可能选择之一。例如，在领域风险最小化(VRM)原理中(Chapelle等，2000)，分布\\(P\\)近似为: \\[\n\\begin{align}\nP_{\\nu}(\\tilde{x}, \\tilde{y})=\\frac{1}{n} \\sum_{i=1}^{n} \\nu\\left(\\tilde{x}, \\tilde{y} | x_{i}, y_{i}\\right)\n\\end{align}\n\\]\n其中\\(v\\)是vicinity distributionth用于测量在训练特征-目标对\\((x_{i}, y_{i})\\)的领域内找到虚拟特征-目标对\\((\\hat{x}, \\hat{y})\\)的概率，特别是，chapellpe等人(2000年)考虑了高斯领域内\\(\\nu\\left(\\tilde{x}, \\tilde{y} | x_{i}, y_{i}\\right)=\\mathcal{N}\\left(\\tilde{x}-x_{i}, \\sigma^{2}\\right) \\delta\\left(\\tilde{y}=y_{i}\\right)\\)，相对与用加性高斯噪声来增强训练数据。要学习使用VRM，我们对附近的分布进行采样以构建数据\\(\\mathcal{D}_{\\nu}:=\\left\\{\\left(\\tilde{x}_{i}, \\tilde{y}_{i}\\right)\\right\\}_{i=1}^{m}\\)，然后最小化经验风险损失：\n\\[\n\\begin{align}\nR_{\\nu}(f)=\\frac{1}{m} \\sum_{i=1}^{m} \\ell\\left(f\\left(\\tilde{x}_{i}\\right), \\tilde{y}_{i}\\right)\n\\end{align}\n\\]\n此文章的作用是提出一种通用的邻域分布，称为mixup：\n\\[\n\\begin{align}\n\\mu\\left(\\tilde{x}, \\tilde{y} | x_{i}, y_{i}\\right)=\\frac{1}{n} \\sum_{j}^{n} \\underset{\\lambda}{\\mathbb{E}}\\left[\\delta\\left(\\tilde{x}=\\lambda \\cdot x_{i}+(1-\\lambda) \\cdot x_{j}, \\tilde{y}=\\lambda \\cdot y_{i}+(1-\\lambda) \\cdot y_{j}\\right)\\right]\n\\end{align}\n\\]\n其中\\(\\lambda\\sim \\text{Beta}(\\alpha,\\alpha),\\alpha\\in(0,\\infty)\\)。从混合邻域分布中采样会生成虚拟特征目标向量： \\[\n\\begin{align}\n\\begin{aligned}\n&\\tilde{x}=\\lambda x_{i}+(1-\\lambda) x_{j}\\\\\n&\\tilde{y}=\\lambda y_{i}+(1-\\lambda) y_{j}\n\\end{aligned}\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/ssl-mixup.html#总结",
    "href": "posts/ssl-mixup.html#总结",
    "title": "半监督学习：mixup",
    "section": "总结",
    "text": "总结\n实际上mixup的目标是使的模型更加线性，联想到之前的几个算法，就如mean-teacher算法第一幅图所展示的，最终目标是使在两个标注数据间的未标注区域获得更平滑的概率分布输出，这样我们可以找到类别A与类别B间更加合适的决策面，从而提升分类准确性，下图可以看到输出的概率分布更加平滑了。"
  },
  {
    "objectID": "posts/ssl-pseudo-label.html",
    "href": "posts/ssl-pseudo-label.html",
    "title": "半监督学习：pseudo label",
    "section": "",
    "text": "入坑半监督学习苦于找不到好的学习资料，不过就在昨天我发现了一个宝藏repo，那就是谷歌research的fixmatch仓库，是一套半监督算法的框架，包含数十种半监督算法，简直是入坑半监督学习的最佳教程😆\n话不多说，先来看第一个算法pseudo label,发表与2013年。"
  },
  {
    "objectID": "posts/ssl-pseudo-label.html#类别之间的低密度分离",
    "href": "posts/ssl-pseudo-label.html#类别之间的低密度分离",
    "title": "半监督学习：pseudo label",
    "section": "类别之间的低密度分离",
    "text": "类别之间的低密度分离\n半监督学习的目标是使用未标记的数据来提高泛化性能。集群假设指出决策边界应位于低密度区域以提高泛化性能（Chapelle et al。，2005）。最近提出了使用流形学习的神经网络训练方法，例如半监督嵌入和流形切空间分类器，利用了这种假设。嵌入（Westonet等人，2008）使用基于嵌入的正则化器来提高深度神经网络的泛化性能。由于通过基于嵌入的惩罚项，数据样本的邻居与样本具有相似的激活，因此高密度区域中的数据样本更有可能具有相同的标签.ManifoldTangent分类器（Rifai等人，2011b）鼓励网络输出不敏感低维流形方向的变化因此达到了相同的目的。"
  },
  {
    "objectID": "posts/ssl-pseudo-label.html#熵正则化",
    "href": "posts/ssl-pseudo-label.html#熵正则化",
    "title": "半监督学习：pseudo label",
    "section": "熵正则化",
    "text": "熵正则化\n熵正则化（Grandvalet et al。，2006）意味着可以从最大后验估计框架中的未标记数据中受益。该方案通过最小化未标记数据的类概率的条件熵，有利于类之间的低密度分离，而无需对密度进行任何建模。 \\[\n\\begin{align}\nH\\left(y | x^{\\prime}\\right)=-\\frac{1}{n^{\\prime}} \\sum_{m=1}^{n^{\\prime}} \\sum_{i=1}^{C} P\\left(y_{i}^{m}=1 | x^{\\prime m}\\right) \\log P\\left(y_{i}^{m}=1 | x^{\\prime m}\\right)\n\\end{align}\n\\]\n其中，\\(n'\\)是未标记数据的数量，\\(C\\)是类的数量，\\(y^m_i\\)是未标记样本的未知标签，\\(x'\\)是第\\(m\\)个输入的未标记向量，熵是类重叠的度量。随着类重叠的减少，决策点的数据点密度降低，MAP估计定义为后验分布的最大化： \\[\n\\begin{align}\nC(\\theta, \\lambda)=\\sum_{m=1}^{n} \\log P\\left(y^{m} | x^{m} ; \\theta\\right)-\\lambda H\\left(y | x^{\\prime} ; \\theta\\right)\n\\end{align}\n\\]\n其中\\(n\\)是标记数据的数量，\\(x^m\\)是第\\(m\\)个标记样本，\\(\\lambda\\)是使两项平衡的系数。通过最大化带标签数据的条件对数似然性（第一项），同时使未标记数据的熵（第二项）最小，我们可以使用未标记的数据获得更好的泛化性能。"
  },
  {
    "objectID": "posts/ssl-pseudo-label.html#以伪标签作为熵正则化训练",
    "href": "posts/ssl-pseudo-label.html#以伪标签作为熵正则化训练",
    "title": "半监督学习：pseudo label",
    "section": "以伪标签作为熵正则化训练",
    "text": "以伪标签作为熵正则化训练\n通过使用未标记的数据和伪标签进行训练来鼓励预测的类概率接近其中一项，因此将伪标签条件熵减到最小。因此，我们的方法等效于熵正则化。后验分布的第一项对应于损失函数的第一项，后验分布的第二项对应于损失的第二项，\\(\\alpha(t)\\)对应于\\(\\lambda\\)。\n图1显示了t-SNE（Van der Maaten等人，2008年）MNISTtest数据（不包含在未标记数据中）的网络输出的2D嵌入结果。用600个标记数据训练了神经网络，60000个未标记数据和用或没用伪标签训练了神经网络。尽管在两种情况下训练误差为零，但通过训练，使用伪标签的网络输出测试数据更集中在每一项附近，换句话说，将MAP估计熵最小化。\n\n表2显示了MAP的估计熵。尽管两种情况下标记数据的熵都接近于零，但是通过伪标签训练，未标记数据的熵变低，此外，测试数据的熵也随之变低。这甚至使测试数据的分类问题也变得更加容易，并使决策边界处的数据点密度降低。根据聚类假设，我们可以获得更好的泛化性能。"
  },
  {
    "objectID": "posts/ssl-simclr.html",
    "href": "posts/ssl-simclr.html",
    "title": "半监督学习：SimCLR",
    "section": "",
    "text": "SimCLR实际上是Geoffrey Hinton和谷歌合作的论文A Simple Framework for Contrastive Learning of Visual Representations，严格来说他是一个自监督算法，不过我这里也把他归入半监督中了，他实际上是先无监督预训练然后进行监督微调的。"
  },
  {
    "objectID": "posts/ssl-simclr.html#数据输入",
    "href": "posts/ssl-simclr.html#数据输入",
    "title": "半监督学习：SimCLR",
    "section": "数据输入",
    "text": "数据输入\ndef map_fn(image, label):\n  \"\"\"Produces multiple transformations of the same batch.\"\"\"\n  if FLAGS.train_mode == 'pretrain':\n    xs = []\n    for _ in range(2):  # Two transformations\n      # 预训练的时候是同一张图像\n      xs.append(preprocess_fn_pretrain(image))\n    image = tf.concat(xs, -1) # [h,w,2*c]\n    label = tf.zeros([num_classes])\n  else:\n    image = preprocess_fn_finetune(image)\n    label = tf.one_hot(label, num_classes)\n  return image, label, 1.0\n在无监督预训练的时候对于同一张图像执行两次数据增强，然后concat为[h,w,2*c]的图像。\n# Split channels, and optionally apply extra batched augmentation.\n# 前面变成了[h,w,2*c]，这里再拆分出来\nfeatures_list = tf.split(\n    features, num_or_size_splits=num_transforms, axis=-1)\nif FLAGS.use_blur and is_training and FLAGS.train_mode == 'pretrain':\n  # 再做一些数据增强\n  features_list = data_util.batch_random_blur(\n      features_list, FLAGS.image_size, FLAGS.image_size)\n# 现在变成了(num_transforms * bsz, h, w, c)\nfeatures = tf.concat(features_list, 0)  # (num_transforms * bsz, h, w, c)\n注意他这里的features实际上还是图像，因为之前是[h,w,2*c]的形状，他这里重新分离的同时再分别加了一些数据增强，得到了[n*batch, h, w, c]的数据。"
  },
  {
    "objectID": "posts/ssl-simclr.html#投影获取",
    "href": "posts/ssl-simclr.html#投影获取",
    "title": "半监督学习：SimCLR",
    "section": "投影获取",
    "text": "投影获取\nwith tf.variable_scope('base_model'):\n  if FLAGS.train_mode == 'finetune' and FLAGS.fine_tune_after_block &gt;= 4:\n    # Finetune just supervised (linear) head will not update BN stats.\n    model_train_mode = False\n  else:\n    # Pretrain or finetuen anything else will update BN stats.\n    model_train_mode = is_training\n  hiddens = model(features, is_training=model_train_mode)\n\n# Add head and loss.\nif FLAGS.train_mode == 'pretrain':\n  tpu_context = params['context'] if 'context' in params else None\n  hiddens_proj = model_util.projection_head(hiddens, is_training)\n图像输入到basemodel得到隐含层输出，再通过投影头得到隐含层投影hiddens_proj。"
  },
  {
    "objectID": "posts/ssl-simclr.html#对比损失计算",
    "href": "posts/ssl-simclr.html#对比损失计算",
    "title": "半监督学习：SimCLR",
    "section": "对比损失计算",
    "text": "对比损失计算\ndef add_contrastive_loss(hidden,\n                         hidden_norm=True,\n                         temperature=1.0,\n                         tpu_context=None,\n                         weights=1.0):\n  \"\"\"Compute loss for model.\n\n  Args:\n    hidden: hidden vector (`Tensor`) of shape (bsz, dim).\n    hidden_norm: whether or not to use normalization on the hidden vector.\n    temperature: a `floating` number for temperature scaling.\n    tpu_context: context information for tpu.\n    weights: a weighting number or vector.\n\n  Returns:\n    A loss scalar.\n    The logits for contrastive prediction task.\n    The labels for contrastive prediction task.\n  \"\"\"\n  # Get (normalized) hidden1 and hidden2.\n  if hidden_norm:\n    hidden = tf.math.l2_normalize(hidden, -1)\n  hidden1, hidden2 = tf.split(hidden, 2, 0)\n  batch_size = tf.shape(hidden1)[0]\n\n  # Gather hidden1/hidden2 across replicas and create local labels.\n  if tpu_context is not None:\n    hidden1_large = tpu_cross_replica_concat(hidden1, tpu_context)\n    hidden2_large = tpu_cross_replica_concat(hidden2, tpu_context)\n    enlarged_batch_size = tf.shape(hidden1_large)[0]\n    # TODO(iamtingchen): more elegant way to convert u32 to s32 for replica_id.\n    replica_id = tf.cast(tf.cast(xla.replica_id(), tf.uint32), tf.int32)\n    labels_idx = tf.range(batch_size) + replica_id * batch_size\n    labels = tf.one_hot(labels_idx, enlarged_batch_size * 2)\n    masks = tf.one_hot(labels_idx, enlarged_batch_size)\n  else:\n    hidden1_large = hidden1\n    hidden2_large = hidden2\n    labels = tf.one_hot(tf.range(batch_size), batch_size * 2)\n    masks = tf.one_hot(tf.range(batch_size), batch_size)\n  \n  logits_aa = tf.matmul(hidden1, hidden1_large, transpose_b=True) / temperature\n  logits_aa = logits_aa - masks * LARGE_NUM \n  logits_bb = tf.matmul(hidden2, hidden2_large, transpose_b=True) / temperature\n  logits_bb = logits_bb - masks * LARGE_NUM \n  logits_ab = tf.matmul(hidden1, hidden2_large, transpose_b=True) / temperature\n  logits_ba = tf.matmul(hidden2, hidden1_large, transpose_b=True) / temperature\n\n  loss_a = tf.losses.softmax_cross_entropy(\n      labels, tf.concat([logits_ab, logits_aa], 1), weights=weights)\n  loss_b = tf.losses.softmax_cross_entropy(\n      labels, tf.concat([logits_ba, logits_bb], 1), weights=weights)\n  loss = loss_a + loss_b\n\n  return loss, logits_ab, labels\n因为现在的隐含层投影前一半和后一半是对同一张图像的输出，所以拆分为hidden1, hidden2 = tf.split(hidden, 2, 0)，接下来是得到hidden1_large，这里其实我不是很懂tpu上会和gpu有多大区别，不过对于gpu来说hidden1_large = hidden1。\n有了投影，接下来制作标签，标签实际是batch单位矩阵并上一个零矩阵，mask是单位矩阵，矩阵的大小都是[batch,batch]，比如当batch=2时：\nlabel = [[1., 0., 0., 0.],\n        [0., 1., 0., 0.]] \n\nmask =  [[1., 0.],\n        [0., 1.]] \n下面就是logits_aa,logits_bb，他们就是一个batch内的图像隐含层投影交叉做內积，而其中的对角线元素是相同向量做內积，那么因为他要做对比损失，希望将相似度转换成概率(越相似概率越大，越不相似概率越小)，向量自身的內积肯定是最大的(概率值最大)，所以这里就没有必要把对角线上的结果算到损失里面，他就利用mask将对角线相似度都减去一个极大值，将概率强行降为最小。\n对于logits_ab,logits_ba，和上面一样类似，只不过现在的对角线元素是一张源图像两个不同增强后的表征投影向量內积(相似度)。\n损失值就很明确了，即最大化一张源图像两个不同增强后的表征投影向量的相似度，最小化不同图像间表征投影向量的相似度，一张源图像一个增强的表征投影相似度被mask矩阵排除了。"
  },
  {
    "objectID": "posts/ssl-vat.html",
    "href": "posts/ssl-vat.html",
    "title": "半监督学习：Virtual Adversarial Training",
    "section": "",
    "text": "第四个算法Virtual Adversarial Training(虚拟对抗训练)，出自论文Virtual Adversarial Training:A Regularization Method for Supervised and Semi-Supervised Learning,下面简称为vat。"
  },
  {
    "objectID": "posts/ssl-vat.html#方法细节",
    "href": "posts/ssl-vat.html#方法细节",
    "title": "半监督学习：Virtual Adversarial Training",
    "section": "方法细节",
    "text": "方法细节\n首先定义符号：令\\(x\\in R^I,y\\in Q\\)表示输入向量与输出标签，\\(I\\)表示输入维度，\\(Q\\)表示标签空间。此外，我们将输出分布通过\\(\\theta\\)参数化为\\(p(y|x,\\theta)\\)，我们使用\\(\\hat{\\theta}\\)来表示模型参数在训练过程的特定迭代步骤的向量。使用\\(\\mathcal{D}_{l}=\\left\\{x_{l}^{(n)}, y_{l}^{(n)} | n=1, \\ldots, N_{l}\\right\\}\\)表示带标签数据集，\\(\\mathcal{D}_{ul}=\\left\\{x_{ul}^{(m)}, y_{l}^{(m)} | m=1, \\ldots, N_{ul}\\right\\}\\)表示无标签数据集，我们使用\\(\\mathcal{D}_l,\\mathcal{D}_{ul}\\)训练模型\\(p(y|x,\\theta)\\)。"
  },
  {
    "objectID": "posts/ssl-vat.html#对抗训练",
    "href": "posts/ssl-vat.html#对抗训练",
    "title": "半监督学习：Virtual Adversarial Training",
    "section": "对抗训练",
    "text": "对抗训练\n因为vat继承自对抗训练，因此，在介绍它之前，需要介绍对抗训练。对抗训练的损失函数可以写成： \\[\n\\begin{equation}\nL_{\\mathrm{adv}}\\left(x_{l}, \\theta\\right):=D\\left[q\\left(y | x_{l}\\right), p\\left(y | x_{l}+r_{\\mathrm{adv}}, \\theta\\right)\\right]\n\\end{equation}\\tag{1}\n\\]\n\\[\n\\begin{equation}\n{\\text { where } r_{\\text {adv }}:=\\underset{r ;\\|r\\| \\leq \\epsilon}{\\arg \\max } D\\left[q\\left(y | x_{l}\\right), p\\left(y | x_{l}+r, \\theta\\right)\\right]}\n\\end{equation}\\tag{2}\n\\]\n其中\\(D\\)为分布\\(p\\)和\\(p'\\)间的差异度量函数，通常，我们无法获得精确的对抗性扰动\\(r_{adv}\\)的封闭形式，不过我们可以通过公式\\(2\\)中的度量\\(D\\)来线性近似\\(r\\)。当使用l2正则时，对抗扰动可以通过此公式近似： \\[\n\\begin{align}\nr_{\\mathrm{adv}} \\approx \\epsilon \\frac{g}{\\|g\\|_{2}}, \\text { where } g=\\nabla_{x_{l}} D\\left[h\\left(y ; y_{l}\\right), p\\left(y | x_{l}, \\theta\\right)\\right]\n\\end{align}\\tag{3}\n\\]\n当使用\\(L_{\\infty}\\)正则时，对抗扰动可以通过此公式近似： \\[\n\\begin{aligned}\n    r_{adv}\\approx\\epsilon sign(g)\n\\end{aligned}\\tag{4}\n\\]\n其中\\(g\\)和公式\\(3\\)相同。传统的对抗训练一般使用公式\\(3\\)来计算。"
  },
  {
    "objectID": "posts/ssl-vat.html#虚拟对抗训练",
    "href": "posts/ssl-vat.html#虚拟对抗训练",
    "title": "半监督学习：Virtual Adversarial Training",
    "section": "虚拟对抗训练",
    "text": "虚拟对抗训练\n对抗训练是一种成功的方法，可以解决任何有监督的问题。但并非始终都有完整的标签信息。 令\\(x_*\\)代表任一\\(x_l\\)或\\(x_{ul}\\)，我们的目标函数现在为： \\[\n\\begin{align}\n\\begin{aligned}\n&D\\left[q\\left(y | x_{*}\\right), p\\left(y | x_{*}+r_{\\mathrm{qadv}}, \\theta\\right)\\right]\\\\\n&\\text { where } r_{\\text {qadv }}:=\\underset{r ;\\|r\\| \\leq \\epsilon}{\\arg \\max } D\\left[q\\left(y | x_{*}\\right), p\\left(y | x_{*}+r, \\theta\\right)\\right]\n\\end{aligned}\n\\end{align}\n\\]\n实际上没有关于\\(q(x|x_{ul})\\)直接的标签信息，因此，我们采取了用当前近似值\\(p(y|x,\\theta)\\)代替\\(q(y|x)\\)的策略，这种近似不一定是naive的，因为当带标签的训练样本数量很大时，\\(p(y|x,\\theta)\\)应该接近\\(q(y|x)\\)。从字面上看，我们使用从\\(p(y|x,\\theta)\\)概率生成的虚拟标签代替用户不知道的标签，并根据虚拟标签计算对抗方向。因此，使用当前估计值\\(p(y|x,\\hat{\\theta})\\)代替\\(q(y|x)\\)。有了这种折衷，我们得出了新的公式\\(2\\)的表达式： \\[\n\\begin{align}\n\\operatorname{LDS}\\left(x_{*}, \\theta\\right):=D\\left[p\\left(y | x_{*}, \\hat{\\theta}\\right), p\\left(y | x_{*}+r_{\\mathrm{vadv}}, \\theta\\right)\\right]\n\\end{align}\\tag{5}\n\\]\n\\[\n\\begin{align}\nr_{\\text {vadv }}:=\\underset{r ;\\|r\\|_{2} \\leq \\epsilon}{\\arg \\max } D\\left[p\\left(y | x_{*}, \\hat{\\theta}\\right), p\\left(y | x_{*}+r\\right)\\right]\n\\end{align}\\tag{6}\n\\]\n\\(r_{\\text {vadv }}\\)定义了我们的虚拟采样扰动，损失函数\\(\\operatorname{LDS}( x_{*}, \\theta)\\)可以视为对每个输入样本\\(x_{*}\\)当前模型的局部平滑度的否定度量，度量的减少将使模型在每个样本点处平滑。同时此损失的正则化项是所有输入样本点上的\\(\\operatorname{LDS}(x_{*}, \\theta)\\)的平均值： \\[\n\\begin{align}\n\\mathcal{R}_{\\mathrm{vadv}}\\left(\\mathcal{D}_{l}, \\mathcal{D}_{u l}, \\theta\\right):=\\frac{1}{N_{l}+N_{u l}} \\sum_{x_{*} \\in \\mathcal{D}_{l}, \\mathcal{D}_{u l}} \\operatorname{LDS}\\left(x_{*}, \\theta\\right)\n\\end{align}\\tag{7}\n\\]\n最终得到完整的目标函数为：\n\\[\n\\begin{align}\n\\ell\\left(\\mathcal{D}_{l}, \\theta\\right)+\\alpha \\mathcal{R}_{\\mathrm{vadv}}\\left(\\mathcal{D}_{l}, \\mathcal{D}_{u l}, \\theta\\right)\n\\end{align}\\tag{8}\n\\]\n其中\\(\\ell(\\mathcal{D}_{l}, \\theta)\\)是标记数据集的负对数似然。vat是使用正则化\\(\\mathcal{R}_{\\mathrm{vadv}}\\)的训练方法。\nvat的一个显着优势是仅有两个标量值超参数：(1)对抗方向的范数约束\\(\\epsilon&gt;0\\)；(2)控制负对数似然与正则化器\\(\\mathcal{R}_{\\mathrm{vadv}}\\)之间相对平衡的正则化系数\\(\\alpha&gt;0\\)。实际上，根据实验，vat仅通过调整超参数(固定\\(\\alpha=1\\))而获得了出色的性能。"
  },
  {
    "objectID": "posts/ssl-vat.html#r_textvadv的快速逼近方法和目标函数的导数",
    "href": "posts/ssl-vat.html#r_textvadv的快速逼近方法和目标函数的导数",
    "title": "半监督学习：Virtual Adversarial Training",
    "section": "\\(r_{\\text{vadv}}\\)的快速逼近方法和目标函数的导数",
    "text": "\\(r_{\\text{vadv}}\\)的快速逼近方法和目标函数的导数\n为了简单起见，我们把\\(D\\left[p\\left(y | x_{*}, \\hat{\\theta}\\right), p\\left(y | x_{*}+r_{\\mathrm{vadv}}, \\theta\\right)\\right]\\)表示成\\(D(r,x_*,\\theta)\\) \\(r_{\\text{vadv}}\\)。我们假设\\(p(y|x_*,\\theta)\\)相对于\\(\\theta\\)和几乎各处的\\(x\\)的差是两倍。因为\\(D(r,x_*,\\theta)\\)取最小值时\\(r=0\\)，其一阶导数\\(\\left.\\nabla_{r} D(r, x, \\hat{\\theta})\\right|_{r=0}\\)是0，因此\\(D\\)的第二项泰勒级数近似为：\n\\[\n\\begin{align}\nD(r, x, \\hat{\\theta})=f(0)+f'(0)r+\\frac{f''(0)}{2!}r^2 \\approx \\frac{1}{2} r^{T} H(x, \\hat{\\theta}) r\n\\end{align}\\tag{9}\n\\]\n\\(H(x, \\hat{\\theta})\\)的Hessian矩阵为\\(H(x,\\hat{\\theta}) = \\nabla \\nabla_r D(r,x,\\hat{\\theta}) \\vert_{r=0}\\)。此时\\(r_{\\text{vadv}}\\)成为\\(H(x, \\hat{\\theta})\\)的第一个特征向量\\(u(x,\\theta)\\)，并且幅度为\\(\\epsilon\\)(二次型在单位元上的最大值和最小值分别对应其最大特征值和最小特征值，此时\\(r\\)等于其对应的特征向量，这个具体的证明将Hermite矩阵正交对角化)： \\[\n\\begin{align}\n\\begin{aligned}\nr_{\\text {vadv }} & \\approx \\underset{r}{\\arg \\max }\\left\\{r^{T} H(x, \\hat{\\theta}) r ;\\|r\\|_{2} \\leq \\epsilon\\right\\} \\\\\n&=\\overline{\\epsilon u(x, \\hat{\\theta})}\n\\end{aligned}\n\\end{align}\\tag{10}\n\\]\n其中\\(\\overline{v}\\)表示方向与其参数向量\\(v\\)相同的单位向量，即\\(\\bar{v} \\equiv \\frac{v}{\\|v\\|_{2}}\\)。下面为简单起见，用\\(H\\)表示\\(H(x, \\hat{\\theta})\\)。接下来，我们需要解决计算Hessian矩阵特征向量所需的\\(O(I^3)\\)运行时间。通过幂迭代法和有限差分法通过逼近来解决此问题。设\\(d\\)为随机采样的单位向量，\\(d\\)如果不垂直于主特征向量\\(u\\)，则迭代计算： \\[\n\\begin{equation}\nd \\gets \\overline{Hd}\n\\end{equation}\\tag{11}\n\\]\n此时\\(d\\)是收敛到主特征向量\\(u\\)的，对于\\(H\\)的计算，不需要直接计算，而是计算近似有限差分： \\[\n\\begin{equation}\n\\begin{aligned}\nHd &\\approx \\frac{\\nabla_r D(r,x,\\hat{\\theta}) \\vert_{r=\\xi d} -\\nabla_r D(r,x,\\hat{\\theta})\\vert_{r=0}}{\\xi} \\\\\n&= \\frac{\\nabla_r D(r,x,\\hat{\\theta})\\vert_{r=\\xi d}}{\\xi}\n\\end{aligned}\n\\end{equation}\\tag{12}\n\\]\n其中\\(\\xi \\neq 0\\)，在上面的计算中，我们可以再次利用\\(\\left.\\nabla_{r} D(r, x, \\hat{\\theta})\\right|_{r=0}=0\\)。总而言之，我们可以通过以下更新的重复应用来近似\\(r_{\\text{vadv}}\\)： \\[\n\\begin{align}\nd \\leftarrow \\overline{\\nabla_{r} D(r, x, \\hat{\\theta})|_{r=\\xi d}}\n\\end{align}\\tag{13}\n\\]\n在幂迭代下，这种近似可以由迭代次数\\(K\\)来单调改善，在实验中\\(K=1\\)就可以实现较好的结果了，此时可以对\\(r_{\\text{vadv}}\\)进一步改写为： \\[\n\\begin{aligned}\n    r_{\\text{vadv}} \\approx \\epsilon\\frac{g}{\\|g\\|_2}\n\\end{aligned}\\tag{14}\n\\]\n\\[\n\\begin{aligned}\n    \\text{where}\\ g=\\left. \\nabla_{r} D[p(y | x, \\hat{\\theta}), p(y | x+r, \\hat{\\theta})]\\right|_{r=\\xi d}\n\\end{aligned}\\tag{15}\n\\]\n计算\\(r_{\\text{vadv}}\\)之后，可以使用神经网络中进行的正向和反向传播轻松计算\\(r_{\\text{vadv}}\\)的导数。但是，加入\\(r_{\\text{vadv}}\\)相对于参数的导数，不仅无用并且计算代价高，而且还为梯度引入了另一种方差来源，并对算法的性能产生负面影响。因此vat忽略了\\(r_{\\text{vadv}}\\)对于\\(\\theta\\)的依赖性。总体而言，包括对数似然项公式\\(8\\)在内的全目标函数的导数可以用\\(K + 2\\)组反向传播来计算。具体迭代过程伪代码如下：\n\n对于幂迭代次数\\(K\\)，可以对vat的正则项做一个表述： \\[\n\\begin{equation}\n\\mathcal R^{(K)}(\\theta ,\\mathcal D_l,\\mathcal D_{ul}) := \\frac{1}{N_l + N_{ul}} \\sum_{x \\in \\mathcal D_l,\\mathcal D_{ul}} \\mathbb E_{r_K}[D[p(y \\vert x,\\hat{\\theta}), p(y \\vert x+r_K,\\theta)]]\n\\end{equation}\\tag{16}\n\\]\n对于vat就是幂迭代次数大于等于1次，即\\(K\\geq1\\)。当\\(K=0\\)时，也就是不采用幂迭代求解\\(r_{\\text{vadv}}\\)，称这种方法为rpt，rpt是vat的降级版本， 不执行幂迭代，rpt仅在每个输入数据点周围各向同性地平滑函数。"
  },
  {
    "objectID": "posts/statis-learn-cp1.html",
    "href": "posts/statis-learn-cp1.html",
    "title": "统计学习方法:感知机",
    "section": "",
    "text": "我觉得自己对于概率视角下的机器学习方法还是不够清晰,因此开个新坑(其实这个基础就应该上上学期打好),现在准备两个月之内把统计学习方法第二版撸完(flag是不是太…).不管了,今天是第一章感知机,为了节约记录的时间,我都只写我觉得比较重要的地方."
  },
  {
    "objectID": "posts/statis-learn-cp1.html#模型",
    "href": "posts/statis-learn-cp1.html#模型",
    "title": "统计学习方法:感知机",
    "section": "模型",
    "text": "模型\n\\[\n\\begin{aligned}\n  f(x)=sign(w\\cdot x+b)\\\\\n  sign(x)=\\begin{cases}+1,x\\geq0\\\\-1,x&lt; 0 \\end{cases}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/statis-learn-cp1.html#学习策略",
    "href": "posts/statis-learn-cp1.html#学习策略",
    "title": "统计学习方法:感知机",
    "section": "学习策略",
    "text": "学习策略\n因为是要对所有数据点进行二分类,所以直观的想法是根据误分类点到决策面\\(S\\)的距离作为损失: \\[\n\\begin{aligned}\n  -\\frac{1}{\\parallel w\\parallel}y_i(w\\cdot x_i+b)\n\\end{aligned}\n\\]\n因为感知机想法比较简单,不需要考虑决策面距离分类点有多远,所以舍去\\(\\frac{1}{\\parallel w\\parallel}\\)得到损失函数:\n\\[\n\\begin{aligned}\n  -y_i(w\\cdot x_i+b)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/statis-learn-cp1.html#训练",
    "href": "posts/statis-learn-cp1.html#训练",
    "title": "统计学习方法:感知机",
    "section": "训练",
    "text": "训练\n感知机虽然是通过求导反向传播更新的,但是要注意直接用batch的方式学习是没有用的!这里我踩了个坑,他的更新方式是选择分类错误的点的loss进行反向传播.\n\n定义初值\\(w_0,b_0\\)\n输入\\(x_i,y_i\\)\n如果\\(-y_i(w\\cdot x_i +b)&gt;0\\)\n\n\\[\n\\begin{aligned}\n  w\\leftarrow w+\\eta y_i x_i\\\\\n  b\\leftarrow b+\\eta y_i\n\\end{aligned}\n\\]\n\n重复2,3"
  },
  {
    "objectID": "posts/statis-learn-cp1.html#模型-1",
    "href": "posts/statis-learn-cp1.html#模型-1",
    "title": "统计学习方法:感知机",
    "section": "模型",
    "text": "模型\n我们注意到\\(w\\)在经过多次更新后,他的增量实际上等于如下: \\[\n\\begin{aligned}\n  \\text{Let}\\ \\ \\ \\ \\alpha_i= n_i \\eta\\\\\n  w=\\sum_{i=1}^N \\alpha_i y_i x_i\\\\\n  b=\\sum_{i=1}^N \\alpha_i y_i \\\\\n\\end{aligned}\n\\]\n将原始感知机中的替换为如下: \\[\n\\begin{aligned}\n  f(x)=sign(\\sum_{j=1}^N \\alpha_j y_j x_j \\cdot x +b)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/statis-learn-cp1.html#训练-1",
    "href": "posts/statis-learn-cp1.html#训练-1",
    "title": "统计学习方法:感知机",
    "section": "训练",
    "text": "训练\n\n模型定义\n\n\\[\n\\begin{aligned}\n  f(x)=sign(\\sum_{j=1}^N \\alpha_j y_j x_j \\cdot x +b)\\\\\n  \\alpha=(\\alpha_1,\\alpha_2,...\\alpha_N)^T\n\\end{aligned}\n\\]\n\n初始化参数\\(\\alpha,b\\)\n输入\\(x_i,y_i\\)\n\nNOTE 这里为了加速计算,首先将所有的\\(\\sum_{j=1}^N \\sum_{i=1}^N x_j \\cdot x_i\\)(称为Gram矩阵)计算出来,训练的时候直接取值即可: \\[\n\\begin{aligned}\n  \\boldsymbol{G}=[x_i\\cdot x_j]_{N\\times N}\n\\end{aligned}\n\\]\n\n如果\\(-y_i(\\sum_{j=1}^N \\alpha_j y_j x_j \\cdot x_i +b)&gt;0\\)\n\n\\[\n\\begin{aligned}\n  \\alpha_i\\leftarrow \\alpha_i+\\eta\\\\\n  b\\leftarrow b+\\eta y_i\n\\end{aligned}\n\\]\n\n重复3,4"
  },
  {
    "objectID": "posts/statis-learn-cp11.html",
    "href": "posts/statis-learn-cp11.html",
    "title": "统计学习方法:聚类方法",
    "section": "",
    "text": "聚类方法书中只讲了Kmeans和层次聚类.这两个比较简单,所以我这里就不讲解了.\n\n其中Kmeans是EM算法的特殊情况.具体可参考这里"
  },
  {
    "objectID": "posts/statis-learn-cp16.html",
    "href": "posts/statis-learn-cp16.html",
    "title": "统计学习方法:马尔科夫链蒙特卡洛法",
    "section": "",
    "text": "蒙特卡洛法是对概率模型进行抽样来近似数值计算的方法.马尔科夫链蒙特卡洛法就是以马尔科夫链作为概率模型的蒙特卡洛法.\n我的理解是传统的蒙特卡洛法需要进行采样,但他的采样还是需要依赖于目标分布\\(p(x)\\),当目标分布\\(p(x)\\)是一个多元函数,或者概率密度函数是奇怪的分布,或者内部分量不独立时,总之就是难以直接采样时,引入一个满足遍历定理的马尔科夫链,让他的平稳分布就是目标分布\\(p(x)\\),接下来我们利用马尔科夫链生成许多样本,生成的样本数量越大,那么这些样本就接近于直接抽样的结果.解决了难以直接抽样的问题.\n\n\n蒙特卡洛接受-拒绝法采样\n原理书本上都有,我这里给出一个具体的例子:\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats._discrete_distns import binom\nfrom scipy.stats._continuous_distns import beta, norm\nfrom scipy.stats._continuous_distns import uniform\n\nplt.rcParams['font.sans-serif'] = ['STZhongsong']  # 用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\nx = np.linspace(0, 1)\nq_x = uniform\n\n\ndef p_x(x):\n  return x * np.cos(71 * x) + np.sin(13 * x) + 2\n\n\npoint = 0.75\nc = np.max(p_x(x))\nupper = c * q_x.pdf(point)\nplt.plot(x, c * q_x.pdf(x), label='$cq(x)$')\nplt.plot(x, p_x(x), label='$p(x)$')\nplt.arrow(point, 0, 0, p_x(point), linewidth=1,\n          head_width=0.03, head_length=0.01, fc='g', ec='g')\nplt.arrow(point, upper, 0, -(upper - p_x(point)), linewidth=1,\n          head_width=0.03, head_length=0.01, fc='r', ec='r')\nplt.text(point + .05, 2., 'Reject', fontsize=16)\nplt.text(point + .05, 0.75, 'Accept', fontsize=16)\nplt.title('接受-拒绝抽样示意图', fontsize=20)\nplt.legend()\nplt.show()\n\nn = 500000\nsample_x = q_x.rvs(size=n)# 从均匀分布中采样x\nu = uniform.rvs(size=n)  # 这里采样u作为比例进行计算\nv = sample_x[u &lt;= (p_x(sample_x) / (c * q_x.pdf(sample_x)))]  # v为接受的样本\n\nhist, bin_edges = np.histogram(v, bins=100, normed=True)\nfactor = 2 # 这个参数本来应该是通过p(x)的cdf计算得到的,我这里偷懒了\nbin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.\nplt.step(bin_centers, hist * factor, linewidth=2, label='sampling')\nplt.plot(x, c * q_x.pdf(x), label='$cq(x)$')\nplt.plot(x, p_x(x), label='$p(x)$')\nplt.legend()\nplt.title(f'接受-拒绝抽样 接受率{np.size(v)/n:.3f}', fontsize=20)\nplt.show()\n当我给定目标分布为\\(p(x)=x\\cos(71x)+\\sin(13*x)+2\\),建议分布为\\(q(x)\\sim U(0,1)\\),\\(c\\)为目标分布大最大值,抽样的示意图如下所示:\n\n根据书中的流程进行编码得到抽样结果如下,因为我这个目标分布是随便定义的,他的累积概率最终不为1,因此在绘制统计图的时候需要乘上一个系数.\n\n\n\n马尔科夫链\n之前隐马尔科夫模型其实已经讲到了,但是那边讲的并不详细.书本上在这一章详细介绍了马尔科夫链.我这里也给出一个简单的例子,假设我们有三个岛屿,岛屿上的人口初始分布不同,有一个转移矩阵表示了每年三个岛屿人口迁移概率,经过数年迭代之后到达平稳分布,中间的状态称为燃烧期: \\[\nT=\\begin{bmatrix}\n    0.9& 0.05& 0.05\\\\\n    0.1& 0.8& 0.1\\\\\n    0.04& 0.01& 0.95\n\\end{bmatrix}\n\\]\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.rcParams['font.sans-serif'] = ['STZhongsong']  # 用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\nA = np.array([\n    [0.9, 0.05, 0.05],\n    [0.1, 0.8, 0.1],\n    [0.04, 0.01, 0.95]\n])\n\nx = np.array([300000, 400000, 100000])\n\n\nxs = [x]\nfor i in range(80):\n  xs.append(xs[i] @ A)\n\nxs = np.array(xs)\nplt.plot(xs[:, 0], label='A')\nplt.plot(xs[:, 1], label='B')\nplt.plot(xs[:, 2], label='C')\nplt.legend()\nplt.title('马尔科夫链转移示意图', fontsize=20)\nplt.tight_layout(True)\nplt.show()\n\n\n\nMetropolis-Hastings采样\n我这里给出一个典型的例子,就是使用Metropolis-Hastings采样方法从参数的先验分布与对应的似然分布得到数据的后验分布.即: \\[\n\\begin{aligned}\nP(\\theta|y)&\\propto P(y|\\theta)P(\\theta)\\\\\n\\text{假设如下:}&\\\\\nP(\\theta)&=Be(\\alpha,\\beta)\\\\\nP(y|\\theta)&=Bin(n,k,\\theta)\\\\\nT&\\sim N(0,\\sigma)\n\\end{aligned}\n\\]\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm, binom, beta, uniform\n\nplt.rcParams['font.sans-serif'] = ['STZhongsong']  # 用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\n\ndef target(like: binom, prior: beta, n, h, theta):\n  if theta &lt; 0 or theta &gt; 1:\n    return 0\n  else:\n    return like(n, theta).pmf(h) * prior.pdf(theta)\n\n\nn = 100\nh = 61\na = 10\nb = 10\nlike = binom\nprior = beta(a, b)\nsigma = 0.3\n\nnaccept = 0\n\nniters = 10000\nthetas = np.zeros(niters + 1)\nthetas[0] = 0.1  # 设定初始参数\nfor i in range(niters):\n  # 以当前状态下,转移矩阵为N(0,sigma)\n  theta_p = norm(thetas[i], sigma).rvs()\n  #\n  rho = min(1, target(like, prior, n, h, theta_p) /\n            target(like, prior, n, h, thetas[i]))\n  u = np.random.uniform()\n  if u &lt; rho:\n    naccept += 1\n    thetas[i + 1] = theta_p\n  else:\n    thetas[i + 1] = thetas[i]\nnmcmc = len(thetas) // 2  # 为了尽量选取平稳状态的采样值\n\n\nx = np.linspace(0, 1, 200)\ntrue_posterior = prior.pdf(x) * like(n, x).pmf(h)  # 先验*似然\ntrue_posterior /= (np.sum(true_posterior) / np.size(true_posterior))\nplt.hist(thetas[nmcmc:], 80, density=True, label='posterior')\nplt.hist(prior.rvs(nmcmc), 80, density=True, label='prior')\nplt.plot(x, true_posterior, label='true posterior', c='red')\nplt.legend()\nplt.title(f\"采样效率 = {naccept / niters:.3f}\", fontsize=20)\nplt.tight_layout(True)\nplt.show()\n\n\"\"\" 收敛状态评估 \"\"\"\n\n\ndef mh_coin(niters, init_theta):\n  thetas = np.zeros(niters + 1)\n  thetas[0] = init_theta  # 设定初始参数\n  for i in range(niters):\n    # 以当前状态下,转移矩阵为N(0,sigma)\n    theta_p = norm(thetas[i], sigma).rvs()\n    #\n    rho = min(1, target(like, prior, n, h, theta_p) /\n              target(like, prior, n, h, thetas[i]))\n    u = np.random.uniform()\n    if u &lt; rho:\n      thetas[i + 1] = theta_p\n    else:\n      thetas[i + 1] = thetas[i]\n\n  return thetas\n\n\nthetass = [mh_coin(100, i) for i in np.arange(0.1, 1.1, 0.2)]\n\nfor thetas, init in zip(thetass, np.arange(0.1, 1.1, 0.2)):\n  plt.plot(thetas, '-', label=f'init={init:.1f}')\nplt.legend()\nplt.title(f\"马尔科夫链收敛状态评估\", fontsize=20)\nplt.tight_layout(True)\nplt.show()\n经过采样后我们统计采样结果的分布,可以发现和后验概率分布相同.不过可以看到他的接受率并不高.\n\n同时重复以上过程,我们可以发现进入马尔科夫链进入平稳分布的速度还是比较快的.\n\n\n\nGibbs采样\n这里我暂时没想明白,如果说依次更新参数,当如果两个参数是独立的时候..\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm, binom, beta, uniform, bernoulli, gaussian_kde, multivariate_normal\nfrom toolz import partial\n\nplt.rcParams['font.sans-serif'] = ['STZhongsong']  # 用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\n\ndef binom2(p1, p2, k1, k2, N1, N2):\n  \"\"\" 二维伯努利分布 \"\"\"\n  return binom.pmf(k1, N1, p1) * binom.pmf(k2, N2, p2)\n\n\ndef make_thetas(xmin, xmax, n):\n  xs = np.linspace(xmin, xmax, n)\n  widths = (xs[1:] - xs[:-1]) / 2.0\n  thetas = xs[:-1] + widths\n  return thetas\n\n\ndef make_plots(X, Y, prior, likelihood, posterior, projection=None):\n  fig, ax = plt.subplots(1, 3, subplot_kw=dict(\n      projection=projection), figsize=(12, 3))\n  if projection == '3d':\n    ax[0].plot_surface(X, Y, prior, alpha=0.3, cmap=plt.cm.jet)\n    ax[1].plot_surface(X, Y, likelihood, alpha=0.3, cmap=plt.cm.jet)\n    ax[2].plot_surface(X, Y, posterior, alpha=0.3, cmap=plt.cm.jet)\n    for ax_ in ax:\n      ax_._axis3don = False\n  else:\n    ax[0].contour(X, Y, prior, cmap=plt.cm.jet)\n    ax[1].contour(X, Y, likelihood, cmap=plt.cm.jet)\n    ax[2].contour(X, Y, posterior, cmap=plt.cm.jet)\n  ax[0].set_title('Prior')\n  ax[1].set_title('Likelihood')\n  ax[2].set_title('Posteior')\n\n\nthetas1 = make_thetas(0, 1, 101)\nthetas2 = make_thetas(0, 1, 101)\nX, Y = np.meshgrid(thetas1, thetas2)\n\n\"\"\" 先验分布参数 \"\"\"\na = 2\nb = 3\n\n\"\"\" 似然分布参数 \"\"\"\nk1 = 11\nN1 = 14\nk2 = 7\nN2 = 14\n\nprior = beta(a, b).pdf(X) * beta(a, b).pdf(Y)\nlikelihood = binom2(X, Y, k1, k2, N1, N2)\nposterior = beta(a + k1, b + N1 - k1).pdf(X) * beta(a + k2, b + N2 - k2).pdf(Y)\nmake_plots(X, Y, prior, likelihood, posterior)\nplt.title(f\"原始分布\", fontsize=20)\nplt.tight_layout(True)\nplt.show()\n\n\"\"\" M-H采样 \"\"\"\n\n\ndef prior(theta1, theta2): return beta(a, b).pdf(theta1) * beta(a, b).pdf(theta2)\n\n\nlik = partial(binom2, k1=k1, k2=k2, N1=N1, N2=N2)\ndef target(theta1, theta2): return prior(theta1, theta2) * lik(theta1, theta2)\n\n\nsigma = np.diag([0.2, 0.2])\ndef proposal(theta): return multivariate_normal(theta, sigma).rvs()\n\n\ndef metro_hastings(niters: int, burnin: int,\n                   theta: np.ndarray, proposal: callable,\n                   target: callable):\n  thetas = np.zeros((niters - burnin, 2), np.float)\n  for i in range(niters):\n    new_theta = proposal(theta)\n    p = min(target(*new_theta) / target(*theta), 1)\n    if np.random.rand() &lt; p:\n      theta = new_theta\n    if i &gt;= burnin:\n      thetas[i - burnin] = theta\n  return thetas\n\n\ninit_theta = np.array([0.5, 0.5])\nniters = 10000\nburnin = 500\nthetas = metro_hastings(niters, burnin, init_theta, proposal, target)\nkde = gaussian_kde(thetas.T)\nXY = np.vstack([X.ravel(), Y.ravel()])\nposterior_metroplis = kde(XY).reshape(X.shape)\nmake_plots(X, Y, prior(X, Y), lik(X, Y), posterior_metroplis)\nplt.title(f\"M-H采样\", fontsize=20)\nplt.tight_layout(True)\nplt.show()\n\n\n\"\"\" gibbs \"\"\"\n\ntheta = np.array([0.5, 0.5])\nniters = 10000\nburnin = 500\n\n\ndef gibbs(niters: int, burnin: int,\n          theta: np.ndarray, proposal: callable,\n          target: callable):\n  thetas = np.zeros((niters - burnin, 2), np.float)\n  for i in range(niters):\n    theta = [beta(a + k1, b + N1 - k1).rvs(), theta[1]]\n    theta = [theta[0], beta(a + k2, b + N2 - k2).rvs()]\n\n    if i &gt;= burnin:\n      thetas[i - burnin] = theta\n  return thetas\n\nthetas = gibbs(niters, burnin, init_theta, proposal, target)\nkde = gaussian_kde(thetas.T)\nXY = np.vstack([X.ravel(), Y.ravel()])\nposterior_metroplis = kde(XY).reshape(X.shape)\nmake_plots(X, Y, prior(X, Y), lik(X, Y), posterior_metroplis)\nplt.title(f\"Gibbs采样\", fontsize=20)\nplt.tight_layout(True)\nplt.show()\n运行结果:"
  },
  {
    "objectID": "posts/statis-learn-cp18.html",
    "href": "posts/statis-learn-cp18.html",
    "title": "统计学习方法:PageRank",
    "section": "",
    "text": "PageRank感觉没有什么东西，就是为了避免通常情况下非强连通的图中进行马尔科夫游动导致概率为0的问题，人为为所有节点添加一个随机游动概率。\n\n迭代计算法也是相当简单，当然在图相当大的时候，如何实现pagerank是一个难题，因此后面有近似法。我也没有找到一个规模比较大的数据，因此就复现了一下书中的例子。"
  },
  {
    "objectID": "posts/statis-learn-cp3.html",
    "href": "posts/statis-learn-cp3.html",
    "title": "统计学习方法:朴素贝叶斯",
    "section": "",
    "text": "明明简单的算法,代码写起来还真不轻松…感觉后面的算法我可能要偷懒了.\n\n\n核心\n因为书上的公式打打太麻烦,因此我不讲太多,建议买本书看.我这里就只记录我是怎么想的.我认为我的理解可能还是不太到位.\n我觉得是因为需要一个分类器,给定\\(Y\\)得到\\(X\\),那么分类器可以定义为\\(P(Y|X)\\),接着我们假设特征间相互独立,则根据贝叶斯公式有:\n\\[ \\begin{aligned}\n  P(Y|X)=\\frac{P(X|Y)P(Y)}{P(X)}\n\\end{aligned} \\]\n然后我们即可从先验概率\\(P(Y)\\),条件概率\\(P(X|Y)\\),计算出后验概率\\(P(Y|X)\\),然后选后验概率最大的一项作为分类类别.\n但是书本上讲的是直接学习联合概率分布\\(P(X,Y)\\),假设他的特征条件独立,那么只需要概率相乘即可: \\[\\begin{aligned}\nP(X=x,Y=c_k)&=P(X^{1}=x^{1},...,X^{n}=x^{n}|Y=c_k),k=1,2,...,K\\\\\n&=\\prod_{j=1}^n P(X^{j}=x^{j}|Y=c_k)\n\\end{aligned}\n\\]\n那么如果我想得到指定\\(x\\)时\\(y\\)的概率就可以通过带入贝叶斯公式得到: \\[\nP(Y=c_k|X=x)=\\frac{\\prod_{j} P(X^j =x^j |Y=c_k)P(Y=c_k)}{\\prod_{j} P(X^j =x^j |Y=c_k) \\sum_k P(Y=c_k) }\n\\]\n分类时选择后验概率最大化的那一项即可.\n\n\n先验分布计算\n先验分布直接统计\\(Y\\)的频率即可.\n\n\n条件概率计算\n对于多项式分布的数据,统计给定\\(Y\\)的情况下\\(X\\)的频率.\n对于高斯分布的数据,统计给定\\(Y\\)的情况下\\(X\\)的均值和方差.\n\n\n后验概率计算\n给定一个数据点.\n对于多项式分布的数据,选择之前统计的条件概率累乘.\n对于高斯分布的数据,选择之前统计的均值方差计算当前数据点的pdf再累乘.\n\n\n分类\n选择后验概率较大的一项输出."
  },
  {
    "objectID": "posts/statis-learn-cp5.html",
    "href": "posts/statis-learn-cp5.html",
    "title": "统计学习方法:逻辑回归",
    "section": "",
    "text": "这一章其实是逻辑回归和最大熵模型,最大熵模型的实现需要数个特征与定义对应的特征函数,因此我暂时没有实现."
  },
  {
    "objectID": "posts/statis-learn-cp5.html#原理",
    "href": "posts/statis-learn-cp5.html#原理",
    "title": "统计学习方法:逻辑回归",
    "section": "原理",
    "text": "原理\n假设是一个二分类问题,我们把分类问题考虑为由样本\\(X\\)得到对应\\(Y\\)的概率,那么模型可以被定义为\\(P(Y|X)\\).为了将输出分配为概率形式,使用logistic分布: \\[\n\\begin{aligned}\n  F(x)=P(X\\leq x)=\\frac{1}{1+e^{-\\theta^T x}}\n\\end{aligned}\n\\]\n那么对于一个样本计算他类别为1的概率的过程可以记做\\(p=\\sigma(\\theta^Tx)\\),此时我们面对的是二分类问题,那么自然类别为0的概率为\\(1-p\\)得到对应的概率质量函数为: \\[\n\\begin{aligned}\n  \\text{Let} \\ \\ \\ p&=\\sigma(\\theta^Tx)\\\\\n  P(y)&=\\begin{cases}p&,y=1\\\\1-p&,y=0  \\end{cases}\\\\\n      &= y^{p}+(1-y)^{1-p}\\\\\n  将P(y)表示为&P(y|x;\\theta)\n\\end{aligned}\n\\]\n有了对应的概率质量函数,我们需要求解最合适的参数\\(\\theta\\),就可以利用最大似然估计的方法,最大似然考虑到了分布函数的联合分布,当他们的联合分布概率最大时,那么\\(\\theta\\)肯定是最优的.: \\[\n\\begin{aligned}\nL(x_1,x_2,...,x_n;\\theta)&=\\prod_{i=1}^n P(y|x_i;\\theta)\\\\\n&=\\prod_{i=1}^n\\left[ \\sigma(\\theta^Tx_i)^{y_i} +(1-\\sigma(\\theta^Tx_i))^{1-y_i}\\right]\\\\\n对数化&\\\\\nL(x_1,x_2,...,x_n;\\theta)&=\\log\\left[ \\prod_{i=1}^n\\left[ \\sigma(\\theta^Tx_i)^{y_i} +(1-\\sigma(\\theta^Tx_i))^{1-y_i}\\right]\\right]\\\\\n&=\\sum_{i=1}^n\\left[ y_i \\log\\sigma(\\theta^Tx_i) +(1-y_i)\\log(1-\\sigma(\\theta^Tx_i))\\right]\\\\\n&=\\sum_{i=1}^n\\left[ y_i \\log\\frac{1}{1+e^{-\\theta^Tx_i}}  +(1-y_i)\\log(1-\\frac{1}{1+e^{-\\theta^Tx_i}})\\right]\\\\\n&=\\sum_{i=1}^n\\left[ y_i (\\theta^Tx_i) - \\log(1+e^{\\theta^Tx_i})\\right]\n\\end{aligned}\n\\]\n注意到极大似然估计对数化后的第二步实际上就等价于负的交叉熵,所以令似然估计最大化,相当于最小化交叉熵,因此模型的损失即为: \\[\n\\begin{aligned}\n  \\mathcal{L}=-\\sum_{i=1}^n\\left[ y_i (\\theta^Tx_i) - \\log(1+e^{\\theta^Tx_i})\\right]\n\\end{aligned}\n\\]\n有了损失,又可以求导.可以采用梯度下降法进行优化,梯度为如下. \\[\n\\begin{aligned}\n  \\frac{d\\mathcal{L}}{d\\theta} &=-(yx-\\frac{e^{\\theta^Tx}\\cdot x}{1+e^{\\theta^Tx}})\\\\\n  &=-(y-\\frac{1}{1+e^{-w^Tx}})x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/statis-learn-cp7.html",
    "href": "posts/statis-learn-cp7.html",
    "title": "统计学习方法:提升方法",
    "section": "",
    "text": "提升方法。这个思路可以被考虑到集成中。提升方法AdaBoost的想法是将相同类型的不同参数的弱分类器的分类结果进行集成,他是一个串行的提升过程。此方法的流程比较清楚，关于原理部分建议大家仔细看书。\n\n\n原理\n他的方法是通过串行的训练：\n\n提升弱分类器分类错误样本点的权重\n提升准确率高的弱分类器输出结果权重\n\n假设一个二类分类的训练数据集： \\[\\begin{aligned}\nD = \\{\\left( x_{1},y_{1} \\right),\\left( x_{2},y_{2} \\right),\\ldots,(x_{n},\\ y_{n})\\}\\\\\nx_{i} \\in X \\subseteq \\mathbb{R}^{n}\\ ;y_{i} \\in Y = \\{ - 1,\\  + 1\\}\n\\end{aligned}\n\\]\n开始训练，假设我们迭代\\(M\\)次，首先初始化权重分布\\(W_k\\)为均匀分布，对于\\(k = 0,1,\\ldots,\\ M - 1\\)：\n\n利用权重\\(W_k\\)训练弱分类器：\n\n\\[\\begin{aligned}\ng_{k}(x):X \\rightarrow \\{ - 1,\\  + 1\\}\n\\end{aligned}\n\\]\n\n计算分类器\\(g_{k}(x)\\)的加权错误率：\n\n\\[\\begin{aligned}\ne_{k} = \\sum_{i = 1}^{N}{w_{\\text{k|i}}I(g_{k}\\left( x_{i} \\right) \\neq y_{i})}\n\\end{aligned}\n\\]\n\n根据加权错误率计算分类器的输出结果权值： \\[\\begin{aligned}\n  \\alpha_{k} = \\frac{1}{2}\\ln\\frac{1 - e_{k}}{e_{k}}\n\\end{aligned}\n\\]\n根据分类器\\(g_{k}(x)\\)的错误率修改训练数据的权重，对于误分类的样本要增大权重，反之则减少。\n\n\\[\n\\begin{aligned}\n  w_{k + 1,i} = \\frac{w_{\\text{k|i}}}{Z_{k}} \\cdot exp( - \\alpha_{k}y_{i}g_{k}(x_{i}))\\\\\n  Z_{k}=\\sum_{i=1}^N w_{\\text{k|i}} \\cdot exp( - \\alpha_{k}y_{i}g_{k}(x_{i}))\n\\end{aligned}\n\\]\n\n重复以上过程，迭代结束后最终分类器如下：\n\n\\[\n\\begin{aligned}\n  g\\left( x \\right) =  \\text{sign}\\left( \\sum_{k = 1}^{M}{\\alpha_{k}g_{k}\\left( x \\right)} \\right)\n\\end{aligned}\n\\]\n\n\n题外话\n错误率-权重映射关系："
  },
  {
    "objectID": "posts/statis-learn-cp9.html",
    "href": "posts/statis-learn-cp9.html",
    "title": "统计学习方法:隐马尔可夫模型",
    "section": "",
    "text": "Hidden Markov Model属于生成模型，是带时序的概率模型。时间关系我没有实现他的参数估计方法，等待有缘人pr一波。"
  },
  {
    "objectID": "posts/statis-learn-cp9.html#基本假设",
    "href": "posts/statis-learn-cp9.html#基本假设",
    "title": "统计学习方法:隐马尔可夫模型",
    "section": "基本假设",
    "text": "基本假设\n\n齐次马尔可夫性假设，即隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态**，与其他时刻状态及观测无关，也与时刻t无关：\n\n\\[\n\\begin{aligned}\nP(i_{t}|i_{t-1},o_{t-1},...,i_{1},o_{1}) = P(i_{t}|i_{t-1}),  t = 1,2,...,T\n\\end{aligned}\n\\]\n\n观察独立性假设，即假设任意时刻的观测只依赖与该时刻的马尔可夫链的状态，与其他观测及状态无关\n\n\\[\n\\begin{aligned}\nP(o_{t}|i_{T},o_{T},i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_{t},i_{t-1},o_{t-1},...,i_{1},o_{1} = P(o_{t}|i_{t})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/statis-learn-cp9.html#应用",
    "href": "posts/statis-learn-cp9.html#应用",
    "title": "统计学习方法:隐马尔可夫模型",
    "section": "应用",
    "text": "应用\n\n概率计算\n\n给定模型为\\(\\lambda = (A, B, \\pi)\\)，在观察到结果序列为\\(O=(o_{1},o_{2},...,o_{T})\\)的情况下，计算此序列的出现概率\\(P(O|\\lambda)\\)\n\n参数学习\n\n已知观测序列\\(O=(o_{1},o_{2},...,o_{T})\\)，求解模型参数\\(\\lambda = (A, B, \\pi)\\)。使得该模型下观测序列的概率值最大\\(\\text{argmax}_\\lambda P(O|\\lambda)\\)\n\n状态序列预测\n\n给定模型为\\(\\lambda = (A, B, \\pi)\\)，在观察到结果序列为\\(O=(o_{1},o_{2},...,o_{T})\\)的情况下，求解使得定观测序列条件概率\\(P(I|O)，I = (i_{1}, i_{2}, i_{3},...,i_{T})\\)最大的状态序列。"
  },
  {
    "objectID": "posts/stft.html",
    "href": "posts/stft.html",
    "title": "声音信号处理-STFT变化",
    "section": "",
    "text": "短时傅里叶变换"
  },
  {
    "objectID": "posts/stft.html#变换方程",
    "href": "posts/stft.html#变换方程",
    "title": "声音信号处理-STFT变化",
    "section": "变换方程",
    "text": "变换方程\n\\[\n\\begin{align}\nX_l[k]&=\\sum^{N/2-1}_{n=-N/2}w[n]x[n+lH]e^{-j2\\pi kn/N}\\ \\ \\ \\ l=0,1,\\ldots, \\\\\nw&= 分析窗口\\\\\nl&= 帧索引\\\\\nH&= 跳跃大小\n\\end{align}\n\\]\n与DFT几乎没有大的区别,但是有了l,H,是对于一个时间段进行分析. 所以输出不是单个频谱而是一系列频谱,每一个频谱都有相同的长度,但是每一个都相同,因为输入的时间片段是不一样的.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.io.wavfile import read\n\n(fs, x) = read('../sounds/oboe-A4.wav')\nM = 256\nH = 128\nstart = int(.8*fs)\n\nplt.figure(1)\nx0 = x[start:start+3*M]/float(max(x))\nplt.plot(x0)\nplt.axis([0, 3*M, min(x0), max(x0)+5.5])\n\noffset = 1.5\nx1 = np.zeros(3*M)+offset\nx1[0:M] += (x0[0:M] * np.hamming(M))\nplt.plot(x1,'b')\n\noffset = 2.5\nx2 = np.zeros(3*M)+offset\nx2[H:M+H] += (x0[H:M+H] * np.hamming(M))\nplt.plot(x2,'b')\n\noffset = 3.5\nx2 = np.zeros(3*M)+offset\nx2[H*2:M+H*2] += (x0[2*H:M+H*2] * np.hamming(M))\nplt.plot(x2,'b')\n\noffset = 4.5\nx2 = np.zeros(3*M)+offset\nx2[H*3:M+H*3] += (x0[3*H:M+H*3] * np.hamming(M))\nplt.plot(x2,'b')\n\noffset = 5.5\nx2 = np.zeros(3*M)+offset\nx2[H*4:M+H*4] += (x0[4*H:M+H*4] * np.hamming(M))\nplt.plot(x2,'b')\n\nplt.tight_layout()\nplt.show()\n\n可以从上图观察到每一个时间片段的计算情况. 通过使用分析窗口将声音加窗与计算.最后能获得所有声音作为基本片段的总和."
  },
  {
    "objectID": "posts/stft.html#转换一个窗口的正弦波",
    "href": "posts/stft.html#转换一个窗口的正弦波",
    "title": "声音信号处理-STFT变化",
    "section": "转换一个窗口的正弦波",
    "text": "转换一个窗口的正弦波\n\\[\n\\begin{aligned}\nx[n]&=A_0cos(2 \\pi k_0 n/N)=\\frac{A_0}{2}e^{j2\\pi k_0 n/N}+\\frac{A_0}{2}e^{-j2\\pi k_0 n/N} \\\\\nX[k]&=\\sum^{N/2-1}_{n=-N/2}w[n]x[n]e^{-j2\\pi kn/N}　\\\\\n&=\\sum^{N/2-1}_{n=-N/2}w[n](\\frac{A_0}{2}e^{j2\\pi k_0 n/N}+\\frac{A_0}{2}e^{-j2\\pi k_0 n/N})e^{-j2\\pi kn/N} \\\\\n&=\\sum^{N/2-1}_{n=-N/2}w[n]\\frac{A_0}{2}e^{j2\\pi k_0 n/N}e^{-j2\\pi kn/N}+\\sum^{N/2-1}_{n=-N/2}w[n]\\frac{A_0}{2}e^{-j2\\pi k_0 n/N}e^{-j2\\pi kn/N}\\\\\n&=\\frac{A_0}{2}\\sum^{N/2-1}_{n=-N/2}w[n]e^{-j2\\pi(k-k_0)n/N}+\\frac{A_0}{2}\\sum^{N/2-1}_{n=-N/2}w[n]e^{-j2\\pi(k+k_0)n/N} \\\\\n&=\\frac{A_0}{2}W[k-k_0]+\\frac{A_0}{2}W[k+k_0]\n\\end{aligned}\n\\]\n首先一个正弦波可以分离成两个复数信号.一个具有正频率一个具有负频率.接下来我们将x带入stft方程中,经过化简得到结果.\n结果即为此窗口信号的频谱,频率偏移了输入信号的频率,并乘上了幅度.当然还有另一半(负频率).\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.fftpack import fft, ifft\n\nN = 256\nM = 63\nf0 = 1000\nfs = 10000\nA0 = .8 \nhN = N//2 \nhM = (M+1)//2\nfftbuffer = np.zeros(N)\nX1 = np.zeros(N, dtype='complex')\nX2 = np.zeros(N, dtype='complex')\n\nx = A0 * np.cos(2*np.pi*f0/fs*np.arange(-hM+1,hM))\n\nplt.figure(1, figsize=(9.5, 7))\nw = np.hanning(M)\nplt.subplot(2,3,1)\nplt.title('w (hanning window)')\nplt.plot(np.arange(-hM+1, hM), w, 'b', lw=1.5)\nplt.axis([-hM+1, hM, 0, 1])\n\nfftbuffer[:hM] = w[hM-1:]\nfftbuffer[N-hM+1:] = w[:hM-1]  \nX = fft(fftbuffer)\nX1[:hN] = X[hN:]\nX1[N-hN:] = X[:hN]\nmX = 20*np.log10(abs(X1))       \n\nplt.subplot(2,3,2)\nplt.title('mW')\nplt.plot(np.arange(-hN, hN), mX, 'r', lw=1.5)\nplt.axis([-hN,hN,-40,max(mX)])\n\npX = np.angle(X1)\nplt.subplot(2,3,3)\nplt.title('pW')\nplt.plot(np.arange(-hN, hN), np.unwrap(pX), 'c', lw=1.5)\nplt.axis([-hN,hN,min(np.unwrap(pX)),max(np.unwrap(pX))])\n\nplt.subplot(2,3,4)\nplt.title('xw (windowed sinewave)')\nxw = x*w\nplt.plot(np.arange(-hM+1, hM), xw, 'b', lw=1.5)\nplt.axis([-hM+1, hM, -1, 1])\n\nfftbuffer = np.zeros(N)\nfftbuffer[0:hM] = xw[hM-1:]\nfftbuffer[N-hM+1:] = xw[:hM-1]\nX = fft(fftbuffer)\nX2[:hN] = X[hN:]\nX2[N-hN:] = X[:hN]\nmX2 = 20*np.log10(abs(X2))  \n\nplt.subplot(2,3,5)\nplt.title('mXW')\nplt.plot(np.arange(-hN, hN), mX2, 'r', lw=1.5)\nplt.axis([-hN,hN,-40,max(mX)])\n\npX = np.angle(X2)\nplt.subplot(2,3,6)\nplt.title('pXW')\nplt.plot(np.arange(-hN, hN), np.unwrap(pX), 'c', lw=1.5)\nplt.axis([-hN,hN,min(np.unwrap(pX)),max(np.unwrap(pX))])\n\nplt.tight_layout()\nplt.show()\n\n\n观察上图:\n第一幅图是一个汉宁窗,下面则是加窗之后的正弦波.上面左边分别为汉宁窗的幅度谱与相位谱.\n当我们对加窗之后的正弦波进行DFT之后,可以看到得到的幅度谱与相位谱与上面相似,只是波峰点移动到了正弦波的频率处.并且由于正负频率特性,波形是对称的."
  },
  {
    "objectID": "posts/stft.html#分析窗口",
    "href": "posts/stft.html#分析窗口",
    "title": "声音信号处理-STFT变化",
    "section": "分析窗口",
    "text": "分析窗口\n分析窗口通常一个实函数,并且在原点周围是对称的.\n关于窗口,中间的主瓣我们主要关注主瓣宽度,对于旁瓣我们关注与旁瓣的最高水平.\n\n矩形窗函数\n\\[\n\\begin{aligned}\nw[n]&=\\begin{cases}\n1,\\ \\ \\ n=-M/2,\\ldots,0,\\ldots,M/2 \\\\\n0,\\ \\ \\ n=elsewhere\n\\end{cases}\\\\\nW[k]&=\\frac{sin(\\pi k)}{sin(\\pi k/M)}\n\\end{aligned}\n\\]\n矩形窗是最基础的窗函数,他的时域图形比较简单,但是频域图形相当有趣.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.fftpack import fft\n\nM = 64\nN = 1024\nhN = N//2     \nhM = M//2\nfftbuffer = np.zeros(N)\nmX1 = np.zeros(N)\n\nplt.figure(1, figsize=(9.5, 6))\nfftbuffer[hN-hM:hN+hM]=np.ones(M)\nplt.subplot(2,1,1)\nplt.plot(np.arange(-hN, hN), fftbuffer, 'b', lw=1.5)\nplt.axis([-hN, hN, 0, 1.1])\nplt.title('w (rectangular window), M = 64')\n\n\nX = fft(fftbuffer)\nmX = 20*np.log10(abs(X)) \nmX1[:hN] = mX[hN:]\nmX1[N-hN:] = mX[:hN]      \n\nplt.subplot(2,1,2)\nplt.plot(np.arange(-hN, hN), mX1-max(mX), 'r', lw=1.5)\nplt.axis([-hN,hN,-40,0])\nplt.title('mW, N = 1024')\nplt.annotate('main-lobe', xy=(0,-10), xytext=(-200, -5), fontsize=16, arrowprops=(dict(facecolor='black', width=2, headwidth=6, shrink=0.01)))\nplt.annotate('highest side-lobe', xy=(32,-13), xytext=(100, -10), fontsize=16, arrowprops=(dict(facecolor='black', width=2, headwidth=6, shrink=0.01)))\n\n\nplt.tight_layout()\n\nplt.show()\nprint(\"主瓣宽度:2bins\\n旁瓣最大值:-13.3dB\")\n\n主瓣宽度:2bins\n旁瓣最大值:-13.3dB\n\n\n汉明窗\n最常用的窗口函数,是一个升余弦函数.\n\\[\n\\begin{aligned}\nw[n]&=0.5+0.5cos(2\\pi n/M),\\ \\ \\ \\ n=-M/2,\\ldots,0,\\ldots,M/2\\\\\nW[k]&=0.5D[k]+0.25(D[k-1]+D[k+1])\\ \\ \\ \\ where D[k]=\\frac{sin(\\pi k)}{sin(\\pi k/M)}\n\\end{aligned}\n  \\]\n他的主瓣宽度更宽,旁瓣宽度更低.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.fftpack import fft\n\nM = 64\nN = 512\nhN = N//2     \nhM = M//2\nfftbuffer = np.zeros(N)\nmX1 = np.zeros(N)\n\nplt.figure(1, figsize=(7.5, 4))\nfftbuffer[hN-hM:hN+hM]=np.hamming(M)\nplt.subplot(2,1,1)\nplt.plot(np.arange(-hN, hN), fftbuffer, 'b', lw=1.5)\nplt.axis([-hN, hN, 0, 1.1])\n\n\nX = fft(fftbuffer)\nmX = 20*np.log10(abs(X)) \nmX1[:hN] = mX[hN:]\nmX1[N-hN:] = mX[:hN]      \n\nplt.subplot(2,1,2)\nplt.plot(np.arange(-hN, hN), mX1-max(mX), 'r', lw=1.5)\nplt.axis([-hN,hN,-60,0])\n\nplt.tight_layout()\nplt.show()\nprint(\"主瓣宽度:4 bins\\n旁瓣最高:-42.7dB\")\n\n主瓣宽度:4 bins\n旁瓣最高:-42.7dB\n\n\n布莱克曼窗\n布莱克曼窗口是两个正弦曲线的总和.\n\\[\n\\begin{aligned}\nw[n]=0.42-0.5cos(2\\pi n/M)+0.08cos(4\\pi n/M)\n\\end{aligned}\n\\]\n他比汉明窗两侧少了一些突变.所以在旁瓣方面有着显著改善.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.fftpack import fft\nfrom scipy import signal\n\nM = 64\nN = 512\nhN = N//2     \nhM = M//2\nfftbuffer = np.zeros(N)\nmX1 = np.zeros(N)\n\nplt.figure(1, figsize=(7.5, 4))\nfftbuffer[hN-hM:hN+hM]=signal.blackmanharris(M)\nplt.subplot(2,1,1)\nplt.plot(np.arange(-hN, hN), fftbuffer, 'b', lw=1.5)\nplt.axis([-hN, hN, 0, 1.1])\n\n\nX = fft(fftbuffer)\nmX = 20*np.log10(abs(X)) \nmX1[:hN] = mX[hN:]\nmX1[N-hN:] = mX[:hN]      \n\nplt.subplot(2,1,2)\nplt.plot(np.arange(-hN, hN), mX1-max(mX), 'r', lw=1.5)\nplt.axis([-hN,hN,-110,0])\n\nplt.tight_layout()\nplt.show()\nprint(\"主瓣宽度:6 bins\\n旁瓣最大值:-58dB\")\n\n主瓣宽度:6 bins\n旁瓣最大值:-58dB\n\n\n布莱克曼哈里斯窗\n这是一个特殊的窗口．\n\\[\n\\begin{aligned}\nw(n)=\\frac{1}{M}\\sum^3_{l=0}\\alpha_lcos(2nl\\pi /M ),\\ \\ \\ \\ n=-M/2,\\ldots,0,\\ldots,M/2\\\\\nwhere \\alpha_0=0.35875\\ \\ \\alpha_1=0.48829\\ \\ \\alpha_0=0.14128\\ \\ \\alpha_0=0.01168\\ \\\n\\end{aligned}\n\\]\n可以说他基本没有旁瓣，他的旁瓣电平低于-92dB。以信噪比的方式去考虑，这是一个非常重要的数字，92dB基本上低于16的底噪。\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.fftpack import fft\nfrom scipy import signal\n\nM = 64\nN = 512\nhN = N//2     \nhM = M//2\nfftbuffer = np.zeros(N)\nmX1 = np.zeros(N)\n\nplt.figure(1, figsize=(7.5, 4))\nfftbuffer[hN-hM:hN+hM]=signal.blackmanharris(M)\nplt.subplot(2,1,1)\nplt.plot(np.arange(-hN, hN), fftbuffer, 'b', lw=1.5)\nplt.axis([-hN, hN, 0, 1.1])\n\n\nX = fft(fftbuffer)\nmX = 20*np.log10(abs(X)) \nmX1[:hN] = mX[hN:]\nmX1[N-hN:] = mX[:hN]      \n\nplt.subplot(2,1,2)\nplt.plot(np.arange(-hN, hN), mX1-max(mX), 'r', lw=1.5)\nplt.axis([-hN,hN,-110,0])\n\nplt.tight_layout()\nplt.show()\nprint(\"主瓣宽度:8 bins\\n旁瓣最大值:-92dB\")\n\n主瓣宽度:8 bins\n旁瓣最大值:-92dB"
  },
  {
    "objectID": "posts/stft.html#结果比较",
    "href": "posts/stft.html#结果比较",
    "title": "声音信号处理-STFT变化",
    "section": "结果比较",
    "text": "结果比较\n现在比较几个窗口的处理的不同输出结果：\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time, os, sys\n\nsys.path.append('../software/models/')\n\nimport dftModel as DF\nimport utilFunctions as UF\nfrom scipy.fftpack import fft, ifft\nimport math\n\n(fs, x) = UF.wavread('../sounds/oboe-A4.wav')\nN = 512\npin = 5000\nw = np.ones(501)\nhM1 = int(math.floor((w.size+1)/2)) \nhM2 = int(math.floor(w.size/2))  \nx1 = x[pin-hM1:pin+hM2]\n\n\nplt.figure(1, figsize=(9.5, 7))\nplt.subplot(4,1,1)\nplt.plot(np.arange(-hM1, hM2), x1, lw=1.5)\nplt.axis([-hM1, hM2, min(x1), max(x1)])\nplt.title('x (oboe-A4.wav)')\n\nmX, pX = DF.dftAnal(x1, w, N)\nmX = mX - max(mX)\n\nplt.subplot(4,1,2)\nplt.plot(np.arange(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,N/4,-70,0])\nplt.title ('mX (rectangular window)')\n\nw = np.hamming(501)\nmX, pX = DF.dftAnal(x1, w, N)\nmX = mX - max(mX)\n\nplt.subplot(4,1,3)\nplt.plot(np.arange(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,N/4,-70,0])\nplt.title ('mX (hamming window)')\n\nw = np.blackman(501)\nmX, pX = DF.dftAnal(x1, w, N)\nmX = mX - max(mX)\n\nplt.subplot(4,1,4)\nplt.plot(np.arange(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,N/4,-70,0])\nplt.title ('mX (blackman window)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/stft.html#窗口尺寸",
    "href": "posts/stft.html#窗口尺寸",
    "title": "声音信号处理-STFT变化",
    "section": "窗口尺寸",
    "text": "窗口尺寸\n窗口的尺寸当然可以变化,但是尺寸的变化影响着什么? 看下面这个例子:\n小窗口我们可以看到谐波很少,信息很少,样本很少.\n大窗口样本很多,可以看到更多的声音细节,但是噪音也更加多.\n所以这将是一个重要的选择.\n(fs, x) = UF.wavread('../sounds/oboe-A4.wav')\nN = 128\nstart = int(.81*fs)\nx1 = x[start:start+N] \nplt.figure(1, figsize=(9.5, 6))\nplt.subplot(321)\nplt.plot(np.arange(start, (start+N), 1.0)/fs, x1*np.hamming(N), 'b', lw=1.5)\nplt.axis([start/fs, (start+N)/fs, min(x1*np.hamming(N)), max(x1*np.hamming(N))])\nplt.title('x1, M = 128')\n\nmX, pX = DF.dftAnal(x1, np.hamming(N), N)\nplt.subplot(323)\nplt.plot((fs/2.0)*np.arange(mX.size)/float(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,fs/2.0,-90,max(mX)])\nplt.title('mX1')\n\nplt.subplot(325)\nplt.plot((fs/2.0)*np.arange(mX.size)/float(mX.size), pX, 'c', lw=1.5)\nplt.axis([0,fs/2.0,min(pX),max(pX)])\nplt.title('pX1')\n\nN = 1024\nstart = int(.81*fs)\nx2 = x[start:start+N]\nmX, pX = DF.dftAnal(x2, np.hamming(N), N)\n\nplt.subplot(322)\nplt.plot(np.arange(start, (start+N), 1.0)/fs, x2*np.hamming(N), 'b', lw=1.5)\nplt.axis([start/fs, (start+N)/fs, min(x2), max(x2)])\nplt.title('x2, M = 1024')\n\nplt.subplot(324)\nplt.plot((fs/2.0)*np.arange(mX.size)/float(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,fs/2.0,-90,max(mX)])\nplt.title('mX2')\n\nplt.subplot(326)\nplt.plot((fs/2.0)*np.arange(mX.size)/float(mX.size), pX, 'c', lw=1.5)\nplt.axis([0,fs/2.0,min(pX),max(pX)])\nplt.title('pX2')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/stft.html#奇偶窗口",
    "href": "posts/stft.html#奇偶窗口",
    "title": "声音信号处理-STFT变化",
    "section": "奇偶窗口",
    "text": "奇偶窗口\n另外的一个问题是,即使一个微小的差异(窗口的奇偶不同),对于相位谱影响巨大.\n当我们做fft的时候,最好使用奇数窗口,这样我们的相位谱可以正好对称.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.fftpack import fft, fftshift\nfrom scipy import signal\n\nM = 32\nN = 128\nhN = N//2     \nhM = M//2\nfftbuffer = np.zeros(N)\nw = signal.blackman(M)\n\nplt.figure(1, figsize=(9.5, 6))\n\nplt.subplot(3,2,1)\nplt.plot(np.arange(-hM, hM), w, 'b', lw=1.5)\nplt.axis([-hM, hM-1, 0, 1.05])\nplt.title('w1, M=32')\n\nfftbuffer = np.zeros(N)                         \nfftbuffer[:hM] = w[hM:] \nfftbuffer[N-hM:] = w[:hM]\nX = fft(fftbuffer)\nmX = 20*np.log10(abs(fftshift(X)))    \nplt.subplot(3,2,3)\nplt.plot(np.arange(-hN, hN), mX-max(mX), 'r', lw=1.5)\nplt.axis([-hN//2,hN//2,-80,1])\nplt.title('mW1')\n\npX = np.angle(fftshift(X))\nplt.subplot(3,2,5)\nplt.plot(np.arange(-hN, hN), pX, 'c', lw=1.5)\nplt.axis([-hN,hN-1,-np.pi,np.pi])\nplt.title('pW1')\n\nM = 31\nN = 128\nhN = N//2     \nhM = (M+1)//2\nfftbuffer = np.zeros(N)\nw = signal.blackman(M)\n\nplt.subplot(3,2,2)\nplt.plot(np.arange(-hM, hM-1), w, 'b', lw=1.5)\nplt.axis([-hM, hM, 0, 1.05])\nplt.title('w2, M=31')\n\nfftbuffer = np.zeros(N) \nfftbuffer[:hM] = w[hM-1:] \nfftbuffer[N-hM+1:] = w[:hM-1]                         \nX = fft(fftbuffer)\nmX = 20*np.log10(abs(fftshift(X)))    \nplt.subplot(3,2,4)\nplt.plot(np.arange(-hN, hN), mX-max(mX), 'r', lw=1.5)\nplt.axis([-hN/2,hN/2-1,-80,1])\nplt.title('mW2')\n\npX = np.angle(fftshift(X))\nplt.subplot(3,2,6)\nplt.plot(np.arange(-hN, hN), pX, 'c', lw=1.5)\nplt.axis([-hN,hN-1,-np.pi,np.pi])\nplt.title('pW2')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/stft.html#fft尺寸",
    "href": "posts/stft.html#fft尺寸",
    "title": "声音信号处理-STFT变化",
    "section": "FFT尺寸",
    "text": "FFT尺寸\n从给定的512窗口大小开始,我们选择不同的FFT尺寸,第一幅图我们可以得到一个不错的图.当尺寸更大的时候我们可以得到更加光滑的图片.\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time, os, sys\n(fs, x) = UF.wavread('../sounds/oboe-A4.wav')\nM = 512\nN = 512\nstart = int(.8*fs)\nx1 = x[start:start+M]\nxw = x1 * np.hamming(M) \n\nplt.figure(1, figsize=(9.5, 6))\nplt.subplot(311)\nplt.plot(np.arange(start, (start+M), 1.0)/fs, xw, 'b', lw=1.5)\nplt.axis([start/fs, (start+M)/fs, min(xw), max(xw)])\nplt.title('x (oboe-A4.wav), M = 512')\nmX, pX = DF.dftAnal(x1, np.hamming(N), N)\n\nplt.subplot(312)\nplt.plot((fs/2.0)*np.arange(mX.size)/float(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,fs/4.0,-85,max(mX)])\nplt.title('mX, N = 512')\n\nM = 512\nN = 2048\nstart = int(.8*fs)\nx1 = x[start:start+M]\nxw = x1 * np.hamming(M)\nmX, pX = DF.dftAnal(x1, np.hamming(M), N)\n         \nplt.subplot(313)\nplt.plot((fs/2.0)*np.arange(mX.size)/float(mX.size), mX, 'r', lw=1.5)\nplt.axis([0,fs/4.0,-85,max(mX)])\nplt.title('mX, N = 2048')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/stft.html#跳跃",
    "href": "posts/stft.html#跳跃",
    "title": "声音信号处理-STFT变化",
    "section": "跳跃",
    "text": "跳跃\n这是窗函数的步进长度,称为跳跃. 下面展示两个不同的跳跃大小所带来的变化.\n第一个例子,窗口大小为201,跳跃大小为100,大约窗大小的一半.红色线条是窗口的总和,可以看到这个总和不是很好,在震荡.\n第二个例子,跳跃大小为窗口大小的四分之一.现在所得到的总和就是我们所需要的.\nplt.figure(1, figsize=(9.5, 6))\n\nN= 1000\nM = 201\nw = signal.blackman(M)\nw1 = w/sum(w)\n\ny = np.zeros(N)\nH = 100\npin = 0\npend = N - M\nplt.subplot(211)\nwhile pin&lt;pend:\n    y [pin:pin+M] += w1*H\n    plt.plot(np.arange(pin, pin+M), w, 'b', lw=1.5)\n    pin += H\nplt.plot(np.arange(0, N), y, 'r', lw=1.5)\nplt.axis([0, N-H, 0, max(y)+.01])\nplt.title('Blackman, M=201, H=100')\n\ny = np.zeros(N)\nH = 50\npin = 0\npend = N - M\nplt.subplot(212)\nwhile pin&lt;pend:\n    y [pin:pin+M] += w1*H\n    plt.plot(np.arange(pin, pin+M), w, 'b', lw=1.5)\n    pin += H\nplt.plot(np.arange(0, N), y, 'r', lw=1.5)\nplt.axis([0, N-H, 0, max(y)+.01])\nplt.title('Blackman, M=201, H=50')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/stft.html#分析真实信号",
    "href": "posts/stft.html#分析真实信号",
    "title": "声音信号处理-STFT变化",
    "section": "分析真实信号",
    "text": "分析真实信号\n终于我们掌握了基本原理,可以来分析一下真实的信号:\n在第一幅图上我们看到的是幅度谱.经过STFT的分析,得到是一个DFT序列.纵轴是频率,横轴是时间,颜色强度是对应的幅值.实际上这就是常说的语谱图.\n这里我们可以清楚的看到钢琴的四次敲击,以及横轴对钢琴的谐波反应.对于窗口的大小,如果窗口小,可以得到一个好的时间分辨率,大的窗口会得到较好的频率分辨率.\nimport numpy as np\nimport time, os, sys\nimport stft as STFT\nimport utilFunctions as UF\nimport matplotlib.pyplot as plt\nfrom scipy.signal import hamming\nfrom scipy.fftpack import fft\nimport math\n\n(fs, x) = UF.wavread('../sounds/piano.wav')\n\nplt.figure(1, figsize=(9.5, 6))\n\nw = np.hamming(256)\nN = 256\nH = 128\nmX1, pX1 = STFT.stftAnal(x, w, N, H)\nplt.subplot(211)\nnumFrames = int(mX1[:,0].size)\nfrmTime = H*np.arange(numFrames)/float(fs)                             \nbinFreq = np.arange(mX1[0,:].size)*float(fs)/N                         \nplt.pcolormesh(frmTime, binFreq, np.transpose(mX1),cmap=plt.get_cmap('rainbow'))\nplt.title('mX (piano.wav), M=256, N=256, H=128')\nplt.autoscale(tight=True)\n\nw = np.hamming(1024)\nN = 1024\nH = 128\nmX2, pX2 = STFT.stftAnal(x, w, N, H)\n\nplt.subplot(212)\nnumFrames = int(mX2[:,0].size)\nfrmTime = H*np.arange(numFrames)/float(fs)                             \nbinFreq = np.arange(mX2[0,:].size)*float(fs)/N                         \nplt.pcolormesh(frmTime, binFreq, np.transpose(mX2),cmap=plt.get_cmap('rainbow'))\nplt.title('mX (piano.wav), M=1024, N=1024, H=128')\nplt.autoscale(tight=True)\n\nplt.tight_layout()\nplt.show()\n\n\n分析STFT的幅度谱和相位谱\n我们也可以可视化幅度谱和相位谱\nimport stft as STFT\nimport utilFunctions as UF\nimport matplotlib.pyplot as plt\nfrom scipy.signal import hamming\nfrom scipy.fftpack import fft\nimport math\n\n(fs, x) = UF.wavread('../sounds/piano.wav')\nw = np.hamming(1001)\nN = 1024\nH = 256\nmX, pX = STFT.stftAnal(x, w, N, H)\n\nplt.figure(1, figsize=(9.5, 6))\n\nplt.subplot(211)\nnumFrames = int(mX[:,0].size)\nfrmTime = H*np.arange(numFrames)/float(fs)                             \nbinFreq = np.arange(N/2+1)*float(fs)/N                         \nplt.pcolormesh(frmTime, binFreq, np.transpose(mX),cmap=plt.get_cmap('rainbow'))\nplt.title('mX (piano.wav), M=1001, N=1024, H=256')\nplt.autoscale(tight=True)\n\nplt.subplot(212)\nnumFrames = int(pX[:,0].size)\nfrmTime = H*np.arange(numFrames)/float(fs)                             \nbinFreq = np.arange(N/2+1)*float(fs)/N                         \nplt.pcolormesh(frmTime, binFreq, np.diff(np.transpose(pX),axis=0),cmap=plt.get_cmap('rainbow'))\nplt.title('pX derivative (piano.wav), M=1001, N=1024, H=256')\nplt.autoscale(tight=True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/stft.html#stft",
    "href": "posts/stft.html#stft",
    "title": "声音信号处理-STFT变化",
    "section": "STFT",
    "text": "STFT\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os,sys\nfrom scipy.signal import get_window\nsys.path.append('../software/models/')\nimport utilFunctions as UF\nimport stft as STFT\n\ninputfile='../sounds/flute-A4.wav'\nwindow='hamming'\nM=801\nN=1024\nH=400\nfs,x=UF.wavread(inputfile)\nw=get_window(window,M)\n\nmX,pX=STFT.stftAnal(x,w,N,H)\nplt.pcolor(mX.T,cmap=plt.get_cmap('rainbow'))"
  },
  {
    "objectID": "posts/tensordsl.html",
    "href": "posts/tensordsl.html",
    "title": "Tensor DSL总结",
    "section": "",
    "text": "本文旨在总结一些张量优化的DSL是如何设计的, 尝试从其中发现一些共同点. 接下来我将统一使用Matmul(Transpose(Conv(lhs)),rhs)的例子在不同的框架中进行测试."
  },
  {
    "objectID": "posts/tensordsl.html#dsl语法",
    "href": "posts/tensordsl.html#dsl语法",
    "title": "Tensor DSL总结",
    "section": "1.1 DSL语法",
    "text": "1.1 DSL语法\n首先结合论文中的例子讲一下reindex的原理:\ndef conv(x, p):\n  N,C,H,W = x.shape \n  o,i,h,w = p.shape \n  xx = x.reindex(shape=(N,o,H,W,i,h,w),\n                 indexes=(\"i0\", \"i4\", \"i2-i5\", \"i3-i6\") )\n  pp = p.broadcast(xx.shape, dims=(0,2,3))\n  yy = xx*pp\n  y = yy.sum(dims=(4,5,6))\n  return y\n这里其实是把shape看作为循环层级, 这里的reindex相当于在7层循环的最内层中做类似xx[N,o,H,W,i,h,w] = x[N,i,H-h,W-w]的索引. 然后再把weights也通过boradcast扩展到同样的循环层级pp[N,o,H,W,i,h,w] = p[o,i,h,w], 在7层循环内部执行xx[N,o,H,W,i,h,w]*pp[N,o,H,W,i,h,w]的操作, 等价于执行x[N,i,H-h,W-w] * p[o,i,h,w], 然后对i,h,w三层循环做求和.\n可以说通过reindex+broadcast操作, 完成了类似于polyhedral中2d+1表示中的loop dimension align和修改access relation(由indexes指定). Jittor这里并没有考虑让开发者自行调度算子, 后续的优化都交给编译器自动化."
  },
  {
    "objectID": "posts/tensordsl.html#测试例子",
    "href": "posts/tensordsl.html#测试例子",
    "title": "Tensor DSL总结",
    "section": "1.2 测试例子",
    "text": "1.2 测试例子\nimport jittor as jt\n\ndef conv(x, p):\n  N,C,H,W = x.shape \n  o,i,h,w = p.shape \n  xx = x.reindex(shape=(N,o,H,W,i,h,w),\n                 indexes=(\"i0\", \"i4\", \"i2-i5\", \"i3-i6\"))\n  pp = p.broadcast(xx.shape, dims=(0,2,3))\n  yy = xx*pp\n  y = yy.sum(dims=(4,5,6))\n  return y\n\ndef matmul(a,b):\n  bc, c, m,k = a.shape\n  _, _, _,n = b.shape\n  shape = [bc, c, m, k, n]\n  a = a.broadcast(shape, [-1]) # [m,k, ] -&gt; [m,k,n]\n  b = b.broadcast(shape, [-3]) # [ ,k,n] -&gt; [m,k,n]\n  return (a*b).sum(-2)\n\nlhs = jt.randn(8,3,32,32)\nkernel = jt.randn(16,3,3,3)\nrhs = jt.randn(8,32,16,64)\njt.flags.compile_options={\"compile_shapes\":1}\nwith jt.profile_scope() as report:\n    output = matmul(jt.transpose(conv(lhs, kernel), [0,2,3,1]), rhs).fetch_sync()\njt.flags.compile_options={}\n编译后得到:\nProfile result, sorted by TotalTime\n('it/s' represent number of iterations per sec)\n      Name  FileName     Count TotalTime    %,cum%   AvgTime   MinTime   MaxTime     Input    Output     InOut   Compute\nTotal time:    12.8ms\nTotal Memory Access:    6.19MB\n[opkey0:broadcast_to[Tx:float32][DIM=7][BCAST=d][JIT:1][JIT_cpu:1][index_t:int32]][opkey1:reindex[Tx:float32][XDIM=4][YDIM=7][OVERFLOW:itof(0x0)][INDEX0:i0][INDEX1:i4][INDEX2:i2-i5][INDEX3:i3-i6][OSIZE=0][ESIZE=0][JIT:1][JIT_cpu:1][index_t:int32]][opkey2:binary[Tx:float32][Ty:float32][Tz:float32][OP:multiply][JIT:1][JIT_cpu:1][index_t:int32]][opkey3:reduce[Tx:float32][Ty:float32][Tz:float32][OP:add][DIM=7][REDUCE=70][JIT:1][JIT_cpu:1][index_t:int32]][JIT:1][JIT_cpu:1][graph:040000,062010,010020,000021,020030,][var_info::041704171724][shapes:[10,3,3,3,],[8,10,20,20,3,3,3,],[8,3,20,20,],[8,10,20,20,3,3,3,],[8,10,20,20,3,3,3,],[8,10,20,20,],][choices:compile_shapes:1,]\n          /root/.cache/jittor/jt1.3.1/g++10.5.0/py3.8.18/Linux-5.4.0-42xae/AMDEPYC7T8364-x8f_debug/default/jit/_opkey0_broadcast_to_Tx_float32__DIM_7__BCAST_d__JIT_1__JIT_cpu_1__index_t_int32___opkey1____hash_a2d65b1fd1c3f3d0_op.cc\n                             1    8.12ms(63.3%,63.3%)    8.12ms    8.12ms    8.12ms  11.7MB/s  61.6MB/s  73.3MB/s  436Mit/s\nrandom[T:float32][R:normal][JIT:1][JIT_cpu:1][index_t:int32]\n          /root/.cache/jittor/jt1.3.1/g++10.5.0/py3.8.18/Linux-5.4.0-42xae/AMDEPYC7T8364-x8f_debug/default/jit/random_T_float32__R_normal__JIT_1__JIT_cpu_1__index_t_int32__hash_c27874d0aacc5d25_op.cc\n                             3    3.68ms(28.7%,91.9%)    1.23ms    5.58us    3.37ms     0 B/s   298MB/s   298MB/s   78Mit/s\n[opkey0:broadcast_to[Tx:float32][DIM=5][BCAST=4][JIT:1][JIT_cpu:1][index_t:int32]][opkey1:broadcast_to[Tx:float32][DIM=5][BCAST=10][JIT:1][JIT_cpu:1][index_t:int32]][opkey2:binary[Tx:float32][Ty:float32][Tz:float32][OP:multiply][JIT:1][JIT_cpu:1][index_t:int32]][opkey3:reduce[Tx:float32][Ty:float32][Tz:float32][OP:add][DIM=5][REDUCE=8][JIT:1][JIT_cpu:1][index_t:int32]][JIT:1][JIT_cpu:1][graph:040000,062010,010020,000021,020030,][var_info::041504151524][shapes:[8,20,10,40,],[8,20,20,10,40,],[8,20,20,10,],[8,20,20,10,40,],[8,20,20,10,40,],[8,20,20,40,],][choices:compile_shapes:1,]\n          /root/.cache/jittor/jt1.3.1/g++10.5.0/py3.8.18/Linux-5.4.0-42xae/AMDEPYC7T8364-x8f_debug/default/jit/_opkey0_broadcast_to_Tx_float32__DIM_5__BCAST_4__JIT_1__JIT_cpu_1__index_t_int32___opkey1____hash_ef35f9063cf2acdf_op.cc\n                             1     828us(6.45%,98.4%)     828us     828us     828us  1.77GB/s  2.36GB/s  4.13GB/s 10.1Git/s\ntranspose[Tx:float32][DIM=4][AXES0=0][AXES2=1][AXES3=2][AXES1=3][JIT:1][JIT_cpu:1][index_t:int32]\n          /root/.cache/jittor/jt1.3.1/g++10.5.0/py3.8.18/Linux-5.4.0-42xae/AMDEPYC7T8364-x8f_debug/default/jit/transpose_Tx_float32__DIM_4__AXES0_0__AXES2_1__AXES3_2__AXES1_3__JIT_1__JIT_cpu_1__index_t_int32__hash_998b34c8052fe15_op.cc\n                             1     208us(1.62%,100%)     208us     208us     208us  2.35GB/s  2.35GB/s  4.71GB/s  632Mit/s\n最终我检查他的输出, 发现是分成了三个部分, _opkey0_broadcast_to_Tx_float32__DIM_7__BCAST_d__JIT_1__JIT_cpu_1__index_t_int32___opkey1____hash_a2d65b1fd1c3f3d0_op为卷积实现, _opkey0_broadcast_to_Tx_float32__DIM_5__BCAST_4__JIT_1__JIT_cpu_1__index_t_int32___opkey1____hash_ef35f9063cf2acdf_op为矩阵乘, transpose_Tx_float32__DIM_4__AXES0_0__AXES2_1__AXES3_2__AXES1_3__JIT_1__JIT_cpu_1__index_t_int32__hash_998b34c8052fe15_op为转置."
  },
  {
    "objectID": "posts/tensordsl.html#dsl语法-1",
    "href": "posts/tensordsl.html#dsl语法-1",
    "title": "Tensor DSL总结",
    "section": "2.1 DSL语法",
    "text": "2.1 DSL语法\nimport halide as hl\ninputLhs = hl.ImageParam(hl.Float(32), 2, \"inputLhs\")\ninputRhs = hl.ImageParam(hl.Float(32), 2, \"inputRhs\")\noutput = hl.Func(\"output\")\n(m, n) = hl.Var(\"m\"), hl.Var(\"n\")\nk = hl.RDom([hl.Range(0, self.K)], \"k\")\noutput[n, m] = 0.0\noutput[n, m] += inputLhs[k.x, m] * inputRhs[n, k.x]\nHalide使用Var来表示循环,对于规约的循环需要用RDom来标识(并且如果定义了规约循环,那么还需要为数据设定初值). 使用Var对张量inputLhs[k.x, m]进行索引操作用于建立access relation，实际计算时的迭代域是分析当前的计算statement所参与的迭代变量来决定的，最终根据Var来构建嵌套循环, 他这里默认应该都会把规约的循环放到最内层. 所以他的循环维度和数据的大小是绑定的，那么怎么去定义一个局部的buffer呢？\n提前声明的循环变量的缺点在于需要开发者手动管理好所有的循环变量, 书写起来较为复杂; 优点在于可以确定上下游操作循环之间的关系, 可以轻易的做到自动fusion上下两层算子."
  },
  {
    "objectID": "posts/tensordsl.html#测试例子-1",
    "href": "posts/tensordsl.html#测试例子-1",
    "title": "Tensor DSL总结",
    "section": "2.2 测试例子",
    "text": "2.2 测试例子\nimport halide as hl\n\ninput = hl.ImageParam(hl.Float(32), 4, \"input\")\nweight = hl.ImageParam(hl.Float(32), 4, \"weight\")\nact = hl.ImageParam(hl.Float(32), 2, \"act\")\npad_w_before = 0  # hl.Param(hl.Int(32), \"pad_w_before\")\npad_h_before = 0  # hl.Param(hl.Int(32), \"pad_h_before\")\nstride_w = 1  # hl.Param(hl.Int(32), \"stride_w\")\nstride_h = 1  # hl.Param(hl.Int(32), \"stride_h\")\n\n\nWO, HO, CI, B, CO = hl.Var(\"WO\"), hl.Var(\"HO\"), hl.Var(\"CI\"), hl.Var(\"B\"), hl.Var(\"CO\")\nPadding, Paded, Conv, Acted, Clamped, Psumed = hl.Func(\"Padding\"), hl.Func(\n    \"Paded\"), hl.Func(\"Conv\"), hl.Func(\"Acted\"), hl.Func(\"Clamped\"), hl.Func(\"Psumed\")\n\nr = hl.RDom([hl.Range(0, weight.width()), hl.Range(0, weight.height()),\n            hl.Range(0, weight.dim(2).extent())])  # w,h,ic\n\nPadding = hl.BoundaryConditions.constant_exterior(\n    input, 0, [hl.Range(0, input.width()), hl.Range(0, input.height())])\n\nin_channels = input.dim(2).extent()\nout_channels = weight.dim(3).extent()\n\nPaded[WO, HO, CI, B] = Padding[WO - pad_w_before, HO - pad_h_before, CI, B]\n\nConv[WO, HO, CO, B] = 0.0\nConv[WO, HO, CO, B] += weight[r[0], r[1], r[2], CO] * Paded[WO * stride_w + r[0], HO * stride_h + r[1], r[2], B]  # use float to sum\n\nActed[WO, HO, CO, B] = hl.select(\n    Conv[WO, HO, CO, B] &lt; act[0, CO],\n    Conv[WO, HO, CO, B] * act[1, CO] + act[2, CO],\n    Conv[WO, HO, CO, B] * act[3, CO] + act[4, CO])  # float\n\n\nTranspose = hl.Func(\"Transpose\")\nTranspose[CO, WO, HO, B] = Acted[WO, HO, CO, B]\n\nrhs = hl.ImageParam(hl.Float(32), 4, \"rhs\")  # [x,x,K,N]\n\nN = hl.Var(\"N\")\n\nkdom = hl.RDom([hl.Range(0, rhs.dim(2).extent())], \"k\")\n\nMatmul = hl.Func(\"Matmul\")\nMatmul[N, WO, HO, B] = 0.0\nMatmul[N, WO, HO, B] += Transpose[kdom.x, WO, HO, B] * rhs[N, kdom.x, HO, B]\n\nMatmul.print_loop_nest()\n得到的循环嵌套如下:\nproduce Matmul:\n  for B:\n    for HO:\n      for WO:\n        for N:\n          Matmul(...) = ...\n  for B:\n    for HO:\n      for WO:\n        for N:\n          for k:\n            produce Conv:\n              Conv(...) = ...\n              for r14:\n                for r14:\n                  for r14:\n                    Conv(...) = ...\n            consume Conv:\n              Matmul(...) = ...\n矩阵层的初始化他默认放到root层级, 下面是自动把Transpose的操作inline了, 也自动把矩阵乘和卷积进行了fusion."
  },
  {
    "objectID": "posts/tensordsl.html#dsl语法-2",
    "href": "posts/tensordsl.html#dsl语法-2",
    "title": "Tensor DSL总结",
    "section": "3.1 DSL语法",
    "text": "3.1 DSL语法\nn = te.var(\"n\")\nA = te.placeholder((n,), name=\"A\")\nB = te.placeholder((n,), name=\"B\")\nC = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n也是使用shape来表示完美循环, fcompute的回调函数的参数映射迭代变量, 同时也会在最内层循环执行它. 同Jittor类似, 不过使用回调函数的方式更增加了灵活性. 可以使用reduce_axis, 类似于RDom, 会自动最内层循环加上规约的循环, 他这里默认的初始化会放到规约循环外面."
  },
  {
    "objectID": "posts/tensordsl.html#测试例子-2",
    "href": "posts/tensordsl.html#测试例子-2",
    "title": "Tensor DSL总结",
    "section": "3.2 测试例子",
    "text": "3.2 测试例子\nimport tvm\nfrom tvm import te\nfrom tvm import tir\n\nbatch_size = 8\nin_channel = 3\nout_channel = 16\nin_height = 32\nin_width = 32\nkernel_height = 3\nkernel_width = 3\n\nN = 64\n\nInput = te.placeholder(\n    (batch_size, in_channel, in_height, in_width), name='Input')\nKernel = te.placeholder(\n    (out_channel, in_channel, kernel_height, kernel_width), name='Kernel')\n\nrc = te.reduce_axis((0, in_channel), name='rc')\nry = te.reduce_axis((0, kernel_height), name='ry')\nrx = te.reduce_axis((0, kernel_width), name='rx')\n\nConv = te.compute(\n    (batch_size, out_channel, in_height -\n     kernel_height + 1, in_width - kernel_width + 1),\n    lambda n, f, y, x: te.sum(\n        Input[n, rc, y + ry, x + rx] * Kernel[f, rc, ry, rx],\n        axis=[rc, ry, rx]\n    ),\n    name='Conv'\n)  # (b,oc,oh,ow)  -&gt; (b,oh,ow,oc)\n\noh, ow = 30, 30\nrhs = te.placeholder((batch_size, oh, out_channel, N), name='rhs')\n\nTrans = te.compute(\n    (batch_size, oh, ow, out_channel),\n    lambda i0, i1, i2, i3: Conv[i0, i3, i1, i2])\n\n\nrk = te.reduce_axis((0, out_channel), name='rk')\nMatMul = te.compute(\n    (batch_size, oh, ow, N),\n    lambda i0, i1, i2, i3: te.sum(\n        Trans[i0, i1, i2, rk] * rhs[i0, i1, rk, i3], axis=[rk]),\n    name='MatMul'\n)\n\ns: te.Schedule = te.create_schedule([Conv.op, MatMul.op])\nir = tvm.lower(s, [Input, Kernel, rhs])\nir.show()\n输出:\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(Input: T.Buffer((8, 3, 32, 32), \"float32\"), Kernel: T.Buffer((16, 3, 3, 3), \"float32\"), rhs: T.Buffer((8, 30, 16, 64), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        Conv = T.allocate([460800], \"float32\", \"global\")\n        compute = T.allocate([115200], \"float32\", \"global\")\n        Conv_1 = T.Buffer((115200,), data=Conv)\n        for n, f, y, x in T.grid(8, 16, 30, 30):\n            Conv_1[n * 14400 + f * 900 + y * 30 + x] = T.float32(0)\n            for rc, ry, rx in T.grid(3, 3, 3):\n                cse_var_1: T.int32 = n * 14400 + f * 900 + y * 30 + x\n                Input_1 = T.Buffer((24576,), data=Input.data)\n                Kernel_1 = T.Buffer((432,), data=Kernel.data)\n                Conv_1[cse_var_1] = Conv_1[cse_var_1] + Input_1[n * 3072 + rc * 1024 + y * 32 + ry * 32 + x + rx] * Kernel_1[f * 27 + rc * 9 + ry * 3 + rx]\n        compute_1 = T.Buffer((115200,), data=compute)\n        for i0, i1, i2, i3 in T.grid(8, 30, 30, 16):\n            cse_var_2: T.int32 = i0 * 14400\n            compute_1[cse_var_2 + i1 * 480 + i2 * 16 + i3] = Conv_1[cse_var_2 + i3 * 900 + i1 * 30 + i2]\n        for i0, i1, i2, i3 in T.grid(8, 30, 30, 64):\n            Conv_2 = T.Buffer((460800,), data=Conv)\n            Conv_2[i0 * 57600 + i1 * 1920 + i2 * 64 + i3] = T.float32(0)\n            for rk in range(16):\n                cse_var_3: T.int32 = i0 * 57600 + i1 * 1920 + i2 * 64 + i3\n                rhs_1 = T.Buffer((245760,), data=rhs.data)\n                Conv_2[cse_var_3] = Conv_2[cse_var_3] + compute_1[i0 * 14400 + i1 * 480 + i2 * 16 + rk] * rhs_1[i0 * 30720 + i1 * 1024 + rk * 64 + i3]\n他这里不像Halide一样需要提前定义好循环变量, 但可以从输出中获取axis然后使用类似Halide的调度, 也可以在lower到tir之后使用基于tensor ir的调度. 这里进行lower依据默认的优化流程后, 并无法自动fusion."
  },
  {
    "objectID": "posts/tensordsl.html#dsl语法-3",
    "href": "posts/tensordsl.html#dsl语法-3",
    "title": "Tensor DSL总结",
    "section": "4.1 DSL语法",
    "text": "4.1 DSL语法\n@linalg_structured_op\ndef conv_2d_nhwc_hwcf(\n    I=TensorDef(T1, S.N, S.OH * S.SH + S.KH * S.DH, S.OW * S.SW + S.KW * S.DW, S.C),\n    K=TensorDef(T2, S.KH, S.KW, S.C, S.F),\n    O=TensorDef(U, S.N, S.OH, S.OW, S.F, output=True),\n    strides=IndexAttrDef(S.SH, S.SW, default=[1, 1]),\n    dilations=IndexAttrDef(S.DH, S.DW, default=[1, 1]),\n):\n    \"\"\"Performs 2-D convolution.\n\n    Layout:\n      * Input: NHWC.\n      * Kernel: HWCF.\n\n    Numeric casting is performed on the operands to the inner multiply, promoting\n    them to the same data type as the accumulator/output.\n    \"\"\"\n    implements(ConvolutionOpInterface)\n    domain(D.n, D.oh, D.ow, D.f, D.kh, D.kw, D.c)\n    O[D.n, D.oh, D.ow, D.f] += TypeFn.cast_signed(\n        U, I[D.n, D.oh * S.SH + D.kh * S.DH, D.ow * S.SW + D.kw * S.DW, D.c]\n    ) * TypeFn.cast_signed(U, K[D.kh, D.kw, D.c, D.f])\nMlir这里不使用shape, 使用和polyhedral更加贴近的称呼domain来表示嵌套循环. 我觉得这里更加激进的一点就是完全抛弃循环中的初始化, 也就是忠实的翻译这个OpDSL所描述的内容."
  },
  {
    "objectID": "posts/tensordsl.html#测试例子-3",
    "href": "posts/tensordsl.html#测试例子-3",
    "title": "Tensor DSL总结",
    "section": "4.2 测试例子",
    "text": "4.2 测试例子\nfrom mlir.dialects import arith, builtin, func, linalg, tensor, memref\nfrom mlir.dialects.linalg.opdsl.lang import *\nfrom mlir.ir import *\n\n\n@linalg_structured_op\ndef transpose_nchw_nhwc(\n    I=TensorDef(TV.T1, S.d0, S.d1, S.d2, S.d3),\n    O=TensorDef(TV.T1, S.d0, S.d2, S.d3, S.d1, output=True)\n):\n    domain(D.d0, D.d1, D.d2, D.d3)\n    implements(ContractionOpInterface)\n    O[D.d0, D.d2, D.d3, D.d1] = I[D.d0, D.d1, D.d2, D.d3]\n\n@linalg_structured_op\ndef matmul_4d(\n    A=TensorDef(TV.T1, S.d0, S.d1, S.M, S.K),\n    B=TensorDef(TV.T1, S.d0, S.d1, S.K, S.N),\n    C=TensorDef(TV.T1, S.d0, S.d2, S.M, S.N, output=True)\n):\n    domain(D.d0, D.d1, D.m, D.n, D.k)\n    implements(ContractionOpInterface)\n    C[D.d0, D.d1, D.m, D.n] += A[D.d0, D.d1, D.m, D.k] * B[D.d0, D.d1, D.k, D.n]\n\ndef testOpResultFromOtherOp():\n    with Context(), Location.unknown():\n        module = Module.create()\n        f32 = F32Type.get()\n        index_type = IndexType.get()\n        with InsertionPoint(module.body):\n            batch_size = 8\n            in_channel = 3\n            out_channel = 16\n            in_height = 32\n            out_height = 30\n            in_width = 32\n            out_width = 30\n            kernel_height = 3\n            kernel_width = 3\n            N = 64\n\n            @func.FuncOp.from_py_func(\n                MemRefType.get(\n                    (batch_size, in_channel, in_height, in_width), f32),\n                MemRefType.get(\n                    (out_channel, in_channel, kernel_height, kernel_width), f32),\n                MemRefType.get((batch_size, out_height, out_channel, N), f32),\n            )\n            def main(lhs, weight, rhs):\n                # conv = tensor.EmptyOp([batch_size, out_channel, out_height, out_width], f32)\n                zero = arith.ConstantOp(F32Type.get(), 0.0)\n                # CHECK: %[[LHS:.*]] = linalg.fill\n                conv = memref.AllocOp(MemRefType.get(\n                    [batch_size, out_channel, out_height, out_width], f32), [], [])\n                linalg.fill(zero, outs=[conv])\n                linalg.conv_2d_nchw_fchw(lhs, weight, outs=[conv])\n                trans = memref.AllocOp(MemRefType.get(\n                    [batch_size, out_height, out_width, out_channel], f32), [], [])\n                transpose_nchw_nhwc(conv, outs=[trans])\n                matmul = memref.AllocOp(MemRefType.get(\n                    [batch_size, out_height, out_width, N], f32), [], [])\n                matmul_4d(trans, rhs, outs=[matmul])\n                return matmul\n\n    print(module)\n\n\ntestOpResultFromOtherOp()\n得到convmatmul.mlir:\n#map = affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;\n#map1 = affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d2, d3, d1)&gt;\n#map2 = affine_map&lt;(d0, d1, d2, d3, d4) -&gt; (d0, d1, d2, d4)&gt;\n#map3 = affine_map&lt;(d0, d1, d2, d3, d4) -&gt; (d0, d1, d4, d3)&gt;\n#map4 = affine_map&lt;(d0, d1, d2, d3, d4) -&gt; (d0, d1, d2, d3)&gt;\nmodule {\n  func.func @main(%arg0: memref&lt;8x3x32x32xf32&gt;, %arg1: memref&lt;16x3x3x3xf32&gt;, %arg2: memref&lt;8x30x16x64xf32&gt;) -&gt; memref&lt;8x30x30x64xf32&gt; {\n    %cst = arith.constant 0.000000e+00 : f32\n    %alloc = memref.alloc() : memref&lt;8x16x30x30xf32&gt;\n    linalg.fill ins(%cst : f32) outs(%alloc : memref&lt;8x16x30x30xf32&gt;)\n    linalg.conv_2d_nchw_fchw ins(%arg0, %arg1 : memref&lt;8x3x32x32xf32&gt;, memref&lt;16x3x3x3xf32&gt;) outs(%alloc : memref&lt;8x16x30x30xf32&gt;)\n    %alloc_0 = memref.alloc() : memref&lt;8x30x30x16xf32&gt;\n    linalg.generic {indexing_maps = [#map, #map1], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%alloc : memref&lt;8x16x30x30xf32&gt;) outs(%alloc_0 : memref&lt;8x30x30x16xf32&gt;) {\n    ^bb0(%in: f32, %out: f32):\n      linalg.yield %in : f32\n    }\n    %alloc_1 = memref.alloc() : memref&lt;8x30x30x64xf32&gt;\n    linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\", \"reduction\"]} ins(%alloc_0, %arg2 : memref&lt;8x30x30x16xf32&gt;, memref&lt;8x30x16x64xf32&gt;) outs(%alloc_1 : memref&lt;8x30x30x64xf32&gt;) {\n    ^bb0(%in: f32, %in_2: f32, %out: f32):\n      %0 = arith.mulf %in, %in_2 : f32\n      %1 = arith.addf %out, %0 : f32\n      linalg.yield %1 : f32\n    }\n    return %alloc_1 : memref&lt;8x30x30x64xf32&gt;\n  }\n}\n使用mlir-opt进行fusion:\nmlir-opt -allow-unregistered-dialect convmatmul.mlir --convert-linalg-to-affine-loops -o convmatmul1.mlir\nmlir-opt -allow-unregistered-dialect convmatmul1.mlir -pass-pipeline='builtin.module(func.func(affine-loop-fusion))' -o convmatmul2.mlir\n得到:\n#map = affine_map&lt;(d0, d1) -&gt; (d0 + d1)&gt;\nmodule {\n  func.func @main(%arg0: memref&lt;8x3x32x32xf32&gt;, %arg1: memref&lt;16x3x3x3xf32&gt;, %arg2: memref&lt;8x30x16x64xf32&gt;) -&gt; memref&lt;8x30x30x64xf32&gt; {\n    %alloc = memref.alloc() : memref&lt;1x1x1x16xf32&gt;\n    %alloc_0 = memref.alloc() : memref&lt;1x1x1x1xf32&gt;\n    %cst = arith.constant 0.000000e+00 : f32\n    %alloc_1 = memref.alloc() : memref&lt;8x30x30x64xf32&gt;\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 16 {\n            affine.store %cst, %alloc_0[0, 0, 0, 0] : memref&lt;1x1x1x1xf32&gt;\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %1 = affine.apply #map(%arg4, %arg8)\n                  %2 = affine.apply #map(%arg5, %arg9)\n                  %3 = affine.load %arg0[%arg3, %arg7, %1, %2] : memref&lt;8x3x32x32xf32&gt;\n                  %4 = affine.load %arg1[%arg6, %arg7, %arg8, %arg9] : memref&lt;16x3x3x3xf32&gt;\n                  %5 = affine.load %alloc_0[0, 0, 0, 0] : memref&lt;1x1x1x1xf32&gt;\n                  %6 = arith.mulf %3, %4 : f32\n                  %7 = arith.addf %5, %6 : f32\n                  affine.store %7, %alloc_0[0, 0, 0, 0] : memref&lt;1x1x1x1xf32&gt;\n                }\n              }\n            }\n            %0 = affine.load %alloc_0[0, 0, 0, 0] : memref&lt;1x1x1x1xf32&gt;\n            affine.store %0, %alloc[0, 0, 0, %arg6] : memref&lt;1x1x1x16xf32&gt;\n          }\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 16 {\n              %0 = affine.load %alloc[0, 0, 0, %arg7] : memref&lt;1x1x1x16xf32&gt;\n              %1 = affine.load %arg2[%arg3, %arg4, %arg7, %arg6] : memref&lt;8x30x16x64xf32&gt;\n              %2 = affine.load %alloc_1[%arg3, %arg4, %arg5, %arg6] : memref&lt;8x30x30x64xf32&gt;\n              %3 = arith.mulf %0, %1 : f32\n              %4 = arith.addf %2, %3 : f32\n              affine.store %4, %alloc_1[%arg3, %arg4, %arg5, %arg6] : memref&lt;8x30x30x64xf32&gt;\n            }\n          }\n        }\n      }\n    }\n    return %alloc_1 : memref&lt;8x30x30x64xf32&gt;\n  }\n}\n经过affine-loop-fusion之后的ir基本符合我的预期."
  },
  {
    "objectID": "posts/tensordsl.html#dsl语法-4",
    "href": "posts/tensordsl.html#dsl语法-4",
    "title": "Tensor DSL总结",
    "section": "5.1 DSL语法",
    "text": "5.1 DSL语法\nimport tiramisu as tm\nimport shutil\nimport os\n\ntm.init(\"matmul\")\n\nM = 64\nK = 256\nN = 128\n\n# Level I: specifies \"what\" should be computed\n\nA = tm.input(\"A\", ['m', 'k'], [M, K], tm.primitive_t.p_float32)\nB = tm.input(\"B\", ['k', 'n'], [K, N], tm.primitive_t.p_float32)\n\nm, k, n = tm.var(\"m\", 0, M), tm.var(\"k\", 0, K), tm.var(\"n\", 0, N)\nC_init = tm.computation(\"C_init\", [m, n], tm.expr(0.0))\nC = tm.computation(\"C\", [m, n, k], tm.primitive_t.p_float32)\nC.set_expression(C[m, n, k - 1] + A[m, k] * B[k, n])\n\n# Level II: level specifies \"when\" and \"where\"\n\n# schedule the computation oerder\n\n# Level III: level specifies \"stored\"\n\nbufA = tm.buffer(\"bufA\", [M, K], tm.primitive_t.p_float32, tm.argument_t.a_input)\nbufB = tm.buffer(\"bufB\", [K, N], tm.primitive_t.p_float32, tm.argument_t.a_input)\nbufC = tm.buffer(\"bufC\", [M, N], tm.primitive_t.p_float32, tm.argument_t.a_output)\nA.store_in(bufA)\nB.store_in(bufB)\nC_init.store_in(bufC, [m, n])\nC.store_in(bufC, [m, n])\n\n\nf = tm.get_implicit_function()\nf.codegen([bufA, bufB, bufC], \"matmul.o\", 0, False)\nf.dump_halide_stmt()\ntiramisu其实是使用更加贴近于polyhedral的思想, computation可以等价于statement. 类似halide一样使用var表示循环, 用于指定当前computation的循环位置. 不过他这里内部都是基于polyhedral, 定义computation后直接得到了iteration domain, 经过loop dimension align可以得到schedule."
  },
  {
    "objectID": "posts/tensordsl.html#测试例子-4",
    "href": "posts/tensordsl.html#测试例子-4",
    "title": "Tensor DSL总结",
    "section": "5.2 测试例子",
    "text": "5.2 测试例子\nimport tiramisu as tm\nimport shutil\nimport os\n\n# f = tm.function(\"matmul\")\ntm.init(\"convmatmul\")\n\nB = 8\nIC = 3\nOC = 16\nIH, OH = 32, 30\nIW, OW = 32, 30\nKH = 3\nKW = 3\nN = 64\n\n# Level I: specifies \"what\" should be computed\n\nlhs = tm.input(\"lhs\", ['B', 'IC', 'IH', 'IW'], [\n               B, IC, IH, IW], tm.primitive_t.p_float64)\nrhs = tm.input(\"rhs\", ['B', 'OH', 'OC', 'N'], [\n               B, OH, OC, N], tm.primitive_t.p_float64)\nkernel = tm.input(\"kernel\", ['OC', 'IC', 'KH', 'KW'], [\n    OC, IC, KH, KW], tm.primitive_t.p_float64)\n\nb, oc, oh, ow, ic, kh, kw = tm.var('b', 0, B), tm.var('oc', 0, OC), tm.var('oh', 0, OH), tm.var(\n    'ow', 0,  OW), tm.var('ic', 0, IC), tm.var('kh', 0, KH), tm.var('kw', 0, KW)\nConvInit = tm.computation(\"conv_init\", [b, oc, oh, ow], tm.expr(0.0))\nConv = tm.computation(\n    \"Conv\", [b, oc, oh, ow, ic, kh, kw], tm.primitive_t.p_float64)\nConv.set_expression(Conv[b, oc, oh, ow, ic, kh, kw] +\n                    lhs[b, oc, oh + kh, ow + kw] * kernel[oc, ic, kh, kw])\nn = tm.var('n', 0, N)\nTranspose = tm.computation(\"transpose\", [b, oh, ow, oc], ConvInit[b, oc, oh, ow])\n\nMatmulInit = tm.computation(\"matmul_init\", [b, oh, ow, n], tm.expr(0.0))\nMatmul = tm.computation(\"Matmul\", [b, oh, ow, n], tm.primitive_t.p_float64)\nMatmul.set_expression(\n    MatmulInit[b, oh, ow, n] + Transpose[b, oh, ow, oc] * rhs[b, oh, oc, n])\n\n# Level II: level specifies \"when\" and \"where\"\n\n# Level III: level specifies \"stored\"\n\nbuflhs = tm.buffer(\"buflhs\",  [B, IC, IH, IW], tm.primitive_t.p_float64, tm.argument_t.a_input)\nbufrhs = tm.buffer(\"bufrhs\",  [B, OH, OC, N], tm.primitive_t.p_float64, tm.argument_t.a_input)\nbufkernel = tm.buffer(\"bufkernel\",  [OC, IC, KH, KW], tm.primitive_t.p_float64, tm.argument_t.a_input)\nbufconv = tm.buffer(\"bufconv\",  [B, OC, OH, OW], tm.primitive_t.p_float64, tm.argument_t.a_temporary)\nbufmatmul = tm.buffer(\"bufmatmul\",  [B, OH, OW, N], tm.primitive_t.p_float64, tm.argument_t.a_output)\n\nlhs.store_in(buflhs)\nrhs.store_in(bufrhs)\nkernel.store_in(bufkernel)\n\nConvInit.store_in(bufconv, [b, oc, oh, ow])\nConv.store_in(bufconv, [b, oc, oh, ow])\nTranspose.store_in(bufconv, [b, oh, ow, oc])\nMatmulInit.store_in(bufmatmul, [b, oh, ow, n])\nMatmul.store_in(bufmatmul, [b, oh, ow, n])\n\nf = tm.get_implicit_function()\nf.codegen([buflhs, bufrhs, bufkernel, bufconv, bufmatmul], \"matmul.o\", 0, False)\nf.dump_halide_stmt()\n输出:\nproduce  {\n allocate _transpose_b5[float64 * (16 - 0) * (30 - 0) * (30 - 0) * (8 - 0)] in Heap\n allocate _rhs_b1[float64 * (64 - 0) * (16 - 0) * (30 - 0) * (8 - 0)] in Heap\n allocate _matmul_init_b6[float64 * (64 - 0) * (30 - 0) * (30 - 0) * (8 - 0)] in Heap\n allocate _lhs_b0[float64 * (32 - 0) * (32 - 0) * (3 - 0) * (8 - 0)] in Heap\n allocate _kernel_b2[float64 * (3 - 0) * (3 - 0) * (3 - 0) * (16 - 0)] in Heap\n allocate _conv_init_b3[float64 * (30 - 0) * (30 - 0) * (16 - 0) * (8 - 0)] in Heap\n allocate _Matmul_b7[float64 * (64 - 0) * (30 - 0) * (30 - 0) * (8 - 0)] in Heap\n allocate _Conv_b4[float64 * (3 - 0) * (3 - 0) * (3 - 0) * (30 - 0) * (30 - 0) * (16 - 0) * (8 - 0)] in Heap\n for (c1, 0, 8 - 0) {\n  for (c3, 0, 30 - 0) {\n   for (c5, 0, 30 - 0) {\n    for (c7, 0, 64 - 0) {\n     if (c3 &gt;= 16) {\n      bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] = 0.000000\n      bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] = (float64)bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] + ((float64)bufconv[(((0 + (oc*1)) + (c5*30)) + (c3*900)) + (c1*14400)]*(float64)bufrhs[(((0 + (c7*1)) + (oc*64)) + (c3*1024)) + (c1*30720)])\n      if (c7 &lt;= 15) {\n       bufconv[(((0 + (c7*1)) + (c5*30)) + (c3*900)) + (c1*14400)] = (float64)bufconv[(((0 + (c5*1)) + (c3*30)) + (c7*900)) + (c1*14400)]\n      }\n     } else if (c7 &gt;= 30) {\n      bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] = 0.000000\n      bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] = (float64)bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] + ((float64)bufconv[(((0 + (oc*1)) + (c5*30)) + (c3*900)) + (c1*14400)]*(float64)bufrhs[(((0 + (c7*1)) + (oc*64)) + (c3*1024)) + (c1*30720)])\n     } else {\n      for (c9, 0, 3 - 0) {\n       for (c11, 0, 3 - 0) {\n        for (c13, 0, 3 - 0) {\n         if (((c9 == 0) && (c11 == 0)) && (c13 == 0)) {\n          bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] = 0.000000\n         }\n         bufconv[(((0 + (c7*1)) + (c5*30)) + (c3*900)) + (c1*14400)] = (float64)bufconv[(((0 + (c7*1)) + (c5*30)) + (c3*900)) + (c1*14400)] + ((float64)buflhs[(((0 + ((c7 + c13)*1)) + ((c5 + c11)*32)) + (c3*1024)) + (c1*3072)]*(float64)bufkernel[(((0 + (c13*1)) + (c11*3)) + (c9*9)) + (c3*27)])\n         if (((c9 == 0) && (c11 == 0)) && (c13 == 0)) {\n          bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] = (float64)bufmatmul[(((0 + (c7*1)) + (c5*64)) + (c3*1920)) + (c1*57600)] + ((float64)bufconv[(((0 + (oc*1)) + (c5*30)) + (c3*900)) + (c1*14400)]*(float64)bufrhs[(((0 + (c7*1)) + (oc*64)) + (c3*1024)) + (c1*30720)])\n          if (c7 &lt;= 15) {\n           bufconv[(((0 + (c7*1)) + (c5*30)) + (c3*900)) + (c1*14400)] = (float64)bufconv[(((0 + (c5*1)) + (c3*30)) + (c7*900)) + (c1*14400)]\n          }\n          bufconv[(((0 + (c7*1)) + (c5*30)) + (c3*900)) + (c1*14400)] = 0.000000\n         }\n        }\n       }\n      }\n     }\n    }\n   }\n  }\n }\n}\n因为tiramisu是完全依赖手动调度, 所以这里的fusion使用的buffer需要提前手动指定, 对于手工优化算子应该会省事情, 但是集成到编译器并不是很合适."
  },
  {
    "objectID": "posts/tf-all-regularizers.html",
    "href": "posts/tf-all-regularizers.html",
    "title": "tf2.0 全局添加regularizers",
    "section": "",
    "text": "如何给tensorflow中预训练好的模型添加正则化器？\n\n由于tensorflow基于图的定义方式，在定义好模型后，再添加正则化器是无效的，必须要重新建立图才可以。使用tf.keras.models.model_from_json(model.to_json())是一种方法，不过如果是加载的预训练模型，我们还需要重新加载权重才可以。\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nassert len(physical_devices) &gt; 0, \"Not enough GPU hardware devices available\"\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\ninp = tf.keras.Input((28, 28, 1))\nx = tf.keras.layers.Conv2D(32, 3, activation='relu')(inp)\nx = tf.keras.layers.MaxPooling2D()(x)\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dropout(0.1)(x)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dense(10)(x)\nmodel = tf.keras.Model(inp, x)\n\nfor layer in model.layers:\n    for attr in ['kernel_regularizer']:\n        if hasattr(layer, attr):\n            setattr(layer, attr, tf.keras.regularizers.l2(0.004))\n\ntrain_data = tf.ones(shape=(1, 28, 28, 1))\ntest_data = tf.ones(shape=(1, 28, 28, 1))\nmodel = tf.keras.models.model_from_json(model.to_json())\n\ntrain_out = model(train_data, training=True)\nprint(model.losses)"
  },
  {
    "objectID": "posts/tf-custom-loss-error.html",
    "href": "posts/tf-custom-loss-error.html",
    "title": "tf.keras自定义loss报错shape mismatch",
    "section": "",
    "text": "在上次的文章中我写了如何自定义loss，但是我真正想要的使用的场景比那些还要复杂一些。\n\n\n问题出现\n是想在自定义loss函数中对y_pred进行reshape然后进行sigmoid_cross_entropy_with_logits，但是keras他是将loss构建成一个graph，并且loss中的y_true并不是占位符，他的shape是根据模型最终的输出维度来确定的虚占位符，如果模型最后输出的维度和真正的y_true维度不匹配，那么是肯定报错的。\n看下面这个例子：\nimport tensorflow as tf\nfrom tensorflow.python import keras\nimport numpy as np\nkeras.backend.clear_session()\nx = keras.Input(shape=(10))\nx_1 = keras.layers.Dense(35)(x)\nx_2 = keras.layers.Dense(70)(x)\nmodel = keras.Model(inputs=x, outputs=[x_1, x_2])\nmodel.summary()\n\ndef l_1(true, pred):\n    pred = tf.reshape(pred, (-1, 5, 7))\n    print(true.shape, pred.shape)\n    # NOTE reshape之后 shape是匹配的，但是检查维度时候会报错\n    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=true, logits=pred))\n\ndef l_2(true, pred):\n    pred = tf.reshape(pred, (-1, 10, 7))\n    print(true.shape, pred.shape)\n    # NOTE reshape之后 shape是匹配的，但是检查维度时候会报错\n    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=true, logits=pred))\n\ntrain_set = tf.data.Dataset.from_tensor_slices((np.random.rand(100, 10), np.random.rand(100, 5, 7),\n                                                np.random.rand(100, 10, 7))).repeat()  # type: tf.data.Dataset\ntrain_set = train_set.map(lambda x, y, z: (x, (y, z))).batch(32)\n\nmodel.compile('adam', loss=[l_1, l_2])\nmodel.fit(train_set, steps_per_epoch=30)  # NOTE 不可训练\n输出：\n(?, ?) (?, 5, 7)\nValueError: logits and labels must have the same shape ((?, 5, 7) vs (?, ?))\n可以看到我打印出y_true的shape就是和网络输出尺寸相同的，但实际上y_true输入是正确的，这个实在是让人蛋疼。\n\n\n错误解决\n因为y_true的shape就是和网络输出尺寸相同，所以就从网络上面下手，构建一个model_warrper用于训练。等待训练完成之后直接保存model即可。\nimport tensorflow as tf\nfrom tensorflow.python import keras\nimport numpy as np\nkeras.backend.clear_session()\nx = keras.Input(shape=(10))\nx_1 = keras.layers.Dense(35)(x)\nx_2 = keras.layers.Dense(70)(x)\nmodel = keras.Model(inputs=x, outputs=[x_1, x_2])\nmodel.summary()\n\n\ndef l_1(true, pred):\n    pred = tf.reshape(pred, (-1, 5, 7))\n    print(true.shape, pred.shape)\n    # NOTE reshape之后 shape是匹配的，但是检查维度时候会报错\n    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=true, logits=pred))\n\n\ndef l_2(true, pred):\n    pred = tf.reshape(pred, (-1, 10, 7))\n    print(true.shape, pred.shape)\n    # NOTE reshape之后 shape是匹配的，但是检查维度时候会报错\n    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=true, logits=pred))\n\n\ntrain_set = tf.data.Dataset.from_tensor_slices((np.random.rand(100, 10), np.random.rand(100, 5, 7),\n                                                np.random.rand(100, 10, 7))).repeat()  # type: tf.data.Dataset\ntrain_set = train_set.map(lambda x, y, z: (x, (y, z))).batch(32)\n\n# model.compile('adam', loss=[l_1, l_2])\n# model.fit(train_set, steps_per_epoch=30)  # NOTE 不可训练\n\nx_1 = keras.layers.Reshape((5, 7))(x_1)\nx_2 = keras.layers.Reshape((10, 7))(x_2)\nmodel_warpper = keras.Model(inputs=x, outputs=[x_1, x_2])\nmodel_warpper.summary()\nmodel_warpper.compile('adam', loss=[l_1, l_2])\nmodel_warpper.fit(train_set, steps_per_epoch=30)  # NOTE 可训练"
  },
  {
    "objectID": "posts/tf-data-in-keras.html",
    "href": "posts/tf-data-in-keras.html",
    "title": "tf.Keras完美使用tf.data API",
    "section": "",
    "text": "最近看了苏剑林的几篇博客， 我忽然对keras不是那么抵触了,才发现之前认为Keras使用不灵活完全是因为的认识不够深入。所以我准备使用Tensorflow 2.0中的tf.Keras来 构建Yolo v3，在tensorflow中我们可以更加灵活的优化我们的数据输入管道，这次介绍一下多输入的model如何结合tf.data，基础的使用方式在这里学习。\n\n\n起因\n大家都知道，YOLO v3是一个多输出的模型，在构建模型的时候，label需要多个，所以模型输入也是多个。如果输入是numpy数组，可以保存不同维度的数组，但是tf.Tensor只能保存相同尺寸的数组。\n然后在model.fit中使用tf.data,一般情况下是保证返回值对应x、y,但是现在yolo需要输入[x1,x2,x3,x4],y,这需要tf.data返回一个包含元组的列表。\n\n\n解决方案\n首先我尝试在map中直接返回嵌套的列表，但是因为我使用的是py_function，返回值不能嵌套(暂时没有尝试纯tensorflow的函数能否返回嵌套列表)。还好找了一个好用的方法：tf.data.dataset.zip，可以将两个dataset对象制作成嵌套列表，所以只需要小改动程序如下，把样本与标签分开制作，然后再合并即可！：\ndef parser(lines):\n    image_data = []\n    box_data = []\n    for line in lines:\n        image, box = get_random_data(line.numpy().decode(), input_shape, random=True)\n        image_data.append(image)\n        box_data.append(box)\n\n    image_data = np.array(image_data)\n    box_data = np.array(box_data)\n\n    y_true = [tf.convert_to_tensor(y, tf.float32) for y in preprocess_true_boxes(box_data, input_shape, anchors, num_classes)]\n    image_data = tf.convert_to_tensor(image_data, tf.float32)\n    return (image_data, *y_true)\n\nx_set = (tf.data.Dataset.from_tensor_slices(annotation_lines).\n         apply(tf.data.experimental.shuffle_and_repeat(batch_size * 300, seed=66)).\n         batch(batch_size, drop_remainder=True).\n         map(lambda lines: py_function(parser, [lines], [tf.float32] * (1 + len(anchors) // 3))))\ny_set = tf.data.Dataset.from_tensors(tf.zeros(batch_size, tf.float32)).repeat()\ndataset = tf.data.Dataset.zip((x_set, y_set))\n\nsample = next(iter(dataset))\n。\n。\n。\n。\ntrain_set = create_dataset(lines[:num_train], batch_size, input_shape, anchors, num_classes)\n\nmodel.fit(train_set,\n          epochs=20,\n          steps_per_epoch=max(1, num_train // batch_size),\n          callbacks=[logging, checkpoint])"
  },
  {
    "objectID": "posts/tf-focal-loss.html",
    "href": "posts/tf-focal-loss.html",
    "title": "bce focal loss",
    "section": "",
    "text": "简单记录一下bce focal loss。\n\n\nbce loss\n公式为： \\[\n\\begin{aligned}\nloss=&y * -\\log(sigmoid(p)) + (1 - y) * -\\log(1 - sigmoid(p))\\\\\n    =&\\begin{cases} -\\log(\\sigma(p))&,\\ \\ \\ y=1\\\\\n                    -\\log(1-\\sigma(p))&,\\ \\ \\ y=0\\\\\n     \\end{cases}\n\\end{aligned}\n\\]\n为了解决正负样本的不平衡问题，通常添加参数\\(\\alpha\\)平衡损失： \\[\n\\begin{aligned}\n    loss=&\\begin{cases} -\\alpha \\log(\\sigma(p))&,\\ \\ \\ y=1\\\\\n                    -(1-\\alpha)\\log(1-\\sigma(p))&,\\ \\ \\ y=0\\\\\n     \\end{cases}\n\\end{aligned}\n\\]\n为了解决样本的难易程度不平衡问题，基于预测置信度平衡难易程度：\n\\[\n\\begin{aligned}\n    loss=&\\begin{cases} -(1-\\sigma(p))^\\gamma \\log(\\sigma(p))&,\\ \\ \\ y=1\\\\\n                    -\\sigma(p)^\\gamma\\log(1-\\sigma(p))&,\\ \\ \\ y=0\\\\\n     \\end{cases}\n\\end{aligned}\n\\]\n这样置信度越高的正样本和置信度高的负样本损失衰减都会较大。\n\n\nfocal loss\n结合类别不平衡问题与难易程度不平衡问题，得到focal loss：\n\\[\n\\begin{aligned}\n    loss=&\\begin{cases} -\\alpha(1-\\sigma(p))^\\gamma \\log(\\sigma(p))&,\\ \\ \\ y=1\\\\\n                    -(1-\\alpha)\\sigma(p)^\\gamma\\log(1-\\sigma(p))&,\\ \\ \\ y=0\\\\\n     \\end{cases}\n\\end{aligned}\n\\]\ndef focal_sigmoid_cross_entropy_with_logits(labels: tf.Tensor, logits: tf.Tensor,\n                                            gamma: float = 2.0,\n                                            alpha: float = 0.25):\n    pred_sigmoid = tf.nn.sigmoid(logits)\n    pt = (1 - pred_sigmoid) * labels + pred_sigmoid * (1 - labels)\n    focal_weight = (alpha * labels + (1 - alpha) * (1 - labels)) * tf.math.pow(pt, gamma)\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels, logits) * focal_weight\n    return loss"
  },
  {
    "objectID": "posts/tf-item-assign.html",
    "href": "posts/tf-item-assign.html",
    "title": "tf2.0数组索引与赋值",
    "section": "",
    "text": "用了挺久的tensorflow，目前也尝试了一些别的框架，感觉最让我难受的一点就是没法很方便的按索引赋值。\n原因主要有： 1. tensorflow的索引方式与numpy不同，写起来别扭。 2. 必须要变量类型才可以进行赋值。 我写完这篇文章后找到对普通tensor的赋值方式了\n这次就来说下一些数组操作在tensorflow里面的写法。"
  },
  {
    "objectID": "posts/tf-item-assign.html#numpy中索引",
    "href": "posts/tf-item-assign.html#numpy中索引",
    "title": "tf2.0数组索引与赋值",
    "section": "numpy中索引",
    "text": "numpy中索引\nnumpy中是给定每一维下标，然后进行索引。比如一个4维的数组，我们需要索引到他的那个元素，那我们就需要给出一个元组，这个元组里面需要包含不大于维度个数组，每个数组的维度要保证可以适配broadcast机制。\narr = np.random.randn(7, 10, 5, 2)\nidx = (np.array([2]), np.array([3]), np.array([4]), np.array([1]))\narr[idx] # array([0.41087443])\n\nidx = (np.array([2]), np.array([3]), np.array([4]), np.array([1, 0]))\narr[idx]  # array([ 0.41087443, -0.83541546])\n\nidx = (np.array([2]), np.array([3, 2, 3]), np.array([4]), np.array([1, 0]))\narr[idx]  # error"
  },
  {
    "objectID": "posts/tf-item-assign.html#tf.gather索引",
    "href": "posts/tf-item-assign.html#tf.gather索引",
    "title": "tf2.0数组索引与赋值",
    "section": "tf.gather索引",
    "text": "tf.gather索引\n而tensorflow中索引，首先就有两个函数：tf.gather和tf.gather_nd。 先说gather，这个是只索引单一维度的，比较好理解，并且会自动将索引出来的值按索引的维度进行stack。\narr = tf.random.normal((7, 10, 5, 2))\n# shape = (3, 10, 5, 2)\ntf.assert_equal(tf.gather(arr, [1, 2, 3]), tf.stack([arr[1], arr[2], arr[3]]))\n# shape = (7, 10, 3, 2)\ntf.assert_equal(tf.gather(arr, [1, 2, 3], axis=2),\n                tf.stack([arr[:, :, 1, :],\n                          arr[:, :, 2, :],\n                          arr[:, :, 3, :]], axis=2))\n上面看起来和stack差不多，但是gather可以通过给索引参数indices增加维度来增加最终的输出维度，相当于先添加维度再stack。\n# shape = (3, 1, 10, 5, 2)\ntf.assert_equal(tf.gather(arr, [[1], [2], [3]]),\n                tf.stack([arr[1][None, ...],\n                          arr[2][None, ...],\n                          arr[3][None, ...]]))\n# shape = (7, 10, 3, 1, 2)\ntf.assert_equal(tf.gather(arr, [[1], [2], [3]], axis=2),\n                tf.stack([arr[:, :, 1, :][:, :, None, :],\n                          arr[:, :, 2, :][:, :, None, :],\n                          arr[:, :, 3, :][:, :, None, :]], axis=2))"
  },
  {
    "objectID": "posts/tf-item-assign.html#tf.gather_nd索引",
    "href": "posts/tf-item-assign.html#tf.gather_nd索引",
    "title": "tf2.0数组索引与赋值",
    "section": "tf.gather_nd索引",
    "text": "tf.gather_nd索引\ngather_nd里面，是索引参数indices的最后一维所对应的值对应了数组的索引值，下面的例子可以发现：\narr = tf.random.normal((7, 10, 5, 2))\n# NOTE 单一维度索引\ntf.gather(arr, [2]) # shape=(1, 10, 5, 2)\ntf.gather_nd(arr, [2]) # shape=(10, 5, 2)\n# NOTE 多维度索引\ntf.gather(arr, [2, 2]) # shape=(2, 10, 5, 2)\ntf.gather_nd(arr, [2, 2]) # shape=(5, 2)\narr[2, 2]  # shape=(5, 2)\n但是当多个多维度索引时，gather_nd就不是很友好了，就相当于每个位置进行一次索引，再组合成新数组。\ntf.gather_nd(arr, [[2, 2]])  # shape=(1, 5, 2)\ntf.convert_to_tensor([arr[2, 2]])  # shape=(1, 5, 2)"
  },
  {
    "objectID": "posts/tf-item-assign.html#总结",
    "href": "posts/tf-item-assign.html#总结",
    "title": "tf2.0数组索引与赋值",
    "section": "总结",
    "text": "总结\n本来我想说这个索引问题很头大，但是实际上按需求使用其实也还好。"
  },
  {
    "objectID": "posts/tf-item-assign.html#scatter_update赋值",
    "href": "posts/tf-item-assign.html#scatter_update赋值",
    "title": "tf2.0数组索引与赋值",
    "section": "scatter_update赋值",
    "text": "scatter_update赋值\n与gather对应，scatter_update也是只索引某一个维度，并且我还没找到他如何指定维度，所以目前我就演示下在第一个维度下的批量赋值：\narr = tf.Variable(tf.random.normal((7, 10, 5, 2)), False)\narr2 = tf.random.normal((10, 5, 2))\narr.scatter_update(tf.IndexedSlices(2, [0]))\narr[0]  # all is 2.\narr.scatter_update(tf.IndexedSlices(arr2, [1]))\ntf.assert_equal(arr[1], arr2)  # true"
  },
  {
    "objectID": "posts/tf-item-assign.html#scatter_nd_update赋值",
    "href": "posts/tf-item-assign.html#scatter_nd_update赋值",
    "title": "tf2.0数组索引与赋值",
    "section": "scatter_nd_update赋值",
    "text": "scatter_nd_update赋值\n不过还好有这个scatter_nd_update，对应gather_nd，有两点要注意： 1. indices的shape[0]必须等于updates的shape[0] 2. indices的shape[-1]必须小于变量的dim 3. indices的dim加updates的dim必须等于变量的dim\narr = tf.Variable(tf.random.normal((2, 2, 2)), False)\n\narr.scatter_nd_update([[1, 1], [0, 1]], [[3, 2], [3, 3]])\narr[1, 1, :]  # [3., 2.]\narr[0, 1, :]  # [3., 3.]\n\narr.scatter_nd_update([[1, 1, 0], [0, 1, 1]], [6, 7])\narr[1, 1, 0]  # 6.\narr[0, 1, 1]  # 7.\n\narr = tf.Variable(tf.random.normal((7, 10, 5, 2)), False)\narr.scatter_nd_update([[1, 1, 1, 0], [1, 1, 1, 1]], [2, 3])\narr[1, 1, 1, 0]  # 2.0\narr[1, 1, 1, 1]  # 3.0\n注意!\n对于变量的scatter_nd_update是不支持使用-1进行更新的! 而tensor_scatter_nd_update是支持的.\narr = tf.Variable(tf.random.normal((2, 2, 2)))\n\narr.scatter_nd_update([[1, -1], [0, -1]], [[3, 2], [3, 3]])\narr[1, 1, :]  # [ 0.6016006, -1.7807052]\narr[0, 1, :]  # [-2.4027731, -0.6694494]\narr.scatter_nd_update([[1, 1], [0, 1]], [[3, 2], [3, 3]])\narr[1, 1, :]  # [3., 2.]\narr[0, 1, :]  # [3., 3.]"
  },
  {
    "objectID": "posts/tf-item-assign.html#tensor_scatter_nd_update赋值",
    "href": "posts/tf-item-assign.html#tensor_scatter_nd_update赋值",
    "title": "tf2.0数组索引与赋值",
    "section": "tensor_scatter_nd_update赋值",
    "text": "tensor_scatter_nd_update赋值\n如果按位置赋值必须使用变量的话实在是太令人蛋疼了，因为变量是不能多次创建的，那我们必须要用很多全局变量，极大的破坏代码的结构。万幸tensorflow提供了tensor_scatter_nd_update可以对普通的tensor进行操作。其使用方式与scatter_nd_update相同，不过他并不能修改原始值。\narr = tf.random.normal((2, 2, 2))\n\narr = tf.tensor_scatter_nd_update(arr, [[1, 1], [0, 1]], [[3, 2], [3, 3]])\narr[1, 1, :]  # [3., 2.]\narr[0, 1, :]  # [3., 3.]\n\narr = tf.tensor_scatter_nd_update(arr, [[1, 1, 0], [0, 1, 1]], [6, 7])\narr[1, 1, 0]  # 6.\narr[0, 1, 1]  # 7.\n\narr = tf.random.normal((7, 10, 5, 2))\narr = tf.tensor_scatter_nd_update(arr, [[1, 1, 1, 0], [1, 1, 1, 1]], [2, 3])\narr[1, 1, 1, 0]  # 2.0\narr[1, 1, 1, 1]  # 3.0"
  },
  {
    "objectID": "posts/tf-item-assign.html#总结-1",
    "href": "posts/tf-item-assign.html#总结-1",
    "title": "tf2.0数组索引与赋值",
    "section": "总结",
    "text": "总结\n这样总结一次相信对tensorflow中的索引与赋值更加熟悉，需要根据不同的场景选择合适的函数。当然如果需要更深入的了解，还是需要查看api文档。最后，根据我的测试，将原来的数据生成过程的标签制作、图像读取、图像归一化等过程全部利用tensorflow的函数重写之后，数据读取加快了5倍有余，已经好了很多，就是一些图像旋转等等的数据增强不好用tensorflow重写，不然速度还可以更快。tensorflow的使用过程虽然比较别扭，但是构建静态图之后有xla、图优化等方法使程序运行更高效。"
  },
  {
    "objectID": "posts/tf-keras-dynamic-input.html",
    "href": "posts/tf-keras-dynamic-input.html",
    "title": "tf.keras实现动态多尺度训练",
    "section": "",
    "text": "哇,今天真的好累,就写了个动态多尺度训练(差点又被tensorflow劝退.),下面写几个要注意的点.\n\n\ninput_shape的修改\n首先网络为了要支持多尺度训练,那我们的输入是不能指定的,所以要修改成\\([None,None,3]\\)才可以.\n\n\noutput_shape的修改\n这里就是一个很蛋疼的地方,因为我们不知道输入的维度,所以得到的输出维度为\\([None,None,75]\\),之前我们可以计算出前两个维度然后reshape,但是现在我们不知道这个维度reshape中传入None就会报错.但是如果不Reshape,在tf.keras的Loss中会默认检查y_true和y_pred的维度是否一致,所以我只能用折中的方式,使用expand_dims加一个虚的维度,然后在Loss中重新reshape\n\n\nxy_offset的计算\n我本来的想法是修改Helper对象中的输入输出数组值即可,但是改了之后我才发现,由于tf.keras用的是静态图的方式,所以我使用的那个数组早就在建立图的时候被转换为Tensor固化了,我再去修改他也并没有实际用处…所以得在Loss里面重新计算xy_offset了.\n\n\n分别处理训练和测试\n根据测试tf.keras中callback的运行状态中测试的方式,添加Callback对象进行处理,这种方式唯一的缺点就是在测试开始之后,有一部分batch本来应该是多尺度的,但是被强制改成原始尺度了.哎,实在想不到好点子了…..日常劝退tensorflow.\n\n\n总结\n花了半天时间搞定了这个多尺度训练,接下来可以做别的去了…之前lffd算法的内存泄漏问题调试的我头疼,都没时间测试算法的效果."
  },
  {
    "objectID": "posts/tf-keras-profile.html",
    "href": "posts/tf-keras-profile.html",
    "title": "tf.keras中分析性能",
    "section": "",
    "text": "tensorflow和keras的结合太好了，今天训练模型超级慢，我准备使用tensorflow中的timeline来进行分析，一番寻找发现根本不需要以前那么麻烦。\n\n\n升级Tensorflow\n首先要升级一下，我2.0版本的都还没有这个特性，需要安装：\npip install tf-nightly-gpu-2.0-preview -U\n然后在TensorBoard回调中添加：\ntb_call = TensorBoard(log_dir=str(log_dir), profile_batch=3, update_freq='batch')\n这样他会自动分析batch=3时的函数调用分析。\n\n\n效果\n可以看到基本上所有的等待时间都在读取文件与频谱图的生成了，所以我下一步要想办法去提升性能。"
  },
  {
    "objectID": "posts/tf-learn-err.html",
    "href": "posts/tf-learn-err.html",
    "title": "Tensorflow中动态学习率无效",
    "section": "",
    "text": "我昨天刚刚上传了Mobilenet Flowers项目,今天在修改勘智官方的的demo,我简单粗暴的把他的代码改成我的写法,然后测试,忽然发现我的动态学习率一直不变.找了半天才解决."
  },
  {
    "objectID": "posts/tf-learn-err.html#问题解决",
    "href": "posts/tf-learn-err.html#问题解决",
    "title": "Tensorflow中动态学习率无效",
    "section": "问题解决",
    "text": "问题解决\n我的是把train_op使用一个函数输入的,问题就是在这个函数的minimize的时候,我忘记把第二个参数global_step传入了.在我以前没有被函数包裹的情况下,可以不传入这个参数,现在必须传入.\n    train_op = create_train_op(total_loss, global_step, args.optimizer,\n                               current_learning_rate, args.moving_average_decay)\n    .\n    .\n    .\n    \ndef create_train_op(total_loss, global_step, optimizer, learning_rate, moving_average_decay):\n    # Generate moving averages of all losses and associated summaries.\n    # loss_averages_op = _add_loss_summaries(total_loss)\n\n    # Compute gradients.\n    # with tf.control_dependencies([loss_averages_op]):\n    if optimizer == 'ADAGRAD':\n        opt = tf.train.AdagradOptimizer(learning_rate)\n    elif optimizer == 'ADADELTA':\n        opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n    elif optimizer == 'ADAM':\n        opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    elif optimizer == 'RMSPROP':\n        opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n    elif optimizer == 'MOM':\n        opt = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n    else:\n        raise ValueError('Invalid optimization algorithm')\n    # ! 因为minimize中包含了compute_gradients和apply_gradients,所以他们的思路是:\n    # ! 首先计算梯度,然后给梯度增加滑动平均,最后把滑动平均的梯度应用到梯度下降\n    #     grads = opt.compute_gradients(total_loss, update_gradient_vars)\n\n    # # Apply gradients.\n    # apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    # # Track the moving averages of all trainable variables.\n    # variable_averages = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)\n    # variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    # with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    #     train_op = tf.no_op(name='train')\n    # ! 我先用原始的方式进行训练\n    train_op = opt.minimize(total_loss, global_step)\n\n    return train_op"
  },
  {
    "objectID": "posts/tf-pb-error.html",
    "href": "posts/tf-pb-error.html",
    "title": "Tensorflow加载pb推理输出不正确",
    "section": "",
    "text": "我最近做网络测试的时候,我发现推理出的结果和我训练的时候结果完全不相同.我做了一番测试."
  },
  {
    "objectID": "posts/tf-pb-error.html#测试代码",
    "href": "posts/tf-pb-error.html#测试代码",
    "title": "Tensorflow加载pb推理输出不正确",
    "section": "测试代码",
    "text": "测试代码\nfrom models.yolonet import yoloconv\nimport tensorflow as tf\nfrom tools.utils import helper\nimport skimage\nimport numpy as np\nfrom scipy.special import expit\n\nif __name__ == \"__main__\":\n    tf.enable_eager_execution()\n    fddb = helper('/home/zqh/Documents/faces/data/train.list', (224, 320), (7, 10))\n    gen = fddb.generator()\n    tf.reset_default_graph()\n    g = tf.Graph()\n    g1 = tf.Graph()\n    with g.as_default():\n        with tf.gfile.GFile('/home/zqh/Documents/faces/Training_save.pb', 'rb') as f:\n            Training_save = tf.GraphDef()\n            Training_save.ParseFromString(f.read())\n        tf.import_graph_def(Training_save, name='')\n\n    with g1.as_default():\n        with tf.gfile.GFile('/home/zqh/Documents/faces/Freeze_save.pb.pb', 'rb') as f:\n            Freeze_save = tf.GraphDef()\n            Freeze_save.ParseFromString(f.read())\n        tf.import_graph_def(Freeze_save, name='')\n\n    train_weights = [opt for opt in g.get_operations() if 'read' in opt.name]\n    train_weights = [g.get_tensor_by_name(opt.name+':0') for opt in train_weights]\n\n    freeze_weights = [opt for opt in g1.get_operations() if 'read' in opt.name]\n    freeze_weights = [g1.get_tensor_by_name(opt.name+':0') for opt in freeze_weights]\n\n    \"\"\" start compare weights \"\"\"\n    with tf.Session(graph=g) as t_sess:\n        t_var_dict = {}\n        for var in train_weights:\n            t_var_dict[var.name] = t_sess.run(var)\n\n    with tf.Session(graph=g1) as f_sess:\n        f_var_dict = {}\n        for var in freeze_weights:\n            f_var_dict[var.name] = f_sess.run(var)\n\n    for name, value in t_var_dict.items():\n        # ! all weights are equal\n        assert np.array_equal(t_var_dict[name], f_var_dict[name])\n\n    \"\"\" start test output \"\"\"\n    img, label = next(gen)\n    img = img[np.newaxis, :, :, :]\n    with tf.Session(graph=g) as t_sess:\n        t_output = t_sess.run(g.get_tensor_by_name('predict:0'), feed_dict={g.get_tensor_by_name('Input_image:0'): img})\n\n    with tf.Session(graph=g1) as f_sess:\n        f_output = f_sess.run(g1.get_tensor_by_name('predict:0'), feed_dict={g1.get_tensor_by_name('Input_image:0'): img})\n\n    t_output = expit(t_output)\n    f_output = expit(f_output)\n    # ! array not equal    Why ??\n    if np.array_equal(t_output, f_output):\n        print('test success!')\n    else:\n        print('results not equal. Why ??')"
  },
  {
    "objectID": "posts/tf-pb-error.html#运行结果",
    "href": "posts/tf-pb-error.html#运行结果",
    "title": "Tensorflow加载pb推理输出不正确",
    "section": "运行结果",
    "text": "运行结果\nresults not equal. Why ??\n以上代码中,我加载了两种不同保存方式的权重,然后一一对比了所有权重,权重完全相同,证明了保存pb的方式都没有问题.但是同一张图像,两个网络的输出结果确实不相同的.这个就十分诡异."
  },
  {
    "objectID": "posts/tf-pb-error.html#解决步骤1",
    "href": "posts/tf-pb-error.html#解决步骤1",
    "title": "Tensorflow加载pb推理输出不正确",
    "section": "解决步骤1",
    "text": "解决步骤1\n\n首先我们需要让这个两个参数进行更新,将train_op定义为如下:\n\n    \"\"\" define train_op \"\"\"\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = tf.train.AdamOptimizer(current_learning_rate).minimize(total_loss, global_steps)\n\n接下来我们需要让模型保存滑动平均参数\n\n注意:正是因为滑动平均参数是需要在输入数据上计算出来的,所以这两个参数是默认不保存的. 将所有变量都传入,进行保存.\n    \"\"\" must save the bn paramter! \"\"\"\n    var_list = tf.global_variables()\n    saver = tf.train.Saver(var_list)\n加载时也同样:\nloader = tf.train.Saver(tf.global_variables())\n以上就是代码中需要加的地方."
  },
  {
    "objectID": "posts/tf-pb-error.html#解决步骤2",
    "href": "posts/tf-pb-error.html#解决步骤2",
    "title": "Tensorflow加载pb推理输出不正确",
    "section": "解决步骤2",
    "text": "解决步骤2\n现在我们需要训练模型.并且需要保证他有良好的泛化能力. 1. 设置占位符控制\ntraining_control = tf.placeholder_with_default(True, shape=[], name='training_control')\nnets, endpoints = network(batch_image, depth_multiplier, is_training=training_control)\n\n在训练的同时测试\n\n我们一直观察着模型的验证误差,这样在做推理的时候才知道效果.\nif j % 40 == 0:\n    summary, test_con_acc, _, step_cnt = sess.run(\n        [merged,  test_confidence_acc, test_acc_op, global_steps], feed_dict={training_control: False})\nelse:\n    summary, _, total_l, con_acc, _,  lr, step_cnt = sess.run(\n        [merged, train_op, total_loss, confidence_acc, acc_op, current_learning_rate, global_steps])"
  },
  {
    "objectID": "posts/tf-pb-error.html#解决",
    "href": "posts/tf-pb-error.html#解决",
    "title": "Tensorflow加载pb推理输出不正确",
    "section": "解决",
    "text": "解决\n说是解决,实际上没有解决.因为选择False以及True一定会导致模型输出不同,但是经过之前的步骤,是可以得出近似相同的结果的."
  },
  {
    "objectID": "posts/tf15-tb-error.html",
    "href": "posts/tf15-tb-error.html",
    "title": "Tensorflow 1.15中TensorBoard错误",
    "section": "",
    "text": "升级了tensorflow到1.15,发现一个用tf.keras中TensorBoard的时候就会报错的问题.\n\n\n问题描述\n错误信息如下:\nTypeError: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: create_file_writer/SummaryWriter:0\n\n\n问题解决\n这个问题主要还是因为tensorflow要兼容两个版本的问题,也有我用vscode开发的原因.\n现在使用的tensorflow.keras的时候最好还是用如下的方式:\nTensorBoard = tf.keras.callbacks.TensorBoard\n我用的是如下:\nfrom tensorflow.python.keras.callbacks import TensorBoard\n问题就在这里了,现在tensorflow 1.15默认的TensorBoard是使用tf 2.0的写法的,如果用第一个方式调用是没问题的.但因为tf.keras.callbacks.TensorBoard是从from tensorflow.python.keras.callbacks_v1导入的,所以我的写法调用的是对应tf 2.0的代码,导致错误.\n所以我的写法要改成:\nfrom tensorflow.python.keras.callbacks_v1 import TensorBoard"
  },
  {
    "objectID": "posts/tflite-kmodel.html",
    "href": "posts/tflite-kmodel.html",
    "title": "比较kmdoel和tflite推理输出",
    "section": "",
    "text": "我的yolo3模型在k210里面输出结果完全不对，所以我十分怀疑是量化出了问题，但是我又找不到问题。还好昨天case小姐姐帮忙更新了nncase，可以在pc上推理kmdoel.然后我推理了几个图像，这次就是记录一下这个脚本，免得下次要用找不到了。。。\n\n\n代码\nimport numpy as np\nfrom scipy.special import expit, softmax\nfrom tensorflow import lite\nfrom pathlib import Path\nfrom skimage.io import imread\nfrom termcolor import colored\nnp.set_printoptions(suppress=True)\n\n\n\"\"\" 加载tflite \"\"\"\ninterpreter = lite.Interpreter(model_path=str('mobile_yolo.tflite'))\ninterpreter.allocate_tensors()\n\ninput_index = [details[\"index\"] for details in interpreter.get_input_details()]\noutput_index = [details[\"index\"] for details in interpreter.get_output_details()]\n\n\n\"\"\" 加载图片 \"\"\"\nimg_paths = list(Path('test_logs').glob('*.jpg'))\nimg_paths.sort()\nimgs = np.array([imread(str(path)) for path in img_paths])\nimgs = imgs / 255\n\n\n\"\"\" 推理 \"\"\"\n\n\ndef infer(img: np.ndarray) -&gt; [np.ndarray, np.ndarray]:\n    inp = img[np.newaxis, ...].astype('float32')\n    interpreter.set_tensor(input_index[0], inp)\n    interpreter.invoke()\n    predictions = [interpreter.get_tensor(idx)[0] for idx in output_index]\n    return predictions\n\n\ntf_res = [infer(img) for img in imgs]\n\n\"\"\" 加载bin \"\"\"\nbin_paths = list(Path('output').glob('*.bin'))\nbin_paths.sort()\n\n\ndef parser_bin(path: Path) -&gt; [np.ndarray, np.ndarray]:\n    content = path.open('rb').read()  # type:bytes\n    assert len(content) / 4 == 7 * 10 * 75 + 14 * 20 * 75\n    # out = np.fromstring(content, dtype='&lt;f4')\n    out = np.array(np.frombuffer(content, '&lt;f4'))\n    # out = [out[:7 * 10 * 75].reshape(7, 10, 75),\n    #        out[7 * 10 * 75:].reshape(14, 20, 75)]\n    out = [np.transpose(out[:7 * 10 * 75].reshape(75, 7, 10), (1, 2, 0)),\n           np.transpose(out[7 * 10 * 75:].reshape(75, 14, 20), (1, 2, 0))]\n\n    return out\n\n\nkmd_res = [parser_bin(path) for path in bin_paths]\n\n\n\"\"\" 解析输出 \"\"\"\ninshape = (224, 320)\nanchors = np.array([[[81, 82], [135, 169], [344, 319]], [[10, 14], [23, 27], [37, 58]]])\n\nfor i in range(len(kmd_res)):\n    for j in range(len(output_index)):\n        grid_shape = tf_res[i][j].shape[0:2]\n        a = tf_res[i][j].reshape(grid_shape + (3, 25))\n        b = kmd_res[i][j].reshape(grid_shape + (3, 25))\n\n        \"\"\" 解析xy \"\"\"\n        grid_y = np.tile(np.reshape(np.arange(0, grid_shape[0]), [-1, 1, 1, 1]), [1, grid_shape[1], 1, 1])\n        grid_x = np.tile(np.reshape(np.arange(0, grid_shape[1]), [1, -1, 1, 1]), [grid_shape[0], 1, 1, 1])\n        grid = np.concatenate([grid_x, grid_y], axis=-1)\n\n        a[..., 0:2] = (expit(a[..., 0:2]) + grid) / grid_shape[::-1] * inshape[::-1]\n        b[..., 0:2] = (expit(b[..., 0:2]) + grid) / grid_shape[::-1] * inshape[::-1]\n\n        \"\"\" 解析wh \"\"\"\n        a[..., 2:4] = np.exp(a[..., 2:4]) * anchors[j]\n        b[..., 2:4] = np.exp(b[..., 2:4]) * anchors[j]\n\n        \"\"\" 解析confendice \"\"\"\n        a[..., 4:5] = expit(a[..., 4:5])\n        b[..., 4:5] = expit(b[..., 4:5])\n\n        \"\"\" 解析类别 \"\"\"\n        a[..., 5] = np.argmax(softmax(a[..., 5:], axis=-1), axis=-1)\n        b[..., 5] = np.argmax(softmax(b[..., 5:], axis=-1), axis=-1)\n\n        print(colored(f'img {i} layer {j} tflite :\\n', 'blue'), a[np.where(a[..., 4] &gt; .6)][:, :5])\n        print(colored(f'img {i} layer {j} kmodel :\\n', 'green'), b[np.where(b[..., 4] &gt; .6)][:, :5])\n\n\n结果\nINFO: Initialized TensorFlow Lite runtime.\nimg 0 layer 0 tflite :\n []\nimg 0 layer 0 kmodel :\n []\nimg 0 layer 1 tflite :\n [[ 50.671265  184.66817    16.581987   14.750296    0.7672671]\n [135.05923   188.98267    14.629576    9.189983    0.7180456]\n [180.27417   193.83612    18.99109    10.469457    0.7107621]\n [258.86105   197.36963    41.65087    16.02682     0.8539901]\n [259.66782   197.74883    33.983326   23.298958    0.720852 ]]\nimg 0 layer 1 kmodel :\n [[ 51.26226    184.          13.131495    14.           0.69366026]]\nimg 1 layer 0 tflite :\n [[172.43805   147.88477   210.21072   149.22597     0.7295683]]\nimg 1 layer 0 kmodel :\n [[173.1715     147.51495    211.01398    141.3503       0.72762936]]\nimg 1 layer 1 tflite :\n []\nimg 1 layer 1 kmodel :\n [[107.401764   186.7845      63.801384    52.96505      0.73084366]\n [117.872925   187.09857     63.801384    52.96505      0.6740316 ]]\nimg 2 layer 0 tflite :\n [[181.06418    138.47464    224.90767    197.36978      0.9352028 ]\n [172.32611    137.85072    261.53143    188.90297      0.7145112 ]\n [195.62141    136.05687    213.14252    194.8197       0.85339344]]\nimg 2 layer 0 kmodel :\n [[180.8456     137.29376    230.73112    184.79173      0.87710017]\n [170.5144     137.29376    263.13138    186.64618      0.6714251 ]\n [195.9328     135.123      211.01398    184.79173      0.87710017]]\nimg 2 layer 1 tflite :\n []\nimg 2 layer 1 kmodel :\n []\nimg 3 layer 0 tflite :\n [[200.14052    100.00058     52.676952   117.090515     0.75216126]]\nimg 3 layer 0 kmodel :\n [[199.123     100.954346   56.663612  107.2014      0.7615662]]\nimg 3 layer 1 tflite :\n [[197.36374     98.02013     22.453339    80.50932      0.63156927]\n [244.84764     98.41268     23.07575     50.217842     0.63718444]]\nimg 3 layer 1 kmodel :\n [[131.26227   100.90143    19.180124   66.949135    0.6537735]\n [197.21548    98.61127    19.594486   83.40284     0.8488212]\n [245.87292    98.81582    23.         50.983635    0.7126105]\n [281.7856     98.23825    21.457172   91.33108     0.7650425]]\n\n\n分析\n现在看起来量化应该是没什么问题，差距不是很大。我得去找找c代码是不是出错了。。好累。。 🙄"
  },
  {
    "objectID": "posts/tfp-ch2.html",
    "href": "posts/tfp-ch2.html",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "",
    "text": "Tensorflow 概率模型学习，代码运行于Tensorflow 1.14，文字半机器翻译。"
  },
  {
    "objectID": "posts/tfp-ch2.html#a-little-more-on-tensorflow-and-tensorflow-probability",
    "href": "posts/tfp-ch2.html#a-little-more-on-tensorflow-and-tensorflow-probability",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "A little more on TensorFlow and TensorFlow Probability",
    "text": "A little more on TensorFlow and TensorFlow Probability\n为了解释TensorFlow概率，值得研究使用Tensorflow张量的各种方法。在这里，我们介绍了Tensorflow图的概念，以及我们如何使用某些编码模式使我们的张量处理工作流更加快速和优雅。\n\nTensorFlow图和Eager模式\nTFP通过主张量流库实现了大部分繁重的工作。张量流库还包含许多熟悉的NumPy计算元素，并使用类似的表示法。当NumPy直接执行计算时（例如，当您运行+ b时），图形模式中的张量流会构建一个“计算图形”，跟踪您要对元素a和b执行+运算。只有在评估张量流表达式时才会进行计算 - tensorflow是惰性求值的。使用Tensorflow而不是NumPy的好处是图形可以实现数学优化（例如简化），通过自动微分进行梯度计算，将整个图形编译为C以机器速度运行，以及编译它以在GPU或TPU上运行。\n从根本上说，TensorFlow使用图形进行计算，其中图形表示计算作为各个操作之间的依赖关系。在Tensorflow图的编程范例中，我们首先定义数据流图，然后创建TensorFlow会话以运行图的部分。 Tensorflow tf.Session（）对象运行图形以获取我们想要建模的变量。在下面的示例中，我们使用了一个全局会话对象sess，我们在上面的“Imports and Global Variables”部分中创建了它。\n为了避免懒惰评估有时令人困惑的方面，Tensorflow的渴望模式会立即对结果进行评估，从而为使用NumPy提供更加相似的感觉。使用Tensorflow eager模式，您可以立即评估操作，而无需显式构建图形：操作返回具体值，而不是构建计算图形以便稍后运行。如果我们处于急切模式，我们会看到可以立即转换为NumPy数组等效的张量。 Eager模式使您可以轻松开始使用TensorFlow和调试模型。\nTFP is essentially:\n\n各种概率分布的张量流符号表达式的集合，它们被组合成一个大的计算图\n一组推理算法，使用该图来计算概率和梯度。\n\n出于实际目的，这意味着为了构建某些模型，我们有时必须使用核心Tensorflow。泊松采样的这个简单示例是我们如何使用图形和急切模式：\nparameter = tfd.Exponential(rate=1., name=\"poisson_param\").sample() # 构建一个指数分布并进行采样\nrv_data_generator = tfd.Poisson(parameter, name=\"data_generator\") # 构建一个泊松分布\ndata_generator = rv_data_generator.sample() # 取得泊松分布的样本\n\nif tf.executing_eagerly():\n    data_generator_ = tf.contrib.framework.nest.pack_sequence_as(data_generator,[t.numpy() if tf.contrib.framework.is_tensor(t) else t for t in tf.contrib.framework.nest.flatten(data_generator)])\nelse:\n    data_generator_ = sess.run(data_generator)\n    \nprint(\"Value of sample from data generator random variable:\", data_generator_)\nValue of sample from data generator random variable: 1.0\n在图形模式下，Tensorflow会自动将任何变量分配给图形;然后可以在会话中对它们进行评估，也可以在急切模式下使用它们。如果在会话已关闭或处于最终状态时尝试定义变量，则会出现错误。在“导入和全局变量”部分中，我们定义了一种特定类型的会话，称为InteractiveSession。全局InteractiveSession的这个定义允许我们通过shell或笔记本以交互方式访问我们的会话变量。\n使用全局会话的模式，我们可以递增地构建图形并运行它的子集以获得结果。\n热切执行进一步简化了我们的代码，无需显式调用会话功能。实际上，如果您尝试在急切模式下运行图形模式语义，您将收到如下错误消息：\nAttributeError: Tensor.graph is meaningless when eager execution is enabled.\nAs mentioned in the previous chapter, we have a nifty tool that allows us to create code that’s usable in both graph mode and eager mode. The custom evaluate() function allows us to evaluate tensors whether we are operating in TF graph or eager mode. A generalization of our data generator example above, the function looks like the following:\n\ndef evaluate(tensors):\n    if tf.executing_eagerly():\n         return tf.contrib.framework.nest.pack_sequence_as(\n             tensors,\n             [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n             for t in tf.contrib.framework.nest.flatten(tensors)])\n    with tf.Session() as sess:\n        return sess.run(tensors)\nEach of the tensors corresponds to a NumPy-like output. To distinguish the tensors from their NumPy-like counterparts, we will use the convention of appending an underscore to the version of the tensor that one can use NumPy-like arrays on. In other words, the output of evaluate() gets named as variable + _ = variable_ . Now, we can do our Poisson sampling using both the evaluate() function and this new convention for naming Python variables in TFP.\n# 定义我们的假设\nparameter = tfd.Exponential(rate=1., name=\"poisson_param\").sample()\n\n# 转换为numpy\n[ parameter_ ] = evaluate([ parameter ])\n\nprint(\"Sample from exponential distribution before evaluation: \", parameter)\nprint(\"Evaluated sample from exponential distribution: \", parameter_)\nSample from exponential distribution before evaluation:  Tensor(\"poisson_param_1/sample/Reshape:0\", shape=(), dtype=float32)\nEvaluated sample from exponential distribution:  0.34844726\n更一般地说，我们可以使用我们的evaluate()函数在Tensorflow tensor数据类型和我们可以运行操作的数据类型之间进行转换：\n[ \n    parameter_,\n    data_generator_,\n] = evaluate([ \n    parameter, \n    data_generator,\n])\n\nprint(\"'parameter_' evaluated Tensor :\", parameter_)\nprint(\"'data_generator_' sample evaluated Tensor :\", data_generator_)\n'parameter_' evaluated Tensor : 0.7298943\n'data_generator_' sample evaluated Tensor : 0.0\n在TensorFlow中编程的一般经验法则是，如果您需要进行任何需要NumPy函数的类似数组的计算，则应在TensorFlow中使用它们的等价物。这种做法是必要的，因为NumPy只能产生常数值，但TensorFlow张量是计算图的动态部分。如果以错误的方式混合和匹配这些，通常会出现有关不兼容类型的错误。\n\n\nTFP Distributions\n让我们看看tfp.distributions如何工作。\nTFP使用分布子类来表示随机随机变量。当满足以下条件时，变量是随机的：即使您知道变量的参数和组件的所有值，它仍然是随机的。此类别中包括Poisson，Uniform和Exponential类的实例。\n您可以从随机变量中抽取随机样本。当您绘制样本时，这些样本将成为tensorflow.Tensors从该点开始具有确定性。快速的心理检查以确定某些东西是否具有确定性：如果我知道创建变量foo的所有输入，我可以计算foo的值。您可以通过下面讨论的各种方式添加，减去和操纵张量。这些操作几乎总是确定的。\n\n初始化分布\n初始化随机变量或随机变量需要一些特定于类的参数来描述分布的形状，例如位置和比例。例如：\nsome_distribution = tfd.Uniform(0., 4.)\n初始化随机或随机的均匀分布，其下限为0，上限为4.在分布上调用sample（）会返回一个张量，该张量将从该点开始确定性地表现：\nsampled_tensor = some_distribution.sample()\n下一个例子说明了当我们说分布是随机的但是张量是确定性时我们的意思：\nderived_tensor_1 = 1 + sampled_tensor\nderived_tensor_2 = 1 + sampled_tensor  # equal to 1\n\nderived_tensor_3 = 1 + some_distribution.sample()\nderived_tensor_4 = 1 + some_distribution.sample()  # different from 3\n前两行产生相同的值，因为它们引用相同的采样张量。最后两行可能产生不同的值，因为它们指的是从相同分布中提取的独立样本。\n要定义多变量分布，只需传入具有您希望输出在创建分布时的形状的参数。例如：\nbetas = tfd.Uniform([0., 0.], [1., 1.])\n使用batch_shape（2，）创建分布。现在，当您调用betas.sample（）时，将返回两个值而不是一个。您可以在TFP文档中阅读有关TFP形状语义的更多信息，但本书中的大多数用法应该是不言自明的。\n\n\n确定变量\n我们可以创建一个确定性分布，类似于我们如何创建随机分布。我们只是从Tensorflow Distributions中调用Deterministic类，并传递我们想要的确定性值\ndeterministic_variable = tfd.Deterministic(name=\"deterministic_variable\", loc=some_function_of_variables)\n调用tfd.Deterministic对于创建始终具有相同值的分布非常有用。但是，在TFP中使用确定性变量的更常见模式是从分布中创建张量或样本：\nlambda_1 = tfd.Exponential(rate=1., name=\"lambda_1\") #随机变量\nlambda_2 = tfd.Exponential(rate=1., name=\"lambda_2\") #随机变量\ntau = tfd.Uniform(name=\"tau\", low=0., high=10.) #随机变量\n\n# 因为我们在采样后得到lambda的结果，所以确定性变量\nnew_deterministic_variable = tfd.Deterministic(name=\"deterministic_variable\", loc=(lambda_1.sample() + lambda_2.sample()))\nnew_deterministic_variable\n&lt;tfp.distributions.Deterministic 'deterministic_variable/' batch_shape=[] event_shape=[] dtype=float32&gt;\n在前一章的文本消息示例中可以看到确定性变量的使用。回想一下λ的模型看起来像\n\\[\n\\lambda =\n\\begin{cases}\\lambda_1  & \\text{if } t \\lt \\tau \\cr\n\\lambda_2 & \\text{if } t \\ge \\tau\n\\end{cases}\n\\]\nAnd in TFP code:\n# 构建图\n\n# 日子\nn_data_points = 5  # in CH1 we had ~70 data points\nidx = np.arange(n_data_points)\n# 对于n_data_points样本，如果采样tau&gt; = day值，则从lambda_2中选择，否则为lambda_1\nrv_lambda_deterministic = tfd.Deterministic(tf.gather([lambda_1.sample(), lambda_2.sample()],\n                    indices=tf.cast(\n                        tau.sample() &gt;= idx,tf.int32)))\nlambda_deterministic = rv_lambda_deterministic.sample()\n\n# Execute graph\n[lambda_deterministic_] = evaluate([lambda_deterministic])\n\n# Show results\n\nprint(\"{} samples from our deterministic lambda model: \\n\".format(n_data_points), lambda_deterministic_ )\n5 samples from our deterministic lambda model: \n [0.24393924 0.24393924 0.24393924 0.24393924 0.24393924]\n显然，如果已知τ，\\(\\lambda_1\\)，λ1和λ2，那么λ是完全已知的，因此它是一个确定性变量。我们在这里使用索引在适当的时间从λ1切换到λ2\n\n\n\n包括模型中的观察\n在这一点上，它可能看起来不像，但我们已经完全指定了我们的先验。例如，我们可以提出并回答诸如“我之前分配\\(\\lambda_1\\)的内容是什么样的问题？”之类的问题。\n为此，我们将从分发中进行抽样。方法.sample()有一个非常简单的作用：从给定的分布中获取数据点。然后我们可以评估生成的张量以获得类似NumPy数组的对象。\n# 定义观测变量为指数分布\nrv_lambda_1 = tfd.Exponential(rate=1., name=\"lambda_1\")\nlambda_1 = rv_lambda_1.sample(sample_shape=20000)\n    \n# 执行图\n[ lambda_1_ ] = evaluate([ lambda_1 ])\n\n# 可视化先验分布\nplt.figure(figsize(12.5, 5))\nplt.hist(lambda_1_, bins=70, normed=True, histtype=\"stepfilled\")\nplt.title(r\"Prior distribution for$\\lambda_1$\")\nplt.xlim(0, 8)\n\n# 定义观测变量为正态分布\nrv_lambda_1 = tfd.Normal(loc=0,scale=1,name=\"lambda_1\")\nlambda_1 = rv_lambda_1.sample(sample_shape=20000)\n    \n# 执行图\n[ lambda_1_ ] = evaluate([ lambda_1 ])\n\n# 可视化先验分布\nplt.figure(figsize(12.5, 5))\nplt.hist(lambda_1_, bins=70, normed=True, histtype=\"stepfilled\")\nplt.title(r\"Prior distribution for$\\lambda_1$\")\nplt.xlim(-8, 8)\n\n为了在第一章的符号中描述这一点，虽然这是对符号的轻微滥用，但我们已经指定了\\(P(A)\\)。我们的下一个目标是将数据/证据/观察结果\\(X\\)包含在我们的模型中。\n有时我们可能希望将分布的属性与观察数据的属性相匹配。为此，我们从数据本身获取分布参数。在此示例中，泊松率（平均事件数）在数据平均值上显式设置为1：\n# 构建图\ndata = tf.constant([10., 5.], dtype=tf.float32)\nrv_poisson = tfd.Poisson(rate=1./tf.reduce_mean(data))\npoisson = rv_poisson.sample()\n\n# Execute graph\n[ data_, poisson_, ] = evaluate([ data, poisson ])\n\n# Show results\nprint(\"two predetermined data points: \", data_)\nprint(\"\\n mean of our data: \", np.mean(data_))\nprint(\"\\n random sample from poisson distribution \\n with the mean as the poisson's rate: \\n\", poisson_)\ntwo predetermined data points:  [10.  5.]\n\n mean of our data:  7.5\n\n random sample from poisson distribution \n with the mean as the poisson's rate: \n 1.0"
  },
  {
    "objectID": "posts/tfp-ch2.html#建模方法",
    "href": "posts/tfp-ch2.html#建模方法",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "建模方法",
    "text": "建模方法\n对贝叶斯建模的良好开端思考是考虑如何生成数据。将自己置于无所不知的位置，并尝试想象如何重新创建数据集。\n在上一章中，我们研究了文本消息数据。我们首先询问我们的观察结果如何产生：\n\n我们首先想到“描述这个计数数据的最佳随机变量是什么？”泊松随机变量是一个很好的候选变量，因为它可以表示计数数据。因此，我们模拟从泊松分布中采样的短信的数量。\n接下来，我们认为，“好吧，假设短信是泊松分布的，那么泊松分布需要什么？”那么，泊松分布有一个参数\\(\\lambda\\)。\n我们知道\\(\\lambda\\)吗？不。实际上，我们怀疑有两个\\(\\lambda\\)值，一个用于早期行为，一个用于后面的行为。我们不知道行为何时切换，但称切换点为\\(\\tau\\)。\n这两个\\(\\lambda\\)的好分布是什么？指数是好的，因为它将概率分配给正实数。那么指数分布也有一个参数，称之为\\(\\alpha\\)。\n我们知道参数\\(\\alpha\\)可能是什么吗？没有。此时，我们可以继续并将分配分配给\\(\\alpha\\)，但是一旦达到设定的无知水平，最好停止：而我们先前有关于\\(\\lambda\\)的信念，（“它可能会改变随着时间的推移，“它可能在10到30”之间，等等，我们对\\(\\alpha\\)没有任何强烈的信念。所以最好停在这里。\n那么\\(\\alpha\\)有什么好处呢？我们认为\\(\\lambda\\)s在10-30之间，所以如果我们将\\(\\alpha\\)设置得非常低（相当于较高值的较大概率），我们就不会反映我们之前的好。类似的，太高的阿尔法也错过了我们先前的信念。反映我们信念的\\(\\alpha\\)的一个好主意是设置值，以便\\(\\alpha\\)的\\(\\lambda\\)的平均值等于我们观察到的平均值。这在最后一章中有所体现。\n\n我们对\\(\\tau\\)可能发生的时间没有专家意见。所以我们假设\\(\\tau\\)来自整个时间跨度的离散均匀分布。\n下面我们给出了这个的图形可视化，其中箭头表示父子关系。 （由Daft Python库提供）\n\nTFP和其他概率编程语言旨在告诉这些数据生成故事。更一般地说，B。Cronin写道[2]：\n\n概率编程将解读数据的叙述性解释，这是商业分析的圣杯之一，也是科学说服的无名英雄。人们从故事的角度思考 - 因此轶事的不合理的力量推动决策，有充分根据或没有。但现有的分析很大程度上无法提供这种故事;相反，数字似乎凭空出现，人们在权衡他们的选择时几乎没有因果关系。\n\n\n相同的故事;不同的结局。\n有趣的是，我们可以通过重述故事来创建新数据集。\n例如，如果我们颠倒上述步骤，我们可以模拟数据集的可能实现。\n1 通过从\\(\\text {DiscreteUniform}（0,80）\\)中抽样来指定用户行为的切换时间：\ntau = tf.random_uniform(shape=[1], minval=0, maxval=80, dtype=tf.int32)\n\n[ tau_ ] = evaluate([ tau ])\n\nprint(\"Value of Tau (randomly taken from DiscreteUniform(0, 80)):\", tau_)\nValue of Tau (randomly taken from DiscreteUniform(0, 80)): [58]\n2 绘制出\\(\\lambda_1\\)和\\(\\lambda_2\\)从\\(\\text{Gamma}(\\alpha)\\)分布:\n注意：伽玛分布是指数分布的推广。形状参数\\(α= 1\\)和尺度参数\\(β\\)的伽玛分布是指数（\\(β\\)）分布。在这里，我们使用伽玛分布比我们用指数建模时具有更大的灵活性。我们可以返回远大于\\(1\\)的值（即，在每日短信计数中会出现的数字种类），而不是返回\\(0\\)和\\(1\\)之间的值。\nalpha = 1./8.\n\nlambdas  = tfd.Gamma(concentration=1/alpha, rate=0.3).sample(sample_shape=[2])  \n[ lambda_1_, lambda_2_ ] = evaluate( lambdas )\nprint(\"Lambda 1 (randomly taken from Gamma(α) distribution): \", lambda_1_)\nprint(\"Lambda 2 (randomly taken from Gamma(α) distribution): \", lambda_2_)\nLambda 1 (randomly taken from Gamma(α) distribution):  57.477856\nLambda 2 (randomly taken from Gamma(α) distribution):  23.423761\n3 在τ之前的几天，通过从Poi（λ1）采样来表示用户接收的短信计数，并且在τ之后的几天表示来自Poi（λ2）的样本。例如：\ndata = tf.concat([tfd.Poisson(rate=lambda_1_).sample(sample_shape=tau_),\n                      tfd.Poisson(rate=lambda_2_).sample(sample_shape= (80 - tau_))], axis=0)\ndays_range = tf.range(80)\n[ data_, days_range_ ] = evaluate([ data, days_range ])\nprint(\"Artificial day-by-day user SMS count created by sampling: \\n\", data_)\nArtificial day-by-day user SMS count created by sampling: \n [62. 65. 63. 63. 61. 70. 61. 49. 58. 78. 60. 51. 54. 61. 54. 59. 64. 54.\n 68. 54. 45. 59. 71. 55. 51. 37. 52. 51. 55. 71. 60. 47. 59. 58. 61. 53.\n 49. 44. 51. 56. 67. 64. 71. 52. 72. 48. 55. 62. 58. 49. 52. 62. 57. 56.\n 52. 61. 69. 46. 29. 23. 20. 27. 26. 25. 20. 27. 23. 27. 30. 25. 21. 29.\n 24. 17. 39. 25. 30. 22. 22. 22.]\n4 Plot the artificial dataset:\n4 画出人造的数据\nplt.bar(days_range_, data_, color=TFColor[3])\nplt.bar(tau_ - 1, data_[tau_ - 1], color=\"r\", label=\"user behaviour changed\")\nplt.xlabel(\"Time (days)\")\nplt.ylabel(\"count of text-msgs received\")\nplt.title(\"Artificial dataset\")\nplt.xlim(0, 80)\nplt.legend()\n\n我们的虚构数据集看起来不像我们观察到的数据集是正常的：它的概率确实很小。 TFP的引擎旨在找到最大化此概率的良好参数\\(\\lambda_i, \\tau\\)\n生成人工数据集的能力是我们建模的一个有趣的副作用，我们将看到这种能力是贝叶斯推理的一个非常重要的方法。我们在下面生成一些数据集：\ndef plot_artificial_sms_dataset():   \n    tau = tf.random_uniform(shape=[1], \n                            minval=0, \n                            maxval=80,\n                            dtype=tf.int32)\n    alpha = 1./8.\n    lambdas  = tfd.Gamma(concentration=1/alpha, rate=0.3).sample(sample_shape=[2]) \n    [ lambda_1_, lambda_2_ ] = evaluate( lambdas )\n    data = tf.concat([tfd.Poisson(rate=lambda_1_).sample(sample_shape=tau),\n                      tfd.Poisson(rate=lambda_2_).sample(sample_shape= (80 - tau))], axis=0)\n    days_range = tf.range(80)\n    \n    [ \n        tau_,\n        data_,\n        days_range_,\n    ] = evaluate([ \n        tau,\n        data,\n        days_range,\n    ])\n    \n    plt.bar(days_range_, data_, color=TFColor[3])\n    plt.bar(tau_ - 1, data_[tau_ - 1], color=\"r\", label=\"user behaviour changed\")\n    plt.xlim(0, 80)\n\nplt.figure(figsize(12.5, 8))\nfor i in range(4):\n    plt.subplot(4, 1, i+1)\n    plot_artificial_sms_dataset()\n\n稍后我们将看到我们如何使用它来进行预测并测试模型的适当性。\n\n\n示例：贝叶斯A / B测试\nA / B测试是用于确定两种不同处理之间的有效性差异的统计设计模式。例如，一家制药公司对药物A与药物B的有效性感兴趣。该公司将在其试验的某些部分测试药物A，在另一部分测试药物B（该部分通常是1/2，但我们将放松这个假设）。在进行了足够的试验后，内部统计人员筛选数据以确定哪种药物产生了更好的结果。\n同样，前端Web开发人员对他们的网站设计产生更多销售额或其他一些感兴趣的指标感兴趣。他们将一部分访问者路由到站点A，将另一部分路由到站点B，并记录访问是否产生了销售。记录数据（实时），然后进行分析。\n通常，实验后分析使用称为假设检验的方法进行，例如平均值测试或比例差异测试。这通常会误解为“Z分数”，甚至更令人困惑的“p值”（请不要问）。如果您已经学过统计学课程，那么您可能已经学过这种技术（尽管不一定学习这种技术）。如果你像我一样，你可能会对他们的推导感到不舒服 - 好的：贝叶斯方法解决这个问题要自然得多。\n\n\n一个简单的例子\n由于这是一本黑客书，我们将继续使用web-dev示例。目前，我们只关注网站A的分析。假设在显示站点A时最终从站点购买的用户有一些真正的\\(0 \\lt p_A \\lt 1\\)概率。这是网站A的真正有效性。目前，我们不知道这个数量\n假设站点A显示为\\(N\\)人，并且从站点购买\\(n\\)人。有人可能会急忙得出结论：\\(p_A = \\frac{n}{N}\\)。不幸的是，观察频率\\(\\frac{n}{N}\\)不一定等于\\(p_A\\)- 观察频率与事件的真实频率之间存在差异。真实频率可以解释为事件发生的概率。例如，在6面骰子上滚动1的真实频率是\\(\\frac{1}{6}\\)。了解事件的真实频率，例如：\n\n购买用户的比例\n社会属性的频率\n有猫等互联网用户的百分比\n\n是我们对大自然提出的常见要求。不幸的是，通常现实中充满了噪音和干扰隐藏了真实频率，我们必须从观察到的数据中推断它。\n然后观察到的频率是我们观察到的频率：比如摇动色子100次，你可以观察到20个1.观察到的频率0.2，与真实频率不同，\\(\\frac{1}{6}\\)。我们可以使用贝叶斯统计来使用适当的先验和观测数据来推断真实频率的可能值。\n关于我们的A / B示例，我们有兴趣使用我们所知的\\(N\\)（管理的总试验次数）和\\(n\\)（转换次数）来估算\\(p_A\\)，买家的真实频率， 可能。\n要设置贝叶斯模型，我们需要为我们的未知量分配先验分布先验，我们认为\\(p_A\\)可能是什么？对于这个例子，我们对\\(p_A\\)没有强烈的信念，所以现在，让我们假设\\(p_A\\)统一超过\\([0,1]\\)：\nreset_sess()\n\n# 设定一个平均分布\nrv_p = tfd.Uniform(low=0., high=1., name='p')\n如果我们有更强烈的置信度，我们可以在上面的内容中表达它们。\n对于此示例，请考虑\\(p_A = 0.05\\)，\\(N = 1500\\)用户访问了站点A，我们将模拟用户是否进行了购买。为了从\\(N\\)试验中模拟这个，我们将使用伯努利分布：如果\\(X \\ \\sim \\text{Ber}（p）\\)，则\\(X\\)为1，概率为\\(p\\)，0为概率\\(1 - p\\)。当然，在实践中我们不知道\\(p_A\\)，但我们将在此处使用它来模拟数据。我们可以假设我们可以使用以下生成模型：\n\\[\n\\begin{aligned}\np &\\sim \\text{Uniform}[\\text{low}=0,\\text{high}=1) \\\\\nX\\ &\\sim \\text{Bernoulli}(\\text{prob}=p) \\\\\n\\text{for }  i &= 1\\ldots N:\\text{ Users} \\\\\nX_i\\ &\\sim \\text{Bernoulli}(p_i)\n\\end{aligned}\n\\]\nreset_sess()\n\n#set constants\nprob_true = 0.05  # 假设P_a是0.05\nN = 1500\n\n# 样本N来自 Ber(0.05).\n# 每个变量有0.05的概率为1\n# 这是数据生成的步骤\n\noccurrences = tfd.Bernoulli(probs=prob_true).sample(sample_shape=N, seed=10) # 生成1500的样本\noccurrences_sum = tf.reduce_sum(occurrences) # 求和\noccurrences_mean = tf.reduce_mean(tf.cast(occurrences,tf.float32)) # 求平均\n\n[ \n    occurrences_,\n    occurrences_sum_,\n    occurrences_mean_,\n] = evaluate([ \n    occurrences, \n    occurrences_sum,\n    occurrences_mean,\n])\n\nprint(\"Array of {} Occurences:\".format(N), occurrences_) \nprint(\"(Remember: Python treats True == 1, and False == 0)\")\nprint(\"Sum of (True == 1) Occurences:\", occurrences_sum_)\nArray of 1500 Occurences: [0 0 0 ... 0 1 0]\n(Remember: Python treats True == 1, and False == 0)\nSum of (True == 1) Occurences: 76\n观测频率如下:\n# Occurrences.mean is equal to n/N.\nprint(\"A组中观察到的频率是多少? %.4f\" % occurrences_mean_)\nprint(\"他是否等于正式的频率? %s\" % (occurrences_mean_ == prob_true))\nA组中观察到的频率是多少? 0.0507\n他是否等于正式的频率? False\n我们可以将我们的伯努利分布和我们观察到的事件组合成基于二者的对数概率函数。\ndef joint_log_prob(occurrences, prob_A):\n    \"\"\"\n    联合对数概率优化函数\n        \n    Args:\n      事件: 一个二进制数组 (0 & 1), 表现观测频率\n      prob_A: 标量估计出现1的概率\n    Returns: \n      来自所有先验和条件分布的联合对数概率之和\n    \"\"\"  \n    rv_prob_A = tfd.Uniform(low=0., high=1.) # 这里直接把概率P_A的分布设置为均匀分布\n    rv_occurrences = tfd.Bernoulli(probs=prob_A) # 概率为P_A的二项分布\n    return (rv_prob_A.log_prob(prob_A)+ tf.reduce_sum(rv_occurrences.log_prob(occurrences)))\n概率推断的目标是找到可以解释您观察到的数据的模型参数。 TFP通过使用joint_log_prob函数评估模型参数来执行概率推断。 joint_log_prob的参数是数据和模型参数 - 用于在joint_log_prob函数本身中定义的模型。该函数返回参数化模型的联合概率的对数，该模型按照输入参数生成观察数据。\n所有的 joint_log_prob 函数都有共同的结构:\n\n该函数需要一组输入来评估。每个输入都是观察值或模型参数。\njoint_log_prob函数使用概率分布来定义用于评估输入的模型。这些分布测量输入值的可能性。 （按照惯例，测量变量foo的可能性的分布将被命名为rv_foo以注意它是一个随机变量。）我们在joint_log_prob函数中使用两种类型的分布：\n\n先前的分布测量输入值的可能性。先前的分配决不依赖于输入值。每个先前分布都测量单个输入值的可能性。每个未知变量 - 一个未被直接观察到的变量 - 需要相应的先验变量。关于哪些值可能合理的信念决定了先前的分布。选择先验可能很棘手，因此我们将在第6章深入介绍。\n条件分布测量给定其他输入值的输入值的可能性。通常，条件分布返回给定模型中参数的当前猜测的观察数据的可能性，p（observed_data | model_parameters）。\n\n最后，我们计算并返回输入的联合对数概率。联合对数概率是来自所有先验分布和条件分布的对数概率的总和。 （由于数值稳定性的原因，我们采用对数概率之和而不是直接乘以概率：计算机中的浮点数不能表示计算联合对数概率所需的非常小的值，除非它们在对数空间中。）概率之和实际上是一个非标准化的密度;虽然所有可能输入的概率总和可能不等于1，但概率之和与真实概率密度成正比。这种比例分布足以估计可能输入的分布。\n\n让我们将这些术语映射到上面的代码中。在这个例子中，输入值是occurrence的观察值和prob_A的未知值。 joint_log_prob获取prob_A的当前猜测并回答，如果prob_A是occurrence的概率，数据的可能性有多大。答案取决于两个分布：\n\n先前的分布rv_prob_A表示prob_A的当前值本身的可能性。\n如果prob_A是伯努利分布的概率，则条件分布rv_occurrences表示“发生”的可能性。\n\n这些概率的对数之和是 联合对数概率。\njoint_log_prob与tfp.mcmc模块一起使用时特别有用。马尔可夫链蒙特卡罗（MCMC）算法通过对未知输入值进行有根据的猜测并计算这组参数的可能性来进行。 （我们将在第3章中讨论它是如何进行这些猜测的。）通过多次重复此过程，MCMC构建了可能参数的分布。构建此分布是概率推理的目标。\nThen we run our inference algorithm:\n让我运行推理算法\nnumber_of_steps = 48000 #@param {type:\"slider\", min:2000, max:50000, step:100} #@markdown (Default is 18000).\nburnin = 25000 #@param {type:\"slider\", min:0, max:30000, step:100} #@markdown (Default is 1000).\nleapfrog_steps=2 #@param {type:\"slider\", min:1, max:9, step:1} #@markdown (Default is 6).\n\n# 设置链的开始状态\ninitial_chain_state = [tf.reduce_mean(tf.to_float(occurrences)) * tf.ones([], dtype=tf.float32, name=\"init_prob_A\")]\n\n# 由于HMC在无约束空间上运行，我们需要对样本进行变换，使它们存在于真实空间中。\nunconstraining_bijectors = [\n    tfp.bijectors.Identity()   # Maps R to R.  \n]\n\n# 在我们的joint_log_prob上定义一个闭包\n# 闭包使得HMC不会尝试改变“出现次数”，而是确定可能产生我们观察到的“出现次数”的其他参数的分布。\nunnormalized_posterior_log_prob = lambda *args: joint_log_prob(occurrences, *args)\n\n# 初始化step_size。 （它将自动调整。）\nwith tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n    step_size = tf.get_variable(name='step_size',initializer=tf.constant(0.5, dtype=tf.float32),trainable=False)\n    # 定义 HMC\n    hmc = tfp.mcmc.TransformedTransitionKernel(\n    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_posterior_log_prob,\n        num_leapfrog_steps=leapfrog_steps,\n        step_size=step_size,\n        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n        state_gradients_are_stopped=True),\n        bijector=unconstraining_bijectors)\nW0726 19:11:30.352059 140237485999936 deprecation.py:323] From &lt;ipython-input-19-e4e347c50353&gt;:6: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nW0726 19:11:30.359435 140237485999936 deprecation.py:323] From &lt;ipython-input-19-e4e347c50353&gt;:26: make_simple_step_size_update_policy (from tensorflow_probability.python.mcmc.hmc) is deprecated and will be removed after 2019-05-22.\nInstructions for updating:\nUse tfp.mcmc.SimpleStepSizeAdaptation instead.\nW0726 19:11:30.363615 140237485999936 deprecation.py:506] From &lt;ipython-input-19-e4e347c50353&gt;:27: calling HamiltonianMonteCarlo.__init__ (from tensorflow_probability.python.mcmc.hmc) with step_size_update_fn is deprecated and will be removed after 2019-05-22.\nInstructions for updating:\nThe `step_size_update_fn` argument is deprecated. Use `tfp.mcmc.SimpleStepSizeAdaptation` instead.\n# 从链里面采样\n[posterior_prob_A], kernel_results = tfp.mcmc.sample_chain(\n    num_results=number_of_steps,\n    num_burnin_steps=burnin,\n    current_state=initial_chain_state,\n    kernel=hmc)\n\n# 初始化变量\ninit_g = tf.global_variables_initializer()\ninit_l = tf.local_variables_initializer()\nW0726 19:11:30.403772 140237485999936 deprecation.py:323] From /home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/uniform.py:182: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n\n执行TF图以从后验采样\nevaluate(init_g)\nevaluate(init_l)\n[posterior_prob_A_,kernel_results_,] = evaluate([posterior_prob_A,kernel_results,])\n\n    \nprint(f'acceptance rate: {kernel_results_.inner_results.is_accepted.mean()}')\n\nburned_prob_A_trace_ = posterior_prob_A_[burnin:]\nacceptance rate: 0.7248958333333333\n我们绘制下面未知\\(p_A\\)的后验分布：\nplt.figure(figsize(12.5, 4))\nplt.title(\"Posterior distribution of$p_A$, the true effectiveness of site A\")\nplt.vlines(prob_true, 0, 90, linestyle=\"--\", label=\"true$p_A$(unknown)\")\nplt.hist(burned_prob_A_trace_, bins=25, histtype=\"stepfilled\", normed=True)\nplt.legend()\n\n我们的后验分布使得大部分权重接近\\(p_A\\)的真实值，但尾部也有一些权重。根据我们的观察，这可以衡量我们应该多么不确定。尝试改变观察数N，并观察后验分布如何变化\n\n\n\nA 和 B 一起\n可以对站点B的响应数据进行类似的分析，以确定类似的\\(p_B\\)。但我们真正感兴趣的是\\(p_A\\)和\\(p_B\\)之间的差异。我们一下子推断\\(p_A\\)，\\(p_B\\)，和\\(\\text{delta} = p_A - p_B\\)。我们可以使用TFP的确定性变量来做到这一点。 （我们假设这个练习\\(p_B = 0.04\\)，所以\\(\\text{delta} = 0.01\\)，\\(N_B = 750\\)（显著低于\\(N_A\\)）我们将像我们一样模拟站点B的数据网站A的数据）。我们的模型现在如下所示：\n\\[\\begin{align*}\np_A &\\sim \\text{Uniform}[\\text{low}=0,\\text{high}=1) \\\\\np_B &\\sim \\text{Uniform}[\\text{low}=0,\\text{high}=1) \\\\\nX\\ &\\sim \\text{Bernoulli}(\\text{prob}=p) \\\\\n\\text{for }  i &= 1\\ldots N: \\\\\nX_i\\ &\\sim \\text{Bernoulli}(p_i)\n\\end{align*}\\]\nreset_sess()\n\n# 假设两个概率\ntrue_prob_A_ = 0.05\ntrue_prob_B_ = 0.04\n\n# 注意不相等的样本大小 - 贝叶斯分析没有问题。\nN_A_ = 1500\nN_B_ = 750\n\n# 生成观测值\nobservations_A = tfd.Bernoulli(name=\"obs_A\", \n                          probs=true_prob_A_).sample(sample_shape=N_A_, seed=6.45)\nobservations_B = tfd.Bernoulli(name=\"obs_B\", \n                          probs=true_prob_B_).sample(sample_shape=N_B_, seed=6.45)\n[   observations_A_,\n    observations_B_,\n] = evaluate([ \n    observations_A, \n    observations_B, \n])\n\nprint(\"站点A观测值: \", observations_A_[:30], \"...\")\nprint(\"Prob_A观测值: \", np.mean(observations_A_), \"...\")\nprint(\"站点B观测值: \", observations_B_[:30], \"...\")\nprint(\"Prob_B观测值: \", np.mean(observations_B_))\nprint(\"发现观测值的均值收敛于概率\")\n站点A观测值:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ...\nProb_A观测值:  0.050666666666666665 ...\n站点B观测值:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ...\nProb_B观测值:  0.04\n发现观测值的均值收敛于概率\n下面我们推理新的模型\ndef delta(prob_A, prob_B):\n    \"\"\"\n    定义确定性delta函数。这是我们未知的兴趣。\n        \n    Args:\n      prob_A: 标量1出现在观测集A中的估计概率\n      prob_B: 标量1出现在观测集B中的估计概率\n    Returns: \n      prob_A 和 prob_B 之间的差值\n    \"\"\"\n    return prob_A - prob_B\n\n  \ndef double_joint_log_prob(observations_A, observations_B, \n                   prob_A, prob_B):\n    \"\"\"\n    定义新的联合对数概率优化函数,我个人感觉这里有点像最大似然\n        \n    Args:\n      observations_A: 表示站点A的观察集的二进制值数组\n      observations_B: 表示站点B的观察集的二进制值数组\n      prob_A: 标量1出现在观测集A中的估计概率\n      prob_B: 标量1出现在观测集B中的估计概率\n    Returns: \n      联合概率优化函数\n    \"\"\"\n    tfd = tfp.distributions\n  \n    rv_prob_A = tfd.Uniform(low=0., high=1.) # 假设P_A的分布\n    rv_prob_B = tfd.Uniform(low=0., high=1.) # 假设P_B的分布\n  \n    rv_obs_A = tfd.Bernoulli(probs=prob_A) # 生成数据A\n    rv_obs_B = tfd.Bernoulli(probs=prob_B) # 生成数据B\n  \n    return (rv_prob_A.log_prob(prob_A)+ # P_A 分布的对数概率\n            rv_prob_B.log_prob(prob_B)+ # P_B 分布的对数概率\n            tf.reduce_sum(rv_obs_A.log_prob(observations_A))+  # 所有观测值的对数概率\n            tf.reduce_sum(rv_obs_B.log_prob(observations_B)))\n    \nnumber_of_steps = 37200 #@param {type:\"slider\", min:2000, max:50000, step:100}\n#@markdown (Default is 18000).\nburnin = 1000 #@param {type:\"slider\", min:0, max:30000, step:100}\n#@markdown (Default is 1000).\nleapfrog_steps=3 #@param {type:\"slider\", min:1, max:9, step:1}\n#@markdown (Default is 6).\n\n\n# 设置初始状态\ninitial_chain_state = [    \n    tf.reduce_mean(tf.cast(observations_A,tf.float32)) * tf.ones([], dtype=tf.float32, name=\"init_prob_A\"),\n    tf.reduce_mean(tf.cast(observations_B,tf.float32)) * tf.ones([], dtype=tf.float32, name=\"init_prob_B\")\n]\n\n# 由于HMC在无约束空间上运行，我们需要对样本进行变换，使它们存在于真实空间中。\nunconstraining_bijectors = [\n    tfp.bijectors.Identity(),   # Maps R to R.\n    tfp.bijectors.Identity()    # Maps R to R.\n]\n\n# 将joint_log_prob.闭包\nunnormalized_posterior_log_prob = lambda *args: double_joint_log_prob(observations_A, observations_B, *args)\n\n# 初始化step\nwith tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    step_size = tf.get_variable(\n        name='step_size',\n        initializer=tf.constant(0.5, dtype=tf.float32),\n        trainable=False,\n        use_resource=True)\n\n    # 定义 HMC\n    hmc=tfp.mcmc.TransformedTransitionKernel(\n    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_posterior_log_prob,\n        num_leapfrog_steps=3,\n        step_size=step_size,\n        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n        state_gradients_are_stopped=True),\n    bijector=unconstraining_bijectors)\n\n# Sample from the chain.\n[posterior_prob_A,posterior_prob_B], kernel_results = tfp.mcmc.sample_chain(\n    num_results=number_of_steps,\n    num_burnin_steps=burnin,\n    current_state=initial_chain_state,\n    kernel=hmc)\n\n# Initialize any created variables.\ninit_g = tf.global_variables_initializer()\ninit_l = tf.local_variables_initializer()\n\n执行TF图以从后验采样\nevaluate(init_g)\nevaluate(init_l)\n[\n    posterior_prob_A_,\n    posterior_prob_B_,\n    kernel_results_\n] = evaluate([\n    posterior_prob_A,\n    posterior_prob_B,\n    kernel_results\n])\n    \nprint(\"接受率: {}\".format(\n    kernel_results_.inner_results.is_accepted.mean()))\n\nburned_prob_A_trace_ = posterior_prob_A_[burnin:]\nburned_prob_B_trace_ = posterior_prob_B_[burnin:]\nburned_delta_trace_ = (posterior_prob_A_ - posterior_prob_B_)[burnin:]\n接受率: 0.6146505376344086\n下面我们绘制三个未知数的后验分布：\nplt.figure(figsize(12.5, 12.5))\n\n#histogram of posteriors\n\nax = plt.subplot(311)\n\nplt.xlim(0, .1)\nplt.hist(burned_prob_A_trace_, histtype='stepfilled', bins=25, alpha=0.85,\n         label=\"posterior of$p_A$\", color=TFColor[0], normed=True)\nplt.vlines(true_prob_A_, 0, 80, linestyle=\"--\", label=\"true$p_A$(unknown)\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Posterior distributions of$p_A$,$p_B$, and delta unknowns\")\n\nax = plt.subplot(312)\n\nplt.xlim(0, .1)\nplt.hist(burned_prob_B_trace_, histtype='stepfilled', bins=25, alpha=0.85,\n         label=\"posterior of$p_B$\", color=TFColor[2], normed=True)\nplt.vlines(true_prob_B_, 0, 80, linestyle=\"--\", label=\"true$p_B$(unknown)\")\nplt.legend(loc=\"upper right\")\n\nax = plt.subplot(313)\nplt.hist(burned_delta_trace_, histtype='stepfilled', bins=30, alpha=0.85,\n         label=\"posterior of delta\", color=TFColor[6], normed=True)\nplt.vlines(true_prob_A_ - true_prob_B_, 0, 60, linestyle=\"--\",\n           label=\"true delta (unknown)\")\nplt.vlines(0, 0, 60, color=\"black\", alpha=0.2)\nplt.legend(loc=\"upper right\")\n\n请注意由于N_B &lt; N_A. 我们从网站B获得的数据较少，我们后来的\\(p_B\\)分布比较宽，这意味着我们相比于\\(p_A\\)不太确定\\(p_B\\)的真实值.\n关于\\(\\text{delta}\\)的后验分布，我们可以看到大部分分布都高于\\(\\text{delta} = 0\\)，这意味着网站A的响应可能比网站B的响应更好。这种推断不正确的概率很容易计算：\n# 计算小于0的样本数，即曲线下面积\n# 在0之前，表示站点A比站点B更差的概率。\nprint(\" site A 比 site B 差的概率: %.3f\" % \\\n    np.mean(burned_delta_trace_ &lt; 0))\n\nprint(\" site A 比 site B 好的概率: %.3f\" % \\\n    np.mean(burned_delta_trace_ &gt; 0))\n site A 比 site B 差的概率: 0.298\n site A 比 site B 好的概率: 0.702\n如果这个概率对于舒适的决策来说太高了，我们可以在站点B上进行更多的试验（因为站点B开始时的样本较少，站点B的每个附加数据点比每个附加数据点比网站A提供更多的推理功率）。\n尝试使用参数true_prob_A，true_prob_B，N_A和N_B进行测试，看看\\(\\text{delta}\\)的后验是什么样的。请注意，在所有这些中，从未提及站点A和站点B之间的样本大小差异：它自然适合贝叶斯分析。\n我希望读者觉得这种A / B测试方式比假设测试更自然，假设测试可能比帮助从业者更困惑。在本书的后面，我们将看到这个模型的两个扩展：第一个帮助动态调整不良站点，第二个将通过将分析减少到单个方程来提高计算的速度。"
  },
  {
    "objectID": "posts/tfp-ch2.html#一种人为欺骗的算法",
    "href": "posts/tfp-ch2.html#一种人为欺骗的算法",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "一种人为欺骗的算法",
    "text": "一种人为欺骗的算法\n社交数据还有一层额外的兴趣，因为人们并不总是诚实地回应，这进一步增加了推理的复杂性。例如，简单地询问个人“你有没有在考试中作弊？”肯定会包含一些不诚实的行为。你可以肯定的是，真实的比率低于你观察到的比率（假设个人谎言只是关于不作弊;我无法想象一个人会承认是作弊，而事实上他们没有作弊）。\n为了提出一个优雅的解决方案来规避这个不诚实的问题，并演示贝叶斯模型，我们首先需要介绍二项分布。"
  },
  {
    "objectID": "posts/tfp-ch2.html#二项分布",
    "href": "posts/tfp-ch2.html#二项分布",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "二项分布",
    "text": "二项分布\n二项分布是最受欢迎的分布之一，主要是因为它的简单性和实用性。与本书迄今为止遇到的其他分布不同，二项分布有2个参数：\\(N\\)，表示\\(N\\)试验的正整数或潜在事件的实例数，以及\\(p\\)，事件的概率发生在一次试验中。像泊松分布一样，它是一个离散分布，但与泊松分布不同，它只能权衡从\\(0\\)到\\(N\\)的整数。质量分布如下：\n\\[P( X = k ) =  { {N}\\choose{k} }  p^k(1-p)^{N-k}\\]\n如果\\(X\\)是一个带有参数\\(p\\)和\\(N\\)的二项式随机变量，表示为\\(X \\sim \\text{Bin}（N，p）\\)，则\\(X\\)是\\(中发生的事件数N\\)次试验（显然是\\(0 \\le X \\le N\\)）。较大的\\(p\\)（仍然保持在0和1之间），可能发生的事件越多。二项式的期望值等于\\(Np\\)。下面我们绘制不同参数的质量概率分布。\nN = 10.\nk_values = tf.range(start=0, limit=(N + 1), dtype=tf.float32)\nrv_probs_1 = tfd.Binomial(total_count=N, probs=.4).prob(k_values) # 计算样本对应概率\nrv_probs_2 = tfd.Binomial(total_count=N, probs=.9).prob(k_values) # 计算样本对应概率\n\n# 执行图\n[   k_values_,\n    rv_probs_1_,\n    rv_probs_2_,\n] = evaluate([\n    k_values,\n    rv_probs_1,\n    rv_probs_2,\n])\n\n# 显示结果\nplt.figure(figsize=(12.5, 4))\ncolors = [TFColor[3], TFColor[0]] \n\nplt.bar(k_values_ - 0.5, rv_probs_1_, color=colors[0],\n        edgecolor=colors[0],\n        alpha=0.6,\n        label=\"$N$: %d,$p$: %.1f\" % (10., .4),\n        linewidth=3)\nplt.bar(k_values_ - 0.5, rv_probs_2_, color=colors[1],\n        edgecolor=colors[1],\n        alpha=0.6,\n        label=\"$N$: %d,$p$: %.1f\" % (10., .9),\n        linewidth=3)\n\nplt.legend(loc=\"upper left\")\nplt.xlim(0, 10.5)\nplt.xlabel(\"$k$\")\nplt.ylabel(\"$P(X = k)$\")\nplt.title(\"Probability mass distributions of binomial random variables\")\n\n\\(N = 1\\)的特殊情况对应于伯努利分布。伯努利和二项式随机变量之间存在另一种联系。如果我们有\\(X_1，X_2，...，X_N\\)Bernoulli随机变量具有相同的\\(p\\)，那么\\(Z = X_1 + X_2 + ... + X_N \\sim \\text{Binomial}（N，p）\\)。\n伯努利随机变量的期望值是\\(p\\)。通过注意更一般的二项式随机变量具有预期值\\(Np\\)并设置\\(N = 1\\)可以看出这一点"
  },
  {
    "objectID": "posts/tfp-ch2.html#例子-在学生之间作弊",
    "href": "posts/tfp-ch2.html#例子-在学生之间作弊",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "例子: 在学生之间作弊",
    "text": "例子: 在学生之间作弊\n我们将使用二项分布来确定学生在考试期间作弊的频率。如果我们让\\(N\\)成为参加考试的学生总数，并假设每个学生在考试后接受面试（回答无后果），我们将收到整数\\(X\\)“是的，我做了作弊”的答案。然后我们找到\\(p\\)的后验分布，给定\\(N\\)，一些在\\(p\\)之前指定，观察数据\\(X\\)。\n这是一个完全荒谬的模型。没有学生，即使有免费通过惩罚，也会承认作弊。我们需要的是一个更好的算法来询问学生是否有欺骗行为。理想情况下，该算法应鼓励个人在保护隐私的同时保持诚实。以下提出的算法是我非常钦佩的解决方案，因为它的独创性和有效性：\n\n在每个学生的面试过程中，学生翻转一个隐藏在面试官面前的硬币。如果硬币正面，学生同意诚实地回答。否则，如果硬币反面，学生（秘密地）再次翻转硬币，如果硬币翻转落地为正面，则回答“是的，我做了作弊”，如果硬币翻转落地为反面，则回答“不，我没有作弊”。这样，面试官不知道“是”是认罪的结果，还是第二次掷硬币的正面。因此保护了隐私，研究人员得到了诚实的答案。\n\n我称之为隐私算法。人们当然可以争辩说，采访者仍在接收错误的数据，因为有些Yes不是供词而是随机性，但另一种观点是研究人员丢弃其原始数据集的大约一半，因为一半的回复将是噪声。但他们已经获得了可以建模的系统数据生成过程。此外，他们没有必要（或许有点天真）加入欺骗性答案的可能性。我们可以使用TFP来挖掘这个嘈杂的模型，并找到一个关于作弊真实频率的后验分布。\n假设有100名学生正在接受作弊调查，我们希望找到\\(p\\)，作弊者的比例。我们可以通过几种方式在TFP中对此进行建模。我将演示最明确的方式，稍后会显示简化版本。两个版本都得出相同的推论。在我们的数据生成模型中，我们从之前的样本中抽取了\\(p\\)，这是作弊者的真实比例。由于我们对\\(p\\)一无所知，我们将先分配一个\\(\\text{Uniform}（0,1）\\)。\nreset_sess()\n\nN = 100\nrv_p = tfd.Uniform(name=\"freq_cheating\", low=0., high=1.)\n再次，考虑到我们的数据生成模型，我们将伯努利随机变量分配给100名学生：1表示他们作弊，0表示他们没有。\nN = 100\nreset_sess()\nrv_p = tfd.Uniform(name=\"freq_cheating\", low=0., high=1.)\ntrue_answers = tfd.Bernoulli(name=\"truths\", probs=rv_p.sample()).sample(sample_shape=N, seed=5)\n# 执行图\n[true_answers_,] = evaluate([true_answers,])\n\nprint(true_answers_)\nprint(true_answers_.sum())\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n98\n如果我们执行算法，下一步发生的是每个学生做的第一次硬币翻转。这可以通过采样100个伯努利随机变量再次建模，其中𝑝= 1/2 表示1为头部，0表示尾部。\nN = 100\nfirst_coin_flips = tfd.Bernoulli(name=\"first_flips\", probs=0.5).sample(sample_shape=N, seed=5)\n# Execute graph\n[first_coin_flips_,] = evaluate([first_coin_flips,])\n\nprint(first_coin_flips_)\n[1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1\n 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0\n 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1]\n虽然不是每个人第二次翻转，但我们仍然可以模拟第二次翻转的可能实现：\nN = 100\nsecond_coin_flips = tfd.Bernoulli(name=\"second_flips\", probs=0.5).sample(sample_shape=N, seed=5)\n# 执行\n[second_coin_flips_,] = evaluate([second_coin_flips,])\n\nprint(second_coin_flips_)\n[1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1\n 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0\n 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1]\n使用这些变量，我们可以返回观察到的“是”响应比例的概率。\ndef observed_proportion_calc(t_a = true_answers, \n                             fc = first_coin_flips,\n                             sc = second_coin_flips):\n    \"\"\"\n    非标准化的log后验分布函数\n        \n    Args:\n      t_a: 表示真实答案的二进制变量数组\n      fc:  表示模拟的第一次翻转的二进制变量数组\n      sc:  表示模拟的第二次翻转的二进制变量数组\n    Returns: \n      观察到硬币翻转的比例\n    Closure over: N\n    \"\"\"\n    observed = fc * t_a + (1 - fc) * sc\n    observed_proportion = tf.cast(tf.reduce_sum(observed),tf.float32) / tf.cast(N,tf.float32)\n    \n    return tf.cast(observed_proportion,tf.float32)\n线fc * t_a +（1-fc）* sc包含隐私算法的核心。当且仅当 1. 第一次投掷是头并且学生被欺骗或 2. 第一次投掷是尾巴，第二次投掷是正面.并且是0否则时，该数组中的元素是1。\n最后，最后一行将此向量相加并除以浮点数（N），产生一个比例。\nobserved_proportion_val = observed_proportion_calc(t_a=true_answers_,fc=first_coin_flips_,sc=second_coin_flips_)\n\n# Execute graph\n[observed_proportion_val_,] = evaluate([observed_proportion_val,])\n\nprint(observed_proportion_val_)\n0.48\n接下来我们需要一个数据集。在进行了硬币翻转访谈后，研究人员收到了35条“是”回复。从相对的角度来看，如果确实没有作弊者，我们应该期望平均看到所有回答中的1/4是“是”（第一次投入硬币土地的一半机会，以及另外一次获得第二次硬币的机会）硬币土地负责人），在一个无欺诈的世界中大约有25个回应。另一方面，如果所有学生都作弊了，我们应该会看到大约3/4的答案都是“是”。\n研究人员观察到二项式随机变量，其中“N = 100”和“total_yes = 35”：\ntotal_count = 100\ntotal_yes = 35\ndef coin_joint_log_prob(total_yes, total_count, lies_prob):\n    \"\"\"\n    联合对数概率优化函数。\n\n    Args:\n      headsflips：观察到的头部翻转总数的整数\n      N: 观察的整数\n      lies_prob: 测试二项分布的头翻转（1）的概率\n    Returns: \n      联合对数概率优化函数。\n    \"\"\"\n  \n    rv_lies_prob = tfd.Uniform(name=\"rv_lies_prob\",low=0., high=1.)\n\n    cheated = tfd.Bernoulli(probs=tf.to_float(lies_prob)).sample(total_count)\n    first_flips = tfd.Bernoulli(probs=0.5).sample(total_count)\n    second_flips = tfd.Bernoulli(probs=0.5).sample(total_count)\n    observed_probability = tf.reduce_sum(tf.to_float(\n        cheated * first_flips + (1 - first_flips) * second_flips)) / total_count\n\n    rv_yeses = tfd.Binomial(name=\"rv_yeses\",\n                total_count=float(total_count),\n                probs=observed_probability)\n    \n    return (\n        rv_lies_prob.log_prob(lies_prob)\n        + tf.reduce_sum(rv_yeses.log_prob(tf.to_float(total_yes)))\n        )\n下面我们将所有感兴趣的变量添加到我们的Metropolis-Hastings采样器中，并在模型上运行我们的黑盒算法。值得注意的是，我们正在使用Metropolis-Hastings MCMC而不是汉密尔顿主义者，因为我们正在内部采样。\nburnin = 15000\nnum_of_steps = 40000\ntotal_count=100\n\n# 设置链的开始状态。\ninitial_chain_state = [0.4 * tf.ones([], dtype=tf.float32, name=\"init_prob\")]\n\n# 闭包\nunnormalized_posterior_log_prob = lambda *args: coin_joint_log_prob(total_yes, total_count,  *args)\n\n# Defining the Metropolis-Hastings\n# 我们在这里使用Metropolis-Hastings方法而不是哈密顿方法，因为上面例子中的硬币翻转是不可微分的，不能与HMC一起使用。\nmetropolis=tfp.mcmc.RandomWalkMetropolis(target_log_prob_fn=unnormalized_posterior_log_prob,seed=54)\n\n# Sample from the chain.\n[posterior_p], kernel_results = tfp.mcmc.sample_chain(\n    num_results=num_of_steps,\n    num_burnin_steps=burnin,\n    current_state=initial_chain_state,\n    kernel=metropolis,\n    parallel_iterations=1,\n    name='Metropolis-Hastings_coin-flips')\n\n执行TF图以从后验采样\n# Content Warning: This cell can take up to 5 minutes in Graph Mode\n[posterior_p_,kernel_results_] = evaluate([posterior_p,kernel_results,])\n \nprint(\"接受率: {}\".format(\n    kernel_results_.is_accepted.mean()))\n# print(\"prob_p trace: \", posterior_p_)\n# print(\"prob_p burned trace: \", posterior_p_[burnin:])\nburned_cheating_freq_samples_ = posterior_p_[burnin:]\n接受率: 0.1058\n最后我们可以绘制结果。\nplt.figure(figsize(12.5, 6))\np_trace_ = burned_cheating_freq_samples_\nplt.hist(p_trace_, histtype=\"stepfilled\", density=True, alpha=0.85, bins=30, \n         label=\"posterior distribution\", color=TFColor[3])\nplt.vlines([.1, .40], [0, 0], [5, 5], alpha=0.3)\nplt.xlim(0, 1)\nplt.legend()\n\n关于上面的情节，我们仍然非常不确定作弊者的真实频率，但我们已将其缩小到0.1到0.4之间的范围（用实线标出）。这是非常好的，因为先验我们不知道有多少学生可能被欺骗（因此我们之前的统一分布）。另一方面，这也是非常糟糕的，因为有一个.3长度窗口可能存在的真实价值。我们甚至获得了什么，或者我们是否仍然对真实频率不确定？\n我会说，是的，我们发现了一些东西。根据我们的后验，它是不可信的，即没有欺骗者，即后验分配给\\(p=0\\)的概率很低。由于我们从一个统一的先验开始，将\\(p\\)的所有值视为同样合理，但数据排除了\\(p = 0\\)作为一种可能性，我们可以确信有欺骗者。\n这种算法可用于从用户收集私人信息，并且合理地确信数据虽然有噪声但是是真实的。\n\n\n替代TFP模型\n给定\\(p\\)的值（我们知道我们的置信度），我们可以找到学生回答的概率是： \\[\n\\begin{align}\nP(\\text{\"Yes\"}) &= P( \\text{Heads on first coin} )P( \\text{cheater} ) + P( \\text{Tails on first coin} )P( \\text{Heads on second coin} ) \\\\\n&= \\frac{1}{2}p + \\frac{1}{2}\\frac{1}{2}\\\\\n&= \\frac{p}{2} + \\frac{1}{4}\n\\end{align}\n\\] 因此，知道\\(p\\)我们知道学生回答“是”的概率。\n如果我们知道受访者说“是”的概率，即p_skewed，并且我们有𝑁= 100名学生，则“是”回答的数量是具有参数N和p_skewed的二项式随机变量。 这是我们在总共100个中包含我们观察到的35个“是”响应的地方，然后将其传递给下面进一步的代码部分中的joint_log_prob，在此我们通过thejoint_log_prob定义我们的闭包。\nN = 100.\ntotal_yes = 35.\n\ndef alt_joint_log_prob(yes_responses, N, prob_cheating):\n    \"\"\"\n    Alternative joint log probability optimization function.\n        \n    Args:\n      yes_responses: Integer for total number of affirmative responses\n      N: Integer for number of total observation\n      prob_cheating: Test probability of a student actually cheating\n    Returns: \n      Joint log probability optimization function.\n    \"\"\"\n    tfd = tfp.distributions\n  \n    rv_prob = tfd.Uniform(name=\"rv_prob\", low=0., high=1.)\n    p_skewed = 0.5 * prob_cheating + 0.25\n    rv_yes_responses = tfd.Binomial(name=\"rv_yes_responses\",\n                                     total_count=tf.to_float(N), \n                                     probs=p_skewed)\n\n    return (\n        rv_prob.log_prob(prob_cheating)\n        + tf.reduce_sum(rv_yes_responses.log_prob(tf.to_float(yes_responses)))\n    )\n下面我们将所有感兴趣的变量添加到我们的HMC组件定义单元格中，并在模型上运行我们的黑盒算法。\nnumber_of_steps = 25000\nburnin = 2500\n\n# Set the chain's start state.\ninitial_chain_state = [\n    0.2 * tf.ones([], dtype=tf.float32, name=\"init_skewed_p\")\n]\n\n# Since HMC operates over unconstrained space, we need to transform the\n# samples so they live in real-space.\nunconstraining_bijectors = [\n    tfp.bijectors.Sigmoid(),   # Maps [0,1] to R.\n]\n\n# Define a closure over our joint_log_prob.\n# unnormalized_posterior_log_prob = lambda *args: alt_joint_log_prob(headsflips, total_yes, N, *args)\nunnormalized_posterior_log_prob = lambda *args: alt_joint_log_prob(total_yes, N, *args)\n\n# Initialize the step_size. (It will be automatically adapted.)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    step_size = tf.get_variable(\n        name='skewed_step_size',\n        initializer=tf.constant(0.5, dtype=tf.float32),\n        trainable=False,\n        use_resource=True\n    ) \n\n# Defining the HMC\nhmc=tfp.mcmc.TransformedTransitionKernel(\n    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_posterior_log_prob,\n        num_leapfrog_steps=2,\n        step_size=step_size,\n        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n        state_gradients_are_stopped=True),\n    bijector=unconstraining_bijectors)\n\n# Sample from the chain.\n[\n    posterior_skewed_p\n], kernel_results = tfp.mcmc.sample_chain(\n    num_results=number_of_steps,\n    num_burnin_steps=burnin,\n    current_state=initial_chain_state,\n    kernel=hmc)\n\n# Initialize any created variables.\n# This prevents a FailedPreconditionError\ninit_g = tf.global_variables_initializer()\ninit_l = tf.local_variables_initializer()\n\n执行TF图以从后验采样\n# This cell may take 5 minutes in Graph Mode\nevaluate(init_g)\nevaluate(init_l)\n[posterior_skewed_p_,kernel_results_] = evaluate([posterior_skewed_p,kernel_results])\n\nprint(\"acceptance rate: {}\".format(\n    kernel_results_.inner_results.is_accepted.mean()))\n# print(\"final step size: {}\".format(\n#     kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))\n\n# print(\"p_skewed trace: \", posterior_skewed_p_)\n# print(\"p_skewed burned trace: \", posterior_skewed_p_[burnin:])\nfreq_cheating_samples_ = posterior_skewed_p_[burnin:]\nacceptance rate: 0.6818\nNow we can plot our results\nplt.figure(figsize(12.5, 6))\np_trace_ = freq_cheating_samples_\nplt.hist(p_trace_, histtype=\"stepfilled\", normed=True, alpha=0.85, bins=30, \n         label=\"posterior distribution\", color=TFColor[3])\nplt.vlines([.1, .40], [0, 0], [5, 5], alpha=0.2)\nplt.xlim(0, 1)\nplt.legend()\n\n本章的其余部分将介绍TFP和TFP建模的一些实际示例："
  },
  {
    "objectID": "posts/tfp-ch2.html#example-挑战者航天飞机灾难",
    "href": "posts/tfp-ch2.html#example-挑战者航天飞机灾难",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "Example: 挑战者航天飞机灾难",
    "text": "Example: 挑战者航天飞机灾难\n1986年1月28日，美国航天飞机计划的第二十五次飞行结束，当一架航天飞机挑战者的火箭助推器在升空后不久爆炸，造成所有七名机组人员死亡。事故总统委员会得出结论认为，这是由于火箭助推器上的现场接头中的O形环失效造成的，而这种失败是由于设计错误导致O形圈对一些人不可接受的敏感。因素包括室外温度。在之前的24个航班中，有关23个O形圈故障的数据（其中一个在海上丢失），这些数据在挑战者发射前的晚上进行了讨论，但不幸的是只有与7个航班相对应的数据。其中有一个损坏事件被认为是重要的，这些被认为没有明显的趋势。数据如下所示（见[1]）：\nreset_sess()\n\nimport wget\nurl = 'https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv'\nfilename = wget.download(url)\nfilename\n'challenger_data.csv'\nplt.figure(figsize(12.5, 3.5))\nnp.set_printoptions(precision=3, suppress=True)\nchallenger_data_ = np.genfromtxt(\"challenger_data.csv\", skip_header=1,\n                                usecols=[1, 2], missing_values=\"NA\",\n                                delimiter=\",\")\n#drop the NA values\nchallenger_data_ = challenger_data_[~np.isnan(challenger_data_[:, 1])]\n\n#plot it, as a function of tempature (the first column)\nprint(\"温度 (F), O型环是否损坏?\")\nprint(challenger_data_)\n\nplt.scatter(challenger_data_[:, 0], challenger_data_[:, 1], s=75, color=\"k\",\n            alpha=0.5)\nplt.yticks([0, 1])\nplt.ylabel(\"Damage Incident?\")\nplt.xlabel(\"Outside temperature (Fahrenheit)\")\nplt.title(\"Defects of the Space Shuttle O-Rings vs temperature\")\n温度 (F), O型环是否损坏?\n[[66.  0.]\n [70.  1.]\n [69.  0.]\n [68.  0.]\n [67.  0.]\n [72.  0.]\n [73.  0.]\n [70.  0.]\n [57.  1.]\n [63.  1.]\n [70.  1.]\n [78.  0.]\n [67.  0.]\n [53.  1.]\n [67.  0.]\n [75.  0.]\n [70.  0.]\n [81.  0.]\n [76.  0.]\n [79.  0.]\n [75.  1.]\n [76.  0.]\n [58.  1.]]\n\n很明显随着室外温度的降低，发生的损害事故的概率会增加。我们对这里的概率建模感兴趣，因为它看起来不像温度和损坏事件之间存在严格的截止点。我们能做的最好的事情就是“在温度\\(t\\)时，发生损坏事故的概率是多少？”。这个例子的目标是回答这个问题。\n我们需要一个温度函数，称为\\(p（t）\\)，它在0和1之间（以便模拟概率），并随着温度的升高从1变为0。实际上有很多这样的功能，但最受欢迎的选择是logits 函数\n\\[p(t) = \\frac{1}{ 1 + e^{ \\;\\beta t } }\\]\n在这个模型,\\(\\beta\\)是我们不确定的变量. 下面是为\\(\\beta = 1,3，-5\\)绘制的函数。\ndef logistic(x, beta):\n    \"\"\"\n    Logistic Function\n        \n    Args:\n      x: independent variable\n      beta: beta term\n    Returns: \n      Logistic function\n    \"\"\"\n    return 1.0 / (1.0 + tf.exp(beta * x))\n\nx_vals = tf.linspace(start=-4., stop=4., num=100)\nlog_beta_1 = logistic(x_vals, 1.)\nlog_beta_3 = logistic(x_vals, 3.)\nlog_beta_m5 = logistic(x_vals, -5.)\nlog_beta_m1 = logistic(x_vals, -1.)\n\n[\n    x_vals_,\n    log_beta_1_,\n    log_beta_3_,\n    log_beta_m5_,\n    log_beta_m1_,\n] = evaluate([\n    x_vals,\n    log_beta_1,\n    log_beta_3,\n    log_beta_m5,\n    log_beta_m1,\n])\n\nplt.figure(figsize(12.5, 3))\nplt.plot(x_vals_, log_beta_1_, label=r\"$\\beta = 1$\", color=TFColor[0])\nplt.plot(x_vals_, log_beta_3_, label=r\"$\\beta = 3$\", color=TFColor[3])\nplt.plot(x_vals_, log_beta_m5_, label=r\"$\\beta = -5$\", color=TFColor[6])\nplt.plot(x_vals_, log_beta_m5_, label=r\"$\\beta = -1$\", color=TFColor[9])\nplt.legend()\n\n但缺少一些东西。在逻辑函数的图中，概率仅在零附近变化，但在我们的数据中，概率在65到70左右变化。我们需要在逻辑函数中添加一个偏差项：\n\\[p(t) = \\frac{1}{ 1 + e^{ \\;\\beta t + \\alpha } }\\]\n下面有一些情节，不同的\\(\\alpha\\)。\ndef logistic(x, beta, alpha=0):\n    \"\"\"\n    带偏移的Logistic函数\n        \n    Args:\n        x: 独立变量\n        beta: beta term \n        alpha: alpha term\n    Returns: \n        Logistic function\n    \"\"\"\n    return 1.0 / (1.0 + tf.exp((beta * x) + alpha))\n\nx_vals = tf.linspace(start=-4., stop=4., num=100)\nlog_beta_1_alpha_1 = logistic(x_vals, 1, 1)\nlog_beta_3_alpha_m2 = logistic(x_vals, 3, -2)\nlog_beta_m5_alpha_7 = logistic(x_vals, -5, 7)\n\n[\n    x_vals_,\n    log_beta_1_alpha_1_,\n    log_beta_3_alpha_m2_,\n    log_beta_m5_alpha_7_,\n] = evaluate([\n    x_vals,\n    log_beta_1_alpha_1,\n    log_beta_3_alpha_m2,\n    log_beta_m5_alpha_7,\n])\n\nplt.figure(figsize(12.5, 3))\nplt.plot(x_vals_, log_beta_1_, label=r\"$\\beta = 1$\", ls=\"--\", lw=1, color=TFColor[0])\nplt.plot(x_vals_, log_beta_3_, label=r\"$\\beta = 3$\", ls=\"--\", lw=1, color=TFColor[3])\nplt.plot(x_vals_, log_beta_m5_, label=r\"$\\beta = -5$\", ls=\"--\", lw=1, color=TFColor[6])\nplt.plot(x_vals_, log_beta_1_alpha_1_, label=r\"$\\beta = 1, \\alpha = 1$\", color=TFColor[0])\nplt.plot(x_vals_, log_beta_3_alpha_m2_, label=r\"$\\beta = 3, \\alpha = -2$\", color=TFColor[3])\nplt.plot(x_vals_, log_beta_m5_alpha_7_, label=r\"$\\beta = -5, \\alpha = 7$\", color=TFColor[6])\nplt.legend(loc=\"lower left\")\n\n添加一个常数项\\(\\alpha\\)相当于向左或向右移动曲线（因此它被称为偏差）。\n让我们开始在TFP中对此进行建模。\\(\\beta，\\alpha\\)参数没有理由为正，有界或相对较大，所以它们最好用正态随机变量建模，接下来介绍。\n\n正态分布\n一个普通的随机变量，表示为\\(X \\sim N（\\mu，1/\\tau）\\)，有一个包含两个参数的分布：均值，\\(\\mu\\)和* precision*，\\(\\tau\\)。那些熟悉Normal分布的人可能已经看到\\(\\sigma^2\\)而不是\\(\\tau^{-1}\\)。它们实际上是彼此的倒数。这种变化的动机是简单的数学分析，是旧贝叶斯方法的工件。请记住：\\(\\tau\\)越小，分布的分布越大（即我们更不确定）;\\(\\tau\\)越大，分布越紧（即我们更确定）。无论如何，\\(\\tau\\)总是正的的。\n\\(N（\\mu，1/\\tau）\\)随机变量的概率密度函数是：\n\\[f(x ; \\mu, \\tau) = \\sqrt{\\frac{\\tau}{2\\pi} } \\exp\\left( -\\frac{\\tau}{2} (x-\\mu)^2 \\right)\\]\n我们在下面绘制一些不同的密度函数\nrand_x_vals = tf.linspace(start=-8., stop=7., num=150)\n\ndensity_func_1 = tfd.Normal(loc=float(-2.), scale=float(1./.7)).prob(rand_x_vals)\ndensity_func_2 = tfd.Normal(loc=float(0.), scale=float(1./1)).prob(rand_x_vals)\ndensity_func_3 = tfd.Normal(loc=float(3.), scale=float(1./2.8)).prob(rand_x_vals)\n\n[\n    rand_x_vals_,\n    density_func_1_,\n    density_func_2_,\n    density_func_3_,\n] = evaluate([\n    rand_x_vals,\n    density_func_1,\n    density_func_2,\n    density_func_3,\n])\n\ncolors = [TFColor[3], TFColor[0], TFColor[6]]\n\nplt.figure(figsize(12.5, 3))\nplt.plot(rand_x_vals_, density_func_1_,\n         label=r\"$\\mu = %d, \\tau = %.1f$\" % (-2., .7), color=TFColor[3])\nplt.fill_between(rand_x_vals_, density_func_1_, color=TFColor[3], alpha=.33)\nplt.plot(rand_x_vals_, density_func_2_, \n         label=r\"$\\mu = %d, \\tau = %.1f$\" % (0., 1), color=TFColor[0])\nplt.fill_between(rand_x_vals_, density_func_2_, color=TFColor[0], alpha=.33)\nplt.plot(rand_x_vals_, density_func_3_,\n         label=r\"$\\mu = %d, \\tau = %.1f$\" % (3., 2.8), color=TFColor[6])\nplt.fill_between(rand_x_vals_, density_func_3_, color=TFColor[6], alpha=.33)\n\nplt.legend(loc=r\"upper right\")\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"density function at$x$\")\nplt.title(r\"Probability distribution of three different Normal random variables\")\n\n普通随机变量可以取任何实数，但变量很可能相对接近\\(\\mu\\)。实际上，Normal的预期值等于其\\(\\mu\\)参数：\n\\[E[ X ; \\mu, \\tau] = \\mu\\]\n并且它的方差等于\\(\\tau\\)的倒数：\n\\[\\text{Var}( X ; \\mu, \\tau ) = \\frac{1}{\\tau}\\]\n下面我们继续我们对挑战者太空飞船的建模：\nreset_sess()\n\ntemperature_ = challenger_data_[:, 0]\ntemperature = tf.convert_to_tensor(temperature_, dtype=tf.float32)\nD_ = challenger_data_[:, 1]                # defect or not?\nD = tf.convert_to_tensor(D_, dtype=tf.float32)\n\nbeta = tfd.Normal(name=\"beta\", loc=0.3, scale=1000.).sample()\nalpha = tfd.Normal(name=\"alpha\", loc=-15., scale=1000.).sample()\np_deterministic = tfd.Deterministic(name=\"p\", loc=1.0/(1. + tf.exp(beta * temperature_ + alpha))).sample()\n\n[\n    prior_alpha_,\n    prior_beta_,\n    p_deterministic_,\n    D_,\n] = evaluate([\n    alpha,\n    beta,\n    p_deterministic,\n    D,\n])\n我们有自己的概率，但我们如何将它们与我们观察到的数据联系起来？带参数\\(p\\)的A Bernoulli随机变量，表示为\\(\\text{Ber}（p）\\)，是一个随机变量，取值为1，概率为\\(p\\)，0为else。因此，我们的模型看起来像：\n\\[\\text{Defect Incident, }D_i \\sim \\text{Ber}( \\;p(t_i)\\; ), \\;\\; i=1..N\\]\n其中\\(p（t）\\)是我们的logistic函数，\\(t_i\\)是我们观察到的温度。请注意，在下面的代码中，我们在initial_chain_state中将beta和alpha的值设置为0。这样做的原因是，如果beta和alpha非常大，它们会使p等于1或0.不幸的是，tfd.Bernoulli不喜欢0或1的概率，尽管它们是数学上的定义明确的概率。因此，通过将系数值设置为“0”，我们将变量“p”设置为合理的起始值。这对我们的结果没有影响，也不意味着我们在之前的内容中包含任何其他信息。这只是TFP中的计算警告。\ndef challenger_joint_log_prob(D, temperature_, alpha, beta):\n    \"\"\"\n    联合对数概率优化函数。\n        \n    Args:\n      D: 来自挑战者灾难的数据表示存在或不存在缺陷\n      temperature_: 来自挑战者灾难的数据，特别是观察是否存在缺陷的温度\n      alpha: one of the inputs of the HMC\n      beta: one of the inputs of the HMC\n    Returns: \n      Joint log probability optimization function.\n    \"\"\"\n    rv_alpha = tfd.Normal(loc=0., scale=1000.)\n    rv_beta = tfd.Normal(loc=0., scale=1000.)\n\n    # make this into a logit\n    logistic_p = 1.0/(1. + tf.exp(beta * tf.to_float(temperature_) + alpha))\n    rv_observed = tfd.Bernoulli(probs=logistic_p)\n    \n    return (\n        rv_alpha.log_prob(alpha)\n        + rv_beta.log_prob(beta)\n        + tf.reduce_sum(rv_observed.log_prob(D))\n    )\nnumber_of_steps = 10000 #@param {type:\"slider\", min:2500, max:120000, step:100}\nburnin = 2000 #@param {type:\"slider\", min:2000, max:100000, step:100}\n\n# 初始化参数都是0\ninitial_chain_state = [\n    1. * tf.ones([], dtype=tf.float32, name=\"init_alpha\"),\n    2. * tf.ones([], dtype=tf.float32, name=\"init_beta\")\n]\n\n# Since HMC operates over unconstrained space, we need to transform the\n# samples so they live in real-space.\n# Alpha is 100x of beta approximately, so apply Affine scalar bijector\n# to multiply the unconstrained alpha by 100 to get back to \n# the Challenger problem space\nunconstraining_bijectors = [\n    tfp.bijectors.AffineScalar(100.),\n    tfp.bijectors.Identity()\n]\n\n# Define a closure over our joint_log_prob.\nunnormalized_posterior_log_prob = lambda *args: challenger_joint_log_prob(D, temperature_, *args)\n\n# Initialize the step_size. (It will be automatically adapted.)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    step_size = tf.get_variable(\n        name='step_size',\n        initializer=tf.constant(0.01, dtype=tf.float32),\n        trainable=False,\n        use_resource=True\n    )\n\n# Defining the HMC\nhmc=tfp.mcmc.TransformedTransitionKernel(\n    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_posterior_log_prob,\n        num_leapfrog_steps=40, #to improve convergence\n        step_size=step_size,\n        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(\n            num_adaptation_steps=int(burnin * 0.8)),\n        state_gradients_are_stopped=True),\n    bijector=unconstraining_bijectors)\n\n# Sampling from the chain.\n[posterior_alpha,posterior_beta], kernel_results = tfp.mcmc.sample_chain(\n    num_results = number_of_steps,\n    num_burnin_steps = burnin,\n    current_state=initial_chain_state,\n    kernel=hmc)\n\n## Initialize any created variables for preconditions\ninit_g = tf.global_variables_initializer()\n\n执行TF图以从后验采样\n%%time\n# 在图形模式下，此单元格最多可能需要15分钟\nevaluate(init_g)\n[posterior_alpha_, posterior_beta_, kernel_results_] = evaluate([\n    posterior_alpha,\n    posterior_beta,\n    kernel_results\n])\n    \nprint(\"接受率: {}\".format(\n    kernel_results_.inner_results.is_accepted.mean()))\nprint(\"结束步: {}\".format(\n    kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))\n接受率: 0.388\n结束步: 0.016446322202682495\nCPU times: user 10min 51s, sys: 1min 2s, total: 11min 53s\nWall time: 5min 5s\n我们已经在观察到的数据上训练了我们的模型，所以让我们看看\\(\\alpha\\)和\\(\\beta\\)的后验分布：\nposterior_alpha_, posterior_beta_=evaluate([\n    posterior_alpha,\n    posterior_beta,\n])\nplt.figure(figsize(12.5, 6))\n\n# 样本的直方图:\nplt.subplot(211)\nplt.title(r\"Posterior distributions of the variables$\\alpha, \\beta$\")\nplt.hist(posterior_beta_, histtype='stepfilled', bins=35, alpha=0.85,\n         label=r\"posterior of$\\beta$\", color=TFColor[6], density=True)\nplt.legend()\n\nplt.subplot(212)\nplt.hist(posterior_alpha_, histtype='stepfilled', bins=35, alpha=0.85,\n         label=r\"posterior of$\\alpha$\", color=TFColor[0], density=True)\nplt.legend()\n\n\\(\\beta\\)的所有样本都大于0.如果后验以0为中心，我们可能会怀疑\\(\\beta = 0\\)，这意味着温度对缺陷概率没有影响。\n同样，所有\\(\\alpha\\)后验值都是负数且远离0，这意味着认为\\(\\alpha\\)明显小于0是正确的。\n关于数据的传播，我们非常不确定真实参数可能是什么（尽管考虑到样本量较小以及缺陷与非缺陷的大量重叠，这种行为可能是预期的）。\n接下来，让我们看一下特定温度值的预期概率。也就是说，我们对来自后验的所有样本求平均值，得到\\(p（t_i）\\)的可能值。\nalpha_samples_1d_ = posterior_alpha_[:, None]  # best to make them 1d\nbeta_samples_1d_ = posterior_beta_[:, None]\n\nbeta_mean = tf.reduce_mean(beta_samples_1d_.T[0])\nalpha_mean = tf.reduce_mean(alpha_samples_1d_.T[0])\n[ beta_mean_, alpha_mean_ ] = evaluate([ beta_mean, alpha_mean ])\n\n\nprint(\"beta mean:\", beta_mean_)\nprint(\"alpha mean:\", alpha_mean_)\ndef logistic(x, beta, alpha=0):\n    \"\"\"\n    Logistic function with alpha and beta.\n        \n    Args:\n      x: independent variable\n      beta: beta term \n      alpha: alpha term\n    Returns: \n      Logistic function\n    \"\"\"\n    return 1.0 / (1.0 + tf.exp((beta * x) + alpha))\n\nt_ = np.linspace(temperature_.min() - 5, temperature_.max() + 5, 2500)[:, None]\np_t = logistic(t_.T, beta_samples_1d_, alpha_samples_1d_)\nmean_prob_t = logistic(t_.T, beta_mean_, alpha_mean_)\n[ \n    p_t_, mean_prob_t_\n] = evaluate([ \n    p_t, mean_prob_t\n])\nbeta mean: 0.034827046\nalpha mean: -1.5132124\nplt.figure(figsize(12.5, 4))\n\nplt.plot(t_, mean_prob_t_.T, lw=3, label=\"average posterior \\nprobability \\\nof defect\")\nplt.plot(t_, p_t_.T[:, 0], ls=\"--\", label=\"realization from posterior\")\nplt.plot(t_, p_t_.T[:, -8], ls=\"--\", label=\"realization from posterior\")\nplt.scatter(temperature_, D_, color=\"k\", s=50, alpha=0.5)\nplt.title(\"Posterior expected value of probability of defect; \\\nplus realizations\")\nplt.legend(loc=\"lower left\")\nplt.ylim(-0.1, 1.1)\nplt.xlim(t_.min(), t_.max())\nplt.ylabel(\"probability\")\nplt.xlabel(\"temperature\")\n\n上面我们还绘制了实际底层系统可能实现的两种可能的实现。两者都与其他任何平局一样可能。当我们将所有20000条可能的虚线平均在一起时，会出现蓝线。\nfrom scipy.stats.mstats import mquantiles\n\n# “置信区间”的矢量化底部和顶部2.5％分位数\nqs = mquantiles(p_t_, [0.025, 0.975], axis=0)\nplt.fill_between(t_[:, 0], *qs, alpha=0.7,\n                 color=\"#7A68A6\")\n\nplt.plot(t_[:, 0], qs[0], label=\"95% CI\", color=\"#7A68A6\", alpha=0.7)\n\nplt.plot(t_[:, 0], mean_prob_t_[0,:], lw=1, ls=\"--\", color=\"k\",\n         label=\"average posterior \\nprobability of defect\")\n\nplt.xlim(t_.min(), t_.max())\nplt.ylim(-0.02, 1.02)\nplt.legend(loc=\"lower left\")\nplt.scatter(temperature_, D_, color=\"k\", s=50, alpha=0.5)\nplt.xlabel(\"temp,$t$\")\n\nplt.ylabel(\"probability estimate\")\nplt.title(\"Posterior probability estimates given temp.$t$\")\n\n95％可信区间，或95％CI，涂成紫色，表示每个温度的间隔，包含95％的分布。例如，在65度时，我们可以95％确定缺陷的概率介于0.25和0.85之间。\n更一般地说，我们可以看到，当温度接近60度时，CI迅速扩散到\\([0,1]\\)以上。当我们通过70度时，CI再次收紧。这可以让我们深入了解如何继续下一步：我们应该在60-65温度附近测试更多的O形环，以更好地估计该范围内的概率。同样地，在向科学家报告您的估计时，您应该非常谨慎地告诉他们预期的概率，因为我们可以看到这并不能反映后验分布的宽。\n\n\n\n挑战者灾难的那天怎么样？\n在挑战者灾难当天，室外温度为31华氏度。在这个温度下，出现缺陷的后验分布是什么？分布如下图所示。看起来几乎可以保证挑战者将受到有缺陷的O形圈的影响。\nplt.figure(figsize(12.5, 3))\n\nprob_31 = logistic(31, posterior_beta_, posterior_alpha_)\n\n[ prob_31_ ] = evaluate([ prob_31 ])\n\nplt.xlim(0.98, 1)\nplt.hist(prob_31_, bins=10, density=True, histtype='stepfilled')\nplt.title(\"Posterior distribution of probability of defect, given$t = 31$\")\nplt.xlabel(\"probability of defect occurring in O-ring\")\n\n\n\n我们的模型是否合适?\n持怀疑态度的读者会说：“你故意为\\(p（t）\\)和特定的先验选择了logistic函数。或许其他函数或先验会给出不同的结果。我怎么知道我选择了一个好模特？”这绝对是真的。考虑一个极端情况，如果我选择了函数\\(p（t）= 1，\\; \\forall t\\)，它保证了一直存在的缺陷：我将在1月28日再次预测灾难。然而，这显然是一个选择不当的模型。另一方面，如果我确实为\\(p（t）\\)选择了逻辑函数，但指定我的所有先验在0附近非常紧，可能我们会有非常不同的后验分布。我们怎么知道我们的模型是数据的表达？这鼓励我们衡量模型的适合度。\n我们可以想：我们如何测试我们的模型是否合适？一个想法是将观测数据与我们可以模拟的人工数据集进行比较。基本原理是，如果模拟数据集在统计上与观察到的数据集不相似，则可能我们的模型未准确地表示观察到的数据。\n在本章的前面，我们为SMS示例模拟了一个人工数据集。为此，我们从先验中采样值。我们看到了生成的数据集看起来多么多样，而且它们很少模仿我们观察到的数据集。在当前示例中，我们应该从后验分布中进行采样，以创建非常合理的数据集。幸运的是，我们的贝叶斯框架使这很容易。我们只需要从选择的分布中收集样本，并指定样本的数量，样本的形状（我们在原始数据集中有21个观测量，因此我们将使每个样本的形状为21），以及概率我们想用来确定1个观测值与0个观测值的比率。\n因此我们创造了以下内容:\nsimulated_data = tfd.Bernoulli(name=\"simulation_data\", probs=p).sample(sample_shape=N)\n模拟 10 000:\nalpha = alpha_mean_ # 我们将这些值基于上述模型的输出\nbeta = beta_mean_\np_deterministic = tfd.Deterministic(name=\"p\", loc=1.0/(1. + tf.exp(beta * temperature_ + alpha))).sample()#seed=6.45)\nsimulated_data = tfd.Bernoulli(name=\"bernoulli_sim\", \n                               probs=p_deterministic_).sample(sample_shape=10000)\n[ \n    bernoulli_sim_samples_,\n    p_deterministic_\n] =evaluate([\n    simulated_data,\n    p_deterministic\n])\nsimulations_ = bernoulli_sim_samples_\nprint(\"Number of simulations:             \", simulations_.shape[0])\nprint(\"Number data points per simulation: \", simulations_.shape[1])\n\nplt.figure(figsize(12.5, 12))\nplt.title(\"Simulated dataset using posterior parameters\")\nfor i in range(4):\n    ax = plt.subplot(4, 1, i+1)\n    plt.scatter(temperature_, simulations_[1000*i, :], color=\"k\",\n                s=50, alpha=0.6)\n    \nNumber of simulations:              10000\nNumber data points per simulation:  23\n\n请注意，上面的图表是不同的（如果您能想到一个更清晰的方式来呈现此，请发送拉取请求并回答此处!).\n我们希望评估我们的模型有多好。 “好”当然是一个主观的术语，因此结果必须与其他模型相关。\n我们也将以图形方式进行此操作，这似乎是一种更不客观的方法。另一种方法是使用贝叶斯p值。这些仍然是主观的，因为好与坏之间的恰当截止是任意的。格尔曼强调，图形测试比p值测试更有启发性[3]。我们同意。\n以下图形测试是一种新颖的数据 - 逻辑回归方法。这些图称为分离图 [4]。对于我们希望比较的一组模型，每个模型都绘制在单独的分离图上。我将关于分离图的大部分技术细节留给了非常容易获得的原始论文, 但我会在这里总结一下它们的用途。\n对于每个模型，我们计算后验模拟为特定温度提出值1的次数，即通过平均计算\\(P（\\; \\text{缺陷} = 1 | t，\\alpha，\\beta）\\)。这为我们提供了数据集中每个数据点的缺陷后验概率。例如，对于我们上面使用的模型：\nposterior_probability_ = simulations_.mean(axis=0)\nprint(\"posterior prob of defect | realized defect \")\nfor i in range(len(D_)):\n    print(\"%.2f                     |   %d\" % (posterior_probability_[i], D_[i]))\nposterior prob of defect | realized defect \n0.00                     |   0\n0.00                     |   1\n0.00                     |   0\n0.00                     |   0\n0.00                     |   0\n0.00                     |   0\n0.00                     |   0\n0.00                     |   0\n0.00                     |   1\n0.00                     |   1\n0.00                     |   1\n0.00                     |   0\n0.00                     |   0\n0.00                     |   1\n0.00                     |   0\n0.00                     |   0\n0.00                     |   0\n0.00                     |   0\n0.00                     |   0\n0.00                     |   0\n0.00                     |   1\n0.00                     |   0\n0.00                     |   1\n接下来，我们按后验概率对每列进行排序:\nix_ = np.argsort(posterior_probability_)\nprint(\"probb | defect \")\nfor i in range(len(D_)):\n    print(\"%.2f  |   %d\" % (posterior_probability_[ix_[i]], D_[ix_[i]]))\nprobb | defect \n0.00  |   0\n0.00  |   1\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   1\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   1\n0.00  |   1\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   0\n0.00  |   1\n0.00  |   1\n0.00  |   1\n我们可以在图中更好地呈现上述数据：我们创建了一个separation_plot函数。\nimport matplotlib.pyplot as plt\n\ndef separation_plot( p, y, **kwargs ):\n    \"\"\"\n    This function creates a separation plot for logistic and probit classification. \n    See http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf\n    \n    p: 比例/概率可以是表示M个模型的n×M矩阵。\n    y: 0-1响应变量。\n    \n    \"\"\"    \n    assert p.shape[0] == y.shape[0], \"p.shape[0] != y.shape[0]\"\n    n = p.shape[0]\n\n    try:\n        M = p.shape[1]\n    except:\n        p = p.reshape( n, 1 )\n        M = p.shape[1]\n\n    colors_bmh = np.array( [\"#eeeeee\", \"#348ABD\"] )\n\n\n    fig = plt.figure( )\n    \n    for i in range(M):\n        ax = fig.add_subplot(M, 1, i+1)\n        ix = np.argsort( p[:,i] )\n        #plot the different bars\n        bars = ax.bar( np.arange(n), np.ones(n), width=1.,\n                color = colors_bmh[ y[ix].astype(int) ], \n                edgecolor = 'none')\n        ax.plot( np.arange(n+1), np.append(p[ix,i], p[ix,i][-1]), \"k\",\n                 linewidth = 1.,drawstyle=\"steps-post\" )\n        #create expected value bar.\n        ax.vlines( [(1-p[ix,i]).sum()], [0], [1] )\n        plt.xlim( 0, n)\n        \n    plt.tight_layout()\n    \n    return\n\nplt.figure(figsize(11., 3))\nseparation_plot(posterior_probability_, D_)\n\n蛇形线是排序的概率，蓝色条表示缺陷，空白空间（或乐观读者的灰色条）表示非缺陷。随着概率的上升，我们发现越来越多的缺陷发生。在右侧，该图表明，随着后验概率很大（线接近1），则实现了更多的缺陷。这是一种很好的行为。理想情况下，所有蓝条应该靠近右侧，与此相反的偏差反映了错过的预测。\n在给定此模型的情况下，黑色垂直线是我们应该观察到的预期缺陷数。这允许用户查看模型预测的事件总数与数据中的实际事件数量的比较。\n将其与其他模型的分离图进行比较会提供更多信息。下面我们比较我们的模型（顶部）与其他三个模型：\n\n完美模型，如果确实发生缺陷，则预测后验概率等于1。\n一个完全随机的模型，它可以预测随机概率，而不管温度如何。\n常数模型：其中\\(P（D = 1 \\; | \\; t）= c，\\; \\; \\forall t\\)。\\(c\\)的最佳选择是观察到的缺陷频率，在这种情况下为7/23。\n\nplt.figure(figsize(11., 2))\n\n# Our temperature-dependent model\nseparation_plot(posterior_probability_, D_)\nplt.title(\"Temperature-dependent model\")\n\n# 完美模型\n# i.e. the probability of defect is equal to if a defect occurred or not.\np_ = D_\nseparation_plot(p_, D_)\nplt.title(\"Perfect model\")\n\n# 随机模型\np_ = np.random.rand(23)\nseparation_plot(p_, D_)\nplt.title(\"Random model\")\n\n# 常量模型\nconstant_prob_ = 7./23 * np.ones(23)\nseparation_plot(constant_prob_, D_)\nplt.title(\"Constant-prediction model\")\n\n\n\n\nIn the random model, we can see that as the probability increases there is no clustering of defects to the right-hand side. Similarly for the constant model.\nIn the perfect model, the probability line is not well shown, as it is stuck to the bottom and top of the figure. Of course the perfect model is only for demonstration, and we cannot infer any scientific inference from it."
  },
  {
    "objectID": "posts/tfp-ch2.html#exercises",
    "href": "posts/tfp-ch2.html#exercises",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "Exercises",
    "text": "Exercises\n1 Try putting in extreme values for our observations in the cheating example. What happens if we observe 25 affirmative responses? 10? 50?\n#type your code here.\n2 Try plotting\\(\\alpha\\)samples versus\\(\\beta\\)samples. Why might the resulting plot look like this?\n#type your code here.\nplt.figure(figsize(12.5, 4))\n\nplt.scatter(alpha_samples_, beta_samples_, alpha=0.1)\nplt.title(\"Why does the plot look like this?\")\nplt.xlabel(r\"$\\alpha$\")\nplt.ylabel(r\"$\\beta$\")"
  },
  {
    "objectID": "posts/tfp-ch2.html#references",
    "href": "posts/tfp-ch2.html#references",
    "title": "概率模型第二章 ： A little more on TFP",
    "section": "References",
    "text": "References\n[1] Dalal, Fowlkes and Hoadley (1989),JASA, 84, 945-957.\n[2] Cronin, Beau. “Why Probabilistic Programming Matters.” 24 Mar 2013. Google, Online Posting to Google . Web. 24 Mar. 2013. https://plus.google.com/u/0/+BeauCronin/posts/KpeRdJKR6Z1.\n[3] Gelman, Andrew. “Philosophy and the practice of Bayesian statistics.” British Journal of Mathematical and Statistical Psychology. (2012): n. page. Web. 2 Apr. 2013.\n[4] Greenhill, Brian, Michael D. Ward, and Audrey Sacks. “The Separation Plot: A New Visual Method for Evaluating the Fit of Binary Models.” American Journal of Political Science. 55.No.4 (2011): n. page. Web. 2 Apr. 2013."
  },
  {
    "objectID": "posts/tfp-ch4.html",
    "href": "posts/tfp-ch4.html",
    "title": "概率模型第四章 ： 大数定理",
    "section": "",
    "text": "Tensorflow 概率模型学习，代码运行于Tensorflow 1.14，文字半机器翻译。\n\n\nProbabilistic Programming and Bayesian Methods for Hackers Chapter 4\n\n\n\n### Table of Contents - Dependencies & Prerequisites - The greatest theorem never told - The Law of Large Numbers - Intuition - How do we compute \\(Var(Z)\\) though? - Expected values and probabilities - What does this all have to do with Bayesian statistics? - The Disorder of Small Numbers - Example: Aggregated geographic data - Example: Kaggle’s U.S. Census Return Rate Challenge - Example: How to order Reddit submissions - Setting up the Praw Reddit API - Register your Application on Reddit - Reddit API Setup - Sorting! - But this is too slow for real-time! - Extension to Starred rating systems - Example: Counting Github stars - Conclusion - Appendix - Exercises - Kicker Careers Ranked by Make Percentage - Average Household Income by Programming Language - References\n\n\n______\n\n\n```python #@title Imports and Global Variables { display-mode: “form” } ““” The book uses a custom matplotlibrc file, which provides the unique styles for matplotlib plots. If executing this book, and you wish to use the book’s styling, provided are two options: 1. Overwrite your own matplotlibrc file with the rc-file provided in the book’s styles/ dir. See http://matplotlib.org/users/customizing.html 2. Also in the styles is bmh_matplotlibrc.json file. This can be used to update the styles in only this notebook. Try running the following code:\n\n\nimport json s = json.load(open(“../styles/bmh_matplotlibrc.json”)) matplotlib.rcParams.update(s) ““” from future import absolute_import, division, print_function\n\n\n#@markdown This sets the warning status (default is ignore, since this notebook runs correctly) warning_status = “ignore” #@param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"] import warnings warnings.filterwarnings(warning_status) with warnings.catch_warnings(): warnings.filterwarnings(warning_status, category=DeprecationWarning) warnings.filterwarnings(warning_status, category=UserWarning)\n\n\nimport numpy as np import os #@markdown This sets the styles of the plotting (default is styled like plots from FiveThirtyeight.com) matplotlib_style = ‘fivethirtyeight’ #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook'] import matplotlib.pyplot as plt; plt.style.use(matplotlib_style) import matplotlib.axes as axes; from matplotlib.patches import Ellipse from mpl_toolkits.mplot3d import Axes3D import pandas_datareader.data as web %matplotlib inline import seaborn as sns; sns.set_context(‘notebook’) from IPython.core.pylabtools import figsize #@markdown This sets the resolution of the plot outputs (retina is the highest resolution) notebook_screen_res = ‘retina’ #@param ['retina', 'png', 'jpeg', 'svg', 'pdf'] %config InlineBackend.figure_format = notebook_screen_res plt.rcParams[‘font.sans-serif’]=[‘YaHei Consolas Hybrid’] import tensorflow as tf tfe = tf.contrib.eager\n\n\n# Eager Execution #@markdown Check the box below if you want to use Eager Execution #@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the Google AI Blog use_tf_eager = False #@param {type:“boolean”}\n\n\n# Use try/except so we can easily re-execute the whole notebook. if use_tf_eager: try: tf.enable_eager_execution() except: pass\n\n\nimport tensorflow_probability as tfp tfd = tfp.distributions tfb = tfp.bijectors\n\n\ndef evaluate(tensors): “““Evaluates Tensor or EagerTensor to Numpy ndarrays. Args: tensors: Object of Tensor or EagerTensors; can belist,tuple,namedtuple` or combinations thereof.\n\n\nReturns: ndarrays: Object with same structure as tensors except with Tensor or EagerTensors replaced by Numpy ndarrays. ““” if tf.executing_eagerly(): return tf.contrib.framework.nest.pack_sequence_as( tensors, [t.numpy() if tf.contrib.framework.is_tensor(t) else t for t in tf.contrib.framework.nest.flatten(tensors)]) return sess.run(tensors)\n\n\nclass _TFColor(object): “““Enum of colors used in TF docs.”“” red = ‘#F15854’ blue = ‘#5DA5DA’ orange = ‘#FAA43A’ green = ‘#60BD68’ pink = ‘#F17CB0’ brown = ‘#B2912F’ purple = ‘#B276B2’ yellow = ‘#DECF3F’ gray = ‘#4D4D4D’ def getitem(self, i): return [ self.red, self.orange, self.green, self.blue, self.pink, self.brown, self.purple, self.yellow, self.gray,][i % 9] TFColor = _TFColor()\n\n\ndef session_options(enable_gpu_ram_resizing=True, enable_xla=True): ““” Allowing the notebook to make use of GPUs if they’re available.\n\n\nXLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that optimizes TensorFlow computations. ““” config = tf.ConfigProto() config.log_device_placement = True if enable_gpu_ram_resizing: # allow_growth=True makes it possible to connect multiple colabs to your # GPU. Otherwise the colab malloc’s all GPU ram. config.gpu_options.allow_growth = True if enable_xla: # Enable on XLA. https://www.tensorflow.org/performance/xla/. config.graph_options.optimizer_options.global_jit_level = (tf.OptimizerOptions.ON_1) return config\n\n\ndef reset_sess(config=None): ““” Convenience function to create the TF graph & session or reset them. ““” if config is None: config = session_options() global sess tf.reset_default_graph() try: sess.close() except: pass sess = tf.InteractiveSession(config=config)\n\n\nreset_sess() ```\n\n\nWARNING: Logging before flag parsing goes to stderr. W0728 19:21:27.291700 139811709552448 lazy_loader.py:50] The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops) If you depend on functionality not listed there, please file an issue.\n\n\n## 最伟大的定理从未被告知过\n\n\n本章的重点是一个总是在我们脑海中蹦蹦跳跳的想法，但很少在专门用于统计的书籍之外明确表达。事实上，到目前为止，我们在每个例子中都使用过这个简单的想法。\n\n\n### 大数定律\n\n\n让$ Z_i \\(为\\) N \\(来自某些概率分布的独立样本。根据*大数定律*，只要预期值\\) E[Z] $是有限的，以下成立，\n\n\n\\[\\frac{1}{N} \\sum_{i=1}^N Z_i \\rightarrow E[ Z ],  \\;\\;\\; N \\rightarrow \\infty.\\]\n\n\n文字表述:\n\n\n&gt; 来自相同分布的随机变量序列的平均值收敛于该分布的期望。\n\n\n这似乎是一个无聊的结果，但它将是您使用的最有用的工具。他是计算机数值计算的重要手段。\n\n\n### 直觉\n\n\n如果上述法律有些令人惊讶，可以通过研究一个简单的例子来更清楚地说明。\n\n\n考虑一个随机变量$ Z \\(，它只能带两个值，\\) c_1 \\(和\\) c_2 \\(。假设我们有大量\\) Z \\(的样本，表示一个特定的样本\\) Z_i \\(。该定理规定，我们可以通过平均所有样本来估计\\) Z $的预期值。考虑平均值：\n\n\n\\[ \\frac{1}{N} \\sum_{i=1}^N \\;Z_i \\]\n\n\n通过构造，$ Z_i \\(只能接受\\) c_1 \\(或\\) c_2 \\(，因此我们可以对这两个值进行分区：\\)$ \\[\\begin{align}\n\\frac{1}{N} \\sum_{i=1}^N \\;Z_i & =\\frac{1}{N} \\big(  \\sum_{ Z_i = c_1}c_1 + \\sum_{Z_i=c_2}c_2 \\big) \\\\\n& = c_1 \\sum_{ Z_i = c_1}\\frac{1}{N} + c_2 \\sum_{ Z_i = c_2}\\frac{1}{N} \\\\\n& = c_1 \\times \\text{ (approximate frequency of $c_1$) } \\\\\n& \\;\\;\\;\\;\\;\\;\\;\\;\\; + c_2 \\times \\text{ (approximate frequency of $c_2$) } \\\\\n& \\approx c_1 \\times P(Z = c_1) + c_2 \\times P(Z = c_2 ) \\\\\n& = E[Z]\n\\end{align}\\] $$\n\n\n在极限平等保持，但我们可以通过在平均值中使用越来越多的样本来越来越近。该法适用于几乎任何分布，减去我们稍后将遇到的一些重要案例。\n\n\n### 例子 ____\n\n\n下面是三个不同泊松随机变量序列的大数定律图。\n\n\n我们用参数$ = 4.5 \\(对`sample_size = 100000`泊松随机变量进行抽样。 （回想一下泊松随机变量的期望值等于它的参数。）我们计算前\\) n \\(个样本的平均值，\\)n=1$到sample_size。\n\n\n```python sample_size_ = 100000 expected_value_ = lambda_val_ = 4.5 N_samples = tf.range(start=1, limit=sample_size_, delta=100)\n\n\nplt.figure(figsize(12.5, 4)) for k in range(3): samples = tfd.Poisson(rate=lambda_val_).sample(sample_shape=(sample_size_)) [ samples_, N_samples_ ] = evaluate([ samples, N_samples ])\n\n\npartial_average_ = [ samples_[:i].mean() for i in N_samples_ ]\n\n\nplt.plot( N_samples_, partial_average_, lw=1.5,label=“\\(n\\)个样本的平均值 ; seq. %d”%k)\n\n\nplt.plot( N_samples_, expected_value_ * np.ones_like( partial_average_), ls = “–”, label = “真实期望值”, c = “k” )\n\n\nplt.ylim( 4.35, 4.65) plt.title( “随机变量的平均值与其期望的收敛性” ) plt.ylabel( “average of \\(n\\) samples” ) plt.xlabel( “# of samples, \\(n\\)”) plt.legend(); ```\n\n\n\n\n\n看一下上面的图，很明显，当样本量很小时，平均值会有较大的变化（比较跳跃的最初平均值，然后平滑）。所有三条路径接近值4.5，但随着$ N $变大，只是调整它。数学家和统计学家有另一个名字：收敛。\n\n\n我们可以问的另一个非常相关的问题是我收敛到预期值的速度有多快？让我们绘制一些新的东西。对于特定的$ N \\(，让我们进行上述试验数千次，并计算出我们与真实预期值的平均距离。但等等——*平均计算*？这只是大数法则！例如，我们感兴趣的是，对于特定的\\) N $，数量：\n\n\n\\[D(N) = \\sqrt{ \\;E\\left[\\;\\; \\left( \\frac{1}{N}\\sum_{i=1}^NZ_i  - 4.5 \\;\\right)^2 \\;\\;\\right] \\;\\;}\\]\n\n\n对于某些$ N \\(，上述公式可解释为距离真实值（平均值）的距离。 （我们取平方根，因此上述数量的维数和我们的随机变量是相同的）。由于上面是一个期望值，它可以使用大数定律近似：我们计算以下多次并平均它们，而不是平均\\) Z_i $：\n\n\n\\[ Y_k = \\left( \\;\\frac{1}{N}\\sum_{i=1}^NZ_i  - 4.5 \\; \\right)^2 \\]\n\n\n通过计算上面的$ N_y $次（记住，它是随机的），并对它们求平均值：\n\n\n\\[ \\frac{1}{N_Y} \\sum_{k=1}^{N_Y} Y_k \\rightarrow E[ Y_k ] = E\\;\\left[\\;\\; \\left( \\frac{1}{N}\\sum_{i=1}^NZ_i  - 4.5 \\;\\right)^2 \\right]\\]\n\n\n最后，取平方根：\n\n\n\\[ \\sqrt{\\frac{1}{N_Y} \\sum_{k=1}^{N_Y} Y_k} \\approx D(N) \\]\n\n\n```python N_Y = tf.constant(250) # 用这么多来近似 D(N) N_array = tf.range(1000., 50000., 2500) # 在大约使用这么多样品。差异。 D_N_results = tf.zeros(tf.shape(N_array)[0]) lambda_val = tf.constant(4.5) expected_value = tf.constant(4.5) #for X ~ Poi(lambda) , E[ X ] = lambda\n\n\n[ N_Y_, N_array_, D_N_results_, expected_value_, lambda_val_,] = evaluate([ N_Y, N_array, D_N_results, expected_value, lambda_val,])\n\n\ndef D_N(n): ““” This function approx. D_n, the average variance of using n samples. ““” Z = tfd.Poisson(rate=lambda_val_).sample(sample_shape=(int(n), int(N_Y_))) average_Z = tf.reduce_mean(Z, axis=0) average_Z_ = evaluate(average_Z)\n\n\nreturn np.sqrt(((average_Z_ - expected_value_)**2).mean())\n\n\nfor i,n in enumerate(N_array_): D_N_results_[i] = D_N(n)\n\n\nplt.figure(figsize(12.5, 3)) plt.xlabel( “\\(N\\)” ) plt.ylabel( “预期与真实值的平方距离” ) plt.plot(N_array_, D_N_results_, lw = 3, label=“随机变量\\(N\\)的预期值与平均值之间的预期距离。”) plt.plot( N_array_, np.sqrt(expected_value_)/np.sqrt(N_array_), lw = 3, ls = “–”, label = r”\\(\\frac{\\sqrt{\\lambda}}{\\sqrt{N}}\\)” ) plt.legend() plt.title( “样本平均收敛的“快”程度如何？ ” ); ```\n\n\n\n\n\n正如预期的那样，随着$ N $增长，我们的样本平均值与实际预期值之间的预期距离会缩小。但也注意到收敛率降低，也就是说，我们只需要10 000个额外样本从0.020移动到0.015，相差0.005，但是20000更多样本再次从0.015降低到0.010，再次只有0.005减少。\n\n\n事实证明，我们可以衡量这种收敛速度。上面我绘制了第二行，函数$ / \\(。这不是任意选择的。在大多数情况下，给定一系列随机变量分布如\\) Z \\(，大数定律的收敛率为\\) E [Z] $\n\n\n\\[ \\frac{ \\sqrt{ \\; Var(Z) \\; } }{\\sqrt{N} }\\]\n\n\n这有用的知识：对于给定的大$ N \\(，我们知道（平均而言）我们与估计的距离。另一方面，在贝叶斯环境中，这似乎是一个无用的结果：贝叶斯分析是不确定的，那么添加额外精确数字的*统计*点是什么？虽然绘图样本的计算成本可以很低，但*越大的*\\)N$也很好。\n\n\n### 我们如何计算$ Var（Z）$？\n\n\n方差只是另一个可以近似的预期值！考虑以下情况，一旦我们得到预期值（通过使用大数定律来估计它，表示$ $），我们可以估计方差：\n\n\n\\[ \\frac{1}{N}\\sum_{i=1}^N \\;(Z_i - \\mu)^2 \\rightarrow E[ \\;( Z - \\mu)^2 \\;] = Var( Z )\\]\n\n\n### 期望值和概率\n\n\n期望值与估计概率之间的关系更为明显。定义指标功能\n\n\n\\[\\mathbb{1}_A(x) =\n\\begin{cases} 1 &  x \\in A \\\\\\\\\n0 &  else\n\\end{cases}\n\\]\n\n\n然后，根据大数定律，如果我们有许多样本$ X_i \\(，我们可以通过以下方式估计事件\\) A \\(的概率，表示为\\) P（A）$：\n\n\n\\[ \\frac{1}{N} \\sum_{i=1}^N \\mathbb{1}_A(X_i) \\rightarrow E[\\mathbb{1}_A(X)] =  P(A) \\]\n\n\n同样，经过一段时间的观测后，这是相当明显的：如果事件发生，指标函数只有1，所以我们只将事件发生的时间相加并除以试验总数（考虑我们通常如何使用频率逼近概率） 。例如，假设我们希望估计$ Z Exp（.5）\\(大于5的概率，并且我们从\\) Exp（.5）$分布中得到许多样本。\n\n\n\\[ P( Z &gt; 5 ) =  \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{z &gt; 5 }(Z_i) \\]\n\n\n```python N = 10000\n\n\nprint(“概率估计:”, len(np.where(evaluate(tfd.Exponential(rate=0.5).sample(sample_shape=N)) &gt; 5))/N ) ```\n\n\n概率估计: 0.0001\n\n\n### 这与贝叶斯统计有什么关系？\n\n\n使用期望值计算贝叶斯推断中将在下一章中介绍的点估计。在更多分析的贝叶斯推断中，我们需要评估表示为多维积分的复杂期望值。不再。如果我们可以直接从后验分布中进行采样，我们只需要评估平均值。更容易。如果准确性是一个优先事项，那么上面的图表显示你收敛速度有多快。如果需要进一步的精确度，只需从后验中采集更多样本。\n\n\n什么时候够了？你何时可以停止从后方抽取样本？这是从业者的决定，也取决于样本的方差（从上面回忆高方差意味着平均值会收敛得更慢）。\n\n\n我们也应该理解大数定律何时失败。顾名思义，并将上面的图表与小\\(N\\)进行比较，该法则适用于大样本量。没有这个，渐近结果是不可靠的。了解定理失败的情况可以让我们对自己应该多么不自信充满信心。下一节将讨论此问题。\n\n\n### 小数目的紊乱\n\n\n大数定律只有在$ N $得到无限大时才有效：永远不可能实现。虽然定理是一个强有力的工具，但广泛地应用它是蛮干的。我们的下一个例子说明了这\n\n\n### Example: 汇总的地理数据\n\n\n数据通常以汇总形式出现。例如，数据可以按州，县或城市级别分组。当然，人口数量因地理区域而异。如果数据是每个地理区域的某些特征的平均值，那么我们必须意识到大数定律以及它对于人口较少的区域如何失败。\n\n\n我们将在玩具数据集上观察到这一点。假设我们的数据集中有五千个县。此外，每个州的人口数量均匀分布在100到1500之间。人口数量的生成方式与讨论无关，因此我们不能证明这一点。我们感兴趣的是测量每个县的平均身高。我们不知道，身高不会因县而异，每个人，无论他或她目前居住在哪个县，都有与他们身高相同的分布：\n\n\n\\[ \\text{height} \\sim \\text{Normal}(\\text{mu}=150, \\text{sd}=15 ) \\]\n\n\n我们汇总了县级的个人，因此我们只有县内平均值的数据。我们的数据集可能是什么样的？\n\n\n```python plt.figure(figsize(12.5, 4))\n\n\nstd_height = 15. mean_height = 150. n_counties = 500 smallest_population = 100 largest_population = 1500 pop_generator = np.random.randint norm = np.random.normal\n\n\npopulation_ = pop_generator(smallest_population, largest_population, n_counties)\n\n\n# Our strategy to vectorize this problem will be to end-to-end concatenate the # number of draws we need. Then we’ll loop over the pieces. d = tfp.distributions.Normal(loc=mean_height, scale= 1. / std_height) x = d.sample(np.sum(population_))\n\n\naverage_across_county = [] seen = 0 for p in population_: average_across_county.append(tf.reduce_mean(x[seen:seen+p])) seen += p average_across_county_full = tf.stack(average_across_county)\n\n\n##located the counties with the apparently most extreme average heights. [ average_across_county_, i_min, i_max] = evaluate([ average_across_county_full, tf.argmin( average_across_county_full ), tf.argmax( average_across_county_full )])\n\n\n#plot population size vs. recorded average plt.scatter( population_, average_across_county_, alpha = 0.5, c=TFColor[6]) plt.scatter( [ population_[i_min], population_[i_max] ], [average_across_county_[i_min], average_across_county_[i_max] ], s = 60, marker = “o”, facecolors = “none”, edgecolors = TFColor[0], linewidths = 1.5, label=“极端的高度”)\n\n\nplt.xlim( smallest_population, largest_population ) plt.title( “平均高度 vs. 县人口”) plt.xlabel(“县人口”) plt.ylabel(“县的平均身高”) plt.plot( [smallest_population, largest_population], [mean_height, mean_height], color = “k”, label = “true expected\nheight”, ls=“–” ) plt.legend(scatterpoints = 1); ```\n\n\n\n\n\n我们观察到了什么？ 如果不考虑人口规模 我们冒着造成巨大推理错误的风险：如果我们忽略了人口规模，我们会说最短和最高个体的县已被正确圈出。但由于以下原因，这种推断是错误的。这两个县不一定具有最极端的高度。计算出的较小种群的平均值不能很好地反映出人口的真实预期价值（事实上应该是$ = 150 $）。样本大小/人口规模/ $ N $，无论你想要什么，它都太小了，无法有效地调用大数定律。\n\n\n我们提供了更多反对这种推论的证据。回想一下，人口数量均匀分布在100到1500之间。我们的直觉应该告诉我们，人口最高极度的县也应该统一分布在100到1500之间，当然也不依赖于该县的人口。不是这样。以下是具有最极端高度的县的人口规模。\n\n\npython print(\"10个最少县的人口规模：\") print(population_[ np.argsort( average_across_county_ )[:10] ], '\\n') print(\"10个最高县的人口规模： \") print(population_[ np.argsort( -average_across_county_ )[:10] ])\n\n\n10个最少县的人口规模： [160 134 280 129 207 176 411 256 247 176]\n\n\n10个最高县的人口规模： [113 127 258 362 185 224 478 310 148 312]\n\n\n在100到1500之间根本没有统一。这是大数定律的绝对失败。\n\n\n### 示例：Kaggle的美国。人口普查退货率挑战\n\n\n以下是2010年美国人口普查的数据，该数据将县以外的人口划分为街区集团（城市街区或同等城市的集合）。这个数据集来自Kaggle机器学习竞赛，一些同事和我参与了。目的是使用人口普查变量（中位数收入，女性人数）预测一组群体的人口普查信件回邮率，测量值在0到100之间。街区集团，拖车停车场数量，平均儿童人数等）。下面我们绘制人口普查邮件回复率与块组人口的关系：\n\n\n```python reset_sess()\n\n\nimport wget url = ‘https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter4_TheGreatestTheoremNeverTold/data/census_data.csv’ filename = wget.download(url) filename ```\n\n\n‘census_data.csv’\n\n\n```python plt.figure(figsize(12.5, 6.5)) data_ = np.genfromtxt( “census_data.csv”, skip_header=1, delimiter= “,”) plt.scatter( data_[:,1], data_[:,0], alpha = 0.5, c=TFColor[6]) plt.title(“人口普查邮寄回邮率与人口”) plt.ylabel(“邮寄回复率”) plt.xlabel(“块组人口”) plt.xlim(-100, 15e3 ) plt.ylim( -5, 105)\n\n\ni_min = tf.argmin( data_[:,0] ) i_max = tf.argmax( data_[:,0] )\n\n\n[ i_min_, i_max_ ] = evaluate([ i_min, i_max ])\n\n\nplt.scatter( [ data_[i_min_,1], data_[i_max_, 1] ], [ data_[i_min_,0], data_[i_max_,0] ], s = 60, marker = “o”, facecolors = “none”, edgecolors = TFColor[0], linewidths = 1.5, label=“最极端的点”)\n\n\nplt.legend(scatterpoints = 1); ```\n\n\n\n\n\n以上是统计学中的经典现象。我说经典指的是上面散点图的“形状”。它遵循经典的三角形形式，随着我们增加样本大小而紧缩（随着大数定律变得更加精确）。\n\n\n我可能会过分强调这一点，也许我应该把这本书命名为“你没有大数据问题！”，但这里再次举例说明小数据集的问题，而不是大数据集。简单地说，使用大数定律不能处理小数据集。与对大数据集（例如大数据）毫不费力地应用定理相比较。我之前提到矛盾的是，大数据预测问题是通过相对简单的算法解决的。通过理解大数定律创建稳定的解决方案，即加入或减少一些数据点不会对解决方案产生太大影响，可以部分解决悖论。另一方面，向小型数据集添加或删除数据点可能会产生截然不同的结果。\n\n\n为了进一步阅读大数定律的隐患，我强烈推荐优秀的手稿最危险的方程式\n\n\n### Example: 计算Github星\n\n\nGithub存储库的平均星数是多少？你怎么算这个？有超过600万个存储库，因此有足够的数据来调用大数定律。让我们开始提取一些数据。\n\n\n```python reset_sess()\n\n\nimport wget url = ‘https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/data/github_data.csv’ filename = wget.download(url) filename ```\n\n\n‘github_data (1).csv’\n\n\n```python # Github data scrapper # See documentation_url: https://developer.github.com/v3/\n\n\nfrom json import loads import datetime import numpy as np from requests import get\n\n\n““” variables of interest: indp. variables - language, given as a binary variable. Need 4 positions for 5 langagues - #number of days created ago, 1 position - has wiki? Boolean, 1 position - followers, 1 position - following, 1 position - constant\n\n\ndep. variables -stars/watchers -forks ““”\n\n\nMAX = 8000000 today = datetime.datetime.today() randint = np.random.randint N = 10 #sample size. auth = (“zhen8838”, “zqh19960305” )\n\n\nlanguage_mappings = {“Python”: 0, “JavaScript”: 1, “Ruby”: 2, “Java”:3, “Shell”:4, “PHP”:5}\n\n\n#define data matrix: X = np.zeros( (N , 12), dtype = int )\n\n\nfor i in range(N): is_fork = True is_valid_language = False\n\n\nwhile is_fork == True or is_valid_language == False: is_fork = True is_valid_language = False\n\n\nparams = {“since”:randint(0, MAX ) } r = get(“https://api.github.com/repositories”, params = params, auth=auth ) results = loads( r.text )[0] #im only interested in the first one, and if it is not a fork. # print(results) is_fork = results[“fork”]\n\n\nr = get( results[“url”], auth = auth)\n\n\n#check the language repo_results = loads( r.text ) try: language_mappings[ repo_results[“language” ] ] is_valid_language = True except: pass\n\n\n#languages X[ i, language_mappings[ repo_results[“language” ] ] ] = 1\n\n\n#delta time X[ i, 6] = ( today - datetime.datetime.strptime( repo_results[“created_at”][:10], “%Y-%m-%d” ) ).days\n\n\n#haswiki X[i, 7] = repo_results[“has_wiki”]\n\n\n#get user information r = get( results[“owner”][“url”] , auth = auth) user_results = loads( r.text ) X[i, 8] = user_results[“following”] X[i, 9] = user_results[“followers”]\n\n\n#get dep. data X[i, 10] = repo_results[“watchers_count”] X[i, 11] = repo_results[“forks_count”] print() print(” ————– “) print(i,”: “, results[”full_name”], repo_results[”language” ], repo_results[”watchers_count”], repo_results[”forks_count”]) print(” ————– “) print()\n\n\nnp.savetxt(“github_data.csv”, X, delimiter=“,”, fmt=“%d” ) ```\n\n\n————– 0 : hbradlow/autograde Python 0 0 ————–\n\n\n————– 1 : pkellett/test JavaScript 2 0 ————–\n\n\n————– 2 : sputnikus/cmdoro Python 0 0 ————–\n\n\n————– 3 : theteam/vagrant-django-template Python 36 12 ————–\n\n\n————– 4 : contra/JMOT Java 17 9 ————–\n\n\n————– 5 : jcDesigns99/sample_app Ruby 1 0 ————–\n\n\n————– 6 : tbarho/base_app Ruby 1 0 ————–\n\n\n————– 7 : lvh/txscrypt Python 6 1 ————–\n\n\n————– 8 : Xand0r/Treebook JavaScript 1 0 ————–\n\n\n————– 9 : wingertge/ThumbsApplyGroupManager Java 1 0 ————–\n\n\n### 结论\n\n\n虽然大数定律很酷，但只有它的名字暗示它才真实：只有大样本量。我们已经看到了如何通过不考虑数据的形状来影响我们的推理。\n\n\n1. 通过（简单的）从后验分布中抽取许多样本，我们可以确保大数定律适用于我们接近期望值（我们将在下一章中进行）。\n\n\n2. 贝叶斯推理理解，对于小样本，我们可以观察到野生随机性。我们的后验分布将通过更广泛而不是紧密集中来反映这一点。因此，我们的推论应该是可纠正的。\n\n\n3. 不考虑样本大小有重大影响，尝试对不稳定的对象进行排序会导致病态排序。上面提供的方法解决了这个问题。\n\n\n##### Exercises\n\n\n1. How would you estimate the quantity \\(E\\left[ \\cos{X} \\right]\\), where \\(X \\sim \\text{Exp}(4)\\)? What about \\(E\\left[ \\cos{X} | X \\lt 1\\right]\\), i.e. the expected value given we know \\(X\\) is less than 1? Would you need more samples than the original samples size to be equally accurate?\n\n\n你如何估计$ E \\(，其中\\) X （4）\\(？或者\\) E \\(，即预期值*给定*我们知道\\) X $小于1？您是否需要比原始样本大小更多的样本才能同样准确？\n\n\n```python ## Enter code here import tensorflow as tf import tensorflow_probability as tfp tfd = tf.distributions\n\n\nreset_sess()\n\n\nexp = tfd.Exponential(rate=4.) N = 10000 X = exp.sample(sample_shape=int(N))\n\n\ne_cos_x=evaluate(tf.reduce_sum(tf.math.cos(X)/N)) e_cos_x_le_1=evaluate(tf.reduce_sum(tf.math.cos(X)*tf.cast(X&lt;1,tf.float32)/N))\n\n\nprint(‘𝐸[cos𝑋]’,e_cos_x) print(‘𝐸[cos𝑋|𝑋&lt;1]’,e_cos_x_le_1) ```\n\n\n𝐸[cos𝑋] 0.9411032 𝐸[cos𝑋|𝑋&lt;1] 0.9354135\n\n\n2. The following table was located in the paper “Going for Three: Predicting the Likelihood of Field Goal Success with Logistic Regression” [2]. The table ranks football field-goal kickers by their percent of non-misses. What mistake have the researchers made?\n\n\n\n\nKicker Careers Ranked by Make Percentage\n\n\n\n\nRank\n\n\nKicker\n\n\nMake %\n\n\nNumber of Kicks\n\n\n\n\n1\n\n\nGarrett Hartley\n\n\n87.7\n\n\n57\n\n\n\n\n2\n\n\nMatt Stover\n\n\n86.8\n\n\n335\n\n\n\n\n3\n\n\nRobbie Gould\n\n\n86.2\n\n\n224\n\n\n\n\n4\n\n\nRob Bironas\n\n\n86.1\n\n\n223\n\n\n\n\n5\n\n\nShayne Graham\n\n\n85.4\n\n\n254\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n51\n\n\nDave Rayner\n\n\n72.2\n\n\n90\n\n\n\n\n52\n\n\nNick Novak\n\n\n71.9\n\n\n64\n\n\n\n\n53\n\n\nTim Seder\n\n\n71.0\n\n\n62\n\n\n\n\n54\n\n\nJose Cortez\n\n\n70.7\n\n\n75\n\n\n\n\n55\n\n\nWade Richey\n\n\n66.1\n\n\n56\n\n\n\n\nIn August 2013, a popular post on the average income per programmer of different languages was trending. Here’s the summary chart: (reproduced without permission, cause when you lie with stats, you gunna get the hammer). What do you notice about the extremes?\n\n\n\nAverage household income by programming language\n\n\n\nLanguage\n\n\nAverage Household Income ($)\n\n\nData Points\n\n\n\n\nPuppet\n\n\n87,589.29\n\n\n112\n\n\n\n\nHaskell\n\n\n89,973.82\n\n\n191\n\n\n\n\nPHP\n\n\n94,031.19\n\n\n978\n\n\n\n\nCoffeeScript\n\n\n94,890.80\n\n\n435\n\n\n\n\nVimL\n\n\n94,967.11\n\n\n532\n\n\n\n\nShell\n\n\n96,930.54\n\n\n979\n\n\n\n\nLua\n\n\n96,930.69\n\n\n101\n\n\n\n\nErlang\n\n\n97,306.55\n\n\n168\n\n\n\n\nClojure\n\n\n97,500.00\n\n\n269\n\n\n\n\nPython\n\n\n97,578.87\n\n\n2314\n\n\n\n\nJavaScript\n\n\n97,598.75\n\n\n3443\n\n\n\n\nEmacs Lisp\n\n\n97,774.65\n\n\n355\n\n\n\n\nC#\n\n\n97,823.31\n\n\n665\n\n\n\n\nRuby\n\n\n98,238.74\n\n\n3242\n\n\n\n\nC++\n\n\n99,147.93\n\n\n845\n\n\n\n\nCSS\n\n\n99,881.40\n\n\n527\n\n\n\n\nPerl\n\n\n100,295.45\n\n\n990\n\n\n\n\nC\n\n\n100,766.51\n\n\n2120\n\n\n\n\nGo\n\n\n101,158.01\n\n\n231\n\n\n\n\nScala\n\n\n101,460.91\n\n\n243\n\n\n\n\nColdFusion\n\n\n101,536.70\n\n\n109\n\n\n\n\nObjective-C\n\n\n101,801.60\n\n\n562\n\n\n\n\nGroovy\n\n\n102,650.86\n\n\n116\n\n\n\n\nJava\n\n\n103,179.39\n\n\n1402\n\n\n\n\nXSLT\n\n\n106,199.19\n\n\n123\n\n\n\n\nActionScript\n\n\n108,119.47\n\n\n113\n\n\n\n\n\nReferences\n\nWainer, Howard. The Most Dangerous Equation. American Scientist, Volume 95.\nClarck, Torin K., Aaron W. Johnson, and Alexander J. Stimpson. “Going for Three: Predicting the Likelihood of Field Goal Success with Logistic Regression.” (2013): n. page. Web. 20 Feb. 2013.\nhttp://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function"
  },
  {
    "objectID": "posts/tiramisu.html",
    "href": "posts/tiramisu.html",
    "title": "Tiramisu Compiler Internals",
    "section": "",
    "text": "Tiramisu 是一个基于polyhedral的DL compiler, 通过探索他的实现细节来学习如何利用polyhedral compilation并整合到他的dsl中的."
  },
  {
    "objectID": "posts/tiramisu.html#tiramisu-schedule-commands",
    "href": "posts/tiramisu.html#tiramisu-schedule-commands",
    "title": "Tiramisu Compiler Internals",
    "section": "Tiramisu Schedule Commands",
    "text": "Tiramisu Schedule Commands\n假设C和P为computation,b为buffer,i/j为迭代变量.来说明一些调度命令：\n\n循环变化\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nC.tile(i,j,t1,t2, i0,j0,i1,j1)\n使用t1 × t2来tile C的循环 (i, j) ,其中i0为最外层循环,j1为最内侧循环\n\n\nC.interchange(i, j)\n交换C的循环 i ,j\n\n\nC.shift(i, s)\n偏移循环i ｜\n\n\nC.split(i, s, i0, i1)\n循环分块\n\n\nP.compute_at(C, j)\n定义计算发生的位置,不合理的调度可能会引起冗余计算.\n\n\nC.unroll(i, v)\n根据参数v展开循环i\n\n\nC.after(B, i)\n指定循环i中C发生在B之后\n\n\nC.inline()\ninline C到他所有的消费者处\n\n\nC.set_schedule()\n直接通过isl语法指定C的 affine 关系（可以表示layer I和II）\n\n\n\n\n硬件映射\n\n\n\n\nCommand\nDescription\n\n\n\n\nC.parallelize(i)\n在共享内存的系统中并行化执行循环i\n\n\nC.vectorize(i, v)\n通过vector size v向量化循环i\n\n\nC.gpu(i0, i1, i2, i3)\n标记循环i0, i1, i2, i3 在GPU上执行\n\n\nC.tile_gpu(i0,i1,t1,t2)\nTile 循环 i0 and i1 并映射到 GPU.\n\n\nC.distribute(i)\n在分布式内存系统上并行话循环i.\n\n\n\n\n高层数据操作\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nC.store_in(b,{i, j})\n将computation C(i,j) 输出存在buffer b[i,j].\n\n\nC.cache_shared_at(P,i)\nCache (copy) C 的buffer 到 shared memory. 从 global 到 shared GPU memory会发生在循环i的P操作中, 这个过程中的数据访问和同步会自动完成.\n\n\nC.cache_local_at(P, i)\n类似上一个但是存储在 local GPU memory.\n\n\nsend(d, src, s, q, p)\n构造send操作. d: 迭代位置向量; src: 源buffer; s: size; q: 目标节点; p: 属性 (synchronous, asynchronous, blocking, …).\n\n\nreceive(d,dst,s,q,p)\n构造receive操作.\n\n\n\n\n底层数据操作\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nBuffer b(sizes, type)\n声明 buffer (sizes: buffer维度).\n\n\nb.allocate_at(p, i)\n在循环i的计算P处添加allocate操作\n\n\nC.buffer()\n返回 C所关联的buffer.\n\n\nb.set_size(sizes)\n设定buffer的维度\n\n\nb.tag_gpu_global()\n设定buffer位于global GPU memory.\n\n\nb.tag_gpu_shared()\n设定buffer位于shared GPU memory.\n\n\nb.tag_gpu_local()\n设定buffer位于local GPU memory.\n\n\nb.tag_gpu_constant()\n设定buffer位于constant GPU memory.\n\n\nC.host_to_device()\ncopy C.buffer() 从 host 到 device.\n\n\nC.device_to_host()\ncopy C.buffer() 从 device 到 host.\n\n\ncopy_at(p, i, bs, bd)\n在循环i的操作P中copy bs 到 bd .可以在 global, shared and local中进行.\n\n\n\n\n同步操作\n\n\n\n\nCommand\nDescription\n\n\n\n\nbarrier_at(p,i)\n在循环i的操作p处构造屏障"
  },
  {
    "objectID": "posts/tiramisu.html#tiramisu-multi-layer-ir",
    "href": "posts/tiramisu.html#tiramisu-multi-layer-ir",
    "title": "Tiramisu Compiler Internals",
    "section": "Tiramisu Multi-Layer IR",
    "text": "Tiramisu Multi-Layer IR\n当前大多数IR都使用内存在程序语句之间进行通信. 这会在程序中创建基于内存的依赖关系,并迫使编译器在决定优化并映射到硬件之前选择数据布局. 为不同的硬件体系结构优化程序通常需要修改数据布局并消除基于内存的依赖关系,因为它们限制了优化. 因此,必须撤消在调度之前指定的任何数据布局,以允许更多的调度自由度,并且必须调整代码以使用最适合目标硬件的数据列表. 应用这些数据变换并消除基于内存的依赖性很具有挑战性.\n举个例子, 将buffer传输到GPU上的shared和local memory. 需要复制到shared memory以及何时执行同步的数据量都取决于代码的优化方式(例如, 代码是否具有两级tile). 同样的适用于决定生成分布式代码时要发送或接收的数据量. 因此, buffer传输到内存层次结构以及通信管理和同步不应在调度之前决定.\nTiramisu通过使用架构无关算法、循环变换、数据布局、通信等完全分开的多层IR来解决代码生成中的这些复杂问题. 1. 第一层表示使用无存储位置的生产者-消费者关系描述了纯算法. 2. 第二层指定计算顺序, 以及处理器计算每个值的计算顺序, 该层适合执行大量优化, 而无需处理具体的内存布局. 3. 第三层指定在消耗中间数据之前存储中间数据的位置. 4. 第四层添加了所有必要的通信和同步操作.\n分层的设计决定了应用优化的特定顺序, 并确保编译器在给定层中通过不必担心修改或撤消早期层中做出的决策. 例如, 指定计算顺序及其发生的阶段可以安全地假设不需要数据划分转换. 这个简单的假设使Tiramisu无需考虑调度是否会影响数据布局.\n\n\n\nOverview\n\n\n通过下面一段简单的例子来说明每层IR内部的表示形式:\nimport tiramisu as tm\ntm.init(\"test\")\n\nN = 100\nM = 200\nC = 3\n\ninput = tm.input(\"input\", [\"i\", \"j\", \"c\"], [N, M, C], tm.primitive_t.p_float32)\n\ni, j, c = tm.var(\"i\", 0, N - 2), tm.var('j', 0, M - 2), tm.var('c', 0, 3)\n\nd = tm.expr(3).cast(tm.primitive_t.p_float32)\nbx = tm.computation('bx', [i, j, c], (input[i, j, c] + input[i, j + 1, c] + input[i, j + 2, c]) / d)\nby = tm.computation('by', [i, j, c], (bx[i, j, c] + bx[i + 1, j, c] + bx[i + 2, j, c]) / d)\ntiramisu内部使用presubger set来表示.\n\n第一层: 算法抽象\n\n直接通过简单的表达式之间的生产消费关系来定义,比如by的表示为:\n\\[\n\\begin{align}\n{by(i, j, c) : 0 ≤ i &lt; N − 2 ∧ 0 ≤ j &lt; M − 2 ∧ 0 ≤ c &lt; 3} : \\\\\n(bx(i, j, c) + bx(i + 1, j, c) + bx(i + 2, j, c))/3\n\\end{align}\n\\]\n其中第一部分指定计算的迭代域,第二部分为计算的表达式. 第一部分声明了i,j,c的循环顺序,但是循环的执行顺序却并没有指定.\n\n第二层: 计算管理\n\n定义计算的顺序以及处理器编号等. 这里不指定中间变量如何存储在内存中, 这样执行优化操作的就不需要考虑如何去修改内存布局. 从第一层转到第二层只需要使用调度指令即可:\nby.gpu_tile(i, j, 32, 32, i0, j0, i1, j1)\nbx.compute_at(by, j0)\nbx.cache_shared(by, j0)\n调度后内部的IR表示如下: \\[\n\\begin{align}\n{ by(1, i0(gpuB), j0(gpuB), i1(gpuT ), j1(gpuT), c) : \\\\\ni0 = floor(i/32)∧j0 = floor(j/32)∧i1 = i\\%32∧j1 = j\\%32∧0 ≤ i &lt; N −2∧0 ≤ j &lt; M −2∧0 ≤ c &lt; 3} : \\\\\n(bx(i0*32+i1,j0*32+j1,c)+ bx(i0*32+i1+1, j0*32+j1, c)+bx(i0*32+i1+2, j0*32+j1, c))/3\n\\end{align}\n\\]\n此时这一层中的Computations被排序以及分配到特定的处理器. 排序可以被分为时间维度和空间维度, 其中时间维度指的是当前计算相对于其他计算的执行顺序,是通过一个词典序的set来表示的; 空间维度指的是计算发生在哪一个处理器, 同时处理器位置则是通过tag表示(目前支持cpu,node,gpuB,gpuT).\n\n第三层: 数据管理\n\n这一层通过指定中间变量如何存储来确定数据布局. 任何必要的buffer分配和释放都在这个级别构造, 转换到本层也是通过使用数据调度命令自动转换, 本层还可以指定数据存储的内存层级位置.\n本层表示由第二层的IR组成, 添加buffer申请/释放语句, 以及对数组读写的access relation(其中标量也被看作单元素数组). 对于每个buffer,需要指定他们申请的大小和类型.\n有时候数据映射可以很复杂,比如存储的时候减少循环的维度,或者可以展平, 这个时候我们也只需要提供复杂的映射函数即可, 比如将计算c(i, j)映射到c(i%2, j%2)或者c(j, i)上.\n比如指定bx,by的数据布局:\nbx.store_in({c, i, j})\nby.store_in({c, i, j})\n可以得到如下的affine relation表示:\n\\[\n\\begin{align}\n  {by(1, i0(gpuB), j0(gpuB), i1(gpuT ), j1(gpuT ), c) \\rightarrow by[c, i0 * 32 + i1,j0*32+j1] :\\\\\n   i0 = floor(i/32)∧j0 = floor(j/32)∧i1 = i\\%32∧j1 = j\\%32 ∧ 0 ≤ i &lt; N − 2 ∧ 0 ≤ j &lt; M − 2 ∧ 0 ≤ c &lt; 3}\n\\end{align}\n\\]\n\n第四层: 通信管理\n\n这一层添加同步/通信操作到IR中, 并将这些操作映射到时间顺序维度, 并且在buffer分配和释放的时候实例化. 这一层也是通过调度命令来进行转换的."
  },
  {
    "objectID": "posts/tiramisu.html#数据结构",
    "href": "posts/tiramisu.html#数据结构",
    "title": "Tiramisu Compiler Internals",
    "section": "数据结构",
    "text": "数据结构\nexpr的数据结构和halide类似, 主要用于表示每个statement中的运算表达式. 可以用于表示常量, 变量, 通信/运算/调用操作."
  },
  {
    "objectID": "posts/tiramisu.html#内部执行流程",
    "href": "posts/tiramisu.html#内部执行流程",
    "title": "Tiramisu Compiler Internals",
    "section": "内部执行流程",
    "text": "内部执行流程\n\n1. 定义输入\n下面就通过python执行和内部的log进行分析. 首先我们定义一个input:\nN = 100\nM = 200\nC = 3\n\ninput = tm.input(\"input\", [\"i\", \"j\", \"c\"], [N, M, C], tm.primitive_t.p_float32)\n在上面一节的数据结构定义input是继承自computation, 因此所调用的构造函数还是computation. 给到computation的迭代变量即为自动构造的i,j,c, 将三个迭代变量构造出对应的迭代域{input[i, j, c] : 0&lt;=i&lt;100 and 0&lt;=j&lt;200 and 0&lt;=c&lt;3}.\n然后有一个gen_identity_schedule_for_iteration_domain将原始的迭代域进行扩展为{ input[i, j, c] -&gt; input[0, 0, i' = i, 0, j' = j, 0, c' = c, 0] : 0 &lt;= i &lt;= 99 and 0 &lt;= j &lt;= 199 and 0 &lt;= c &lt;= 2 }的形式, 并赋值到computation的schedule.\n其实就是2d+1的执行顺序表示, 其中对应关系如下:\n* Loop level               :    -1         0      1      2\n* Schedule dimension number:        0, 1   2  3   4  5   6  7\n* Schedule:                        [0, 0, i1, 0, i2, 0, i3, 0]\n而后设定当前computation的表达式, 因为是输入, 因此表达式为空.\n接下来再构造出unique的名字代替原来的变量名, 以避免用户起的变量名出现冲突的情况, 然后修改schedule的维度名字为:{ input[t1, t2, t3] -&gt; input[0, 0, i = t1, 0, j = t2, 0, c = t3, 0] : 0 &lt;= t1 &lt;= 99 and 0 &lt;= t2 &lt;= 199 and 0 &lt;= t3 &lt;= 2 }\n最后是检测到当前 computation具备有效的边界值, 自动构造了buffer,并将当前computationstore_in到这个buffer中.\ninput log\n\n\n2. 定义计算\ni, j, c = tm.var(\"i\", 0, N - 2), tm.var('j', 0, M - 2), tm.var('c', 0, 3)\n\nd = tm.expr(3).cast(tm.primitive_t.p_float32)\nbx = tm.computation('bx', [i, j, c], (input[i, j, c] + input[i, j + 1, c] + input[i, j + 2, c]) / d)\nby = tm.computation('by', [i, j, c], (bx[i, j, c] + bx[i + 1, j, c] + bx[i + 2, j, c]) / d)]\n定义bx和by也是类似同上的逻辑, 不过这里因为存在了表达式, 所以多了一步递归的将表达式转换为非仿射的访问关系.\ncomputation log\n\n\n3. 调度计算\ntiramisu的调度都被记录在function中, function更像是所有computation的聚合. 与halide不同, 这里一份代码中只有一个全局function, 许多调度默认操作这个全局变量.\n\n接下来我们看一下function是如何记录调度操作:\n\nafter\n\nbx.after(by, j)\n首先是找到迭代变量j所对应的循环层级为1, 然后在sched_graph添加by指向bx的连接, 同时将循环层级设置为1.\n[src/tiramisu_core.cpp:2354 after]\n|   [src/tiramisu_core.cpp:2728 get_loop_level_numbers_from_dimension_names]\n|   |   Searching for the dimension j\n|   |   Searching in the range of  { bx[t5, t6, t7] -&gt; bx[0, 0, i = t5, 0, j = t6, 0, c = t7, 0] : 0 &lt;= t5 &lt;= 97 and 0 &lt;= t6 &lt;= 197 and 0 &lt;= t7 &lt;= 2 }\n|   |   Corresponding loop level is 1\n|   |   Checking the validity of loop level 1\n|   The loop level that corresponds to j is 1\n|   [src/tiramisu_core.cpp:2374 after]\n|   |   Scheduling bx to be executed after by at level 1\n|   |   sched_graph[by, bx] = 1\n\ninterchange\n\nbx.interchange(i, j)\n交互循环, 对于基于isl的ir来说非常简单. 首先获取两个迭代变量所对应的循环层级,然后构造一个transformation map, 然后对computation的schedule进行apply range即可完成变换. 具体的细节如下:\ninterchange log\n\nshift\n\nbx.shift(i, 10)\n同样也是找到对应的循环维度然后对实施变换即可.\nshift log\n\nsplit\n\ni0,  i1 = tm.var(tm.primitive_t.p_int32, \"i0\"), tm.var(tm.primitive_t.p_int32, \"i1\")\nbx.split(i, 16, i0, i1)\n获取循环维度后同样构造Transformation map, 首先是通过当前参数构造Origin Transformation map的字符串{bx[t15,t16,t17,t18,t19,t20,t21,t22] -&gt; bx[t15,t16,t12, t13, t14,t18,t19,t20,t21,t22] : t15 = 0 and t12 = floor(t17/16) and t14 = (t17%16) and t13 = 0}, 然后使用isl自动构造出优化后的Transformation map :{ bx[t15 = 0, t16, t17, t18, t19, t20, t21, t22] -&gt; bx[t15' = 0, t16' = t16, t12, t13 = 0, t14, t18' = t18, t19' = t19, t20' = t20, t21' = t21, t22' = t22] : (-t17 + t14) mod 16 = 0 and -15 + t17 &lt;= 16t12 &lt;= t17 and 0 &lt;= t14 &lt;= 15 }, 再应用变换到schedule, 最后再把变换后的schedule中维度的名字进行修改.\nsplit log\n\nunroll/vectorize/parallelize/distribute\n\n作者为了通用化起见,其实这这些调度都走了同一的逻辑, 都是按一定约束系数分离出原始的计算,然后把循环变量保存到对应的列表中. 这里就以unroll为例子(按作者的代码实现看,他在论文中应该把unroll的调度放到IR的第二层才对):\nbx.unroll(c, 3)\n首先拿到循环维度, 然后通过separateAndSplit函数检查当前unroll的大小是否可以保证被split. 如果可以被split, 循环维度将扩展两个, 否则只扩展一个. 在separateAndSplit函数中, 生成time_space_domain, 也就是获得实际执行的order, 然后获取maximal_AST_depth为后续使用. 再收集当前循环维度的上下界, 这里收集到的上下界分别是2和0, 总次数即(2-0+1). 接下来就可以调用separate函数, 将原始的computation分离两部分, 一部分是i &lt; v * floor(loop_bound/v)另一部分为i &gt;= v * floor(loop_bound/v), 称前面部分为full computation后面为separated computation. 在这个例子中,separated computation为空, 因此并不会添加新的computation, 但是之前所构造的约束还是要通过add_schedule_constraint函数添加到原始的computation中. 后续的的split_with_lower_bound也因为没有分离所以并没有实际的操作.\n中间有一步tag_unroll_level就是将当前循环变量保存到对应的列表中, 同样vectorize/parallelize/distribute则是保存到不同的列表中.\nunroll log\n\ngpu_tile\n\ni0, j0, i1, j1 = tm.var(tm.primitive_t.p_int32, \"i0\"), tm.var(tm.primitive_t.p_int32, \"j0\"), tm.var(\n    tm.primitive_t.p_int32, \"i1\"), tm.var(tm.primitive_t.p_int32, \"j1\")\nby.gpu_tile(i, j, 32, 32, i0, j0, i1, j1)\n看代码实现上, 其实gpu tile就是调用两次split并交换循环维度, 然后将循环维度分别映射到block/thread上, 我理解这个scheudle应该也是可以放到IR的第一层中.\ngpu_tile log\n\ncompute_at\n\n这个调度我是在gpu tile之后继续进行的, 因为在不同的循环层级去计算, 需要考虑数据依赖等问题, 所以这个调度实现就比较复杂.\nbx.compute_at(by, j0)\ncompute_at函数假设by作为消费者消费由bx(生产者)产生的值. 根据消费者的每个唯一值按需计算此computation, 此时by消费的值在bx中的循环维度计算, 并与消费者处于同一循环嵌套中. 如果消费者by需要冗余计算, 则调度会创建必要的冗余计算并在消费者之前对其进行调度.\n此函数会执行: - 按需在消费者之前调度此计算的执行 - 如果需要冗余计算此计算,则创建冗余计算\n此函数不执行: - 不为此计算创建任何数据映射. 用户需要为此计算提供访问关系, 就像对任何其他正常计算所做的那样. - 不为此计算分配任何缓冲区. 用户需要声明一个缓冲区以存储此计算的结果.\n如果此函数创建了计算的副本,则用户不需要设置其访问关系, 复制的计算将自动具有与原始计算相同的访问关系.此函数不返回用于操作副本计算的句柄, 它不允许用户自由操作副本. 副本将自动安排在消费者之前执行.\n下面开始正式的调度流程:\n\n通过compute_needed_and_produced计算producer(即this)和consumer之间的needed关系,也就是consumer需要读取producer的哪些数据\n\n获取producer_domain, producer_sched以及consumer_domain,consumer_sched\n根据consumer的expression得到对于producer的访问关系consumer_accesses, 并过滤其中无效的部分得到 { by[i, j, c] -&gt; bx[i', j' = j, c' = c] : i &gt;= 0 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 and i &lt;= i' &lt;= 97 and i' &lt;= 2 + i }\n获取producer_domain,consumer_domain到time-processor上, 也就是apply对应的sched.\n将consumer_accesses也转换到time-processor上, 也就是对consumer_accesses的domain应用producer_sched, 他的range应用consumer_sched\n对齐上述集合和关系的维度\n在consumer_domain上应用consumer_accesses, 得到needed, 即为consumer需要读取的producer的数据集合: [t40, t41] -&gt; { bx[0, 0, i, 0, j, 0, c, 0, t38 = 0, 0, t39 = 0, 0] : t40 &gt;= 0 and 32t40 &lt;= i &lt;= 97 and i &lt;= 33 + 32t40 and j &gt;= 32t41 and 0 &lt;= j &lt;= 197 and j &lt;= 31 + 32t41 and 0 &lt;= c &lt;= 2 and -32 - 32t41 - j &lt;= 32*floor((-1 - j)/32) &lt;= 165 - 32t41 - j }\n返回needed和producer_domain(此时的producer_domain表示是produced数据集合) :[t40, t41] -&gt; { bx[0, 0, i = t40, 0, j = t41, 0, c, 0, t38 = 0, 0, t39 = 0, 0] : 0 &lt;= t40 &lt;= 97 and 0 &lt;= t41 &lt;= 197 and 0 &lt;= c &lt;= 2 }\n\n计算producer本身的迭代域produced\n计算missing = needed - produced(减号表示difference), 即consumer需要但producer没有produce的部分(这个例子中,missing的区域被分为了两块):[t40, t41] -&gt; { bx[0, 0, i, 0, j, 0, c, 0, t38 = 0, 0, t39 = 0, 0] : t40 &gt;= 0 and 32t40 &lt;= i &lt;= 97 and i &lt;= 33 + 32t40 and j &gt;= 32t41 and t41 &lt; j &lt;= 197 and j &lt;= 31 + 32t41 and 0 &lt;= c &lt;= 2 and -32 - 32t41 - j &lt;= 32*floor((-1 - j)/32) &lt;= 165 - 32t41 - j; bx[0, 0, i, 0, j = 0, 0, c, 0, t38 = 0, 0, t39 = 0, 0] : t41 = 0 and t40 &gt;= 0 and i &gt;= 32t40 and t40 &lt; i &lt;= 97 and i &lt;= 33 + 32t40 and 0 &lt;= c &lt;= 2 and 32*floor((-1)/1) &lt; 0 }\n如果missing不为空, 那么开始对missing进行操作\n首先使用get_shift_degrees对missing计算位移度\n\n首先遍历missing集合的每个循环层级, 这里是循环1, 对应维度则是2和4.\n然后将missing集合投影到除当前维度之外的所有维度, 这里的实现是通过isl_set_project_out删除从[0,dim)和[1,max_dim-1)两个部分,从而获得只有当前维度的投影, 比如投影维度为2则得到:[t40, t41] -&gt; { [i] : t40 &gt;= 0 and 32t40 &lt;= i &lt;= 97 and i &lt;= 33 + 32t40 and ((0 &lt;= t41 &lt;= 6) or (t41 = 0 and i &gt; t40)) }\n对投影后的集合,假设其只剩下当前维度,并且形式为:[T0]-&gt;{[i0]: i0 = T0 + c}且c是常量, 那么-c就是当前维度的移位度(可以理解成shift的offset)\n将当前循环层级的位移度push到列表中\n\n循环的将参数替换为existential variables并消除他们.\n\n存在变量是代码在程序分析和优化中, 引入新的变量来表示computation中的某个未知变量, 从而可以在不改变原始computation语义的前提下, 进行变换和优化\n遍历missing的参数遍历, 都加上前缀p.\n遍历获取missing的每个basic set, 比如当前的basic set: [t40, t41] -&gt; { bx[0, 0, i, 0, j, 0, c, 0, t38 = 0, 0, t39 = 0, 0, pt40, pt41] : t40 &gt;= 0 and 32t40 &lt;= i &lt;= 97 and i &lt;= 33 + 32t40 and j &gt;= 32t41 and t41 &lt; j &lt;= 197 and j &lt;= 31 + 32t41 and 0 &lt;= c &lt;= 2 and -32 - 32t41 - j &lt;= 32*floor((-1 - j)/32) &lt;= 165 - 32t41 - j }, 其中最后两个维度pt40, pt41就是被新加入前缀的变量.\n遍历basic set每个constraint\n检查每个constraint是不是有原始参数[t40,t41]参与.\n如果有参与,比如constraint为([t40, t41] -&gt; { bx[i0, i1, i, i3, j, i5, c, i7, t38, i9, t39, i11, pt40, pt41] : t40 &gt;= 0 }), 那么修改constraint为中用到的变量为加过前缀的变量:([t40, t41] -&gt; { bx[i0, i1, i, i3, j, i5, c, i7, t38, i9, t39, i11, pt40, pt41] : pt40 &gt;= 0 })\n将所有constraint合并起来后, project out掉之前修改过的变量维度, 也就是[pt40, pt41]\n\n最终原始missing从[t40, t41] -&gt; { bx[0, 0, i, 0, j, 0, c, 0, t38 = 0, 0, t39 = 0, 0] : t40 &gt;= 0 and 32t40 &lt;= i &lt;= 97 and i &lt;= 33 + 32t40 and j &gt;= 32t41 and t41 &lt; j &lt;= 197 and j &lt;= 31 + 32t41 and 0 &lt;= c &lt;= 2 and -32 - 32t41 - j &lt;= 32*floor((-1 - j)/32) &lt;= 165 - 32t41 - j; bx[0, 0, i, 0, j = 0, 0, c, 0, t38 = 0, 0, t39 = 0, 0] : t41 = 0 and t40 &gt;= 0 and i &gt;= 32t40 and t40 &lt; i &lt;= 97 and i &lt;= 33 + 32t40 and 0 &lt;= c &lt;= 2 and 32*floor((-1)/1) &lt; 0 }变为了{ bx[0, 0, i, 0, j, 0, c, 0, t38 = 0, 0, t39 = 0, 0] : 0 &lt;= i &lt;= 97 and 0 &lt; j &lt;= 197 and 0 &lt;= c &lt;= 2 and -32 - j - 32*floor((-1 - j)/32) &lt;= 32*floor((j)/32) &lt;= 165 - j - 32*floor((-1 - j)/32); bx[0, 0, i, 0, j = 0, 0, c, 0, t38 = 0, 0, t39 = 0, 0] : 0 &lt; i &lt;= 97 and 0 &lt;= c &lt;= 2 }\n因为现在missing不为空, 那么by作为consumer就需要bx重新计算出missing的值, 所以接下来需要创建duplicated producer\n\n将此时的missing作为range_constraints_set开始duplicate\nduplicate就是将复制原始的schedule中的并增加第一个维度, 从{ bx[t5, t6, t7] -&gt; bx[0, 0, i = t5, 0, j = t6, 0, c = t7, 0, t38 = 0, 0, t39 = 0, 0] : 0 &lt;= t5 &lt;= 97 and 0 &lt;= t6 &lt;= 197 and 0 &lt;= t7 &lt;= 2 }变为了{ bx[t5, t6, t7] -&gt; bx[1, 0, i = t5, 0, j = t6, 0, c = t7, 0, t38 = 0, 0, t39 = 0, 0] : 0 &lt;= t5 &lt;= 97 and t6 &gt;= 0 and -t5 &lt; t6 &lt;= 197 and 0 &lt;= t7 &lt;= 2 }\n然后copy出原始的computation后将新的schedule赋值到新的computation上\n\n指定duplicated computation -&gt; producer computation -&gt; comsumer computation的执行顺序\n因为此时duplicated computation中的一些计算是发生在指定循环内部的, 但之前copy过来的计算映射是带有指定循环外部的偏移, 也就是之前计算的get_shift_degrees, 此时再将偏移补回去\n\ncompute_at log\n\n\n4. 代码生成\n这里简单的再添加一下host/device数据复制的操作, 然后进行代码生成. 本来我想使用他论文中提到的gpu的方式进行调度, 但可能由于python包装的一些问题, 导致生成有问题, 因此我就使用cpu的数据搬运操作进行模拟.\nN = 100\nM = 200\nC = 3\n\ninput = tm.input(\"input\", [\"i\", \"j\", \"c\"], [N, M, C], tm.primitive_t.p_float32)\n\ni, j, c = tm.var(\"i\", 0, N - 2), tm.var('j', 0, M - 2), tm.var('c', 0, 3)\n\ncp1 = tm.computation('cp1', [i, j, c], input[i, j, c])\nd = tm.expr(3).cast(tm.primitive_t.p_float32)\nbx = tm.computation('bx', [i, j, c], (cp1[i, j, c] + cp1[i, j + 1, c] + cp1[i, j + 2, c]) / d)\nby = tm.computation('by', [i, j, c], (bx[i, j, c] + bx[i + 1, j, c] + bx[i + 2, j, c]) / d)\ncp2 = tm.computation('cp2', [i, j, c], by[i, j, c])\nby.after(bx, j)\ncp1.before(bx, tm.computation.root)\ncp2.after(by, tm.computation.root)\n\ninbuf =  input.get_buffer() # note lifetime problem\noutbuf = cp2.get_buffer()\n\ntm.codegen([inbuf, outbuf], \"func.o\")\n这里调度和构造computation的过程就先不看了, 直接开始看codegen部分:\nvoid tiramisu::function::codegen(const std::vector&lt;tiramisu::buffer *&gt; &arguments, const std::string obj_filename, const bool gen_cuda_stmt, bool gen_python)\n{\n    DEBUG_FCT_NAME(3);\n    this-&gt;set_arguments(arguments);\n    this-&gt;lift_dist_comps();\n    this-&gt;gen_time_space_domain();\n    this-&gt;gen_isl_ast();\n    this-&gt;gen_halide_stmt();\n    this-&gt;gen_halide_obj(obj_filename, gen_python);\n}\n这里作者所分割的流程还是比较清楚的, 我们一个个来看.\n\nset_arguments\n\n将传入的buffers作为function的参数\n\nlift_dist_comps\n\n列出分布式的相关内容, 这里暂时没使用到\n\ngen_time_space_domain (构建时序空间)\n\ngen_ordering_schedules (获取schedule的顺序)\n\ndump_sched_graph, 根据此前before/after的调度时添加的连接关系dump出依赖图 cp1=[root]=&gt;bx ,bx=[1]=&gt;by, by=[root]=&gt;cp2 可以看到是一个比较依赖图, 其中方括号中记录是依赖的循环层级.\n遍历sched graph, 通过调用after_low_level函数来调整computation的执行顺序.\n\n首先将所有的computation的schedule进行对齐, 也就是把schedule中的维度个数进行扩展到相同. 比如{ cp1[t14 = 0] -&gt; cp1[0, 0, t13 = 0, 0] }就被扩展到了{ cp1[t14 = 0] -&gt; cp1[0, 0, t13 = 0, 0, 0, 0, 0, 0] }, 从而匹配bx/by的两级循环.\n接下来修改schedule来实现先后顺序\n\n此时bx需要排在cp1之后, 原本的bx为{ bx[t5, t6, t7] -&gt; bx[0, 0, i = t5, 0, j = t6, 0, c = t7, 0] : 0 &lt;= t5 &lt;= 97 and 0 &lt;= t6 &lt;= 197 and 0 &lt;= t7 &lt;= 2 }, 这里通过增大相同维度词典序(+10)来修改执行顺序,修改后bx为{ bx[t5, t6, t7] -&gt; bx[0, 10, i = t5, 0, j = t6, 0, c = t7, 0] : 0 &lt;= t5 &lt;= 97 and 0 &lt;= t6 &lt;= 197 and 0 &lt;= t7 &lt;= 2 }\nbx修改后,修改by, 原本by为{ by[t9, t10, t11] -&gt; by[0, 0, i = t9, 0, j = t10, 0, c = t11, 0] : 0 &lt;= t9 &lt;= 97 and 0 &lt;= t10 &lt;= 197 and 0 &lt;= t11 &lt;= 2 }, 此时他需要排在bx后面, 那么首先dim 1需要匹配, 同时dim 5需要大于bx因此得到{ by[t9, t10, t11] -&gt; by[0, 10, i = t9, 0, j = t10, 10, c = t11, 0] : 0 &lt;= t9 &lt;= 97 and 0 &lt;= t10 &lt;= 197 and 0 &lt;= t11 &lt;= 2 }\n后续按相同逻辑修改好每个computation的schedule.\n\n\n遍历所有的computation, 调用computation.gen_time_space_domain构建时序空间\n\n直接对当前computation的iter domain应用它的schedule即可得到time_space_domain并返回\n比如{ by[i, j, c] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 } apply schedule: { by[t9, t10, t11] -&gt; by[0, 10, i = t9, 0, j = t10, 10, c = t11, 0] : 0 &lt;= t9 &lt;= 97 and 0 &lt;= t10 &lt;= 197 and 0 &lt;= t11 &lt;= 2 } 得到: { by[0, 10, i, 0, j, 10, c, 0] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }\n\n\n\ngen_isl_ast 构造整个function的isl ast\n\nget_trimmed_time_processor_domain将每个之前schedule中多余的第0维消除掉.\nget_aligned_identity_schedules 获取function内部所有computation的aligned的identity schedule\n\n也就是将他的schedule的domain取出后进行identity映射, 然后再和trimmed_time_processor_domain进行求交集, 比如by的schedule为 { by[0, 10, i, 0, j, 10, c, 0] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }, 它的identity_schedule为{ by[10, i, 0, j, 10, c, 0] -&gt; [10, i' = i, 0, j' = j, 10, c' = c, 0] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 } (这里by的第0维在是与trimmed_time_processor_domain求交集消除的)\n\nrename_computations, 避免名字冲突, 需要给computation添加unique name\n构造isl_ast_build并设定codegen选项\n\natomic_upper_bound=true(表示生成的循环变量在判断上界时只使用一次)\ngroup_coscheduled=true(处理相同位置的两个实例)\n\n为ast build 设定after_each_for以及at_each_domain的callback.\n设定迭代器, 这里他并没有从computation中获取循环名, 而是默认为c0..cn\n将aligned_identity_schedules与trimmed_time_processor_domain进行取domain的交集, 得到Identity schedule intersect trimmed Time-Processor domain : { cp2[20, t16 = 0, 0, t20 = 0, 0, t21 = 0, 0] -&gt; [20, t16' = 0, 0, t20' = 0, 0, t21' = 0, 0]; bx[10, i, 0, j, 0, c, 0] -&gt; [10, i' = i, 0, j' = j, 0, c' = c, 0] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2; cp1[0, t13 = 0, 0, t18 = 0, 0, t19 = 0, 0] -&gt; [0, t13' = 0, 0, t18' = 0, 0, t19' = 0, 0]; by[10, i, 0, j, 10, c, 0] -&gt; [10, i' = i, 0, j' = j, 10, c' = c, 0] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }\n接下来对这个map进行执行isl_ast_build_node_from_schedule_map, 此时会执行到各个callback\n\nafter_each_for的callback为空\nat_each_domain的callback为stmt_code_generator\n\n当前传入的ast node调用get_computation_by_node, 通过ast node的expr, 获得第一个参数的名字得到computation name, 然后再从function获得对应的computation 的vector\nfilter_computations_by_domain, 获得当前build的schedule map domain, 对上一步获得的computation vector进行过滤\n\n假设当前build的ast node的domain为:{ bx[10, i, 0, j, 0, c, 0] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }, 当前的computation vector中存在bx.\n遍历computation vector\n\n获得当前computation的原始domain为:{ bx[i, j, c] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }\ndomain apply 之前trimmed_union_of_schedules得到computation在time space上的domain:{ bx[10, i, 0, j, 0, c, 0] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }\n将当前的build node dmoian与time space上的domain取交集, 如果交集不为空, 那么将当前computation选中放到filtered_comp_vec中.\n\n\n遍历filtered_comp_vec\n\n以当前computation构造出isl id, 并通过isl_ast_node_set_annotation将这个id作为node的标记\n通过get_access_relation_adapted_to_time_processor_domain来获取access(此时的access是lhs, 也就是写入的buffer的访问)\n\n获取原始读写buffer的access为:{ bx[i, j, c] -&gt; _bx_b1[i' = i, j' = j, c' = c] }\n原始的schedule为:{ bx[t5, t6, t7] -&gt; bx[0, 10, i = t5, 0, j = t6, 0, c = t7, 0] : 0 &lt;= t5 &lt;= 97 and 0 &lt;= t6 &lt;= 197 and 0 &lt;= t7 &lt;= 2 }\ntrimmed schedule为: { bx[t5, t6, t7] -&gt; bx[10, i = t5, 0, j = t6, 0, c = t7, 0] : 0 &lt;= t5 &lt;= 97 and 0 &lt;= t6 &lt;= 197 and 0 &lt;= t7 &lt;= 2 }\n将trimmed schedule对access relation进行apply domain得到access function:{ bx[10, i, 0, j, 0, c, 0] -&gt; _bx_b1[i' = i, j' = j, c' = c] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }\n\n通过get_rhs_accesses解析得到的access function (这里是读数据的访问)\n\n获取当前computation的expr, 比如bx原本的表达式为(input[i, j, c] + input[i, j + 1, c] + input[i, j + 2, c]) / d\n递归执行traverse_expr_and_extract_accesses将expr中的accesses提取出来\n\n首先提取到第一个访问的表达式为:input[i, j, c]\n以当前computation的domainbx[i, j, c]构建出Transformation map:{ bx[i, j, c] -&gt; input[i', j', c'] }\n遍历domain的dim和range的dim,如果他们的id相同,那么添加等价约束, 得到:{ bx[i, j, c] -&gt; input[i' = i, j' = j, c' = c] }\n然后依次遍历得到其他两个access relation{ bx[i, j, c] -&gt; input[i' = i, j' = 1 + j, c' = c] }和{ bx[i, j, c] -&gt; input[i' = i, j' = 2 + j, c' = c] }\n\n\n计算iterators map, 假设原来的循环为c[i0, i1], 由于之前重新生成了迭代变量, 所以需要将他们联系起来, 得到类似{ i0 : c0*10+c2, i1: c1*10+c3&gt;}的字典.\n\n获取当前computation的iter domian/schedule\ndomian进行identity之后再使用schedule apply domain得到新的identity: { cp1[0, t13 = 0, 0, t18 = 0, 0, t19 = 0, 0] -&gt; cp1[t13' = 0] (此时map的domain的经过对齐的, 而range则是原始的)\n通过create_isl_ast_index_expression构造取index的isl_ast_expr, 将上一步获取的identity作为access relation传入\n\n拿到原始schedule:{ bx[10, i, 0, j, 0, c, 0] -&gt; [t13 = i, t18 = j, t19 = c] : 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }进行reverse得到: { [t13, t18, t19] -&gt; bx[10, i = t13, 0, j = t18, 0, c = t19, 0] : 0 &lt;= t13 &lt;= 97 and 0 &lt;= t18 &lt;= 197 and 0 &lt;= t19 &lt;= 2 }\n将reversed map转换为pw aff为iterator_map:{ [t13, t18, t19] -&gt; bx[(10), (t13), (0), (t18), (0), (t19), (0)] : 0 &lt;= t13 &lt;= 97 and 0 &lt;= t18 &lt;= 197 and 0 &lt;= t19 &lt;= 2 }\n将access转换为pw aff为index_aff:{ bx[i0, i, i2, j, i4, c, i6] -&gt; bx[(i), (j), (c)] : i0 = 10 and i2 = 0 and i4 = 0 and i6 = 0 and 0 &lt;= i &lt;= 97 and 0 &lt;= j &lt;= 197 and 0 &lt;= c &lt;= 2 }\n将两个pw aff的参数进行对齐得到:{ bx[i0, i, i2, j, i4, c, i6] -&gt; bx[i', j', c'] }和{ [t13, t18, t19] -&gt; bx[o0, i, o2, j, o4, c, o6] }\n然后将两个pw aff进行复合isl_pw_multi_aff_pullback_pw_multi_aff(index_aff,iterator_map)得到{ [t13, t18, t19] -&gt; bx[(t13), (t18), (t19)] : 0 &lt;= t13 &lt;= 97 and 0 &lt;= t18 &lt;= 197 and 0 &lt;= t19 &lt;= 2 }\n通过isl_ast_build_access_from_pw_multi_aff取复合后的pw aff的access表达式得到{ op: access, args: [ { id: bx }, { id: c1 }, { id: c3 }, { id: c5 } ] }就是当前computation的index表达式.\n\n\n得到iterators map 为{ i :  { id: c1 }, j :  { id: c3 }, c :  { id: c5 } }\n\n接下来遍历所有的rhs的access, 为每个rhs 的accesscreate_isl_ast_index_expression:\n\n{ op: access, args: [ { id: _input_b0 }, { id: c1 }, { id: c3 }, { id: c5 } ] }\n{ op: access, args: [ { id: _input_b0 }, { id: c1 }, { op: add, args: [ { id: c3 }, { val: 1 } ] }, { id: c5 } ] }\n{ op: access, args: [ { id: _input_b0 }, { id: c1 }, { op: add, args: [ { id: c3 }, { val: 2 } ] }, { id: c5 } ] }\n\n最终所有的access的index expression为: _bx_b1[c1][c3][c5],  _input_b0[c1][c3][c5],  _input_b0[c1][c3 + 1][c5],  _input_b0[c1][c3 + 2][c5]\n\n然后将当前computation的iterators map以及index expression都保存起来.\n\n\n此时所有的ast node都构造完毕了, 通过isl printer打印最终的ast node为:\n{\n  for (int c1 = 0; c1 &lt;= 97; c1 += 1)\n    for (int c3 = 0; c3 &lt;= 197; c3 += 1)\n      for (int c5 = 0; c5 &lt;= 2; c5 += 1)\n        cp1(0, c1, 0, c3, 0, c5, 0);\n  for (int c1 = 0; c1 &lt;= 97; c1 += 1)\n    for (int c3 = 0; c3 &lt;= 197; c3 += 1) {\n      for (int c5 = 0; c5 &lt;= 2; c5 += 1)\n        bx(10, c1, 0, c3, 0, c5, 0);\n      for (int c5 = 0; c5 &lt;= 2; c5 += 1)\n        by(10, c1, 0, c3, 10, c5, 0);\n    }\n  for (int c1 = 0; c1 &lt;= 97; c1 += 1)\n    for (int c3 = 0; c3 &lt;= 197; c3 += 1)\n      for (int c5 = 0; c5 &lt;= 2; c5 += 1)\n        cp2(20, c1, 0, c3, 0, c5, 0);\n}\n\ngen_halide_stmt, 将isl ast lower到halide的stmt上从而利用halide所提供的codegen功能\n\n调用halide_stmt_from_isl_node函数递归的将isl ast转换为halide stmt, 对于stmt来说主要有以下一些ast类型for,if,block\n对于表达式则是通过之前所构造的isl ast node中所添加的annotation找到对应的computation, 然后通过computation构造出具体的halide expression.\n\n\ncodegen log"
  },
  {
    "objectID": "posts/tqdm-fmt.html",
    "href": "posts/tqdm-fmt.html",
    "title": "tqdm中后缀的添加",
    "section": "",
    "text": "苦于tensorflow中没有好用的训练显示函数,所以我准备用tqdm库显示一下训练过程.既然要显示训练过程中的参数,那肯定要自己对他的默认进度条格式进行修改,所以这里就来说几个使用方式."
  },
  {
    "objectID": "posts/tqdm-fmt.html#l_bar",
    "href": "posts/tqdm-fmt.html#l_bar",
    "title": "tqdm中后缀的添加",
    "section": "l_bar",
    "text": "l_bar\n\n100%|███████████████████████████████████████████| 10/10 [00:01&lt;00:00, 9.96it/s]\n\n进度条的前端,上面的100%即为l_bar,其中l_bar='{desc}: {percentage:3.0f}%|'.desc为描述前缀,可以由desc参数赋值,后面的percentage就是我们看到的100%.\n下面给出一个修改前缀的实例:\nfrom tqdm import trange, tqdm\nfrom random import random, randint\nfrom time import sleep\n\ndef training(epoch: int, step_per_epoch: int):\n    for i in range(epoch):\n        with tqdm(total=step_per_epoch, desc='epoch {}'.format(i), ncols=80) as t:\n            for j in range(step_per_epoch):\n                sleep(.1)\n                t.update()\n\n\ntraining(epoch=3, step_per_epoch=10)\n效果:\nepoch 0: 100%|██████████████████████████████████| 10/10 [00:01&lt;00:00,  9.96it/s]\nepoch 1: 100%|██████████████████████████████████| 10/10 [00:01&lt;00:00,  9.96it/s]\nepoch 2: 100%|██████████████████████████████████| 10/10 [00:01&lt;00:00,  9.95it/s]"
  },
  {
    "objectID": "posts/tqdm-fmt.html#bar",
    "href": "posts/tqdm-fmt.html#bar",
    "title": "tqdm中后缀的添加",
    "section": "bar",
    "text": "bar\n这个就不介绍了,因为进度条的精髓就在这了.并且一般不需要去修改."
  },
  {
    "objectID": "posts/tqdm-fmt.html#r_bar",
    "href": "posts/tqdm-fmt.html#r_bar",
    "title": "tqdm中后缀的添加",
    "section": "r_bar",
    "text": "r_bar\n现在分析r_bar='| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, ' '{rate_fmt}{postfix}]'\n\n100%|████████████████████████████████████████| 10/10 [00:00&lt;00:00, 89813.79it/s]\n\n此处的{n_fmt}/{total_fmt}即为上面的10/10,这个对于我来说是需要的\n此处的{elapsed}&lt;{remaining}即为00:00&lt;00:00,这个对于我来说是不要的\n此处的{rate_fmt}就是9.96it/s,这个也需要的,并且这个还可以进一步的更改\n最后的{postfix}上面没有,是需要自己添加参数来显示的,这里的他可以是任意的\n其实我主要就是要修改他的postfix参数,来随时的显示我的loss,learn rete啥的.\n\n修改后缀实例\n\n例子1\nfrom tqdm import tqdm\nfrom random import random, randint\nfrom time import sleep\n\n\ndef training(epoch: int, step_per_epoch: int):\n    for i in range(epoch):\n        with tqdm(total=step_per_epoch, ncols=80) as t:\n            for j in range(step_per_epoch):\n                t.set_postfix(loss='{:^7.3f}'.format(random()))\n                sleep(0.1)\n                t.update()\n\n\ntraining(epoch=3, step_per_epoch=10)\n\n\n效果1\n100%|███████████████████████████████| 10/10 [00:01&lt;00:00,  9.94it/s, loss=0.279]\n100%|███████████████████████████████| 10/10 [00:01&lt;00:00,  9.94it/s, loss=0.736]\n100%|███████████████████████████████| 10/10 [00:01&lt;00:00,  9.94it/s, loss=0.022]\n\n\n分析1\n这里使用了t.set_postfix函数,我进入这个函数中,找到了重要部分:\nself.postfix = ', '.join(key + '=' + postfix[key].strip()\n                                 for key in postfix.keys())\n这个函数会把输入的参数变成key=xxx的形式复制给自身的self.postfix,这样还是挺好的,并且可以自己随意添加list,str,int型的变量.\n\n\n例子2\nfrom tqdm import trange, tqdm\nfrom random import random, randint\nfrom time import sleep\n\ndef training(epoch: int, step_per_epoch: int):\n    for i in range(epoch):\n        with tqdm(total=step_per_epoch, bar_format='{l_bar}{bar}| {rate_fmt} {postfix[0]}{postfix[1][loss]:&gt;6.3f}',\n                  unit=' batch', postfix=['loss=', dict(loss=0)], ncols=80) as t:\n            for j in range(step_per_epoch):\n                t.postfix[1][\"loss\"] = random()\n                t.update()\n\ntraining(epoch=3, step_per_epoch=10)\n\n\n分析2\n上面我首先改变了bar_format,不过我没有动前面的两部分,只修改了最后一部分.具体看这里:{postfix[0]}{postfix[1][loss]:&gt;6.3f}.这样相当与这个postfix是一个列表.其中t.postfix[1][\"loss\"] = random()意思是向这个列表中的字典项loss复制,最终达到更新loss的效果.\n\n\n例子3\nfrom tqdm import trange, tqdm\nfrom random import random, randint\nfrom time import sleep\n\n\ndef training(epoch: int, step_per_epoch: int):\n    for i in range(epoch):\n        with tqdm(total=step_per_epoch, bar_format='{n_fmt}/{total_fmt} |{bar}| {rate_fmt} {postfix}', ncols=80) as t:\n            for i in range(step_per_epoch):\n                t.set_postfix_str('loss={:^7.3f}'.format(random()))\n                sleep(.1)\n                t.update()\n\n\ntraining(epoch=3, step_per_epoch=10)\n\n\n效果3\n10/10 |███████████████████████████████████████████████|  9.94it/s , loss= 0.093\n10/10 |███████████████████████████████████████████████|  9.94it/s , loss= 0.889\n10/10 |███████████████████████████████████████████████|  9.94it/s , loss= 0.090\n\n\n分析3\n这个最好理解,直接对postfix赋值字符串,可以随便自己操作.我比较喜欢."
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html",
    "href": "posts/triton-cpu-lesson-1.html",
    "title": "triton-cpu初体验",
    "section": "",
    "text": "体验一下triton cpu，看看是否有想象中的效果。"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#llvm-编译",
    "href": "posts/triton-cpu-lesson-1.html#llvm-编译",
    "title": "triton-cpu初体验",
    "section": "2.1 llvm 编译",
    "text": "2.1 llvm 编译\nllvm可以参考他的ci中的编译方式, 他给每个平台都给出了详细的编译命令。"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#triton-编译选项",
    "href": "posts/triton-cpu-lesson-1.html#triton-编译选项",
    "title": "triton-cpu初体验",
    "section": "2.2 triton 编译选项",
    "text": "2.2 triton 编译选项\n如果只有cpu后端， 那么就使用下面的选项:\n{\n  \"LLVM_LIBRARY_DIR\": \"xxxx/llvm-project/install/lib\",\n  \"TRITON_BUILD_PYTHON_MODULE\": true,\n  \"TRITON_BUILD_PROTON\": false,\n  \"TRITON_CODEGEN_BACKENDS\": \"cpu\",\n  \"TRITON_BUILD_UT\": false\n}"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#cmake-bug-修复",
    "href": "posts/triton-cpu-lesson-1.html#cmake-bug-修复",
    "title": "triton-cpu初体验",
    "section": "2.3 cmake bug 修复",
    "text": "2.3 cmake bug 修复\ncpu后端这句话会报错， 先注释掉\n# Override sleef's output directory with our own\n# set_target_properties(sleef PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY})"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#cpp-编译错误修复",
    "href": "posts/triton-cpu-lesson-1.html#cpp-编译错误修复",
    "title": "triton-cpu初体验",
    "section": "2.4 cpp 编译错误修复",
    "text": "2.4 cpp 编译错误修复\n由于triton他的opt里面用了所有的后端的pass， 但是我们又没有编译nv/amd， 所以需要修改bin/RegisterTritonDialects.h, 把编译时报错的地方都注释掉。 然后third_party/nvidia/include/TritonNVIDIAGPUToLLVM/Utility.h中还有一个nv的函数找不到定义， 自己手动写个实现：\nbool canSkipBarSync(Operation *before, Operation *after){\n  return false;\n}"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#python-加载段错误修复",
    "href": "posts/triton-cpu-lesson-1.html#python-加载段错误修复",
    "title": "triton-cpu初体验",
    "section": "2.5 python 加载段错误修复",
    "text": "2.5 python 加载段错误修复\n我用的python3.8， 好像在mac会和pybind会有奇怪的问题，需要修改cmake：\n    if(NOT APPLE)\n      link_libraries(${Python3_LIBRARIES})\n    endif()"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#软链接",
    "href": "posts/triton-cpu-lesson-1.html#软链接",
    "title": "triton-cpu初体验",
    "section": "2.6 软链接",
    "text": "2.6 软链接\ntriton的python编译脚本中有自动copy后端文件到指定目录， 这里我们手动就需要自己软链接\n❯ cd python/triton/backends\n❯ mkdir cpu\n❯ cd cpu\n❯ ln -s xxxx/triton-cpu/third_party/cpu/backend/driver.py driver.py               \n❯ ln -s xxxx/triton-cpu/third_party/cpu/backend/compiler.py compiler.py\n❯ cd triton-cpu/python/triton/_C\n❯ ln -s xxxx/triton-cpu/out/build/debug/libtriton.so libtriton.so              \n❯ ln -s xxxx/triton-cpu/out/build/debug/third_party/cpu/libTritonCPURuntime.dylib libTritonCPURuntime.dylib                \n❯ ln -s xxxx/triton-cpu/out/build/debug/third_party/cpu/sleef/lib/libsleef.dylib libsleef.dylib"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#环境变量配置",
    "href": "posts/triton-cpu-lesson-1.html#环境变量配置",
    "title": "triton-cpu初体验",
    "section": "2.7 环境变量配置",
    "text": "2.7 环境变量配置\n下面三个是debug使用的。\nPYTHONPATH=\"xxxx/triton-cpu/out/build/debug:xxxx/triton-cpu/python\"\nTRITON_KERNEL_DUMP=1\nTRITON_DEBUG=1\nLLVM_IR_ENABLE_DUMP=1"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#python-执行报错",
    "href": "posts/triton-cpu-lesson-1.html#python-执行报错",
    "title": "triton-cpu初体验",
    "section": "2.8 python 执行报错",
    "text": "2.8 python 执行报错\n我这里是python3.8 但是他写了一个3.11才有的特性, 所以修改掉\n    def _init_slots(self):\n        \"\"\" Initialize the slots of this class \"\"\"\n        for name, val in self.arg_properties.items():\n            setattr(self, (name[3:] if name.startswith(\"tt.\") else name) + '_' + str(self.property_values[name]), val)\n还有一个是mac中选择编译器的问题, 修改文件python/triton/runtime/build.py:\n        if system == 'Darwin' and clang is not None:\n            cc = clang\n        else:\n            cc = gcc if gcc is not None else clang"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#vector-add",
    "href": "posts/triton-cpu-lesson-1.html#vector-add",
    "title": "triton-cpu初体验",
    "section": "3.1 vector add",
    "text": "3.1 vector add\n我尝试执行最简单的例子，但是他会卡死在生成asm，然后我发现dump的llvm ir非常奇怪， 有32768个数：  然后发现vector add中又有:\nCPU_ST_THRESHOLD = 65536\n然后尝试一下缩小这个阈值， 果然就可以正常编译了= =。 但是还有omp的问题， 因为apple自带的apple-clang是没有omp功能的。\n可以使用conda安装omp然后修改python/triton/runtime/build.py中mac的配置，并且下面之前添加的openmp需要注释掉，添加上编译选项/头文件/库。\n    if system == \"Darwin\":\n        cc_cmd += [\"-undefined\", \"dynamic_lookup\"]\n        cc_cmd += ['-Xpreprocessor', '-fopenmp', '-lomp']\n        include_dirs += ['xxxxx/miniforge3/envs/ci/include']\n        library_dirs += ['xxxxx/miniforge3/envs/ci/lib']\n        # Don't use libgcc on clang + macos\n        if \"clang\" in cc:\n            libraries.remove(\"gcc\")\n.\n.\n.\n        # if not os.environ.get(\"TRITON_DISABLE_OPENMP\", None):\n        #   cc_cmd += [\"-fopenmp\"]\n现在终于可以正常运行vector add:\n❯ python python/tutorials/01-vector-add.py\ntensor([0.5151, 1.6826, 0.9153,  ..., 0.9852, 1.2714, 1.8192])\ntensor([0.5151, 1.6826, 0.9153,  ..., 0.9852, 1.2714, 1.8192])\nThe maximum difference between torch-cpu and triton-cpu is 0.0\nThe maximum difference between torch-cpu-tiled and triton-cpu is 0.0\n我仔细观察后，发现这个CPU_ST_THRESHOLD其实就是BLOCK_SIZE的最大值，那么其实是BLOCK_SIZE大于512的时候就已经很难编译了，这个可能和triton编译出展开的代码关系比较大。"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#如何并行",
    "href": "posts/triton-cpu-lesson-1.html#如何并行",
    "title": "triton-cpu初体验",
    "section": "4.1 如何并行？",
    "text": "4.1 如何并行？\n首先triton编译出来的kernel他作为一个库函数， 默认都有3维的index作为输入参数。然后runtime的driver部分手写了一个launcher去调用triton kernel，在启动时根据预先设定的gridX, gridY, gridZ大小，将这些index的tuple展平为1维，然后直接用openmp并行循环i即可。这里其实存在一个问题，三个id生成的循环顺序和程序是无关的，但是程序的运行出来的性能却和id的顺序有关。\nauto all_grids = get_all_grids(gridX, gridY, gridZ);\n// For now, use the default chunk size, total iterations / max_threads.\n#ifdef _OPENMP\n#pragma omp parallel for schedule(static) num_threads(max_threads)\n#endif // _OPENMP\n  for (size_t i = 0; i &lt; N; ++i) {{\n    const auto [x, y, z] = all_grids[i];\n    (*kernel_ptr)({kernel_fn_args_list + ', ' if len(kernel_fn_args) &gt; 0 else ''} x, y, z, gridX, gridY, gridZ);\n  }}\n}}"
  },
  {
    "objectID": "posts/triton-cpu-lesson-1.html#gridx-gridy-gridz大小如何设定",
    "href": "posts/triton-cpu-lesson-1.html#gridx-gridy-gridz大小如何设定",
    "title": "triton-cpu初体验",
    "section": "4.2 gridX, gridY, gridZ大小如何设定？",
    "text": "4.2 gridX, gridY, gridZ大小如何设定？\ntriton的runtime有launcher，但runtime也是被python端的jit function所调用的，在jit function启动一个kernel的时候，会把当前python端输入的参数都给到上面提到的grid_callback函数，此时这个函数的返回值就是gridX, gridY, gridZ大小。 也就是说我们可以根据当前输入tensor的尺寸选择合适的工作线程个数。\nclass JITFunction(KernelInterface[T]):\n    ...\n    def run(self, *args, grid, warmup, **kwargs):\n        ...\n        if not warmup:\n            # canonicalize grid\n            assert grid is not None\n            if callable(grid):\n                # Arguments are passed as a dict to `grid`, by contract.\n                # TODO(jlebar): In the new launch API, pass the compiler flags as a\n                # second parameter to `grid`.\n                grid = grid(bound_args)\n            grid_size = len(grid)\n            grid_0 = grid[0]\n            grid_1 = grid[1] if grid_size &gt; 1 else 1\n            grid_2 = grid[2] if grid_size &gt; 2 else 1\n\n            # launch kernel\n            launch_metadata = kernel.launch_metadata(grid, stream, *non_constexpr_vals)\n            kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\n                       self.CompiledKernel.launch_enter_hook, self.CompiledKernel.launch_exit_hook, *non_constexpr_vals)"
  },
  {
    "objectID": "posts/use-lan.html",
    "href": "posts/use-lan.html",
    "title": "设置路由器使用LAN口",
    "section": "",
    "text": "新到了个服务器,为了在Windows下面也可以交叉编译,再配合vscode远程功能,美滋滋.但是很快我发现了一个问题,我们的网络配置如下:\n\n172.24.41 段             路由器--------交换机--------服务器          \n                           |\n                     ------------\n                     |          |\n192.168.1 段      我的电脑      树莓派     \n树莓派可以连接到服务器,但是服务器无法连接到树莓派,所以无法传输文件,所以我就路由器当做交换机来用.\n\n解决\n我用的是DD-WRT现在配置是:\n\n\n\n名称\n配置\n\n\n\n\nWAN\n指派WAN口为交换口\n\n\n路由器IP\n设置为可上网的静态IP\n\n\nDHCP\n从172.24.41.220开始分配\n\n\n\n现在我们的网络连接如下:\n172.24.41 段    我的电脑    树莓派      路由器--------交换机--------服务器          \n                   |        |          |\n                   ---------------------"
  },
  {
    "objectID": "posts/vae2.html",
    "href": "posts/vae2.html",
    "title": "变分自编码器(VAE) 直观推导",
    "section": "",
    "text": "最近一周系统的看下概率论的东西，把公式都看了下，这次重新对VAE做一个直观的推导，我这里就不说VAE为什么要这么做(水平不够)，只说他是怎么做的。"
  },
  {
    "objectID": "posts/vae2.html#重构损失",
    "href": "posts/vae2.html#重构损失",
    "title": "变分自编码器(VAE) 直观推导",
    "section": "重构损失",
    "text": "重构损失\n\\(\\mathcal{L}_{re}\\)计算很简单，直接使用tf.nn.sigmoid_cross_entropy_with_logits即可。"
  },
  {
    "objectID": "posts/vae2.html#kl损失",
    "href": "posts/vae2.html#kl损失",
    "title": "变分自编码器(VAE) 直观推导",
    "section": "KL损失",
    "text": "KL损失\n这个需要好好推导： \\[\n\\begin{aligned}\n    &KL(p({z}|x)\\| N(0,1))=\\int p({z}|x)\\ \\log\\frac{p({z}|x)}{N(0,1)}\\ dz \\\\\n    &=\\int p({z}|x)\\ \\log \\frac{\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(z-\\mu)^2}{2\\sigma^2}}}{\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}}\\ dz \\\\\n    &=\\int p({z}|x)\\ [\\log\\frac{1}{\\sqrt{\\sigma^2}}+\\log e^{\\frac{1}{2}(z^2-\\frac{(z-\\mu)^2}{\\sigma^2})}]\\ dz\\\\\n    &=\\int p({z}|x)\\ [-\\frac{1}{2}\\log \\sigma^2+\\frac{1}{2}(z^2-\\frac{(z-\\mu)^2}{\\sigma^2})]\\ dz \\\\\n    &=\\int p({z}|x)\\ [-\\frac{1}{2}\\log \\sigma^2+\\frac{1}{2}(z^2-\\frac{(z-\\mu)^2}{\\sigma^2})]\\ dz \\\\\n    &=\\frac{1}{2}[-\\int p({z}|x)\\ \\log \\sigma^2 \\ dz +\\int p({z}|x)\\ z^2\\ dz-\\int p({z}|x)\\ \\frac{(z-\\mu)^2}{\\sigma^2}\\ dz] \\\\\n    &=\\frac{1}{2}[-\\log\\sigma^2+E(z^2)-\\frac{D(z)}{\\sigma^2}] \\\\\n    &=\\frac{1}{2}(-\\log\\sigma^2+\\mu^2+\\sigma^2-1)\n\\end{aligned}\n\\]\n注意： 上面的\\(p({z}|x)\\)其实就是正态分布的概率，所以\\(\\int p({z}|x)\\ dz=1\\)。后面两个就是求\\(z^2\\)的期望，和\\(z\\)的方差。"
  },
  {
    "objectID": "posts/vae2.html#隐空间图像",
    "href": "posts/vae2.html#隐空间图像",
    "title": "变分自编码器(VAE) 直观推导",
    "section": "隐空间图像",
    "text": "隐空间图像"
  },
  {
    "objectID": "posts/vae2.html#重构图像",
    "href": "posts/vae2.html#重构图像",
    "title": "变分自编码器(VAE) 直观推导",
    "section": "重构图像",
    "text": "重构图像"
  },
  {
    "objectID": "posts/vscode-pydoc.html",
    "href": "posts/vscode-pydoc.html",
    "title": "vscode Python Docstring Generator",
    "section": "",
    "text": "Python Docstring Generator这个插件我用了挺久了,不过他这里的提供的样式风格和vscode自动提示的风格是不太匹配的.之前改了一个模板我又找不到了,这里记录一下.\n\n主要还是vscode渲染注释的时候会把下划线转义,而且不会自动识别换行,说明他应该不是markdown格式的,所以用下面这个模板替换~/.vscode/extensions/njpwerner.autodocstring-0.5.1/out/docstring/templates/google.mustache,之后自动注释看起来就会清爽许多.\n{{! Google Docstring Template }}\n{{summaryPlaceholder}}\n\n{{extendedSummaryPlaceholder}}\n\n{{#parametersExist}}\nArgs:\n{{#args}}\n    `{var}` ({{typePlaceholder}}): {{descriptionPlaceholder}}\n    \n{{/args}}\n{{#kwargs}}\n    `{var}` ({{typePlaceholder}}, optional): {{descriptionPlaceholder}}. Defaults to {{&default}}.\n    \n{{/kwargs}}\n{{/parametersExist}}\n\n{{#exceptionsExist}}\nRaises:\n{{#exceptions}}\n    {{type}}: {{descriptionPlaceholder}}\n{{/exceptions}}\n{{/exceptionsExist}}\n\n{{#returnsExist}}\nReturns:\n{{#returns}}\n    {{typePlaceholder}}: {{descriptionPlaceholder}}\n{{/returns}}\n{{/returnsExist}}\n\n{{#yieldsExist}}\nYields:\n{{#yields}}\n    {{typePlaceholder}}: {{descriptionPlaceholder}}\n{{/yields}}\n{{/yieldsExist}}"
  },
  {
    "objectID": "posts/wenda-week1.html",
    "href": "posts/wenda-week1.html",
    "title": "机器学习作业第一周",
    "section": "",
    "text": "这是吴恩达老师的机器学习作业….并不是我们学校的 hh\n因为机器学习这个课是老师在2014年讲的,那时候Python还不火,所以老师讲的时候用的是ocatve,现在我准备都用Python写."
  },
  {
    "objectID": "posts/wenda-week1.html#重要公式的推导",
    "href": "posts/wenda-week1.html#重要公式的推导",
    "title": "机器学习作业第一周",
    "section": "重要公式的推导",
    "text": "重要公式的推导\n这里有一个多变量的梯度更新，需要把原来的损失函数进行求导，因为现在是每一个变量都是向量，所以推导的时候我纠结了好一会。具体如下：\n注：上标为行数，下标为列数，设数据X=[m,n]\n\\[ \\begin{aligned}\n    \\theta_j &= \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})*x_j^{(i)} \\\\\n    \\theta_j &-=\\frac{ \\alpha}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})*x_j^{(i)}\n\\end{aligned} \\] 现在我们分析此部分\\(\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})*x_j^{(i)}\\)：\n注：以下都省略了常数项\\(\\alpha/m\\)\n\\[ \\begin{aligned}\n    \\because &x=[m,n]\\ \\ \\ y=[m,1]\\\\\n    \\therefore &x^{(i)} =[1,n]\\ \\ \\ y^{(i)}=[1,1]\\ \\ \\ \\theta=[n,1] \\\\\n    \\Rightarrow &h_\\theta(x^{(i)})=x^{(i)}*\\theta=[1,1]\\\\\n    \\Rightarrow &h_\\theta(x^{(i)})-y^{(i)}=[1,1]=E_\\theta^{(i)}\\\\\n    \\because &\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})*x_j^{(i)}=\\sum_{i=1}^{m}E_\\theta^{(i)}*x_j^{(i)}\\\\\n    &=E_\\theta^{(0)}*x_j^{(0)}+E_\\theta^{(1)}*x_j^{(1)}+\\ldots+E_\\theta^{(m)}*x_j^{(m)}\\\\\n    \\therefore \\Theta&=\n    \\begin{bmatrix}\n    x_1^1E_1 + x_2^1E_1 + \\ldots + x_m^1E_1 \\\\\n    x_1^2E_2 + x_2^2E_2 + \\ldots + x_m^2E_2 \\\\\n    x_1^nE_n + x_2^nE_n + \\ldots + x_m^nE_n \\\\\n    \\end{bmatrix}\n\\end{aligned} \\]\n将其放到全局的矩阵中就是这样：\n\\[ \\begin{aligned}\n    X&=\\begin{bmatrix}\n        x_1^1 & x_2^1 & \\ldots & x_n^1 \\\\\n        \\vdots & \\vdots & \\ldots & \\vdots \\\\\n        x_1^m & x_2^m & \\ldots & x_n^1 \\\\\n    \\end{bmatrix}\\\\\n    Y&=\\begin{bmatrix}\n        y_1^1  \\\\\n        \\vdots \\\\\n        y_1^m  \\\\\n    \\end{bmatrix}\\\\\n    \\theta&=\\begin{bmatrix}\n        \\theta_1^1  \\\\\n        \\vdots \\\\\n        \\theta_1^n  \\\\\n    \\end{bmatrix} \\\\\n    \\Theta-&=X^T(h_\\theta(X) -Y) = X^T(X*\\Theta-Y) \\\\\n    &=\n    \\begin{bmatrix}\n     x_1^1 & x_1^2 & \\ldots & x_1^m \\\\\n    \\vdots & \\vdots & \\ldots & \\vdots \\\\\n    x_n^1 & x_n^2 & \\ldots & x_n^m \\\\    \n    \\end{bmatrix}*\n    (\\begin{bmatrix}\n        x_1^1 & x_2^1 & \\ldots & x_n^1 \\\\\n        \\vdots & \\vdots & \\ldots & \\vdots \\\\\n        x_1^m & x_2^m & \\ldots & x_n^1 \\\\\n    \\end{bmatrix}*\n    \\begin{bmatrix}\n        \\theta_1^1  \\\\\n        \\vdots \\\\\n        \\theta_1^n  \\\\\n    \\end{bmatrix}-\n    \\begin{bmatrix}\n        y_1^1  \\\\\n        \\vdots \\\\\n        y_1^m  \\\\\n    \\end{bmatrix}) \\\\\n    &= \\begin{bmatrix}\n     x_1^1 & x_1^2 & \\ldots & x_1^m \\\\\n    \\vdots & \\vdots & \\ldots & \\vdots \\\\\n    x_n^1 & x_n^2 & \\ldots & x_n^m \\\\    \n    \\end{bmatrix}*\n    \\begin{bmatrix}\n        h_1^1  - y_1^1  \\\\\n        \\vdots          \\\\\n        h_1^m  - y_1^m  \\\\\n    \\end{bmatrix}\\\\\n    &=\n    \\begin{bmatrix}\n    x_1^1E_1 + x_2^1E_1 + \\ldots + x_m^1E_1 \\\\\n    x_1^2E_2 + x_2^2E_2 + \\ldots + x_m^2E_2 \\\\\n    x_1^nE_n + x_2^nE_n + \\ldots + x_m^nE_n \\\\\n    \\end{bmatrix}\n\\end{aligned} \\]\n即推出：\n\\[\\Theta-=\\frac{\\alpha}{m}X^T(X*\\Theta-Y)\\] # 代码\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\n\n\n# 加载数据\ndef load_data(filepath: str)-&gt;np.ndarray:\n    dataset = []\n    f = open(filepath)\n    for line in f:\n        dataset.append(line.strip().split(','))\n    return np.asfarray(dataset)\n\n# 定义推演函数\n\n\ndef h_fuc(x: float, theta: np.ndarray)-&gt;float:\n    return theta[0, 0]+theta[0, 1]*x\n\n\n# z定义损失函数\ndef computeCost(X: np.ndarray, y: np.ndarray, theta: np.ndarray)-&gt;float:\n    m = y.shape[0]\n    return np.sum(np.power(np.dot(X, theta)-y, 2))/(2*m)\n\n\n# 多参数损失函数计算\ndef computeCostMulti(X: np.ndarray, y: np.ndarray, theta: np.ndarray)-&gt;float:\n    m = y.shape[0]\n    return np.sum(np.power(np.dot(X, theta)-y, 2))/(2*m)\n\n\n# 更新参数\ndef gradientDescent(X: np.ndarray, y: np.ndarray, theta: np.ndarray, alpha: float, num_iters: int):\n    m = y.shape[0]\n    j_history = np.zeros((num_iters, 1))\n    theta_s = theta.copy()\n    for i in range(num_iters):\n        theta[0, 0] -= alpha / m * np.sum(np.dot(X, theta_s) - y)\n        theta[1, 0] -= alpha / m * np.sum((np.dot(X, theta_s) - y)*X)\n        # 必须同时更新theta(1)和theta(2)\n        theta_s = theta\n        j_history[i, 0] = computeCost(X, y, theta)\n    return j_history\n\n\ndef gradientDescentMulti(X: np.ndarray, y: np.ndarray, theta: np.ndarray, alpha: float, num_iters: int):\n    m = y.shape[0]\n    j_history = np.zeros((num_iters, 1))\n    for i in range(num_iters):\n        theta -= alpha*(X.T@(X@theta-y))/m\n        j_history[i, 0] = computeCostMulti(X, y, theta)\n    return theta, j_history\n\n\ndef featureNormalize(X: np.ndarray):\n    X_norm = X\n    mu = np.zeros((1, X.shape[0]))\n    sigma = np.zeros((1, X.shape[0]))\n    mu = np.mean(X, axis=0)\n    # 加上ddof=1 因为matlab中默认除以 n-1 np 默认除以 n\n    sigma = np.std(X, axis=0, ddof=1)\n    X_norm = (X-mu)/sigma\n    return X_norm, mu, sigma\n\n\n# 正规方程求解\ndef normalEqn(X: np.ndarray, y: np.ndarray):\n    theta = np.linalg.inv(X.T@X)@X.T@y\n    return theta\n\n\nif __name__ == \"__main__\":\n\n    # 步骤一（替换sans-serif字体）\n    plt.rcParams['font.sans-serif'] = ['YaHei Consolas Hybrid']\n    # 步骤二（解决坐标轴负数的负号显示问题）\n    plt.rcParams['axes.unicode_minus'] = False\n\n    \"\"\" 以下为单变量回归 \"\"\"\n    dataset = load_data('machine_learning_exam/week1/ex1data1.txt')\n    m = dataset.shape[0]\n    # x 添加一列 便于矩阵计算 x=[m,2]\n    X = np.c_[np.ones((m, 1)).reshape(-1, 1),\n              np.array(dataset[:, 0]).reshape(-1, 1)]\n    Y = np.array(dataset[:, 1]).reshape(-1, 1)\n    # # theta 设置为列向量 theta=[2,1] !!!这里一定要设置数据类型!!!\n    theta = np.array([0, 0], dtype=float, ndmin=2).reshape(-1, 1)\n    iterations = 1500\n    alpha = 0.01\n    cost = computeCost(X, Y, theta)\n    j_history = gradientDescent(X, Y, theta, alpha, 1500)\n    plt.figure()\n    plt.scatter(X[:, 1], Y, c='r', marker='x')\n    plt.plot(X[:, 1], np.dot(X, theta))\n    plt.xlabel('Profit in $10,000s')\n    plt.ylabel('Population of city in 10,1000s')\n    plt.title('单边量回归')\n    # 可视化损失\n    theta0_vals = np.linspace(-10, 10, 100.0)\n    theta1_vals = np.linspace(-1, 4, 100.0)\n    # 这里必须要加 不然画出来只用中间一条\n    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)\n    J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n    for i in range(len(theta0_vals)):\n        for j in range(len(theta1_vals)):\n            t = np.array([theta0_vals[i, j], theta1_vals[i, j]]).reshape(-1, 1)\n            J_vals[i, j] = computeCost(X, Y, t)\n            # print(\"J_vals[{}, {}]={}\".format(i,j,J_vals[i,j]))\n\n    fig1 = plt.figure()\n    ax = fig1.gca(projection='3d')\n    surf = ax.plot_surface(theta0_vals, theta1_vals, J_vals)\n    plt.xlabel('theta0')\n    plt.ylabel('theta1')\n    plt.title('可视化损失')\n\n    \"\"\" 以下为多变量回归 \"\"\"\n    # ===========数据标准化=============\n\n    dataset = load_data('machine_learning_exam/week1/ex1data2.txt')\n    m = dataset.shape[0]\n    X = np.array(dataset[:, :2])  # x=[m,2]\n    Y = np.array(dataset[:, 2]).reshape(-1, 1)  # y=[m,1]\n    # 归一化特征值\n    X, mu, sigma = featureNormalize(X)\n    # x add a cloum\n    X = np.c_[np.ones((m, 1)).reshape(-1, 1), X]\n    # ===========梯度下降=============\n\n    # 选择学习率\n    iterations = 8500\n    alpha = 0.01\n\n    # theta 列向量 theta=[3,1]\n    theta = np.zeros((3, 1))\n    theta, j_history = gradientDescentMulti(X, Y, theta, alpha, iterations)\n    # 绘画\n    plt.figure()\n    plt.plot(range(len(j_history)), j_history, '-b')\n    plt.xlabel('Number of iterations')\n    plt.ylabel('Cost J')\n    plt.title('学习率为:{}'.format(alpha))\n    # plt.show()\n    print('Theta computed from gradient descent:', theta[:, 0])\n\n    # 估计\n    price = np.ones((1, 3))\n    price[0, 1:] = (np.array([1650, 3])-mu)/sigma\n    price = price@theta\n    print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent):', price)\n\n    \"\"\" 以下使用正规方程求解 \"\"\"\n    # ===========数据读取=============\n\n    dataset = load_data('machine_learning_exam/week1/ex1data2.txt')\n    m = dataset.shape[0]\n    X = np.array(dataset[:, :2])  # x=[m,2]\n    Y = np.array(dataset[:, 2]).reshape(-1, 1)  # y=[m,1]\n    # x add a cloum\n    X = np.c_[np.ones((m, 1)).reshape(-1, 1), X]\n\n    # ===========梯度下降=============\n    # 选择学习率\n    iterations = 400\n    alpha = 0.01\n\n    # theta 列向量 theta=[3,1]\n    theta = normalEqn(X, Y)\n    # 估计\n    print('Theta computed from normal equations:', theta[:, 0])\n    price = np.array([1, 1650, 3])@theta\n    print('Predicted price of a 1650 sq-ft, 3 br house (using normal equations):', price)\n    plt.show()"
  },
  {
    "objectID": "posts/wenda-week3.html",
    "href": "posts/wenda-week3.html",
    "title": "机器学习作业第三周",
    "section": "",
    "text": "这个是第三周的作业,是一个多类的正则化逻辑回归和神经网络."
  },
  {
    "objectID": "posts/wenda-week3.html#效果",
    "href": "posts/wenda-week3.html#效果",
    "title": "机器学习作业第三周",
    "section": "效果",
    "text": "效果\n\n\n\n图像\n\n\n下面的waring是因为达到我设定的迭代次数\n➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week3/ex3.py\nLoading and Visualizing Data ...\nProgram paused. Press enter to continue.\nTesting lrCostFunction() with regularization\nCost: [[2.5348194]]\nExpected cost: 2.534819\nGradients:\n[[ 0.14656137]\n [-0.54855841]\n [ 0.72472227]\n [ 1.39800296]]\nExpected gradients:\n 0.146561 -0.548558 0.724722 1.398003\nProgram paused. Press enter to continue.\nTraining One-vs-All Logistic Regression...\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.013718\n         Iterations: 50\n         Function evaluations: 183\n         Gradient evaluations: 183\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.056204\n         Iterations: 50\n         Function evaluations: 141\n         Gradient evaluations: 141\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.061957\n         Iterations: 50\n         Function evaluations: 139\n         Gradient evaluations: 139\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.037384\n         Iterations: 50\n         Function evaluations: 154\n         Gradient evaluations: 154\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.064096\n         Iterations: 50\n         Function evaluations: 131\n         Gradient evaluations: 131\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.020031\n         Iterations: 50\n         Function evaluations: 175\n         Gradient evaluations: 175\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.033819\n         Iterations: 50\n         Function evaluations: 161\n         Gradient evaluations: 161\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.085543\n         Iterations: 50\n         Function evaluations: 130\n         Gradient evaluations: 130\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.076294\n         Iterations: 50\n         Function evaluations: 136\n         Gradient evaluations: 136\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.009110\n         Iterations: 50\n         Function evaluations: 182\n         Gradient evaluations: 182\nProgram paused. Press enter to continue.\nTraining Set Accuracy: 95.12%"
  },
  {
    "objectID": "posts/wenda-week3.html#效果-1",
    "href": "posts/wenda-week3.html#效果-1",
    "title": "机器学习作业第三周",
    "section": "效果",
    "text": "效果\n➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week3/ex3_nn.py\nLoading and Visualizing Data ...\nProgram paused. Press enter to continue.\nLoading Saved Neural Network Parameters ...\nTraining Set Accuracy: 97.52%\nProgram paused. Press enter to continue.\nDisplaying Example Image\nNeural Network Prediction: [[8]] (digit [[8]])\nDisplaying Example Image\nNeural Network Prediction: [[5]] (digit [[5]])\nDisplaying Example Image\nNeural Network Prediction: [[10]] (digit [[0]])\nDisplaying Example Image\nNeural Network Prediction: [[5]] (digit [[5]])\nDisplaying Example Image\nNeural Network Prediction: [[10]] (digit [[0]])\nDisplaying Example Image\nNeural Network Prediction: [[8]] (digit [[8]])\nDisplaying Example Image\nNeural Network Prediction: [[6]] (digit [[6]])\nDisplaying Example Image\nNeural Network Prediction: [[10]] (digit [[0]])\nDisplaying Example Image\nNeural Network Prediction: [[2]] (digit [[2]])\nDisplaying Example Image\nNeural Network Prediction: [[3]] (digit [[3]])"
  },
  {
    "objectID": "posts/wenda-week5.html",
    "href": "posts/wenda-week5.html",
    "title": "机器学习作业第五周",
    "section": "",
    "text": "这周没有什么好说的,主要就是根据两个误差与方差曲线去判断算法下一步优化的方向,但是这个我觉得还是要做的多了才知道如何去做下一步的判断,毕竟这么多算法,你肯定得都懂点才能去判断优劣."
  },
  {
    "objectID": "posts/wenda-week5.html#执行效果",
    "href": "posts/wenda-week5.html#执行效果",
    "title": "机器学习作业第五周",
    "section": "执行效果",
    "text": "执行效果\n➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week5/ex5.py\nLoading and Visualizing Data ...\nProgram paused. Press enter to continue.\nCost at theta = [1 ; 1]: 303.9931922202643\n     (this value should be about 303.993192)\nProgram paused. Press enter to continue.\nGradient at theta = [1 ; 1]:  [-15.303015674201186; 598.2507441727035]\n        (this value should be about [-15.303016; 598.250744])\nProgram paused. Press enter to continue.\n.\n.\n.\n# Training Examples     Train Error     Cross Validation Error\n        1               0.000000        205.121096\n        2               0.000000        110.300366\n        3               3.286595        45.010232\n        4               2.842678        48.368911\n        5               13.154049       35.865141\n        6               19.443963       33.829957\n        7               20.098522       31.970987\n        8               18.172859       30.862446\n        9               22.609405       31.135998\n        10              23.261462       28.936207\n        11              24.317250       29.551432\n        12              22.373906       29.433818\nProgram paused. Press enter to continue.\nNormalized Training Example 1:\n  [ 1.         -0.36214078 -0.75508669  0.18222588 -0.70618991  0.30661792\n -0.59087767  0.3445158  -0.50848117]\nProgram paused. Press enter to continue.\n.\n.\n.\nPolynomial Regression (lamda = 0)\n# Training Examples     Train Error     Cross Validation Error\n        1               0.000000        160.721900\n        2               0.000000        160.121510\n        3               0.000000        61.755005\n        4               0.000000        61.928895\n        5               0.000000        6.597673\n        6               0.000000        10.645191\n        7               0.000000        27.988538\n        8               0.000889        20.280259\n        9               0.000236        31.876446\n        10              0.040922        19.687148\n        11              0.045236        15.815669\n        12              0.095506        8.738590\nProgram paused. Press enter to continue.\n.\n.\n.\nlamda           Train Error     Validation Error\n 0.000000       0.095506        8.738590\n 0.001000       0.177934        12.630771\n 0.003000       0.249932        16.348329\n 0.010000       0.385063        17.046746\n 0.030000       0.669275        13.049682\n 0.100000       1.443470        8.149481\n 0.300000       3.101591        5.882464\n 1.000000       7.268148        7.227466\n 3.000000       15.867688       10.089379\n 10.000000      33.372203       19.819785\nProgram paused. Press enter to continue."
  },
  {
    "objectID": "posts/wenda-week7.html",
    "href": "posts/wenda-week7.html",
    "title": "机器学习作业第七周",
    "section": "",
    "text": "这周是无监督学习算法,比较简单.k means之前就已经写过了,PCA主要是有个矩阵的奇异值分解需要看看矩阵相关知识."
  },
  {
    "objectID": "posts/wenda-week7.html#效果",
    "href": "posts/wenda-week7.html#效果",
    "title": "机器学习作业第七周",
    "section": "效果",
    "text": "效果\nFinding closest centroids.\nClosest centroids for the first 3 examples:\n[0 2 1]\n\n(the closest centroids should be 0, 2, 1 respectively)\nProgram paused. Press enter to continue.\n\nComputing centroids means.\nCentroids computed after initial finding of closest centroids:\n[[2.42830111 3.15792418]\n [5.81350331 2.63365645]\n [7.11938687 3.6166844 ]]\n\n(the centroids should be\n   [ 2.428301 3.157924 ]\n   [ 5.813503 2.633656 ]\n   [ 7.119387 3.616684 ]\nProgram paused. Press enter to continue.\n\nRunning K-Means clustering on example dataset.\nK-Means iteration 0/10...\n\nK-Means iteration 1/10...\n\nK-Means iteration 2/10...\n\nK-Means iteration 3/10...\n\nK-Means iteration 4/10...\n\nK-Means iteration 5/10...\n\nK-Means iteration 6/10...\n\nK-Means iteration 7/10...\n\nK-Means iteration 8/10...\n\nK-Means iteration 9/10...\n\n\nK-Means Done.\nProgram paused. Press enter to continue.\n\nRunning K-Means clustering on pixels from an image.\nK-Means iteration 0/10...\n\nK-Means iteration 1/10...\n\nK-Means iteration 2/10...\n\nK-Means iteration 3/10...\n\nK-Means iteration 4/10...\n\nK-Means iteration 5/10...\n\nK-Means iteration 6/10...\n\nK-Means iteration 7/10...\n\nK-Means iteration 8/10...\n\nK-Means iteration 9/10...\n\nProgram paused. Press enter to continue.\n\nApplying K-Means to compress an image.\nProgram paused. Press enter to continue."
  },
  {
    "objectID": "posts/wenda-week7.html#效果-1",
    "href": "posts/wenda-week7.html#效果-1",
    "title": "机器学习作业第七周",
    "section": "效果",
    "text": "效果\nVisualizing example dataset for PCA.\n\nProgram paused. Press enter to continue.\nRunning PCA on example dataset.\n\nTop eigenvector:\n U[:,0] = -0.7071067811865472 -0.7071067811865475\n(you should expect to see -0.707107 -0.707107)\nProgram paused. Press enter to continue.\nDimension reduction on example dataset.\n\nProjection of the first example: [1.48127391]\n(this value should be about 1.481274)\n\nApproximation of the first example: -1.0474188259204957 -1.047418825920496\n(this value should be about  -1.047419 -1.047419)\n\nProgram paused. Press enter to continue.\nLoading face dataset.\n\nProgram paused. Press enter to continue.\nRunning PCA on face dataset.\n(this mght take a minute or two ...)\n\nProgram paused. Press enter to continue.\nDimension reduction for face dataset.\n\nThe projected data Z has a size of:\n(5000, 100)\n\nProgram paused. Press enter to continue.\nVisualizing the projected (reduced dimension) faces.\n\nProgram paused. Press enter to continue.\nK-Means iteration 0/10...\n\nK-Means iteration 1/10...\n\nK-Means iteration 2/10...\n\nK-Means iteration 3/10...\n\nK-Means iteration 4/10...\n\nK-Means iteration 5/10...\n\nK-Means iteration 6/10...\n\nK-Means iteration 7/10...\n\nK-Means iteration 8/10...\n\nK-Means iteration 9/10...\n\nProgram paused. Press enter to continue.\nProgram paused. Press enter to continue."
  },
  {
    "objectID": "posts/whiteboxgan.html",
    "href": "posts/whiteboxgan.html",
    "title": "White-box GAN",
    "section": "",
    "text": "关于论文 Learning to Cartoonize Using White-box Cartoon Representations"
  },
  {
    "objectID": "posts/whiteboxgan.html#learning-from-the-surface-representation",
    "href": "posts/whiteboxgan.html#learning-from-the-surface-representation",
    "title": "White-box GAN",
    "section": "Learning From the Surface Representation",
    "text": "Learning From the Surface Representation\n论文中提出要提取Surface Representation特征，即图像的平滑表面特征。作者通过调研使用differentiable guided filter对图像进行平滑处理。\nNOTE： 这里滤波器方法的原论文为Fast end-to-end trainable guided filter，但作者在使用时将滤波器权重进行了固定。\n定义滤波器为\\(\\mathcal{F}_{dgf}\\)，输入真实图像为\\(\\mathcal{I}_p\\)，输入动画图像为\\(\\mathcal{I}_c\\)，将生成图像的滤波结果与卡通图像的滤波结果进行判别器判别。\n\\[\n\\begin{aligned}\n\\mathcal{L}_{\\text {surface}}\\left(G, D_{s}\\right) &=\\log D_{s}\\left(\\mathcal{F}_{\\text {dgf}}\\left(\\boldsymbol{I}_{c}, \\boldsymbol{I}_{c}\\right)\\right) \\\\\n&+\\log \\left(1-D_{s}\\left(\\mathcal{F}_{\\text {dgf}}\\left(G\\left(\\boldsymbol{I}_{p}\\right), G\\left(\\boldsymbol{I}_{p}\\right)\\right)\\right)\\right)\n\\end{aligned}\\tag{1}\n\\]"
  },
  {
    "objectID": "posts/whiteboxgan.html#learning-from-the-structure-representation",
    "href": "posts/whiteboxgan.html#learning-from-the-structure-representation",
    "title": "White-box GAN",
    "section": "Learning From the Structure representation",
    "text": "Learning From the Structure representation\n这点我觉得比较好，作者想到superpixel后的图像具备较大的图像块以及清晰的边界，从结果上来看superpixel后的图像已经初步具备动画图像的一些特征了，因此很时候来学习原始图像的结构特征。作者在原始的的superpixel基础上加入了选择性搜索来合并一些超像素区域，这样可以获得更加大块的像素，并且考虑到传统的超像素算法是利用平均的方法来合成大像素的，对于图像将会降低部分的亮度与对比度，因此再提出自适应着色算法，提升superpixel的对比度。\n对于结构特征的损失实际上是预训练模型的编码差异损失： \\[\n\\begin{aligned}\n\\mathcal{L}_{\\text {structure}}=\\left\\|\\operatorname{VGG}_{n}\\left(G\\left(\\boldsymbol{I}_{p}\\right)\\right)-\\operatorname{VGG}_{n}\\left(\\mathcal{F}_{\\text {st}}\\left(G\\left(\\boldsymbol{I}_{p}\\right)\\right)\\right)\\right\\|\n\\end{aligned}\\tag{2}\n\\]"
  },
  {
    "objectID": "posts/whiteboxgan.html#learning-from-the-textural-representation",
    "href": "posts/whiteboxgan.html#learning-from-the-textural-representation",
    "title": "White-box GAN",
    "section": "Learning From the Textural Representation",
    "text": "Learning From the Textural Representation\n学习纹理特征，考虑到判别器可以通过颜色亮度等特征很容易的区别出真实图像与动画图像，因此使用灰度化的图像去除其颜色信息，提取其单通道的纹理特征来进行判别。 \\[\n\\begin{aligned}\n  \\mathcal{F}_{r c s}\\left(\\boldsymbol{I}_{r g b}\\right)=(1-\\alpha)\\left(\\beta_{1} * \\boldsymbol{I}_{r}+\\beta_2 * \\boldsymbol{I}_{g}+\\beta_{3} * \\boldsymbol{I}_{b}\\right)+\\alpha * \\boldsymbol{Y}\n\\end{aligned}\\tag{3}\n\\]\nNOTE: 这里和我之前学习过得AnimeGan有相似之处，不过这里的灰度化使用的是随机灰度化，即\\(\\beta\\)的值是从均匀分布中选取的。值得一提的是论文提到的\\(\\alpha\\)在开源代码中并没有出现，作者直接将均匀分布的范围控制在\\((0,1-\\alpha)\\)完成了相同的目标。"
  },
  {
    "objectID": "posts/whiteboxgan.html#image-up-sampling-using-total-variation-regularization-with-a-new-observation-model.",
    "href": "posts/whiteboxgan.html#image-up-sampling-using-total-variation-regularization-with-a-new-observation-model.",
    "title": "White-box GAN",
    "section": "Image up-sampling using total-variation regularization with a new observation model.",
    "text": "Image up-sampling using total-variation regularization with a new observation model.\n作者使用此论文中的total-variation损失强制生成结果更加平滑： \\[\n\\begin{aligned}\n  \\mathcal{L}_{t v}=\\frac{1}{H * W * C}\\left\\|\\nabla_{x}\\left(G\\left(\\boldsymbol{I}_{p}\\right)\\right)+\\nabla_{y}\\left(G\\left(\\boldsymbol{I}_{p}\\right)\\right)\\right\\|\n\\end{aligned}\\tag{4}\n\\]"
  },
  {
    "objectID": "posts/whiteboxgan.html#内容一致性损失",
    "href": "posts/whiteboxgan.html#内容一致性损失",
    "title": "White-box GAN",
    "section": "内容一致性损失",
    "text": "内容一致性损失\n这里也和AnimeGan有相似之处，不过AnimeGan里还使用了生成图像和当前动画图像的style loss。\n\\[\n\\begin{aligned}\n  \\mathcal{L}_{\\text {content}}=\\left\\|\\operatorname{VGG}_{n}\\left(G\\left(\\boldsymbol{I}_{p}\\right)\\right)-\\operatorname{VGG}_{n}\\left(\\boldsymbol{I}_{p}\\right)\\right\\|\n\\end{aligned}\\tag{5}\n\\]"
  },
  {
    "objectID": "posts/whiteboxgan.html#一些问题",
    "href": "posts/whiteboxgan.html#一些问题",
    "title": "White-box GAN",
    "section": "一些问题",
    "text": "一些问题\n我发现原论文开源代码中，貌似\\(G\\left(\\boldsymbol{I}_{p}\\right)\\)其实都被替换成了\\(\\mathcal{F}_{\\text {st}}\\left(G\\left(\\boldsymbol{I}_{p}\\right)\\right)\\)，先问问论文作者看看。"
  },
  {
    "objectID": "posts/x86-instructions.html",
    "href": "posts/x86-instructions.html",
    "title": "x86指令集使用汇总",
    "section": "",
    "text": "一些x86的硬件指令集相关信息。"
  },
  {
    "objectID": "posts/x86-instructions.html#给出硬件切换最少的指令集",
    "href": "posts/x86-instructions.html#给出硬件切换最少的指令集",
    "title": "x86指令集使用汇总",
    "section": "给出硬件切换最少的指令集",
    "text": "给出硬件切换最少的指令集\n对于一些专有npu的指令,一些内存搬运需要跳stride,如果我们是连续的内存,让指令内部硬件执行最少的跳转次数是最好的.\n    shape_n : 8\n    shape_c : 3\n    shape_h : 1\n    shape_w : 48\n    |\n    v\n    shape_n : 1\n    shape_c : 1\n    shape_h : 1\n    shape_w : 576"
  },
  {
    "objectID": "posts/yolo-error.html",
    "href": "posts/yolo-error.html",
    "title": "实现yolo时踩过的坑！",
    "section": "",
    "text": "终于把yolo v3框架写好了。支持多模型、多数据集、任意输出层数量、任意anchor数量、模型剪枝还适配k210.不要太好用～\n这里记录一下我之前的实现的问题出在哪里。"
  },
  {
    "objectID": "posts/yolo-error.html#bbox到达边界值",
    "href": "posts/yolo-error.html#bbox到达边界值",
    "title": "实现yolo时踩过的坑！",
    "section": "bbox到达边界值",
    "text": "bbox到达边界值\n当bbox的中心点位于边界值最大值时,如下图所示. \\[\\begin{aligned}\n    index&=floor(x*w) \\\\\n    \\because w&=3,x=1 \\Rightarrow floor(1*3)=3\n\\end{aligned}\n\\] 但使用3进行索引就会报错,所以我们需要限制一下bbox的中心坐标不能大于等于\\(1\\).\n+-------+-------+-------+\n|       |       |       |\n|       |       |       |\n|       |       |  +---------+\n+-------+-------+--|----+    |\n|       |       |  |    |    |\n|       |       |  |  center |\n|       |       |  |    |    |\n+-------+-------+--|----+    |\n|       |       |  +---------+\n|       |       |       |\n|       |       |       |\n+-------+-------+-------+"
  },
  {
    "objectID": "posts/yolo-error.html#当两个目标的label相同时",
    "href": "posts/yolo-error.html#当两个目标的label相同时",
    "title": "实现yolo时踩过的坑！",
    "section": "当两个目标的label相同时",
    "text": "当两个目标的label相同时\n如下图所示,当两个bbox真的非常靠近时,就会出现他们的label所在的位置都是相同的,就会出现label被覆盖的问题了.目前我将相同label时,后面的label分配给次优的anchor.\n+---------------+-------+\n|  +---------+  |       |\n| +|--------+|  |       |\n| ||    |   ||  |       |\n+-||--------||----------+\n| ||    |   ||  |       |\n| ||    |   ||  |       |\n| ||    |   ||  |       |\n+-|+---------+----------+\n| +---------+   |       |\n|       |       |       |\n|       |       |       |\n+-------+-------+-------+"
  },
  {
    "objectID": "posts/zhihu-markdown.html",
    "href": "posts/zhihu-markdown.html",
    "title": "zhihu markdown导入(2022年6月)",
    "section": "",
    "text": "我最近想把写的东西弄到知乎上,但是发现一堆问题,按照之前的方式出现了老多错误,因此记录一下.\n\n\n公式导入\n\n按照markdown4zhihu的方案,我本地把公式处理成知乎可以预览的模式之后, 本地显示如下:\n\n然后导入知乎后,全部变成空白了:\n\n然后我各种尝试才发现现在知乎可以直接接受$$.*$$的导入,但是不接受$.*$的导入, 因此修改代码如下:\ndef formula_ops(_lines):\n  # _lines = re.sub(r'\\$\\$\\n*([\\s\\S]*?)\\n*\\$\\$',\n  #                 r'\\n&lt;img src=\"https://www.zhihu.com/equation?tex=\\1\" alt=\"\\1\" class=\"ee_img tr_noresize\" eeimg=\"1\"&gt;\\n', _lines)\n  _lines = re.sub(r'\\$(.+?)\\$',\n                  r'$$\\1$$', _lines)\n  return _lines\n\n标注导入\n\n问了客服, 知乎暂时还不支持标注的导入, 而且手写起来特别麻烦, 简直吐血."
  },
  {
    "objectID": "posts/姿态解算.html#四元数",
    "href": "posts/姿态解算.html#四元数",
    "title": "四轴飞行器姿态解算介绍",
    "section": "四元数",
    "text": "四元数\n因为网络上的欧拉角的文章讲解的也比较详细了，所以这里直接开始介绍四元数。\n\n\n基本定义 顾名思义四元数就是由四个元构成的数： \\[\\textbf{Q}(q_0+q_1+q_2+q_3)=q_0+q_1 \\textbf{i} +q_2 \\textbf{j} +q_3 \\textbf{k} \\]\n其中\\(q_0\\)、\\(q_1\\)、\\(q_2\\)、\\(q_3\\)是实数，\\(\\textbf{i}\\)、\\(\\textbf{j}\\)、\\(\\textbf{k}\\) 是互相正交的单位向量，又是虚单位\\(\\sqrt{-1}\\),具体规定的四元数乘法为： \\[\\begin{align}\\textbf{i} \\bigotimes \\textbf{i} &=-1,\\quad  \\textbf{j}\\bigotimes  \\textbf{j} =-1,\\quad \\textbf{k}  \\bigotimes  \\textbf{k} =-1 \\quad \\\\\n  \\textbf{i}  \\bigotimes  \\textbf{j} &= \\textbf{k} ,\\ \\quad  \\textbf{j}  \\bigotimes  \\textbf{k} = \\textbf{i} ,\\ \\quad  \\textbf{k}  \\bigotimes i= \\textbf{j}  \\quad \\\\\n   \\textbf{j}  \\bigotimes  \\textbf{i} &=- \\textbf{k} ,\\quad  \\textbf{k}  \\bigotimes  \\textbf{j} =- \\textbf{i} ,\\quad  \\textbf{i}  \\bigotimes  \\textbf{k} =- \\textbf{j}  \\quad\n   \\end{align} \\]\n表达方式\n\n矢量式 \\[\\textbf{Q}=q_0+\\textbf{q}\\]\n复数式 \\[\\textbf{Q}=q_0+q_1\\textbf{i}+q_2 \\textbf{j} +q_3 \\textbf{k}\\] 记\\(Q\\)的共轭复数为\\(Q^*\\) \\[\\textbf{Q*}=q_0-q_1\\textbf{i}-q_2 \\textbf{j} -q_3 \\textbf{k}\\]\n矩阵式 \\[\\textbf{Q}=\\begin{bmatrix} q_0\\\\q_1\\\\q_2\\\\q_3 \\end{bmatrix}\\]\n\n四元数大小 \\[||\\textbf{Q}||=q_0^2+q_1^2+q_2^2+q_3^2\\]\n四元数运算\n\n加法 设： \\[\\textbf{Q}=q_0+q_1 \\textbf{i} +q_2 \\textbf{j} +q_3 \\textbf{k}\\\\\n  \\textbf{P}=p_0+p_1 \\textbf{i} +p_2 \\textbf{j} +p_3 \\textbf{k}\\] 则： \\[\\textbf{Q}\\pm\\textbf{P}=(q_0\\pm p_0)+(q_1\\pm p_1) \\textbf{i} +(q_2\\pm p_2) \\textbf{j} +(q_3\\pm p_3) \\textbf{k}\\]\n乘法 \\[\\begin{align}\n  \\textbf{P}\\bigotimes \\textbf{Q}& =(\\textbf{P}=p_0+p_1 \\textbf{i} +p_2 \\textbf{j} +p_3               \\textbf{k})\\bigotimes (q_0+q_1 \\textbf{i} +q_2 \\textbf{j} +q_3 \\textbf{k})\\\\\n  &=(p_0q_0-p_1q_1-p_2q_2-p_3q_3)+(p_0q_1+p_1q_0+p_2q_3-p_3q_2)\\textbf{i}\\\\\n  &+(p_0q_2+p_2q_0+p_3q_1-p_1q_3)\\textbf{j}+(p_0q_3+p_3q_0+p_1q_2-p_2q_1)\\textbf{k}\\\\\n  &=r_0+r_1\\textbf{i}+r_2\\textbf{j}+r_3\\textbf{k}\n  \\end{align}\\] 乘法即简单的系数相乘。 当然也可以写成矩阵形式： \\[\\begin{bmatrix} r_0 \\\\ r_1 \\\\r_2 \\\\r_3\\end{bmatrix}= \\begin{gather*}\\begin{bmatrix}\n  & p_0 \\ &-p_1 \\ &-p_2 \\ &-p_3\\\\\n  & p_1 \\ &p_0 \\ &-p_3 \\ &p_2\\\\\n  & p_2 \\ &p_3 \\ &p_0 \\ &-p_1\\\\\n  & p_3 \\ &-p_2 \\ &p_1 \\ &p_0\\\\\n  \\end{bmatrix}\\end{gather*}\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\end{bmatrix}\n  \\] 要注意： \\[\\textbf{P}\\bigotimes\\textbf{Q}\\neq\\textbf{Q}\\bigotimes\\textbf{P}\\] 四元数的乘法也满足分配律和结合律 \\[\\textbf{P}\\bigotimes(\\textbf{Q}+\\textbf{R})=\\textbf{P}\\bigotimes\\textbf{Q}+\\textbf{P}\\bigotimes\\textbf{R}\\]\n\n四元数与姿态阵的关系\n设有参考坐标系\\(R\\),坐标轴为\\(x_0,y_0,z_0\\),坐标轴方向的单位向量为\\(\\textbf{i}_0,\\textbf{j}_0,\\textbf{k}_0\\)。刚体相对\\(R\\)系做定点转动，定点为\\(O\\)。取坐标系\\(b\\)与刚体固联，\\(b\\)系的坐标轴为\\(x,y,z\\)，坐标方向的单位向量为\\(\\textbf{i},\\textbf{j},\\textbf{k}\\)。假设初始状态下\\(b\\)系与\\(R\\)系重合。为了便于分析刚体的空间角位置，在刚体上取一点\\(A\\)，转动点为点\\(O\\)。如下图所示：\n\n设刚体以\\(\\boldsymbol{\\omega}=\\omega_x\\textbf{i}+\\omega_y\\textbf{j}+\\omega_z\\textbf{k}\\)相对于\\(\\textbf{R}\\)系旋转，初始时刻位置向量为\\(\\textbf{OA}=\\textbf{r}\\),经过时间\\(t\\)后位置向量处于\\(\\mathop{\\textbf{OA}'}=\\mathop{\\textbf{r}'}\\)。根据欧拉定理，仅考虑刚体在0时刻和\\(t\\)时刻的角位置时，刚体从\\(\\mathop{\\textbf{A}}\\)位置转动到\\(\\mathop{\\textbf{A}'}\\)可等效成绕瞬轴\\(\\mu\\)(单位向量)转过\\(\\theta\\)角一次完成。这样，单位向量做圆锥运动，\\(\\mathop{\\textbf{A}}\\)和\\(\\mathop{\\textbf{A}'}\\)位于同一个圆上，\\(\\mathop{\\textbf{r}}\\)和\\(\\mathop{\\textbf{r}'}\\)位于同一圆锥面上。\n\n下面分析\\(\\mathop{\\textbf{r}}\\)和\\(\\mathop{\\textbf{r}'}\\)的关系。在圆上取一点\\(B\\)，使\\(\\angle A\\mathop{O'}B=90^\\circ\\)，由图得：\n\\[\n  \\begin{align}\n      \\mathop{\\boldsymbol{O}\\boldsymbol{O}'} &=(\\boldsymbol{r}\\cdot\\boldsymbol{u})\\boldsymbol{u} \\\\\n      \\mathop{\\boldsymbol{O}'\\boldsymbol{A}} &=\\boldsymbol{r}-\\mathop{\\boldsymbol{O}\\boldsymbol{O}'}=\\boldsymbol{r}-(\\boldsymbol{r}\\cdot\\boldsymbol{u})\\boldsymbol{u}\n  \\end{align}\n  \\]"
  },
  {
    "objectID": "posts/资源整理.html",
    "href": "posts/资源整理.html",
    "title": "资源整理",
    "section": "",
    "text": "这是一个资源汇总的地方,我会记录一些好的资源"
  },
  {
    "objectID": "posts/资源整理.html#深度学习类",
    "href": "posts/资源整理.html#深度学习类",
    "title": "资源整理",
    "section": "深度学习类",
    "text": "深度学习类\n\nTensorflow\n\n谷歌官方中文教程 这个比看什么书都好的多,不要去那个什么tensorflow.cn,那个没有这个好."
  },
  {
    "objectID": "posts/资源整理.html#无监督学习算法",
    "href": "posts/资源整理.html#无监督学习算法",
    "title": "资源整理",
    "section": "无监督学习算法",
    "text": "无监督学习算法\n\n聚类分析经典算法讲解已经实现 作者给出了好几个聚类算法的java实现\n\n\nDBSCAN聚类算法\n\nDBSCAN聚类原理 详细介绍了此算法的实现\nscikit-learn中的DBSCAN 详细介绍了sklearn中的dbscan\n聚类算法-DBSCAN-C++实现 这个程序最有参考价值\n\n\n\nSOM聚类\n\n伯明翰大学som教程 仔细看。\nSOM图解加动画 这个非常有意思，能看懂"
  },
  {
    "objectID": "posts/资源整理.html#初步教程",
    "href": "posts/资源整理.html#初步教程",
    "title": "资源整理",
    "section": "初步教程",
    "text": "初步教程\n\n斯坦福免费的语音处理课程 好的教程还不要钱! 学习中~"
  },
  {
    "objectID": "posts/资源整理.html#numpy",
    "href": "posts/资源整理.html#numpy",
    "title": "资源整理",
    "section": "numpy",
    "text": "numpy\n\nnumpy与matlab操作对比 对于会matlab的人来说入门会非常快."
  },
  {
    "objectID": "posts/资源整理.html#simulink",
    "href": "posts/资源整理.html#simulink",
    "title": "资源整理",
    "section": "SIMULINK",
    "text": "SIMULINK\n\n通信系统建模与仿真实例分析 清华大学老师编写的多媒体教案,值得一看"
  },
  {
    "objectID": "posts/资源整理.html#宏定义",
    "href": "posts/资源整理.html#宏定义",
    "title": "资源整理",
    "section": "宏定义",
    "text": "宏定义\n\n编译器,操作系统,cpu预定义宏 跨平台就需要用到他了"
  },
  {
    "objectID": "posts/资源整理.html#c语法参考",
    "href": "posts/资源整理.html#c语法参考",
    "title": "资源整理",
    "section": "C++语法参考",
    "text": "C++语法参考\n\n中文C++参考 好用就是可能少了那么点\nC++官方参考 c++与c全部的语法应该都可以找到"
  },
  {
    "objectID": "posts/资源整理.html#c扩展库",
    "href": "posts/资源整理.html#c扩展库",
    "title": "资源整理",
    "section": "C++扩展库",
    "text": "C++扩展库\n\nEigen\n\nEigen官网 一个C++线性代数库,支持矩阵以及数学运算\nEigen官方教程 学就完事了\neigen与matlab对比 这样快速入门"
  },
  {
    "objectID": "posts/资源整理.html#pcb设计",
    "href": "posts/资源整理.html#pcb设计",
    "title": "资源整理",
    "section": "PCB设计",
    "text": "PCB设计\n\n荔枝one的设计过程 可以在里面看到泽畔是如何设计全志A13的硬件过程"
  },
  {
    "objectID": "posts/资源整理.html#课程",
    "href": "posts/资源整理.html#课程",
    "title": "资源整理",
    "section": "课程",
    "text": "课程\n\n宾夕法尼亚大学matuszek的主页 可以看看这位老师的课件，非常不错\n宾夕法尼亚大学编程语言和技巧全篇课程 上课语言是java，看看别人的大学，是真的牛逼。"
  },
  {
    "objectID": "posts/资源整理.html#回溯法",
    "href": "posts/资源整理.html#回溯法",
    "title": "资源整理",
    "section": "回溯法",
    "text": "回溯法\n\n一位外国老师matuszek写的教程 非常好，只要认真看\n以8皇后为例的回溯法讲解 很 好"
  },
  {
    "objectID": "posts/资源整理.html#数学计算",
    "href": "posts/资源整理.html#数学计算",
    "title": "资源整理",
    "section": "数学计算",
    "text": "数学计算\n\n适用嵌入式的fft程序 stm32f030 128位实测9200us！"
  },
  {
    "objectID": "posts/资源整理.html#rtos",
    "href": "posts/资源整理.html#rtos",
    "title": "资源整理",
    "section": "RTOS",
    "text": "RTOS\n\n自己动手写RTOS 这个必须要学"
  },
  {
    "objectID": "posts/资源整理.html#字符编码",
    "href": "posts/资源整理.html#字符编码",
    "title": "资源整理",
    "section": "字符编码",
    "text": "字符编码\n\nUnicode全集 辅助查询"
  },
  {
    "objectID": "posts/资源整理.html#转换工具",
    "href": "posts/资源整理.html#转换工具",
    "title": "资源整理",
    "section": "转换工具",
    "text": "转换工具\n1.英文转字符画"
  },
  {
    "objectID": "posts/资源整理.html#linux驱动",
    "href": "posts/资源整理.html#linux驱动",
    "title": "资源整理",
    "section": "linux驱动",
    "text": "linux驱动\n\n知秋一叶驱动连载 华清的老师连载的\n庖丁解牛Linux内核 精品课程"
  },
  {
    "objectID": "posts/cute-concepts.html",
    "href": "posts/cute-concepts.html",
    "title": "Cute概念速通",
    "section": "",
    "text": "这篇文章将快速的介绍Cute中的一些基本概念、 layout algorithm、swizzle等，具体代码位于cute概念速通。\nCode\nimport pycute as cute\nimport numpy as np\nimport itertools\nnp.set_printoptions(linewidth=100)"
  },
  {
    "objectID": "posts/cute-concepts.html#mode",
    "href": "posts/cute-concepts.html#mode",
    "title": "Cute概念速通",
    "section": "Mode",
    "text": "Mode\nlayout可以通过mode取其中的每个元素\n\n\nCode\nprint(f'mode 0: {layout[0]}')\nprint(f'mode 1: {layout[1]}')\n\n\nmode 0: 2:3\nmode 1: 3:6"
  },
  {
    "objectID": "posts/cute-concepts.html#domain-codomain",
    "href": "posts/cute-concepts.html#domain-codomain",
    "title": "Cute概念速通",
    "section": "Domain & Codomain",
    "text": "Domain & Codomain\ndomain是layout的索引空间，codomain是layout的访问空间。\n\n\nCode\nprint(f'domain: {layout.size()}')\nprint(f'codomain: {layout.cosize()}')\n\n\ndomain: 6\ncodomain: 16"
  },
  {
    "objectID": "posts/cute-concepts.html#zipped_dividetiled_divide",
    "href": "posts/cute-concepts.html#zipped_dividetiled_divide",
    "title": "Cute概念速通",
    "section": "zipped_divide/tiled_divide",
    "text": "zipped_divide/tiled_divide\n不同divide的区别在于返回的layout的group方式不同:\n\n\nCode\nprint('zipped_divide:', cute.zipped_divide(a, (8, 4)))\nprint('tiled_divide:', cute.tiled_divide(a, (8, 4)))\n\n\nzipped_divide: ((8, 4), (16, 8)):((32, 1), (256, 4))\ntiled_divide: ((8, 4), 16, 8):((32, 1), 256, 4)"
  },
  {
    "objectID": "posts/cute-concepts.html#zipped-product-tiled-product",
    "href": "posts/cute-concepts.html#zipped-product-tiled-product",
    "title": "Cute概念速通",
    "section": "Zipped Product & Tiled Product",
    "text": "Zipped Product & Tiled Product\n同样也是把inner part放到左边，并且也是在group方式上存在区别。\n\n\nCode\nprint('zipped:', cute.zipped_product(a, (8, 4)))\nprint('tiled:', cute.tiled_product(a, (8, 4)))\n\n\nzipped: ((128, 32), (8, 4)):((32, 1), (1, 32))\ntiled: ((128, 32), 8, 4):((32, 1), 1, 32)"
  },
  {
    "objectID": "posts/cute-concepts.html#block-rroduct",
    "href": "posts/cute-concepts.html#block-rroduct",
    "title": "Cute概念速通",
    "section": "Block Rroduct",
    "text": "Block Rroduct\n如果是logical product，那么最后的domain就是(A, B)，按照cute默认的迭代方式进行访问，就是先访问A的所有元素，然后按B的方式访问扩展后的A元素。\n但是有没有可能，通过调整domain，在默认的迭代方式下实现不同的访问模式呢？ 比如我们有一个小的2维矩阵tiledA(m,n)，它匹配硬件计算的粒度，但是在内存中他是以A(3m, 4n)的形式存储的。 那么我们希望在默认的迭代方式下， 先将A的n维度访问完，再访问m维度，这个需求就需要blocked product来实现。\n其实blocked product的本质就是调整logical product的domain顺序， 让inner part的维度在左边， outer part的维度在右边。 这样在默认的迭代方式下，就实现了先访问inner part，再访问outer part的效果。而实际上在内存上的元素顺序并没有任何改变。\n\n\nCode\ndef hier_zip(layoutA, layoutB):\n  assert len(layoutA) == len(layoutB)\n  return cute.make_layout(itertools.chain((cute.make_layout(layoutA[i], layoutB[i]) for i in range(0, len(layoutA)))))\n\ndef blocked_product(block, tiler):\n  res = cute.logical_product(block, tiler)\n  return hier_zip(res[0], res[1])\n\na = cute.Layout((2, 5), (5, 1))\nb = cute.Layout((3, 4), (1, 3))\nlogical_producted = cute.logical_product(a, b)\nblocked_producted = blocked_product(a, b)  # 实际上就是把logical product拆分且zip\nprint(\"logical_producted\", logical_producted)\nprint(\"blocked_producted\", blocked_producted)\n\n\nlogical_producted ((2, 5), (3, 4)):((5, 1), (10, 30))\nblocked_producted ((2, 3), (5, 4)):((5, 10), (1, 30))\n\n\n我写了一个print_offsets的函数，打印按默认的访问顺序下，layout所映射的offset的情况， 用于展示blocked product的效果。 首先是看logical product的情况，这里crd默认也是按col major生成的，所以layout a会按列呈现在offset矩阵中：\n\n\nCode\ndef print_offsets(layout: cute.Layout):\n  rk = len(layout)\n  shape = [layout[i].size() for i in range(rk)]\n  arr = np.zeros(shape, dtype=np.int32)\n  for crd in itertools.product(*[range(shape[i]) for i in range(rk)]):\n    arr[crd] = layout(crd)\n  print(arr)\n\nprint_offsets(logical_producted)\n\n\n[[  0  10  20  30  40  50  60  70  80  90 100 110]\n [  5  15  25  35  45  55  65  75  85  95 105 115]\n [  1  11  21  31  41  51  61  71  81  91 101 111]\n [  6  16  26  36  46  56  66  76  86  96 106 116]\n [  2  12  22  32  42  52  62  72  82  92 102 112]\n [  7  17  27  37  47  57  67  77  87  97 107 117]\n [  3  13  23  33  43  53  63  73  83  93 103 113]\n [  8  18  28  38  48  58  68  78  88  98 108 118]\n [  4  14  24  34  44  54  64  74  84  94 104 114]\n [  9  19  29  39  49  59  69  79  89  99 109 119]]\n\n\n但是如果采用blocked product，则是按[m * 3, n * 4]这样的shape来采样，当我们访问A的n维度时，是优先访问完一个tile的n维度，然后跳到下一个tile的n维度进行访问：\n\n\nCode\nprint_offsets(blocked_producted)\n\n\n[[  0   1   2   3   4  30  31  32  33  34  60  61  62  63  64  90  91  92  93  94]\n [  5   6   7   8   9  35  36  37  38  39  65  66  67  68  69  95  96  97  98  99]\n [ 10  11  12  13  14  40  41  42  43  44  70  71  72  73  74 100 101 102 103 104]\n [ 15  16  17  18  19  45  46  47  48  49  75  76  77  78  79 105 106 107 108 109]\n [ 20  21  22  23  24  50  51  52  53  54  80  81  82  83  84 110 111 112 113 114]\n [ 25  26  27  28  29  55  56  57  58  59  85  86  87  88  89 115 116 117 118 119]]\n\n\n本质上两个layout表达的offset是相同的，区别在于使用cute默认的迭代顺序下他们的行为，这一点其实是十分重要的，这会令logical/blocked layout进行composite的时候得到完全不同的结果。\n当然cute layout的自由度还是很大的，比如对于logical product的结果，我们也可以自行按block coord来采样，再采样tile内部，也是可以得到相同的offset序列的：\n\n\nCode\ntile = logical_producted[0]\nblock = logical_producted[1]\nfor block_crd in itertools.product(*[range(mode.size()) for mode in block]):\n  block_offset = block(block_crd)\n  array = np.zeros(tile.shape, dtype=np.int32)\n  for tile_crd in itertools.product(*[range(mode.size()) for mode in tile]):\n    offset = tile(tile_crd)\n    array[tile_crd] = offset + block_offset\n  print(f\"block {block_crd}:\\n\", array)\n\n\nblock (0, 0):\n [[0 1 2 3 4]\n [5 6 7 8 9]]\nblock (0, 1):\n [[30 31 32 33 34]\n [35 36 37 38 39]]\nblock (0, 2):\n [[60 61 62 63 64]\n [65 66 67 68 69]]\nblock (0, 3):\n [[90 91 92 93 94]\n [95 96 97 98 99]]\nblock (1, 0):\n [[10 11 12 13 14]\n [15 16 17 18 19]]\nblock (1, 1):\n [[40 41 42 43 44]\n [45 46 47 48 49]]\nblock (1, 2):\n [[70 71 72 73 74]\n [75 76 77 78 79]]\nblock (1, 3):\n [[100 101 102 103 104]\n [105 106 107 108 109]]\nblock (2, 0):\n [[20 21 22 23 24]\n [25 26 27 28 29]]\nblock (2, 1):\n [[50 51 52 53 54]\n [55 56 57 58 59]]\nblock (2, 2):\n [[80 81 82 83 84]\n [85 86 87 88 89]]\nblock (2, 3):\n [[110 111 112 113 114]\n [115 116 117 118 119]]"
  },
  {
    "objectID": "posts/cute-concepts.html#raked-product",
    "href": "posts/cute-concepts.html#raked-product",
    "title": "Cute概念速通",
    "section": "Raked product",
    "text": "Raked product\nRaked product和blocked product类似，也是调整logical product的domain顺序，不过是把outer part的维度放到左边， inner part的维度放到右边。 这样在默认的迭代方式下，就实现了先访问outer part，再访问inner part的效果。而实际上在内存上的元素顺序并没有任何改变。\n\n\nCode\ndef raked_product(block, tiler):\n  res = cute.logical_product(block, tiler)\n  return hier_zip(res[1], res[0])\n\na = cute.Layout((2, 5), (5, 1))\nb = cute.Layout((3, 4), (1, 3))\nlogical_producted = cute.logical_product(a, b)\nraked_producted = raked_product(a, b) # 实际上就是把logical layout进行了拆分zip, 不过顺序反过来了。\nprint('logical_producted', logical_producted)\nprint('raked_producted', raked_producted)\n\n\nlogical_producted ((2, 5), (3, 4)):((5, 1), (10, 30))\nraked_producted ((3, 2), (4, 5)):((10, 5), (30, 1))\n\n\n它同样也是先遍历完A的某一个维度，只是遍历维度内部的顺序从inner优先变成了outer优先。\n\n\nCode\nprint_offsets(raked_producted)\n\n\n[[  0  30  60  90   1  31  61  91   2  32  62  92   3  33  63  93   4  34  64  94]\n [ 10  40  70 100  11  41  71 101  12  42  72 102  13  43  73 103  14  44  74 104]\n [ 20  50  80 110  21  51  81 111  22  52  82 112  23  53  83 113  24  54  84 114]\n [  5  35  65  95   6  36  66  96   7  37  67  97   8  38  68  98   9  39  69  99]\n [ 15  45  75 105  16  46  76 106  17  47  77 107  18  48  78 108  19  49  79 109]\n [ 25  55  85 115  26  56  86 116  27  57  87 117  28  58  88 118  29  59  89 119]]"
  },
  {
    "objectID": "posts/cute-concepts.html#right-inverse",
    "href": "posts/cute-concepts.html#right-inverse",
    "title": "Cute概念速通",
    "section": "Right Inverse",
    "text": "Right Inverse\n它的数学表示为：\nLayout_Rinv(Layout(index)) = index\n原始的layout是一个从index到offset的函数，所以right inverse就是得到一个layout invese可以从offset 到 index。\ncrd2idx(coord)\n      |\n      v\n    index\n      |\n      v\n  F_L(index) -&gt; offset\n      ∧             |\n      |             v\n   index &lt;- F_Linv(offset)\n\n\nCode\nlayout = cute.Layout((32, 64), (64, 1))\nx = cute.crd2idx((3, 4), layout.shape)\nprint('layout:', layout, \"index:\", x, \"offset:\", layout(3, 4))  # 3 * 64 + 4\n\n\nlayout: (32, 64):(64, 1) index: 131 offset: 196\n\n\n\n\nCode\nlayout_inv = cute.right_inverse(layout)\nx = layout_inv(cute.idx2crd(196, layout_inv.shape))\nprint('layout_inv', layout_inv, \"index:\", 196, \"offset:\", x)\n\n\nlayout_inv (64, 32):(32, 1) index: 196 offset: 131\n\n\n所以其实它比较大的用处是, 这样可以快速找到原来的一个coord所对应的index\nF_Linv(F_L(coord)) = index\n\n\nCode\ncrd = (3, 4)\nindex = layout_inv(layout(*crd))\nassert layout(index) == layout(*crd)\nprint(f\"we find the coord {crd}'s index is:\", index)\n\n\nwe find the coord (3, 4)'s index is: 131"
  }
]