<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>skimage中resize内存泄漏 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">skimage中resize内存泄漏</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">skimage中resize内存泄漏</h1><div class="post-meta">2019-11-01<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.4k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 17</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%8E%9F%E5%9B%A0"><span class="toc-number">1.</span> <span class="toc-text">问题原因</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E6%9C%AC%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="toc-number">1.1.</span> <span class="toc-text">原本的代码:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E5%90%8E%E4%BB%A3%E7%A0%81"><span class="toc-number">1.2.</span> <span class="toc-text">修改后代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AA%E6%B5%8B%E8%AF%95resize"><span class="toc-number">1.3.</span> <span class="toc-text">只测试resize</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E8%AF%BB%E5%8F%96%E6%96%B9%E5%BC%8F%E5%90%8E"><span class="toc-number">1.4.</span> <span class="toc-text">修改读取方式后</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%AE%9A%E4%BD%8D%E9%97%AE%E9%A2%98"><span class="toc-number">1.5.</span> <span class="toc-text">进一步定位问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5%E4%B8%8B%E5%88%AB%E7%9A%84tf.data%E8%BE%93%E5%85%A5%E6%98%AF%E5%90%A6%E4%BC%9A%E5%87%BA%E7%8E%B0%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98"><span class="toc-number">1.6.</span> <span class="toc-text">检查下别的tf.data输入是否会出现内存泄漏问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.7.</span> <span class="toc-text">新的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%A3%80%E6%9F%A5%E6%98%AF%E4%B8%8D%E6%98%AFtf.example%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%AF%BC%E8%87%B4%E7%9A%84."><span class="toc-number">1.8.</span> <span class="toc-text">进一步检查是不是tf.example的格式导致的.</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8D%E6%8A%98%E8%85%BE%E4%BA%86"><span class="toc-number">2.</span> <span class="toc-text">不折腾了</span></a></li></ol></div></div><div class="post-content"><p>我今天运行个模型,跑着跑着内存就泄漏了,我很奇怪,然后用<code>memory_profiler</code>分析了下内存泄漏的点.发现是<code>skimage</code>的<code>resize</code>中出现了泄漏.</p>
<span id="more"></span>
<h1 id="问题原因">问题原因</h1>
<h2 id="原本的代码">原本的代码:</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = resize(img, [<span class="built_in">int</span>(img.shape[<span class="number">0</span>] * resize_factor),</span><br><span class="line">           <span class="built_in">int</span>(img.shape[<span class="number">1</span>] * resize_factor)],</span><br><span class="line">     preserve_range=<span class="literal">True</span>).astype(np.uint8)</span><br></pre></td></tr></table></figure>
<p>然后分析内存的时候得到如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Line #    Mem usage    Increment   Line Contents</span><br><span class="line">================================================</span><br><span class="line">130 871.9648 MiB   0.0000 MiB           img = img_as_ubyte(resize(img, [int(img.shape[0] * resize_factor),</span><br><span class="line">131 924.0703 MiB  52.1055 MiB                                           int(img.shape[1] * resize_factor)]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">130 924.0703 MiB   0.0000 MiB           img = img_as_ubyte(resize(img, [int(img.shape[0] * resize_factor),</span><br><span class="line">131 946.7148 MiB  22.6445 MiB                                           int(img.shape[1] * resize_factor)]))</span><br><span class="line"></span><br><span class="line">130 875.6328 MiB   0.0000 MiB           img = img_as_ubyte(resize(img, [int(img.shape[0] * resize_factor),</span><br><span class="line">131 932.0742 MiB  56.4414 MiB                                           int(img.shape[1] * resize_factor)]))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/11/01/skimage-mem-leak/Figure_1.png" /></p>
<h2 id="修改后代码">修改后代码</h2>
<p>我参考了网上的人,修改之后发现还是不行 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = img_as_ubyte(resize(img, [<span class="built_in">int</span>(img.shape[<span class="number">0</span>] * resize_factor),</span><br><span class="line">                                <span class="built_in">int</span>(img.shape[<span class="number">1</span>] * resize_factor)]))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Line #    Mem usage    Increment   Line Contents</span><br><span class="line">================================================</span><br><span class="line">132 795.3477 MiB   0.0000 MiB           img = resize(img, [int(img.shape[0] * resize_factor),</span><br><span class="line">133 795.3477 MiB   0.0000 MiB                              int(img.shape[1] * resize_factor)],</span><br><span class="line">134 807.4141 MiB  12.0664 MiB                        preserve_range=True).astype(np.uint8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">132 807.4141 MiB   0.0000 MiB           img = resize(img, [int(img.shape[0] * resize_factor),</span><br><span class="line">133 807.4141 MiB   0.0000 MiB                              int(img.shape[1] * resize_factor)],</span><br><span class="line">134 823.9102 MiB  16.4961 MiB                        preserve_range=True).astype(np.uint8)</span><br><span class="line"></span><br><span class="line">132 824.1680 MiB   0.0000 MiB           img = resize(img, [int(img.shape[0] * resize_factor),</span><br><span class="line">133 824.1680 MiB   0.0000 MiB                              int(img.shape[1] * resize_factor)],</span><br><span class="line">134 824.1758 MiB   0.0078 MiB                        preserve_range=True).astype(np.uint8)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/11/01/skimage-mem-leak/Figure_2.png" /></p>
<h2 id="只测试resize">只测试resize</h2>
<p>之前的分析文件: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Line #    Mem usage    Increment   Line Contents</span><br><span class="line">================================================</span><br><span class="line">282 880.1875 MiB   0.0000 MiB           if boxes is None:</span><br><span class="line">283 895.7812 MiB  15.5938 MiB               self._resize_neg_img(im_in, img)</span><br><span class="line">284                                     else:</span><br><span class="line">285                                         boxes = self._resize_pos_img(im_in, img, boxes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">282 840.2148 MiB   0.0000 MiB           if boxes is None:</span><br><span class="line">283 889.9688 MiB  49.7539 MiB               self._resize_neg_img(im_in, img)</span><br><span class="line">284                                     else:</span><br><span class="line">285                                         boxes = self._resize_pos_img(im_in, img, boxes)</span><br><span class="line"></span><br><span class="line">282 889.9688 MiB   0.0000 MiB           if boxes is None:</span><br><span class="line">283 890.1172 MiB   0.1484 MiB               self._resize_neg_img(im_in, img)</span><br><span class="line">284                                     else:</span><br><span class="line">285                                         boxes = self._resize_pos_img(im_in, img, boxes)</span><br></pre></td></tr></table></figure></p>
<p>我发现都是<code>_resize_neg_img</code>函数出的问题,然后就只测试这个函数看看有没有问题,发现好像并没有内存泄漏.</p>
<p><img src="/2019/11/01/skimage-mem-leak/Figure_2.png" /></p>
<p>我发现内存泄漏还是得在<code>tf.data</code>对象运行起来之后才能发现的.所以我得继续测试.</p>
<p>这里用了那个<code>line_profiler</code>测试了一下代码执行时间.发现了一个大问题:
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">img = img_as_ubyte(rescale(img, resize_factor, multichannel=True))  <span class="comment"># 107486.0ms</span></span><br><span class="line">img = resize(img, None, fx=resize_factor, fy=resize_factor)  <span class="comment"># 849.4ms</span></span><br></pre></td></tr></table></figure>
!用<code>skimage</code>和<code>opencv</code>的<code>resize</code>速度居然差10倍以上.</p>
<h2 id="修改读取方式后">修改读取方式后</h2>
<p>最后我虽然没有找到哪里出现了问题,但是我首先修改了图像处理函数都使用<code>opencv</code>完成.</p>
<p>第二,我看了一下<code>tfrecord</code>的制作方式,将数据转换为了<code>tfrecord</code>的存储方式.
制作时,我们可以选择将图像解码前的<code>buf</code>序列化,也可以先读取出来然后用<code>tf.io.serialize_tensor()</code>的方式进行序列化,后者所存储的<code>tfrecord</code>会大10倍以上.我经过测试之后发现,直接存未序列化的图像数据比较好,因为读取的时候对<code>tensor</code>反序列化和解码图像速度差不太多.
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_example</span>(<span class="params">img_string: <span class="built_in">str</span>, label: <span class="built_in">int</span>, bbox_string: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; make example &quot;&quot;&quot;</span></span><br><span class="line">    feature = &#123;</span><br><span class="line">        <span class="string">&#x27;img_raw&#x27;</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_string])),</span><br><span class="line">        <span class="string">&#x27;label&#x27;</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),</span><br><span class="line">        <span class="string">&#x27;bbox&#x27;</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[bbox_string])),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.io.TFRecordWriter(<span class="built_in">str</span>(record_file)) <span class="keyword">as</span> writer:</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> tqdm(idx_list, total=<span class="built_in">len</span>(idx_list)):</span><br><span class="line">        im_buf, label, bboxes = data[idx]</span><br><span class="line">        bboxes = tf.io.serialize_tensor(bboxes).numpy()</span><br><span class="line">        serialized_example = make_example(im_buf.tostring(), label, bboxes)</span><br><span class="line">        writer.write(serialized_example)</span><br></pre></td></tr></table></figure></p>
<p>读取代码实例,<code>tfrecord</code>总体来说还是比较简单的,唯一的不方便就是我们调试的时候不方便,我之前都是从列表里面读取图像的,那样调试的时候直接执行就能很快找到问题,现在需要先启动一个小的<code>dataset</code>对象才可以进行图像读取,比较难受.综合考虑还是性价比较高,训练的速度有明显提升:
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_datapipe</span>(<span class="params">self, pos_tfrecord: tf.Tensor, neg_tfrecord: tf.Tensor,</span></span><br><span class="line"><span class="params">                   batch_size: <span class="built_in">int</span>, rand_seed: <span class="built_in">int</span>, is_augment: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">                   is_normlize: <span class="built_in">bool</span>, is_training: <span class="built_in">bool</span></span>) -&gt; tf.data.Dataset:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_wapper</span>(<span class="params">raw_img: np.ndarray, ann: np.ndarray, is_augment: <span class="built_in">bool</span></span>) -&gt; [np.ndarray, <span class="built_in">tuple</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot; wapper for process image and ann to label &quot;&quot;&quot;</span></span><br><span class="line">        raw_img, ann = self.process_img(raw_img, ann, is_augment, <span class="literal">True</span>, <span class="literal">False</span>)</span><br><span class="line">        labels = self.ann_to_label(ann)</span><br><span class="line">        <span class="keyword">return</span> (raw_img, *labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parser</span>(<span class="params">stream: <span class="built_in">bytes</span></span>):</span><br><span class="line">        example = tf.io.parse_single_example(stream, &#123;</span><br><span class="line">            <span class="string">&#x27;img_raw&#x27;</span>: tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">            <span class="string">&#x27;label&#x27;</span>: tf.io.FixedLenFeature([], tf.int64),</span><br><span class="line">            <span class="string">&#x27;bbox&#x27;</span>: tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">        &#125;)  <span class="comment"># type:<span class="built_in">dict</span></span></span><br><span class="line"></span><br><span class="line">        raw_img = tf.image.decode_image(example[<span class="string">&#x27;img_raw&#x27;</span>], channels=<span class="number">3</span>)</span><br><span class="line">        label = example[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        bbox = tf.io.parse_tensor(example[<span class="string">&#x27;bbox&#x27;</span>], tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># load image -&gt; resize image -&gt; image augmenter -&gt; make labels</span></span><br><span class="line">        raw_img, *labels = tf.numpy_function(</span><br><span class="line">            _wapper, [raw_img, bbox, is_augment],</span><br><span class="line">            [tf.uint8] + [tf.float32] * self.scale_num, name=<span class="string">&#x27;process_img&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># normlize image</span></span><br><span class="line">        <span class="keyword">if</span> is_normlize:</span><br><span class="line">            img = self.normlize_img(raw_img)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img = tf.cast(raw_img, tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.featuremap_size):</span><br><span class="line">            labels[i].set_shape((v, v, self.out_channels + <span class="number">2</span>))</span><br><span class="line">        img.set_shape((self.in_hw[<span class="number">0</span>], self.in_hw[<span class="number">1</span>], <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, <span class="built_in">tuple</span>(labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        pos_ds = (tf.data.TFRecordDataset(pos_tfrecord, buffer_size=<span class="number">100</span>,</span><br><span class="line">                                          num_parallel_reads=<span class="number">6</span>).</span><br><span class="line">                  shuffle(batch_size * <span class="number">200</span>, rand_seed).repeat().<span class="built_in">map</span>(_parser))</span><br><span class="line">        neg_ds = (tf.data.TFRecordDataset(neg_tfrecord, buffer_size=<span class="number">100</span>,</span><br><span class="line">                                          num_parallel_reads=<span class="number">6</span>).</span><br><span class="line">                  shuffle(batch_size * <span class="number">200</span>, rand_seed).repeat().<span class="built_in">map</span>(_parser))</span><br><span class="line">        ds = (tf.data.experimental.sample_from_datasets(</span><br><span class="line">            [pos_ds, neg_ds], [<span class="number">1</span> - self.neg_sample_ratio,</span><br><span class="line">                               self.neg_sample_ratio]).</span><br><span class="line">            batch(batch_size, <span class="literal">True</span>).prefetch(-<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">&#x27;No support to test eval&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure></p>
<h2 id="进一步定位问题">进一步定位问题</h2>
<p>现在我发现虽然用了<code>tfrecord+tf.data</code>的方式,但还是有内存泄漏的问题,不过这次更加进一步的定位到了问题:</p>
<p>测试代码如下,我检测<code>fn</code>是否报错. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@profile(<span class="params">stream=<span class="built_in">open</span>(<span class="params"><span class="string">&#x27;tmp/process.log&#x27;</span>, <span class="string">&#x27;w&#x27;</span></span>), precision=<span class="number">4</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fn</span>(<span class="params">sess, h: LFFDHelper, imgs, bboxs</span>):</span><br><span class="line">    img, ann = sess.run([imgs, bboxs])</span><br><span class="line">    img, ann = h.resize_img(img, ann)</span><br><span class="line">    img, ann = h.data_augmenter(img, ann)</span><br><span class="line">    label = h.ann_to_label(ann)</span><br><span class="line">    <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_parser</span>(<span class="params">stream: <span class="built_in">bytes</span></span>):</span><br><span class="line">    example = tf.io.parse_single_example(stream, &#123;</span><br><span class="line">        <span class="string">&#x27;img_raw&#x27;</span>: tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">        <span class="string">&#x27;label&#x27;</span>: tf.io.FixedLenFeature([], tf.int64),</span><br><span class="line">        <span class="string">&#x27;bbox&#x27;</span>: tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">    &#125;)  <span class="comment"># type:<span class="built_in">dict</span></span></span><br><span class="line">    raw_img = tf.image.decode_image(example[<span class="string">&#x27;img_raw&#x27;</span>], channels=<span class="number">3</span>)</span><br><span class="line">    label = example[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    <span class="comment"># bbox = tf.io.parse_tensor(example[&#x27;bbox&#x27;], tf.float32)</span></span><br><span class="line">    bbox = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">return</span> raw_img, bbox</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_no_dataset_memory_leak</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 测试以非dataset的方式运行时内存泄漏问题</span></span><br><span class="line"><span class="string">    ! 1.发现问题出现在tf.data读取tfrecord的地方</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line">    neg_resize_factor = np.array([<span class="number">0.5</span>, <span class="number">3.5</span>])</span><br><span class="line">    in_hw = np.array([<span class="number">640</span>, <span class="number">640</span>])</span><br><span class="line">    featuremap_size = np.array([<span class="number">159</span>, <span class="number">79</span>, <span class="number">39</span>, <span class="number">19</span>, <span class="number">9</span>])</span><br><span class="line">    h = LFFDHelper(<span class="string">&#x27;data/lffd_img_ann.npy&#x27;</span>,</span><br><span class="line">                   featuremap_size, in_hw, neg_resize_factor, <span class="number">0.2</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">16</span></span><br><span class="line">    pos_ds = (tf.data.Dataset.list_files(h.train_pos, <span class="literal">True</span>).</span><br><span class="line">              interleave(tf.data.TFRecordDataset, <span class="built_in">len</span>(h.train_pos), <span class="number">1</span>, <span class="number">4</span>).</span><br><span class="line">              shuffle(batch_size * <span class="number">500</span>).repeat().<span class="built_in">map</span>(_parser))</span><br><span class="line">    neg_ds = (tf.data.Dataset.list_files(h.train_neg, <span class="literal">True</span>).</span><br><span class="line">              interleave(tf.data.TFRecordDataset, <span class="built_in">len</span>(h.train_neg), <span class="number">1</span>, <span class="number">4</span>).</span><br><span class="line">              shuffle(batch_size * <span class="number">500</span>).repeat().<span class="built_in">map</span>(_parser))</span><br><span class="line">    ds = (tf.data.experimental.sample_from_datasets(</span><br><span class="line">        [pos_ds, neg_ds],</span><br><span class="line">        [<span class="number">1</span> - h.neg_sample_ratio, h.neg_sample_ratio]).prefetch(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    iters = ds._make_one_shot_iterator()</span><br><span class="line">    imgs, bboxs = iters.get_next()</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">        fn(sess, h, imgs, bboxs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test_no_dataset_memory_leak()</span><br></pre></td></tr></table></figure></p>
<p>log文件如下,可以发现正常的一个读取<code>tfrecord</code>的流程就会出问题:
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Filename: devlop/dev_lffd.py</span><br><span class="line"></span><br><span class="line">Line #    Mem usage    Increment   Line Contents</span><br><span class="line">================================================</span><br><span class="line">  1156 2506.0898 MiB 2506.0898 MiB   @profile(stream=open(&#x27;tmp/process.log&#x27;, &#x27;w&#x27;), precision=4)</span><br><span class="line">  1157                             def fn(sess, h: LFFDHelper, imgs, bboxs):</span><br><span class="line">  1158 2506.3477 MiB   0.2578 MiB       img, ann = sess.run([imgs, bboxs])</span><br><span class="line">  1159 2506.3477 MiB   0.0000 MiB       img, ann = h.resize_img(img, ann)</span><br><span class="line">  1160 2506.3477 MiB   0.0000 MiB       img, ann = h.data_augmenter(img, ann)</span><br><span class="line">  1161 2506.3477 MiB   0.0000 MiB       label = h.ann_to_label(ann)</span><br><span class="line">  1162 2506.3477 MiB   0.0000 MiB       return img, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Filename: devlop/dev_lffd.py</span><br><span class="line"></span><br><span class="line">Line #    Mem usage    Increment   Line Contents</span><br><span class="line">================================================</span><br><span class="line">  1156 2506.3477 MiB 2506.3477 MiB   @profile(stream=open(&#x27;tmp/process.log&#x27;, &#x27;w&#x27;), precision=4)</span><br><span class="line">  1157                             def fn(sess, h: LFFDHelper, imgs, bboxs):</span><br><span class="line">  1158 2506.6055 MiB   0.2578 MiB       img, ann = sess.run([imgs, bboxs])</span><br><span class="line">  1159 2506.6055 MiB   0.0000 MiB       img, ann = h.resize_img(img, ann)</span><br><span class="line">  1160 2506.6055 MiB   0.0000 MiB       img, ann = h.data_augmenter(img, ann)</span><br><span class="line">  1161 2506.6055 MiB   0.0000 MiB       label = h.ann_to_label(ann)</span><br><span class="line">  1162 2506.6055 MiB   0.0000 MiB       return img, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Filename: devlop/dev_lffd.py</span><br><span class="line"></span><br><span class="line">Line #    Mem usage    Increment   Line Contents</span><br><span class="line">================================================</span><br><span class="line">  1156 2506.6055 MiB 2506.6055 MiB   @profile(stream=open(&#x27;tmp/process.log&#x27;, &#x27;w&#x27;), precision=4)</span><br><span class="line">  1157                             def fn(sess, h: LFFDHelper, imgs, bboxs):</span><br><span class="line">  1158 2506.8633 MiB   0.2578 MiB       img, ann = sess.run([imgs, bboxs])</span><br><span class="line">  1159 2506.8633 MiB   0.0000 MiB       img, ann = h.resize_img(img, ann)</span><br><span class="line">  1160 2506.8633 MiB   0.0000 MiB       img, ann = h.data_augmenter(img, ann)</span><br><span class="line">  1161 2506.8633 MiB   0.0000 MiB       label = h.ann_to_label(ann)</span><br><span class="line">  1162 2506.8633 MiB   0.0000 MiB       return img, label</span><br></pre></td></tr></table></figure></p>
<p><img src="/2019/11/01/skimage-mem-leak/Figure_4.png" /></p>
<h2
id="检查下别的tf.data输入是否会出现内存泄漏问题">检查下别的tf.data输入是否会出现内存泄漏问题</h2>
<p>我做了6次实验如下:</p>
<ol type="1">
<li><p>去除<code>prefetch</code><strong>依旧内存泄漏</strong></p></li>
<li><p>两个<code>dataset</code>对象共用一个<code>map</code>函数<strong>依旧内存泄漏</strong></p></li>
<li><p>只读取一个<code>pos dataset</code><strong>依旧内存泄漏</strong></p></li>
<li><p>不使用交错读取就没有问题了</p>
<p>不会内存泄漏的构建过程如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_sample_dataset_memory_leak</span>():</span><br><span class="line">    neg_resize_factor = np.array([<span class="number">0.5</span>, <span class="number">3.5</span>])</span><br><span class="line">    in_hw = np.array([<span class="number">640</span>, <span class="number">640</span>])</span><br><span class="line">    featuremap_size = np.array([<span class="number">159</span>, <span class="number">79</span>, <span class="number">39</span>, <span class="number">19</span>, <span class="number">9</span>])</span><br><span class="line">    h = LFFDHelper(<span class="string">&#x27;data/lffd_img_ann.npy&#x27;</span>,</span><br><span class="line">                   featuremap_size, in_hw, neg_resize_factor, <span class="number">0.2</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">16</span></span><br><span class="line">    pos_ds = (tf.data.TFRecordDataset(h.train_pos, buffer_size=<span class="number">100</span>, num_parallel_reads=<span class="number">4</span>).</span><br><span class="line">              shuffle(batch_size * <span class="number">500</span>).</span><br><span class="line">              repeat().</span><br><span class="line">              <span class="built_in">map</span>(_parser))</span><br><span class="line">    neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=<span class="number">100</span>, num_parallel_reads=<span class="number">4</span>).</span><br><span class="line">              shuffle(batch_size * <span class="number">500</span>).</span><br><span class="line">              repeat().</span><br><span class="line">              <span class="built_in">map</span>(_parser))</span><br><span class="line">    ds = (tf.data.experimental.sample_from_datasets(</span><br><span class="line">        [pos_ds, neg_ds],</span><br><span class="line">        [<span class="number">1</span> - h.neg_sample_ratio, h.neg_sample_ratio]))</span><br><span class="line"></span><br><span class="line">    iters = ds._make_one_shot_iterator()</span><br><span class="line">    imgs, bboxs = iters.get_next()</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">        fn(sess, h, imgs, bboxs)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/11/01/skimage-mem-leak/Figure_5.png" /></p></li>
</ol>
<h2 id="新的问题">新的问题</h2>
<p>我之所以需要使用交错读取,是为了保证对多个<code>tfrecord</code>的随机性,如果不能交错读取...那我不能随机训练起来不是很蛋疼么?我得看看非交错读取和交错读取的读取随机性.</p>
<ol type="1">
<li><p>首先生成一个总长<code>10000</code>的<code>tfrecord</code>文件.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_make_range_tfrecord</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 生成按顺序的tfrecord,用于检查tfrecord生成的随机性 &quot;&quot;&quot;</span></span><br><span class="line">    a = np.arange(<span class="number">10000</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(a), <span class="number">2000</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.io.TFRecordWriter(<span class="string">f&#x27;tmp/<span class="subst">&#123;i&#125;</span>.tfrecords&#x27;</span>) <span class="keyword">as</span> writer:</span><br><span class="line">            <span class="keyword">for</span> num <span class="keyword">in</span> a[i:i + <span class="number">2000</span>]:</span><br><span class="line">                feature = &#123;</span><br><span class="line">                    <span class="string">&#x27;var&#x27;</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[num])),</span><br><span class="line">                &#125;</span><br><span class="line">                serialized_example = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()</span><br><span class="line"></span><br><span class="line">                writer.write(serialized_example)</span><br></pre></td></tr></table></figure></li>
<li><p>测试无交错的读取<code>tfrecord</code>.</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_read_range_tfrecord_no_interleave</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 测试无交错的读取顺序tfrecord dataset &quot;&quot;&quot;</span></span><br><span class="line">    l = [<span class="string">&#x27;tmp/0.tfrecords&#x27;</span>, <span class="string">&#x27;tmp/2000.tfrecords&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;tmp/4000.tfrecords&#x27;</span>, <span class="string">&#x27;tmp/6000.tfrecords&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;tmp/8000.tfrecords&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_par</span>(<span class="params">stream: <span class="built_in">bytes</span></span>):</span><br><span class="line">        example = tf.io.parse_single_example(stream, &#123;</span><br><span class="line">            <span class="string">&#x27;var&#x27;</span>: tf.io.FixedLenFeature([], tf.int64),</span><br><span class="line">        &#125;)  <span class="comment"># type:<span class="built_in">dict</span></span></span><br><span class="line">        var = example[<span class="string">&#x27;var&#x27;</span>]</span><br><span class="line">        <span class="comment"># bbox = tf.io.parse_tensor(example[&#x27;bbox&#x27;], tf.float32)</span></span><br><span class="line">        <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line">    <span class="comment"># @profile(stream=open(&#x27;tmp/_fn.log&#x27;, &#x27;w&#x27;), precision=4)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_fn</span>(<span class="params">sess, next_num</span>):</span><br><span class="line">        num = sess.run([next_num])</span><br><span class="line">        <span class="keyword">return</span> num[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    ds = (tf.data.TFRecordDataset(l, buffer_size=<span class="number">100</span>, num_parallel_reads=<span class="number">4</span>).</span><br><span class="line">        <span class="comment">#   shuffle(16 * 500).</span></span><br><span class="line">        repeat().<span class="built_in">map</span>(_par))</span><br><span class="line"></span><br><span class="line">    iters = ds._make_one_shot_iterator()</span><br><span class="line">    next_num = iters.get_next()</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    arr = np.zeros((<span class="number">10000</span>))</span><br><span class="line">    f = <span class="built_in">open</span>(<span class="string">&#x27;tmp/_fn.log&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">        arr[i] = _fn(sess, next_num)</span><br><span class="line">        <span class="built_in">print</span>(arr[i], file=f)</span><br><span class="line">    plt.plot(arr)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>输出结果,可以发现采样现场为<code>4</code>,就是从<code>4</code>个<code>tfrecord</code>里面按顺序取:
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">0.0</span><br><span class="line">2000.0</span><br><span class="line">4000.0</span><br><span class="line">6000.0</span><br><span class="line">1.0</span><br><span class="line">2001.0</span><br><span class="line">4001.0</span><br><span class="line">6001.0</span><br><span class="line">2.0</span><br><span class="line">2002.0</span><br><span class="line">4002.0</span><br><span class="line">6002.0</span><br><span class="line">3.0</span><br><span class="line">2003.0</span><br><span class="line">4003.0</span><br><span class="line">6003.0</span><br><span class="line">4.0</span><br></pre></td></tr></table></figure> 但采样到后面就会出现比较单一的情况: <img
src="/2019/11/01/skimage-mem-leak/Figure_6.png" />
然后并没有出现内存泄漏的问题: <img
src="/2019/11/01/skimage-mem-leak/Figure_7.png" /></p></li>
<li><p>测试有交错的读取<code>tfrecord</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_read_range_tfrecord_has_interleave</span>():</span><br><span class="line">  <span class="string">&quot;&quot;&quot; 测试使用交错的读取tfrecord dataset &quot;&quot;&quot;</span></span><br><span class="line">  l = [<span class="string">&#x27;tmp/0.tfrecords&#x27;</span>, <span class="string">&#x27;tmp/2000.tfrecords&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;tmp/4000.tfrecords&#x27;</span>, <span class="string">&#x27;tmp/6000.tfrecords&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;tmp/8000.tfrecords&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">_par</span>(<span class="params">stream: <span class="built_in">bytes</span></span>):</span><br><span class="line">      example = tf.io.parse_single_example(stream, &#123;</span><br><span class="line">          <span class="string">&#x27;var&#x27;</span>: tf.io.FixedLenFeature([], tf.int64),</span><br><span class="line">      &#125;)  <span class="comment"># type:<span class="built_in">dict</span></span></span><br><span class="line">      var = example[<span class="string">&#x27;var&#x27;</span>]</span><br><span class="line">      <span class="comment"># bbox = tf.io.parse_tensor(example[&#x27;bbox&#x27;], tf.float32)</span></span><br><span class="line">      <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line">  <span class="comment"># @profile(stream=open(&#x27;tmp/_fn.log&#x27;, &#x27;w&#x27;), precision=4)</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">_fn</span>(<span class="params">sess, next_num</span>):</span><br><span class="line">      num = sess.run([next_num])</span><br><span class="line">      <span class="keyword">return</span> num[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">  ds = (tf.data.Dataset.list_files(l, <span class="literal">True</span>).</span><br><span class="line">      interleave(tf.data.TFRecordDataset, <span class="built_in">len</span>(l), <span class="number">1</span>, <span class="number">4</span>).</span><br><span class="line">      repeat().<span class="built_in">map</span>(_par))</span><br><span class="line"></span><br><span class="line">  iters = ds._make_one_shot_iterator()</span><br><span class="line">  next_num = iters.get_next()</span><br><span class="line">  sess = tf.Session()</span><br><span class="line">  arr = np.zeros((<span class="number">10000</span>))</span><br><span class="line">  f = <span class="built_in">open</span>(<span class="string">&#x27;tmp/_fn.log&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">      arr[i] = (_fn(sess, next_num))</span><br><span class="line">      <span class="built_in">print</span>(arr[i], file=f)</span><br><span class="line">  plt.plot(arr)</span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure>
<p>我发现这个交错<code>4</code>线程读取,他的预期行为和直接<code>4</code>线程读取<code>tfrecords</code>也是一样的:
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">2000.0</span><br><span class="line">0.0</span><br><span class="line">8000.0</span><br><span class="line">4000.0</span><br><span class="line">6000.0</span><br><span class="line">2001.0</span><br><span class="line">1.0</span><br><span class="line">8001.0</span><br><span class="line">4001.0</span><br><span class="line">6001.0</span><br><span class="line">2002.0</span><br><span class="line">2.0</span><br><span class="line">8002.0</span><br><span class="line">4002.0</span><br><span class="line">6002.0</span><br><span class="line">2003.0</span><br><span class="line">3.0</span><br><span class="line">8003.0</span><br><span class="line">4003.0</span><br><span class="line">6003.0</span><br><span class="line">2004.0</span><br><span class="line">4.0</span><br></pre></td></tr></table></figure> 这个采样到最后就不会出现单一数值的情况: <img
src="/2019/11/01/skimage-mem-leak/Figure_8.png" />
但是我发现也没有出现内存泄漏的情况!!我心累了,看来之前的内存泄漏问题<strong>原因还不在这里</strong>.
<img src="/2019/11/01/skimage-mem-leak/Figure_9.png" /></p></li>
</ol>
<h2
id="进一步检查是不是tf.example的格式导致的.">进一步检查是不是<code>tf.example</code>的格式导致的.</h2>
<p>我忽然发现前面定位问题还没定位到,最后一个我认为没有内存泄漏的方式在我运行时间够长之后还是有内存存在的.这个<code>tf.example</code>我尝试了,并不影响.继续尝试:</p>
<ol type="1">
<li><p>之前使用非交错读取: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pos_ds = (tf.data.TFRecordDataset(h.train_pos, buffer_size=<span class="number">100</span>, num_parallel_reads=<span class="number">4</span>).</span><br><span class="line">          shuffle(batch_size * <span class="number">500</span>).</span><br><span class="line">          repeat())</span><br><span class="line">neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=<span class="number">100</span>, num_parallel_reads=<span class="number">4</span>).</span><br><span class="line">          shuffle(batch_size * <span class="number">500</span>).</span><br><span class="line">          repeat())</span><br><span class="line">ds = (tf.data.experimental.sample_from_datasets(</span><br><span class="line">    [pos_ds, neg_ds],</span><br><span class="line">    [<span class="number">1</span> - h.neg_sample_ratio, h.neg_sample_ratio]).<span class="built_in">map</span>(_parser))</span><br></pre></td></tr></table></figure></p>
<p>但是运行时间长了之后: <img
src="/2019/11/01/skimage-mem-leak/Figure_10.png" /></p></li>
<li><p>单个数据集非交错读取</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pos_ds = (tf.data.TFRecordDataset(h.train_pos, buffer_size=<span class="number">100</span>, num_parallel_reads=<span class="number">4</span>).</span><br><span class="line">          shuffle(batch_size * <span class="number">500</span>).</span><br><span class="line">          repeat())</span><br><span class="line"><span class="comment"># neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=100, num_parallel_reads=4).</span></span><br><span class="line"><span class="comment">#           shuffle(batch_size * 500).</span></span><br><span class="line"><span class="comment">#           repeat())</span></span><br><span class="line"><span class="comment"># ds = (tf.data.experimental.sample_from_datasets(</span></span><br><span class="line"><span class="comment">#     [pos_ds, neg_ds],</span></span><br><span class="line"><span class="comment">#     [1 - h.neg_sample_ratio, h.neg_sample_ratio]).map(_parser))</span></span><br><span class="line">ds = pos_ds.<span class="built_in">map</span>(_parser)</span><br></pre></td></tr></table></figure></p>
<p>运行,感觉好像还是有点问题,我人都晕了...: <img
src="/2019/11/01/skimage-mem-leak/Figure_11.png" /></p></li>
<li><p>单个数据集交错读取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pos_ds = (tf.data.Dataset.list_files(h.train_pos, <span class="literal">True</span>).</span><br><span class="line">          interleave(tf.data.TFRecordDataset, <span class="built_in">len</span>(h.train_pos), <span class="number">1</span>, <span class="number">4</span>).</span><br><span class="line">          shuffle(batch_size * <span class="number">500</span>).repeat())</span><br><span class="line"><span class="comment"># neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=100, num_parallel_reads=4).</span></span><br><span class="line"><span class="comment">#           shuffle(batch_size * 500).</span></span><br><span class="line"><span class="comment">#           repeat())</span></span><br><span class="line"><span class="comment"># ds = (tf.data.experimental.sample_from_datasets(</span></span><br><span class="line"><span class="comment">#     [pos_ds, neg_ds],</span></span><br><span class="line"><span class="comment">#     [1 - h.neg_sample_ratio, h.neg_sample_ratio]).map(_parser))</span></span><br><span class="line">ds = pos_ds.<span class="built_in">map</span>(_parser)</span><br></pre></td></tr></table></figure>
<p>运行结果,这个占内存更大并且持续升高比例大: <img
src="/2019/11/01/skimage-mem-leak/Figure_12.png" /></p></li>
<li><p>双数据集无交错采样读取:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pos_ds = (tf.data.TFRecordDataset(h.train_pos, buffer_size=<span class="number">100</span>, num_parallel_reads=<span class="number">8</span>).</span><br><span class="line">          shuffle(batch_size * <span class="number">500</span>).repeat())</span><br><span class="line">neg_ds = (tf.data.TFRecordDataset(h.train_neg, buffer_size=<span class="number">100</span>, num_parallel_reads=<span class="number">8</span>).</span><br><span class="line">          shuffle(batch_size * <span class="number">500</span>).repeat())</span><br><span class="line">ds = (tf.data.experimental.sample_from_datasets(</span><br><span class="line">    [pos_ds, neg_ds],</span><br><span class="line">    [<span class="number">1</span> - h.neg_sample_ratio, h.neg_sample_ratio]).<span class="built_in">map</span>(_parser))</span><br></pre></td></tr></table></figure>
<p>还是在增加,我头大...为什么简单的数据集就不会有这个情况. <img
src="/2019/11/01/skimage-mem-leak/Figure_13.png" /></p></li>
</ol>
<h1 id="不折腾了">不折腾了</h1>
<p>这个<code>tfrecord</code>的问题把我恶心到了,我现在直接用我最常用的方式,从<code>range</code>里面索引数据的方式做,这样是最稳妥的,没有出过问题.
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pos_ds = (tf.data.Dataset.<span class="built_in">range</span>(<span class="built_in">len</span>(pos_list))</span><br></pre></td></tr></table></figure>
修改成这个方式之后,我的运行内存就一直保持在<code>6.1GB</code>左右了.算是勉强解决了这个傻吊问题.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Filename: devlop/dev_lffd.py</span><br><span class="line"></span><br><span class="line">Line <span class="comment">#    Mem usage    Increment   Line Contents</span></span><br><span class="line">================================================</span><br><span class="line">  1212   6133.3 MiB   6133.3 MiB   @mem_profile</span><br><span class="line">  1213                             def fn(sess, next_op):</span><br><span class="line">  1214   6131.7 MiB      0.0 MiB       <span class="built_in">return</span> sess.run(next_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Filename: devlop/dev_lffd.py</span><br><span class="line"></span><br><span class="line">Line <span class="comment">#    Mem usage    Increment   Line Contents</span></span><br><span class="line">================================================</span><br><span class="line">  1212   6131.7 MiB   6131.7 MiB   @mem_profile</span><br><span class="line">  1213                             def fn(sess, next_op):</span><br><span class="line">  1214   6133.0 MiB      1.3 MiB       <span class="built_in">return</span> sess.run(next_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Filename: devlop/dev_lffd.py</span><br><span class="line"></span><br><span class="line">Line <span class="comment">#    Mem usage    Increment   Line Contents</span></span><br><span class="line">================================================</span><br><span class="line">  1212   6133.0 MiB   6133.0 MiB   @mem_profile</span><br><span class="line">  1213                             def fn(sess, next_op):</span><br><span class="line">  1214   6133.1 MiB      0.1 MiB       <span class="built_in">return</span> sess.run(next_op)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" rel="tag">踩坑经验</a></li></ul></div><div class="post-nav"><a class="pre" href="/2019/11/13/tf-keras-callback/">测试tf.keras中callback的运行状态</a><a class="next" href="/2019/10/21/tf15-tb-error/">Tensorflow 1.15中TensorBoard错误</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a> <a href="/tags/%E7%AE%97%E5%AD%90/" style="font-size: 15px;">算子</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/09/26/flashattn/">Flash Attention记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>