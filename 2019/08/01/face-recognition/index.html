<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>人脸识别方法总结 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">人脸识别方法总结</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">人脸识别方法总结</h1><div class="post-meta">2019-08-01<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 18</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#deep-face-recognition-with-keras-dlib-and-opencv"><span class="toc-number">1.</span> <span class="toc-text">Deep face
recognition with Keras, Dlib and OpenCV</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#environment-setup-%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.1.</span> <span class="toc-text">Environment setup 环境设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cnn-architecture-and-training"><span class="toc-number">1.2.</span> <span class="toc-text">CNN architecture and
training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#custom-dataset-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.3.</span> <span class="toc-text">Custom dataset 自定义数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#face-alignment-%E9%9D%A2%E9%83%A8%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.</span> <span class="toc-text">Face alignment 面部对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#embedding-vectors-%E5%B5%8C%E5%85%A5%E5%90%91%E9%87%8F"><span class="toc-number">1.5.</span> <span class="toc-text">Embedding vectors 嵌入向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#distance-threshold-%E8%B7%9D%E7%A6%BB%E9%98%88%E5%80%BC"><span class="toc-number">1.6.</span> <span class="toc-text">Distance threshold 距离阈值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#face-recognition-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB"><span class="toc-number">1.7.</span> <span class="toc-text">Face recognition 人脸识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dataset-visualization-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.8.</span> <span class="toc-text">Dataset visualization
数据集可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#references"><span class="toc-number">1.9.</span> <span class="toc-text">References</span></a></li></ol></li></ol></div></div><div class="post-content"><p>要搞个人脸识别的应用，花了半天时间浏览一下，准备基于<code>open face</code>的模型来做移植。下面是对开源库<a
target="_blank" rel="noopener" href="https://github.com/krasserm/face-recognition">face-recognition</a>的使用指南进行一个翻译，看了一下基本知道了大致流程。不过我记得上次写过<a
href="https://zhen8838.github.io/2019/06/03/l-softmax/">L softmx -&gt; A
softmx -&gt; AM
softmax</a>的这些<code>loss</code>都是用在人脸识别里面的，但是如果基于<code>softmax loss</code>的话，每加一个人脸不都是要重新训练一波吗？不知道是不是这个情况，目前还没看到别的方式。</p>
<span id="more"></span>
<h2 id="deep-face-recognition-with-keras-dlib-and-opencv">Deep face
recognition with Keras, Dlib and OpenCV</h2>
<p>面部识别识别面部图像或视频帧上的人。简而言之，人脸识别系统从输入人脸图像中提取特征，并将其与数据库中标记人脸的特征进行比较。比较基于特征相似性度量，并且最相似的数据库条目的标签用于标记输入图像。如果相似度值低于某个阈值，则输入图像标记为<em>unknown</em>。比较两个面部图像以确定它们是否显示同一个人被称为面部验证。</p>
<p>该笔记本使用深度卷积神经网络（CNN）从输入图像中提取特征。它遵循<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03832">1</a>中描述的方法，其修改受<a
target="_blank" rel="noopener" href="http://cmusatyalab.github.io/openface/">OpenFace</a>项目的启发。
<a target="_blank" rel="noopener" href="https://keras.io/">Keras</a>用于实现CNN，<a
target="_blank" rel="noopener" href="http://dlib.net/">Dlib</a>和<a
target="_blank" rel="noopener" href="https://opencv.org/">OpenCV</a>用于对齐面部在输入图像上。在<a
target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/lfw/">LFW</a>数据集的一小部分上评估面部识别性能，您可以将其替换为您自己的自定义数据集，例如：如果你想进一步试验这款笔记本，请附上你的家人和朋友的照片。在概述了CNN架构以及如何训练模型之后，将演示如何：</p>
<ul>
<li>在输入图像上检测，变换和裁剪面部。这可确保面部在进入CNN之前对齐。该预处理步骤对于神经网络的性能非常重要。</li>
<li>使用CNN从对齐的输入图像中提取面部的128维表示或<em>嵌入</em>。在嵌入空间中，欧几里德距离直接对应于面部相似性的度量。</li>
<li>将输入嵌入向量与数据库中标记的嵌入向量进行比较。这里，支持向量机（SVM）和KNN分类器，在标记的嵌入向量上训练，起到数据库的作用。在此上下文中的面部识别意味着使用这些分类器来预测标签，即新输入的身份。</li>
</ul>
<h3 id="environment-setup-环境设置">Environment setup 环境设置</h3>
<p>For running this notebook, create and activate a new <a
target="_blank" rel="noopener" href="https://docs.python.org/3/tutorial/venv.html">virtual
environment</a> and install the packages listed in <a
href="requirements.txt">requirements.txt</a> with
<code>pip install -r requirements.txt</code>. Furthermore, you'll need a
local copy of Dlib's face landmarks data file for running face
alignment:</p>
<h3 id="cnn-architecture-and-training">CNN architecture and
training</h3>
<p>这里使用的CNN架构是初始架构<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.4842">2</a>的变体。更确切地说，它是<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03832">1</a>中描述的NN4体系结构的变体，并标识为<a
target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/models-and-accuracies/#model-definitions">nn4.small2</a>。这个笔记本使用该模型的Keras实现，其定义取自<a
target="_blank" rel="noopener" href="https://github.com/iwantooxxoox/Keras-OpenFace">Keras-OpenFace</a>项目。这里的体系结构细节并不太重要，只知道有一个完全连接的层，其中有128个隐藏单元，后面是卷积基础顶部的L2规范化层。这两个顶层被称为<em>嵌入层</em>，从中可以获得128维嵌入向量。完整模型在[model.py]（model.py）中定义，图形概述在[model.png]（model.png）中给出。可以使用<code>create_model（）</code>创建nn4.small2模型的Keras版本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> create_model</span><br><span class="line"></span><br><span class="line">nn4_small2 = create_model()</span><br></pre></td></tr></table></figure>
<pre><code>W0801 21:29:26.376736 140043235366720 deprecation.py:506] From /home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor</code></pre>
<p>模型训练旨在学习嵌入<span
class="math inline">\(f(x)\)</span>图像<span
class="math inline">\(x\)</span>，使得相同身份的所有面部之间的平方L2距离较小，并且来自不同身份的一对面部之间的距离较大。当嵌入空间中的锚图像<span
class="math inline">\(x^a_i\)</span>和正图像<span
class="math inline">\(x^p_i\)</span>（相同身份）之间的距离小于两者之间的距离时，可以实现<em>三元组损失</em>
<span class="math inline">\(L\)</span>。锚图像和负图像<span
class="math inline">\(x^n_i\)</span>（不同的身份）至少有一个边缘<span
class="math inline">\(\alpha\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}
L = \sum^{m}_{i=1} \large[ \small {\mid \mid f(x_{i}^{a}) -
f(x_{i}^{p})) \mid \mid_2^2} - {\mid \mid f(x_{i}^{a}) - f(x_{i}^{n}))
\mid \mid_2^2} + \alpha \large ] \small_+
\end{aligned}
\]</span></p>
<p><span class="math inline">\([z]_+\)</span>表示<span
class="math inline">\(\max(z，0)\)</span>和<span
class="math inline">\(m\)</span>是训练集中三元组的数量。
Keras中的三重态损失最好用自定义层实现，因为损失函数不遵循通常的“损失（输入，目标）”模式。该层调用<code>self.add_loss</code>来安装三元组丢失：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Input, Layer</span><br><span class="line"><span class="keyword">import</span> tensorflow.python <span class="keyword">as</span> tf</span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">K.set_session(tf.Session(config=config))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Input for anchor, positive and negative images</span></span><br><span class="line">in_a = Input(shape=(<span class="number">96</span>, <span class="number">96</span>, <span class="number">3</span>))</span><br><span class="line">in_p = Input(shape=(<span class="number">96</span>, <span class="number">96</span>, <span class="number">3</span>))</span><br><span class="line">in_n = Input(shape=(<span class="number">96</span>, <span class="number">96</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output for anchor, positive and negative embedding vectors</span></span><br><span class="line"><span class="comment"># The nn4_small model instance is shared (Siamese network)</span></span><br><span class="line">emb_a = nn4_small2(in_a)</span><br><span class="line">emb_p = nn4_small2(in_p)</span><br><span class="line">emb_n = nn4_small2(in_n)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TripletLossLayer</span>(<span class="title class_ inherited__">Layer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, alpha, **kwargs</span>):</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        <span class="built_in">super</span>(TripletLossLayer, self).__init__(**kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">triplet_loss</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        a, p, n = inputs</span><br><span class="line">        p_dist = K.<span class="built_in">sum</span>(K.square(a-p), axis=-<span class="number">1</span>)</span><br><span class="line">        n_dist = K.<span class="built_in">sum</span>(K.square(a-n), axis=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> K.<span class="built_in">sum</span>(K.maximum(p_dist - n_dist + self.alpha, <span class="number">0</span>), axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        loss = self.triplet_loss(inputs)</span><br><span class="line">        self.add_loss(loss)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Layer that computes the triplet loss from anchor, positive and negative embedding vectors</span></span><br><span class="line">triplet_loss_layer = TripletLossLayer(alpha=<span class="number">0.2</span>, name=<span class="string">&#x27;triplet_loss_layer&#x27;</span>)([emb_a, emb_p, emb_n])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model that can be trained with anchor, positive negative images</span></span><br><span class="line">nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)</span><br></pre></td></tr></table></figure>
<p>在训练期间，选择正对<span
class="math inline">\((x^a_i,x^p_i)\)</span>和负对<span
class="math inline">\((x^a_i，x^n_i)\)</span>难以区分的三元组是很重要的，即它们在嵌入空间中的距离差异应该是低于间距<span
class="math inline">\(\alpha\)</span>，否则，网络无法学习有用的嵌入。因此，每次训练迭代应该基于在前一次迭代中学习的嵌入来选择一批新的三元组。假设从<code>triplet_generator（）</code>调用返回的生成器可以在这些约束下生成三元组，可以通过以下方式训练网络：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> triplet_generator</span><br><span class="line"></span><br><span class="line"><span class="comment"># triplet_generator() creates a generator that continuously returns </span></span><br><span class="line"><span class="comment"># ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch </span></span><br><span class="line"><span class="comment"># and n_batch are batches of anchor, positive and negative RGB images </span></span><br><span class="line"><span class="comment"># each having a shape of (batch_size, 96, 96, 3).</span></span><br><span class="line">generator = triplet_generator() </span><br><span class="line"></span><br><span class="line">nn4_small2_train.<span class="built_in">compile</span>(loss=<span class="literal">None</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>)</span><br><span class="line">nn4_small2_train.fit_generator(generator, epochs=<span class="number">10</span>, steps_per_epoch=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Please note that the current implementation of the generator only generates </span></span><br><span class="line"><span class="comment"># random image data. The main goal of this code snippet is to demonstrate </span></span><br><span class="line"><span class="comment"># the general setup for model training. In the following, we will anyway </span></span><br><span class="line"><span class="comment"># use a pre-trained model so we don&#x27;t need a generator here that operates </span></span><br><span class="line"><span class="comment"># on real training data. I&#x27;ll maybe provide a fully functional generator</span></span><br><span class="line"><span class="comment"># later.</span></span><br></pre></td></tr></table></figure>
<pre><code>W0801 21:29:38.732154 140043235366720 training_utils.py:1101] Output triplet_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_loss_layer.
W0801 21:29:38.856654 140043235366720 deprecation.py:323] From /home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where


Epoch 1/10
100/100 [==============================] - 19s 191ms/step - loss: 0.8117
Epoch 2/10
100/100 [==============================] - 5s 46ms/step - loss: 0.7971
Epoch 3/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8035
Epoch 4/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8018
Epoch 5/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8049
Epoch 6/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8009
Epoch 7/10
100/100 [==============================] - 5s 47ms/step - loss: 0.8003
Epoch 8/10
100/100 [==============================] - 5s 48ms/step - loss: 0.7995
Epoch 9/10
100/100 [==============================] - 5s 46ms/step - loss: 0.8004
Epoch 10/10
100/100 [==============================] - 5s 46ms/step - loss: 0.7998





&lt;tensorflow.python.keras.callbacks.History at 0x7f5cda4385c0&gt;</code></pre>
<p>上面的代码片段应该只演示如何设置模型训练。但是，我们不是从头开始实际训练模型，而是使用预先训练的模型，因为从头开始的训练非常昂贵，并且需要庞大的数据集来实现良好的泛化性能。例如，[1]（https://arxiv.org/abs/1503.03832）使用包含大约8M身份的200M图像的数据集。</p>
<p>OpenFace项目提供了<a
target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/models-and-accuracies/#pre-trained-models">预训练模型</a>，这些模型使用公共人脸识别数据集<a
target="_blank" rel="noopener" href="http://vintage.winklerbros.net/facescrub.html">FaceScrub</a>进行训练,和<a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1411.7923">CASIA-WebFace</a>。
Keras-OpenFace项目将预先训练的nn4.small2.v1模型的权重转换为<a
target="_blank" rel="noopener" href="https://github.com/iwantooxxoox/Keras-OpenFace/tree/master/weights">CSV文件</a>，然后进行<a
target="_blank" rel="noopener" href="http://vintage.winklerbros.net/facescrub.html">转换</a>这里x为一个二进制格式，可由Keras用<code>load_weights</code>加载：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nn4_small2_pretrained = create_model()</span><br><span class="line">nn4_small2_pretrained.load_weights(<span class="string">&#x27;weights/nn4.small2.v1.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="custom-dataset-自定义数据集">Custom dataset 自定义数据集</h3>
<p>为了演示自定义数据集上的人脸识别，使用了<a
target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/lfw/">LFW</a>数据集的一小部分。它由<a
href="images">10个身份</a>的100个面部图像组成。每个图像的元数据（文件和身份名称）被加载到内存中以供以后处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">IdentityMetadata</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base, name, file</span>):</span><br><span class="line">        <span class="comment"># dataset base directory</span></span><br><span class="line">        self.base = base</span><br><span class="line">        <span class="comment"># identity name</span></span><br><span class="line">        self.name = name</span><br><span class="line">        <span class="comment"># image file name</span></span><br><span class="line">        self.file = file</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.image_path()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">image_path</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> os.path.join(self.base, self.name, self.file) </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_metadata</span>(<span class="params">path</span>):</span><br><span class="line">    metadata = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">sorted</span>(os.listdir(path)):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">sorted</span>(os.listdir(os.path.join(path, i))):</span><br><span class="line">            <span class="comment"># Check file extension. Allow only jpg/jpeg&#x27; files.</span></span><br><span class="line">            ext = os.path.splitext(f)[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> ext == <span class="string">&#x27;.jpg&#x27;</span> <span class="keyword">or</span> ext == <span class="string">&#x27;.jpeg&#x27;</span>:</span><br><span class="line">                metadata.append(IdentityMetadata(path, i, f))</span><br><span class="line">    <span class="keyword">return</span> np.array(metadata)</span><br><span class="line"></span><br><span class="line">metadata = load_metadata(<span class="string">&#x27;images&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="face-alignment-面部对齐">Face alignment 面部对齐</h3>
<p>nn4.small2.v1模型使用对齐的面部图像进行训练，因此，自定义数据集中的面部图像也必须对齐。在这里，我们使用<a
target="_blank" rel="noopener" href="http://dlib.net/">Dlib</a>进行人脸检测，使用<a
target="_blank" rel="noopener" href="https://opencv.org/">OpenCV</a>进行图像变换和裁剪，以生成对齐的96x96
RGB人脸图像。通过使用OpenFace项目中的<a
href="align.py">AlignDlib</a>实用程序，这很简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> align <span class="keyword">import</span> AlignDlib</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_image</span>(<span class="params">path</span>):</span><br><span class="line">    img = cv2.imread(path, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># OpenCV loads images with color channels</span></span><br><span class="line">    <span class="comment"># in BGR order. So we need to reverse them</span></span><br><span class="line">    <span class="keyword">return</span> img[...,::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the OpenFace face alignment utility</span></span><br><span class="line">alignment = AlignDlib(<span class="string">&#x27;models/landmarks.dat&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load an image of Jacques Chirac</span></span><br><span class="line">jc_orig = load_image(metadata[<span class="number">77</span>].image_path())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Detect face and return bounding box</span></span><br><span class="line">bb = alignment.getLargestFaceBoundingBox(jc_orig)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Transform image using specified face landmark indices and crop image to 96x96</span></span><br><span class="line">jc_aligned = alignment.align(<span class="number">96</span>, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show original image</span></span><br><span class="line">plt.subplot(<span class="number">131</span>)</span><br><span class="line">plt.imshow(jc_orig)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show original image with bounding box</span></span><br><span class="line">plt.subplot(<span class="number">132</span>)</span><br><span class="line">plt.imshow(jc_orig)</span><br><span class="line">plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=<span class="literal">False</span>, color=<span class="string">&#x27;red&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show aligned image</span></span><br><span class="line">plt.subplot(<span class="number">133</span>)</span><br><span class="line">plt.imshow(jc_aligned)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/08/01/face-recognition/output_14_0.png" /></p>
<p>如OpenFace <a
target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/models-and-accuracies/#pre-trained-models">预训练模型</a>中所述部分,模型nn4.small2.v1需要地标索引<code>OUTER_EYES_AND_NOSE</code>。让我们将面部检测，转换和裁剪实现为<code>align_image</code>函数，以便以后重用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">align_image</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="keyword">return</span> alignment.align(<span class="number">96</span>, img, alignment.getLargestFaceBoundingBox(img), </span><br><span class="line">                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)</span><br></pre></td></tr></table></figure>
<h3 id="embedding-vectors-嵌入向量">Embedding vectors 嵌入向量</h3>
<p>现在可以通过将对齐和缩放的图像馈送到预训练的网络中来计算嵌入向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embedded = np.zeros((metadata.shape[<span class="number">0</span>], <span class="number">128</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, m <span class="keyword">in</span> <span class="built_in">enumerate</span>(metadata):</span><br><span class="line">    img = load_image(m.image_path())</span><br><span class="line">    img = align_image(img)</span><br><span class="line">    <span class="comment"># scale RGB values to interval [0,1]</span></span><br><span class="line">    img = (img / <span class="number">255.</span>).astype(np.float32)</span><br><span class="line">    <span class="comment"># obtain embedding vector for image</span></span><br><span class="line">    embedded[i] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=<span class="number">0</span>))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>Let's verify on a single triplet example that the squared L2 distance
between its anchor-positive pair is smaller than the distance between
its anchor-negative pair.</p>
<p>让我们在单个三元组示例上验证其锚定正对之间的平方L2距离小于其锚定负对之间的距离。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">distance</span>(<span class="params">emb1, emb2</span>):</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.square(emb1 - emb2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_pair</span>(<span class="params">idx1, idx2</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>))</span><br><span class="line">    plt.suptitle(<span class="string">f&#x27;Distance = <span class="subst">&#123;distance(embedded[idx1], embedded[idx2]):<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.imshow(load_image(metadata[idx1].image_path()))</span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.imshow(load_image(metadata[idx2].image_path()))    </span><br><span class="line"></span><br><span class="line">show_pair(<span class="number">77</span>, <span class="number">78</span>)</span><br><span class="line">show_pair(<span class="number">77</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/08/01/face-recognition/output_21_0.png" /></p>
<p><img src="/2019/08/01/face-recognition/output_21_1.png" /></p>
<p>正如预期的那样，Jacques Chirac的两幅图像之间的距离小于Jacques
Chirac图像与GerhardSchröder图像之间的距离（0.30
&lt;1.12）。但是我们仍然不知道距离阈值<span
class="math inline">\(\tau\)</span>是在<em>相同身份</em>和<em>不同身份</em>之间作出决定的最佳边界。</p>
<h3 id="distance-threshold-距离阈值">Distance threshold 距离阈值</h3>
<p>要查找$
$的最佳值，必须在一系列距离阈值上评估面部验证性能。在给定阈值处，所有可能的嵌入向量对被分类为<em>相同的身份</em>或<em>不同的身份</em>并且与基础事实进行比较。因为我们正在处理偏斜的类（比正对更多的负对），我们使用<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/F1_score">F1得分</a>作为评估指标而不是<a
target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">准确度</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, accuracy_score</span><br><span class="line"></span><br><span class="line">distances = [] <span class="comment"># squared L2 distance between pairs</span></span><br><span class="line">identical = [] <span class="comment"># 1 if same identity, 0 otherwise</span></span><br><span class="line"></span><br><span class="line">num = <span class="built_in">len</span>(metadata)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num - <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num):</span><br><span class="line">        distances.append(distance(embedded[i], embedded[j]))</span><br><span class="line">        identical.append(<span class="number">1</span> <span class="keyword">if</span> metadata[i].name == metadata[j].name <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">distances = np.array(distances)</span><br><span class="line">identical = np.array(identical)</span><br><span class="line"></span><br><span class="line">thresholds = np.arange(<span class="number">0.3</span>, <span class="number">1.0</span>, <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">f1_scores = [f1_score(identical, distances &lt; t) <span class="keyword">for</span> t <span class="keyword">in</span> thresholds]</span><br><span class="line">acc_scores = [accuracy_score(identical, distances &lt; t) <span class="keyword">for</span> t <span class="keyword">in</span> thresholds]</span><br><span class="line"></span><br><span class="line">opt_idx = np.argmax(f1_scores)</span><br><span class="line"><span class="comment"># Threshold at maximal F1 score</span></span><br><span class="line">opt_tau = thresholds[opt_idx]</span><br><span class="line"><span class="comment"># Accuracy at maximal F1 score</span></span><br><span class="line">opt_acc = accuracy_score(identical, distances &lt; opt_tau)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot F1 score and accuracy as function of distance threshold</span></span><br><span class="line">plt.plot(thresholds, f1_scores, label=<span class="string">&#x27;F1 score&#x27;</span>)</span><br><span class="line">plt.plot(thresholds, acc_scores, label=<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.axvline(x=opt_tau, linestyle=<span class="string">&#x27;--&#x27;</span>, lw=<span class="number">1</span>, c=<span class="string">&#x27;lightgrey&#x27;</span>, label=<span class="string">&#x27;Threshold&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">f&#x27;Accuracy at threshold <span class="subst">&#123;opt_tau:<span class="number">.2</span>f&#125;</span> = <span class="subst">&#123;opt_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Distance threshold&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192
  return f(*args, **kwds)
/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
  return f(*args, **kwds)
/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
  return f(*args, **kwds)
/home/zqh/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192
  return f(*args, **kwds)</code></pre>
<p><img src="/2019/08/01/face-recognition/output_25_1.png" /></p>
<p><span class="math inline">\(\tau\)</span> =
0.56的面部验证准确率为95.7％。对于总是预测<em>不同身份</em>（有980个pos。对和8821个neg。对）的分类器的基线为89％，这也不错，但由于nn4.small2.v1是一个相对较小的模型，它仍然小于最先进的模型（&gt;
99％）。</p>
<p>以下两个直方图显示了正负对的距离分布和决策边界的位置。这些分布明显分开，这解释了网络的辨别性能。人们也可以发现正对中的一些强异常值，但这里不再进一步分析。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dist_pos = distances[identical == <span class="number">1</span>]</span><br><span class="line">dist_neg = distances[identical == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.hist(dist_pos)</span><br><span class="line">plt.axvline(x=opt_tau, linestyle=<span class="string">&#x27;--&#x27;</span>, lw=<span class="number">1</span>, c=<span class="string">&#x27;lightgrey&#x27;</span>, label=<span class="string">&#x27;Threshold&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Distances (pos. pairs)&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.hist(dist_neg)</span><br><span class="line">plt.axvline(x=opt_tau, linestyle=<span class="string">&#x27;--&#x27;</span>, lw=<span class="number">1</span>, c=<span class="string">&#x27;lightgrey&#x27;</span>, label=<span class="string">&#x27;Threshold&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Distances (neg. pairs)&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/08/01/face-recognition/output_27_0.png" /></p>
<h3 id="face-recognition-人脸识别">Face recognition 人脸识别</h3>
<p>给定距离阈值$ <span
class="math inline">\(的估计，人脸识别现在就像计算输入嵌入向量与数据库中所有嵌入向量之间的距离一样简单。如果输入小于\)</span>
$或标签<em>unknown</em>，则为输入分配具有最小距离的数据库条目的标签（即标识）。此过程还可以扩展到大型数据库，因为它可以轻松并行化。它还支持一次性学习，因为仅添加新标识的单个条目可能足以识别该标识的新示例。</p>
<p>更稳健的方法是使用数据库中的前$ k $评分条目标记输入，该条目基本上是<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">KNN分类</a>，具有欧几里德距离度量。或者，线性<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Support_vector_machine">支持向量机</a>可以用数据库条目训练并用于分类，即识别新输入。为了训练这些分类器，我们使用50％的数据集，用于评估其他50％。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"></span><br><span class="line">targets = np.array([m.name <span class="keyword">for</span> m <span class="keyword">in</span> metadata])</span><br><span class="line"></span><br><span class="line">encoder = LabelEncoder()</span><br><span class="line">encoder.fit(targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Numerical encoding of identities</span></span><br><span class="line">y = encoder.transform(targets)</span><br><span class="line"></span><br><span class="line">train_idx = np.arange(metadata.shape[<span class="number">0</span>]) % <span class="number">2</span> != <span class="number">0</span></span><br><span class="line">test_idx = np.arange(metadata.shape[<span class="number">0</span>]) % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 50 train examples of 10 identities (5 examples each)</span></span><br><span class="line">X_train = embedded[train_idx]</span><br><span class="line"><span class="comment"># 50 test examples of 10 identities (5 examples each)</span></span><br><span class="line">X_test = embedded[test_idx]</span><br><span class="line"></span><br><span class="line">y_train = y[train_idx]</span><br><span class="line">y_test = y[test_idx]</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>, metric=<span class="string">&#x27;euclidean&#x27;</span>)</span><br><span class="line">svc = LinearSVC()</span><br><span class="line"></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">svc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">acc_knn = accuracy_score(y_test, knn.predict(X_test))</span><br><span class="line">acc_svc = accuracy_score(y_test, svc.predict(X_test))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;KNN accuracy = <span class="subst">&#123;acc_knn&#125;</span>, SVM accuracy = <span class="subst">&#123;acc_svc&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>KNN accuracy = 0.96, SVM accuracy = 0.98</code></pre>
<p>KNN分类器在测试集上实现了96％的准确度，SVM分类器为98％。让我们使用SVM分类器来说明单个示例中的人脸识别。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="comment"># Suppress LabelEncoder warning</span></span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line">example_idx = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">example_image = load_image(metadata[test_idx][example_idx].image_path())</span><br><span class="line">example_prediction = svc.predict([embedded[test_idx][example_idx]])</span><br><span class="line">example_identity = encoder.inverse_transform(example_prediction)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.imshow(example_image)</span><br><span class="line">plt.title(<span class="string">f&#x27;Recognized as <span class="subst">&#123;example_identity&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/08/01/face-recognition/output_32_0.png" /></p>
<p>似乎合理:-)实际上应该检查分类结果是否（预测身份的数据库条目的一个子集）的距离小于<span
class="math inline">\(\tau\)</span>，否则应该分配一个<em>未知标签</em>。此处跳过此步骤，但可以轻松添加。</p>
<h3 id="dataset-visualization-数据集可视化">Dataset visualization
数据集可视化</h3>
<p>为了将数据集嵌入到2D空间中以显示身份聚类，将<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-distributed
Stochastic Neighbor
Embedding</a>（t-SNE）应用于128维嵌入向量。除了一些异常值，身份集群很好地分开。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"></span><br><span class="line">X_embedded = TSNE(n_components=<span class="number">2</span>).fit_transform(embedded)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">set</span>(targets)):</span><br><span class="line">    idx = targets == t</span><br><span class="line">    plt.scatter(X_embedded[idx, <span class="number">0</span>], X_embedded[idx, <span class="number">1</span>], label=t)   </span><br><span class="line"></span><br><span class="line">plt.legend(bbox_to_anchor=(<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/08/01/face-recognition/output_36_0.png" /></p>
<h3 id="references">References</h3>
<ul>
<li>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03832">FaceNet: A Unified
Embedding for Face Recognition and Clustering</a></li>
<li>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.4842">Going Deeper with
Convolutions</a></li>
</ul>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" rel="tag">人脸识别</a></li></ul></div><div class="post-nav"><a class="pre" href="/2019/08/12/keras-loss-error/">Tensorflow中的错误记录</a><a class="next" href="/2019/07/31/infor-max/">互信息：无监督提取特征</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>