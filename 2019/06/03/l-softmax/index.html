<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>L softmx -&gt; A softmx -&gt; AM softmax | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">L softmx -&gt; A softmx -&gt; AM softmax</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">L softmx -&gt; A softmx -&gt; AM softmax</h1><div class="post-meta">2019-06-03<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 16</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8B%E7%9A%84softmax"><span class="toc-number">1.</span> <span class="toc-text">原始的softmax</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#large-margin-softmax-loss"><span class="toc-number">2.</span> <span class="toc-text">Large-Margin Softmax Loss</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA"><span class="toc-number">3.</span> <span class="toc-text">动机</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">4.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#a-softmax"><span class="toc-number">5.</span> <span class="toc-text">A softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#modified-softmax"><span class="toc-number">5.1.</span> <span class="toc-text">modified softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#angular-margin-to-softmax-loss"><span class="toc-number">5.2.</span> <span class="toc-text">Angular Margin to Softmax
Loss</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#additive-margin-softmax"><span class="toc-number">6.</span> <span class="toc-text">Additive Margin Softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-1"><span class="toc-number">6.1.</span> <span class="toc-text">定义</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.</span> <span class="toc-text">编程实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#a-softmax-loss"><span class="toc-number">7.1.</span> <span class="toc-text">A softmax loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#additive-margin-softmax-1"><span class="toc-number">7.2.</span> <span class="toc-text">Additive Margin Softmax</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">8.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><p>本篇文章是对<code>Large Margin Softmax loss</code>,<code>Angular Margin to Softmax Loss</code>,<code>Additive Margin Softmax Loss</code>的学习记录。公式我尽量按照原文来写，并加入一点注释。</p>
<span id="more"></span>
<h1 id="原始的softmax">原始的softmax</h1>
<p>首先定义第<code>i</code>个输入特征<span
class="math inline">\(x_i\)</span>和标签<span
class="math inline">\(y_i\)</span>。传统的<code>softmax</code>定义为：</p>
<p><span class="math display">\[
\begin{aligned}
    Loss=\frac{1}{N}\sum_i Loss_i= -\frac{1}{N}log(P_{y_i})=
-\frac{1}{N}log(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}})
\end{aligned}
\]</span></p>
<p><strong>注：</strong> <span
class="math inline">\(f_{y_i}\)</span>应该是指的是输出中对应label分类的那个位置，<span
class="math inline">\(f_j\)</span>对应第<span
class="math inline">\(j\)</span>个元素(<span class="math inline">\(j \in
[1,K)\)</span>,<span
class="math inline">\(K\)</span>对应类别数量)的输出.<span
class="math inline">\(N\)</span>为输入样本数量.</p>
<p>因为在<code>softmax loss</code>中最后都使用全连接层来实现分类,所以<span
class="math inline">\(f_{y_i}=\boldsymbol{W}_{y_i}^Tx_i\)</span>,<span
class="math inline">\(\boldsymbol{W}_{y_i}\)</span>是<span
class="math inline">\(\boldsymbol{W}\)</span>的第<span
class="math inline">\(y_i\)</span>列.(一般来说是全连接层还需要加上偏置,但是为了公式推导的方便这里不加,在实际中直接加偏置即可).再因为这里的<span
class="math inline">\(f_j\)</span>是<span
class="math inline">\(\boldsymbol{W}_j\)</span>和<span
class="math inline">\(x_i\)</span>的內积,所以<span
class="math inline">\(f_j=\boldsymbol{W}_jx_i=\parallel
\boldsymbol{W}_j\parallel \parallel x_i\parallel
cos(\theta_j)\)</span>,<span
class="math inline">\(\theta_j(0\leq\theta_j\leq\pi)\)</span>是向量<span
class="math inline">\(\boldsymbol{W}_i\)</span>和<span
class="math inline">\(x_i\)</span>的夹角.最终损失函数定义为:</p>
<p><span class="math display">\[ \begin{aligned}
    Loss_i=-log(\frac{e^{\parallel \boldsymbol{W}_{j_i}\parallel
\parallel x_i\parallel cos(\theta_{y_i})}}{\sum_j e^{\parallel
\boldsymbol{W}_j\parallel \parallel x_i\parallel cos(\theta_j)}})
\end{aligned} \]</span></p>
<h1 id="large-margin-softmax-loss">Large-Margin Softmax Loss</h1>
<h1 id="动机">动机</h1>
<p>考虑一个二分类的<code>softmax loss</code>,它的目标是使<span
class="math inline">\(\boldsymbol{W}_{1}^Tx&gt;\boldsymbol{W}_{1}^Tx\)</span>或<span
class="math inline">\(\parallel \boldsymbol{W}_{1}\parallel \parallel
x\parallel cos(\theta_{1})&gt;\parallel \boldsymbol{W}_{2}\parallel
\parallel x\parallel cos(\theta_{2})\)</span>来正确分类<span
class="math inline">\(x\)</span>.
现在作者想到构建一个决策余量来更加严格地约束决策间距,所以要求<span
class="math inline">\(\parallel \boldsymbol{W}_{1}\parallel \parallel
x\parallel cos(m\theta_{1})&gt;\parallel \boldsymbol{W}_{2}\parallel
\parallel x\parallel cos(\theta_{2}) (0\leq \theta_1 \leq
\frac{\pi}{m})\)</span>,<span
class="math inline">\(m\)</span>是一个正整数.从而使以下不等式成立: <span
class="math display">\[ \begin{aligned}
    \parallel \boldsymbol{W}_{1}\parallel \parallel x\parallel
cos(\theta_{1})\geq\parallel \boldsymbol{W}_{1}\parallel \parallel
x\parallel cos(m\theta_{1})&gt;\parallel \boldsymbol{W}_{2}\parallel
\parallel x\parallel cos(\theta_{2})
\end{aligned} \]</span></p>
<p>这样对学习<span
class="math inline">\(\boldsymbol{W}_1\boldsymbol{W}_2\)</span>都提出了更高的要求.</p>
<p><strong>注:</strong> 实际上这里是对类内的间距有个限制,假设<span
class="math inline">\(m=6\)</span>时,<span
class="math inline">\(cos(\theta)\geq
cos(m\theta)\)</span>的条件下,<span
class="math inline">\(\theta\)</span>被压缩到一个特定的范围中,如下图所示,只有蓝线大于红线的时候的<span
class="math inline">\(\theta\)</span>取值才是符合条件的,这相当于变相的增加了<span
class="math inline">\(\boldsymbol{W}_1\)</span>与<span
class="math inline">\(x\)</span>的方向限制,也就是学习难度更大,类内间距更小.不过我感觉还得限制一下<span
class="math inline">\(\theta\)</span>的范围,因为符合条件的<span
class="math inline">\(\theta\)</span>范围并不止一个:</p>
<p><img src="/2019/06/03/l-softmax/1.png" /></p>
<h1 id="定义">定义</h1>
<p>下面给出<code>L softmax</code>的定义:</p>
<p><span class="math display">\[ \begin{align}
Loss_{i}=-\log \left(\frac{e^{\left\|\boldsymbol{W}_{y_{i}}\right\|
\boldsymbol{x}_{i} \|
\psi\left(\theta_{y_{i}}\right)}}{e^{\left\|\boldsymbol{W}_{y_{i}}\right\|
\boldsymbol{x}_{i} \| \psi\left(\theta_{y_{i}}\right)}+\sum_{j \neq
y_{i}}
e^{\left\|\boldsymbol{W}_{j}\right\|\left\|\boldsymbol{x}_{i}\right\|
\cos \left(\theta_{j}\right)} )}\right)
\end{align}
\]</span></p>
<p>并:</p>
<p><span class="math display">\[ \begin{align}
\psi(\theta)=\left\{\begin{array}{l}{\cos (m \theta), \quad 0 \leq
\theta \leq \frac{\pi}{m}} \\ {\mathcal{D}(\theta), \quad
\frac{\pi}{m}&lt;\theta \leq \pi}\end{array}\right.
\end{align}
\]</span></p>
<p><span
class="math inline">\(m\)</span>是与分类边界密切相关的参数,<span
class="math inline">\(m\)</span>越大分类学习越难.同时<span
class="math inline">\(\mathcal{D}(\theta)\)</span>需要是一个单调递增函数,且<span
class="math inline">\(\mathcal{D}(\frac{\pi}{m})==cas(\frac{\pi}{m})\)</span>(保证他是连续函数才可以求导).下图显示了不同<span
class="math inline">\(\theta\)</span>值下的两个损失函数的结果,夹角<span
class="math inline">\(\theta\)</span>越大<code>L softmax loss</code>越大.</p>
<p><img src="/2019/06/03/l-softmax/2.png" /></p>
<p>为了简单起见,文章中特化了<span
class="math inline">\(\psi(\theta)\)</span>函数为: <span
class="math display">\[ \begin{align}
\psi(\theta)=(-1)^{k} \cos (m \theta)-2 k, \quad \theta \in\left[\frac{k
\pi}{m}, \frac{(k+1) \pi}{m}\right]
\end{align}
\]</span></p>
<p>其中<span class="math inline">\(k\in[0,m-1]\)</span>且为正整数.</p>
<p>在前向传播和反向传播中作者将<span
class="math inline">\(cos(\theta_j)\)</span>替换成<span
class="math inline">\(\frac{\boldsymbol{W}_{j}^{T}
\boldsymbol{x}_{i}}{\left\|\boldsymbol{W}_{j}\right\|\left\|\boldsymbol{x}_{i}\right\|}\)</span>,<span
class="math inline">\(cos(m\theta_{y_i})\)</span>替换为: <span
class="math display">\[  \begin{align}
\begin{aligned} \cos \left(m \theta_{y_{i}}\right) &amp;=C_{m}^{0} \cos
^{m}\left(\theta_{y_{i}}\right)-C_{m}^{2} \cos
^{m-2}\left(\theta_{y_{i}}\right)\left(1-\cos
^{2}\left(\theta_{y_{i}}\right)\right) \\ &amp;+C_{m}^{4} \cos
^{m-4}\left(\theta_{y_{i}}\right)\left(1-\cos
^{2}\left(\theta_{y_{i}}\right)\right)^{2}+\cdots \\ &amp;(-1)^{n}
C_{m}^{2 n} \cos ^{m-2 n}\left(\theta_{y_{y_{i}}}\right)\left(1-\cos
^{2}\left(\theta_{y_{i}}\right)\right)^{n}+\cdots \end{aligned}
\end{align}
\]</span></p>
<p><span class="math inline">\(n\)</span>是正整数且<span
class="math inline">\(2n\leq
m\)</span>.然后将这些函数带入<code>L softmax loss</code>公式中计算即可拿来做损失.但是这个公式还是太长了,所以我决定还是不用这个损失函数.</p>
<h1 id="a-softmax">A softmax</h1>
<p>这里还是介绍传统的<code>softmax</code>,这里就不赘述了.</p>
<h2 id="modified-softmax">modified softmax</h2>
<p>这个就是作者让传统的<code>softmax</code>的权重<span
class="math inline">\(\boldsymbol{W}\)</span>归一化:<span
class="math inline">\(\parallel \boldsymbol{W}_j\parallel
=1\)</span>(必须是没有bias的),得到了<code>modified softmax loss</code>:
<span class="math display">\[ \begin{align}
Loss=\frac{1}{N} \sum_{i}-\log
\left(\frac{e^{\left\|\boldsymbol{x}_{i}\right\| \cos
\left(\theta_{y_{i}, i}\right)}}{\sum_{j}
e^{\left\|\boldsymbol{x}_{i}\right\| \cos \left(\theta_{j,
i}\right)}}\right)
\end{align}
\]</span></p>
<p>当然我不知道这个归一化有什么好处,从作者给出的结果上来看准确率提高了1%.</p>
<h2 id="angular-margin-to-softmax-loss">Angular Margin to Softmax
Loss</h2>
<p>对于上面的<code>modified softmax loss</code>在二分类问题中,当<span
class="math inline">\(\cos(\theta_1)&gt;\cos(\theta_2)\)</span>可以确定类别为1,但是两个类别的决策面是<span
class="math inline">\(\cos(\theta_1)=\cos(\theta_2)\)</span>,这样的决策面间隔太小,为了让决策面之间的间距更大一些,作者提出做两个决策面:</p>
<p>类别1的决策面为<span
class="math inline">\(\cos(m\theta_1)=\cos(\theta_2)\)</span>;
类别2的决策面为<span
class="math inline">\(\cos(\theta_1)=\cos(m\theta_2)\)</span>; 其中<span
class="math inline">\(m\geq2,m\in N\)</span>,<span
class="math inline">\(m\)</span>取整数可以利用倍角公式;</p>
<p>这样的话,也就是说预测出来的<span
class="math inline">\(\boldsymbol{x}\)</span>与他对应的类的夹角必须要小于他与其他类<strong>最小</strong>的夹角的<span
class="math inline">\(m\)</span>倍,比如2分类问题,其实就有三个决策面,中间两个决策面之间的就是决策间距.然后推导出<code>A softmax loss</code>:
<span class="math display">\[ \begin{aligned}
Loss =
\frac{1}{N}\sum_i-\log(\frac{\exp(\|x_i\|\cos(m\theta_{yi,i}))}{\exp(\|x_i\|\cos(m\theta_{yi,i}))+\sum_{j\neq
y_i}\exp(\|x_i\|\cos(\theta_{j,i}))})
\end{aligned} \]</span></p>
<p>其中<span class="math inline">\(\theta_{yi,i}\in[0,
\frac{\pi}{m}]\)</span>,这就是因为<span
class="math inline">\(cos\)</span>的性质决定,我在上面也提到了,当<span
class="math inline">\(m\theta_{yi,i}&gt;\pi\)</span>时,会使得<span
class="math inline">\(m\theta_{yi,i}&gt;\theta_{j,i}\ ,j\neq
y_i\)</span>,但<span
class="math inline">\(cos(m\theta_{1})&gt;cos(\theta_2)\)</span>也会成立,这就与之前的假设相反.</p>
<p>为了避免<span class="math inline">\(cos\)</span>的问题,就设计 <span
class="math display">\[
\begin{aligned}
\psi(\theta_{y_i,i})=(-1)^k\cos(m\theta_{y_i,i})-2k\ \ \
\theta_{y_i,i}\in[\frac{k\pi}{m},\frac{(k+1)\pi}{m}],且k\in[0,m-1]    
\end{aligned}\]</span></p>
<p>来代替.这样使得<span class="math inline">\(\psi\)</span>随着<span
class="math inline">\(\theta_{y_i,i}\)</span>单调递减,如果<span
class="math inline">\(m\theta_{y_i,i}&gt;\theta_{j,i},j\neq
y_i\)</span>那么必有<span
class="math inline">\(\psi(\theta_{y_i,i})&lt;\cos(\theta_{j,i})\)</span>,这里我们看一下<span
class="math inline">\(\psi(\theta)\)</span>函数的图像(完全符合单独递减的要求,并且是连续函数可导):</p>
<p><img src="/2019/06/03/l-softmax/psi.png" /></p>
<p>最终得出损失函数如下: <span class="math display">\[ \begin{aligned}
L_{ang} =
\frac{1}{N}\sum_i-\log(\frac{\exp(\|x_i\|\psi(\theta_{yi,i}))}{\exp(\|x_i\|\psi(\theta_{yi,i}))+\sum_{j\neq
y_i}\exp(\|x_i\|\cos(\theta_{j,i}))})
\end{aligned} \]</span></p>
<p>这里对比一下三个loss的不同决策界:</p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 90%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">损失函数</th>
<th style="text-align: center;">决策面</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Softmax Loss</td>
<td style="text-align: center;"><span
class="math inline">\(\boldsymbol{W}_1-\boldsymbol{W}_2+b1-b2=0\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Modified Softmax Loss</td>
<td style="text-align: center;"><span
class="math inline">\(\parallel\boldsymbol{x}(cos(\theta_1)-cos(\theta_2))\parallel\)</span>=0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">A Softmax Loss</td>
<td style="text-align: center;"><span
class="math display">\[\begin{aligned}   \parallel\boldsymbol{x}\parallel(cos(m\theta_1)-cos(\theta_2)=0\
\text{for class 1}\\
\parallel\boldsymbol{x}\parallel(cos(\theta_1)-cos(m\theta_2)=0\
\text{for class 2} \end{aligned} \]</span></td>
</tr>
</tbody>
</table>
<p><strong>注:</strong>
写到这里我发现这tm就是一个作者的两篇文章,到这里<code>A softmax</code>还是与<code>L softmax</code>的区别就在于是否对<span
class="math inline">\(\parallel\boldsymbol{W}\parallel\)</span>进行归一化.这样的话对于分类来说可以观察到<code>A softmax</code>的分类结果都是长度会趋近相同,<code>L softmax</code>的分类长度会不相同.</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>A softmax</th>
<th>L softmax</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="/2019/06/03/l-softmax/a_softmax_res.png" /></td>
<td><img src="/2019/06/03/l-softmax/l_softmax_res.png" /></td>
</tr>
</tbody>
</table>
<h1 id="additive-margin-softmax">Additive Margin Softmax</h1>
<p>这里这个作者在2018年发表的文章,这里也一并学习了.这个是效果好并且实现简单的一种方案.</p>
<h2 id="定义-1">定义</h2>
<p>实际上看了上面的<code>Loss</code>函数,所有的变化点其实就在<span
class="math inline">\(e^{?}\)</span>做文章.这里首先替换了<span
class="math inline">\(\psi(\theta)\)</span>函数(<span
class="math inline">\(m\)</span>是偏移): <span class="math display">\[
\begin{aligned}
    \psi(\theta)=cos(\theta)-m
\end{aligned} \]</span></p>
<p>然后又把<span class="math inline">\(W,x\)</span>都归一化: <span
class="math display">\[ \begin{aligned}
    Loss_i=-log(\frac{e^{\parallel \boldsymbol{W}_{j_i}\parallel
\parallel x_i\parallel cos(\theta_{y_i})}}{\sum_j e^{\parallel
\boldsymbol{W}_j\parallel \parallel x_i\parallel cos(\theta_j)}})\ \ \ \
\parallel\boldsymbol{W}\parallel=1,\parallel x\parallel=1
\end{aligned} \]</span></p>
<p>这样內积结果就是: <span class="math display">\[ \begin{aligned}
    &lt;\boldsymbol{W_{y_i}},x&gt;=cos(\theta_{y_i})
\end{aligned} \]</span></p>
<p>接着再来一个偏移<span class="math inline">\(m,\
m&gt;0\)</span>与缩放因子<span
class="math inline">\(s\)</span>,得到最后的损失函数: <span
class="math display">\[ \begin{aligned}
    Loss_i = - \log \frac{e^{s\cdot(\cos\theta_{y_i} -m)}}{e^{s\cdot
(\cos\theta_{y_i} -m)}+\sum^c_{j=1,i\neq t}  e^{s\cdot\cos\theta_j }}
\end{aligned} \]</span></p>
<p>按照原论文,取<span class="math inline">\(m=0.35\)</span>,<span
class="math inline">\(s=30\)</span>.然后就结束了... 😄</p>
<p>再补张图(<code>AM softmax</code>没有乘<span
class="math inline">\(s\)</span>): <img
src="/2019/06/03/l-softmax/psi2.png" /></p>
<h1 id="编程实现">编程实现</h1>
<p>因为<code>A softmax loss</code>是升级版,所以就实现这个.</p>
<h2 id="a-softmax-loss">A softmax loss</h2>
<p>首先是几个代码要注意的点,<span
class="math inline">\(cos(\theta)\)</span>可以通过向量除计算,<span
class="math inline">\(cos(m\theta)\)</span>可以通过倍角公式计算.: <span
class="math display">\[
\begin{split}
\cos \theta_{i,j} &amp;=
\frac{\vec{x_i}\cdot\vec{W_j}}{\|\vec{x_i}\|\cdot\|\vec{W_j}\|}
\frac{\vec{x_i}\cdot\vec{W_{norm_j}}}{\|\vec{x_i}\|} \cr
\cos 2\theta &amp;= 2\cos^2 \theta -1 \cr
\cos 3\theta &amp;= 4\cos^2 \theta -3 \cos \theta \cr
\cos 4\theta &amp;= 8\cos^4 \theta -8\cos^2 \theta + 1 \cr
\end{split}
\]</span></p>
<p>然后还有<span class="math inline">\(k\)</span>的取值,先利用<span
class="math inline">\(sign\)</span>函数判断<span
class="math inline">\(cos(\theta)\)</span>属于哪一个区间,再确定<span
class="math inline">\(k\)</span>的值: <span class="math display">\[
\begin{aligned}
    sign_0&amp;=sign(cos(\theta))\\
    sign_3&amp;=sign(cos(2\theta))*sign_0\\
    sign_4&amp;=2*sign_0+sign_3-3 \\
    \psi(\theta)&amp;=sign_3*cos(4\theta)+sign_4 \\
    &amp;=sign_3*(8\cos^4 \theta -8\cos^2 \theta + 1)+sign_4
\end{aligned} \]</span></p>
<p>下面是<span class="math inline">\(m=4\)</span>时的<span
class="math inline">\(Loss\)</span>计算函数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.python <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> contrib</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Angular_Softmax_Loss</span>(<span class="params">embeddings, labels, margin=<span class="number">4</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Note:(about the value of margin)</span></span><br><span class="line"><span class="string">        as for binary-class case, the minimal value of margin is 2+sqrt(3)</span></span><br><span class="line"><span class="string">        as for multi-class  case, the minimal value of margin is 3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the value of margin proposed by the author of paper is 4.</span></span><br><span class="line"><span class="string">        here the margin value is 4.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        l = <span class="number">0.</span></span><br><span class="line">        embeddings = tf.random_normal((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">        labels = tf.convert_to_tensor([[<span class="number">1</span>], [<span class="number">2</span>]], dtype=tf.int64)</span><br><span class="line">        x_norm = tf.norm(embeddings, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;softmax&quot;</span>):</span><br><span class="line">            weights = tf.get_variable(name=<span class="string">&#x27;embedding_weights&#x27;</span>,</span><br><span class="line">                                      shape=[embeddings.get_shape().as_list()[-<span class="number">1</span>], <span class="number">10</span>],</span><br><span class="line">                                      initializer=contrib.layers.xavier_initializer())</span><br><span class="line">            W = tf.nn.l2_normalize(weights, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># cacualting the cos value of angles between embeddings and W</span></span><br><span class="line">            orgina_logits = tf.matmul(embeddings, W)</span><br><span class="line">            N = embeddings.get_shape()[<span class="number">0</span>]  <span class="comment"># get batch_size</span></span><br><span class="line">            single_sample_label_index = tf.concat([tf.constant(<span class="built_in">list</span>(<span class="built_in">range</span>(N)), tf.int64, shape=(N, <span class="number">1</span>)), labels], axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># N = 128, labels = [1,0,...,9]</span></span><br><span class="line">            <span class="comment"># single_sample_label_index:</span></span><br><span class="line">            <span class="comment"># [ [0,1],</span></span><br><span class="line">            <span class="comment">#   [1,0],</span></span><br><span class="line">            <span class="comment">#   ....</span></span><br><span class="line">            <span class="comment">#   [128,9]]</span></span><br><span class="line">            <span class="comment"># 这里就是F_y_i,根据有目标的位置来选取需要计算的loss位置.</span></span><br><span class="line">            f_y_i = tf.gather_nd(orgina_logits, single_sample_label_index)</span><br><span class="line">            <span class="comment"># NOTE 因为 \parallel W\parallel =1 所以 cos(theta)=f_y_i/x_norm</span></span><br><span class="line">            cos_theta = tf.div(f_y_i, x_norm)</span><br><span class="line">            cos_theta_2 = tf.<span class="built_in">pow</span>(cos_theta, <span class="number">2</span>)</span><br><span class="line">            cos_theta_4 = tf.<span class="built_in">pow</span>(cos_theta, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">            sign0 = tf.sign(cos_theta)</span><br><span class="line">            sign3 = tf.multiply(tf.sign(<span class="number">2</span> * cos_theta_2 - <span class="number">1</span>), sign0)</span><br><span class="line">            sign4 = <span class="number">2</span> * sign0 + sign3 - <span class="number">3</span></span><br><span class="line">            result = sign3 * (<span class="number">8</span> * cos_theta_4 - <span class="number">8</span> * cos_theta_2 + <span class="number">1</span>) + sign4</span><br><span class="line"></span><br><span class="line">            margin_logits = tf.multiply(result, x_norm)</span><br><span class="line">            f = <span class="number">1.0</span> / (<span class="number">1.0</span> + l)</span><br><span class="line">            ff = <span class="number">1.0</span> - f</span><br><span class="line">            combined_logits = tf.add(orgina_logits,</span><br><span class="line">                                     tf.scatter_nd(single_sample_label_index,</span><br><span class="line">                                                   tf.subtract(margin_logits, f_y_i),</span><br><span class="line">                                                   orgina_logits.get_shape()))</span><br><span class="line">            updated_logits = ff * orgina_logits + f * combined_logits</span><br><span class="line">            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits=updated_logits, labels=tf.reshape(labels, (-<span class="number">1</span>,))))</span><br><span class="line">            pred_prob = tf.nn.softmax(logits=updated_logits)</span><br><span class="line">            <span class="keyword">return</span> pred_prob, loss</span><br></pre></td></tr></table></figure>
<h2 id="additive-margin-softmax-1">Additive Margin Softmax</h2>
<p>这个比前面那个简单多了:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> tensorflow.python.keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.constraints <span class="keyword">import</span> unit_norm</span><br><span class="line"></span><br><span class="line">(train_x, train_y), (test_x, test_y) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">train_x = K.reshape(train_x, (-<span class="number">1</span>, <span class="number">784</span>))</span><br><span class="line">train_y = keras.utils.to_categorical(train_y, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = keras.Sequential([Input(shape=(<span class="number">784</span>,)),</span><br><span class="line">                          Dense(<span class="number">512</span>, keras.activations.relu),</span><br><span class="line">                          Dense(<span class="number">256</span>, keras.activations.relu),</span><br><span class="line">                          Dense(<span class="number">128</span>, keras.activations.relu),</span><br><span class="line">                          Lambda(<span class="keyword">lambda</span> x: K.l2_normalize(x, <span class="number">1</span>)),</span><br><span class="line">                          Dense(<span class="number">10</span>, use_bias=<span class="literal">False</span>, kernel_constraint=unit_norm())])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">am_softmax_loss</span>(<span class="params">y_true, y_pred, scale=<span class="number">30</span>, margin=<span class="number">0.35</span></span>):</span><br><span class="line">    <span class="comment"># NOTE 预测出来的x就是归一化后的,并且W也是归一化后的,所以y_pred就是cos(𝜃)</span></span><br><span class="line">    y_pred = (y_true * (y_pred - margin) + (<span class="number">1</span> - y_true) * y_pred) * scale</span><br><span class="line">    <span class="keyword">return</span> K.categorical_crossentropy(y_true, y_pred, from_logits=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=am_softmax_loss, optimizer=keras.optimizers.Adam(),</span><br><span class="line">              metrics=[keras.metrics.CategoricalAccuracy()])</span><br><span class="line"></span><br><span class="line">model.fit(x=train_x, y=train_y, batch_size=<span class="number">128</span>, epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>对于人脸识别问题，还是需要稀疏版本的<code>Additive Margin Softmax</code>实现以节省显存，这里提供一个实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Sparse_AmsoftmaxLoss</span>(kls.Loss):</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">               batch_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">               scale: <span class="built_in">int</span> = <span class="number">30</span>,</span></span><br><span class="line"><span class="params">               margin: <span class="built_in">int</span> = <span class="number">0.35</span>,</span></span><br><span class="line"><span class="params">               reduction=<span class="string">&#x27;auto&#x27;</span>,</span></span><br><span class="line"><span class="params">               name=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; sparse addivate margin softmax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        scale : int, optional</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            by default 30</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        margin : int, optional</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            by default 0.35</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">super</span>().__init__(reduction=reduction, name=name)</span><br><span class="line">    self.scale = scale</span><br><span class="line">    self.margin = margin</span><br><span class="line">    self.batch_idxs = tf.expand_dims(tf.<span class="built_in">range</span>(<span class="number">0</span>, batch_size, dtype=tf.int32),</span><br><span class="line">                                     <span class="number">1</span>)  <span class="comment"># shape [batch,1]</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, y_true: tf.Tensor, y_pred: tf.Tensor</span>) -&gt; tf.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot; loss calc</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        y_true : tf.Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            shape = [batch,1] type = tf.int32</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y_pred : tf.Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            shape = [batch,class num] type = tf.float32</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        tf.Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            loss</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], <span class="number">1</span>)</span><br><span class="line">    y_true_pred = tf.gather_nd(y_pred, idxs)</span><br><span class="line">    y_true_pred = tf.expand_dims(y_true_pred, <span class="number">1</span>)</span><br><span class="line">    y_true_pred_margin = y_true_pred - self.margin</span><br><span class="line">    _Z = tf.concat([y_pred, y_true_pred_margin], <span class="number">1</span>)</span><br><span class="line">    _Z = _Z * self.scale</span><br><span class="line">    logZ = tf.math.reduce_logsumexp(_Z, <span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    logZ = logZ + tf.math.log(<span class="number">1</span> - tf.math.exp(self.scale * y_true_pred - logZ))</span><br><span class="line">    <span class="keyword">return</span> -y_true_pred_margin * self.scale + logZ</span><br></pre></td></tr></table></figure>
<p>稀疏版本的<code>Additive Margin Softmax</code>代码中最后两步的推导过程如下：
<span class="math display">\[
\begin{aligned}
    \text{Let}\ \ p&amp;=y_{pred}\\
    log_Z&amp;=\log\left[e^{s*p_0}+\ldots+e^{s*p_c}+\ldots+e^{s*p_{y_i}}+e^{s*(p_{y_i}-m)}\right]\\
    log_Z&amp;=log_Z+\log\left[1-e^{s*p_{y_i}-log_Z}  \right]\\
    &amp;=log_Z+\log\left[1-\frac{e^{s*p_{y_i}}}{e^{log_Z}}  \right]\\
    &amp;=log_Z+\log\left[1-\frac{e^{s*p_{y_i}}}{e^{\log\left[e^{s*p_0}+\ldots+e^{s*p_c}+\ldots+e^{s*p_{y_i}}+e^{s*(p_{y_i}-m)}\right]}}  \right]\\
    &amp;=\log\left[e^{s*p_0}+\ldots+e^{s*p_c}+e^{s*(p_{y_i}-m)}\right]\\
    \mathcal{L}&amp;=-\log\left[\frac{e^{s*(p_{y_i}-m)}}{e^{s*p_0}+\ldots+e^{s*p_c}+e^{s*(p_{y_i}-m)}}
\right]\\
    &amp;=-\log e^{s*(p_{y_i}-m)}+log_Z\\
    &amp;=-s*(p_{y_i}-m)+log_Z\\
\end{aligned}
\]</span></p>
<h1 id="总结">总结</h1>
<p>看了三个文章,都是通过减小内类间距来达到效果.减小内类间距的途径都是构建<span
class="math inline">\(\psi(\theta)\)</span>代替<span
class="math inline">\(cos(\theta)\)</span>,当然要保证<span
class="math inline">\(\psi(\theta) &lt; \cos\theta\)</span>.</p>
<p>之前的<code>L softmax</code>和<code>A softmax</code>为了保证<span
class="math inline">\(\psi(\theta)=cos(m\theta) &lt;
\cos\theta\)</span>还使用了分段函数,比较麻烦.然后<code>AM softmax</code>就比较简单粗暴了.</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" rel="tag">损失函数</a></li></ul></div><div class="post-nav"><a class="pre" href="/2019/06/08/numpy-slice-range/">numpy中动态范围切片</a><a class="next" href="/2019/05/23/tf-keras-profile/">tf.keras中分析性能</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/03/13/vllm/sglang_attn/">vllm/sglang_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/trt_attn/">vllm/trt_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/vllm_attn/">vllm/vllm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/tvm_attn/">vllm/tvm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/23/torch-trick/">Pytorch中遇到的一些问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>