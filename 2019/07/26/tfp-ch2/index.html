<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>概率模型第二章 ： A little more on TFP | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">概率模型第二章 ： A little more on TFP</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">概率模型第二章 ： A little more on TFP</h1><div class="post-meta">2019-07-26<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 19.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 86</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>Tensorflow
概率模型学习，代码运行于<code>Tensorflow 1.14</code>，文字半机器翻译。</p>
<span id="more"></span>
<h1
id="probabilistic-programming-and-bayesian-methods-for-hackers-chapter-2">Probabilistic
Programming and Bayesian Methods for Hackers Chapter 2</h1>
<h3 id="table-of-contents">Table of Contents</h3>
<ul>
<li>Dependencies &amp; Prerequisites</li>
<li>A little more on TFP
<ul>
<li>TFP Variables
<ul>
<li>Initializing Stochastic Variables</li>
<li>Deterministic variables</li>
</ul></li>
<li>Combining with Tensorflow Core</li>
<li>Including observations in the Model</li>
</ul></li>
<li>Modeling approaches
<ul>
<li>Same story; different ending</li>
<li>Example: Bayesian A/B testing</li>
<li>A Simple Case
<ul>
<li>Execute the TF graph to sample from the posterior</li>
</ul></li>
<li>A and B together
<ul>
<li>Execute the TF graph to sample from the posterior</li>
</ul></li>
</ul></li>
<li>An algorithm for human deceit
<ul>
<li>The Binomial Distribution</li>
<li>Example: Cheating among students
<ul>
<li>Execute the TF graph to sample from the posterior</li>
</ul></li>
<li>Alternative TFP Model
<ul>
<li>Execute the TF graph to sample from the posterior</li>
</ul></li>
<li>More TFP Tricks</li>
<li>Example: Challenger Space Shuttle Disaster
<ul>
<li>Normal Distributions
<ul>
<li>Execute the TF graph to sample from the posterior</li>
</ul></li>
<li>What about the day of the Challenger disaster?</li>
<li>Is our model appropriate?
<ul>
<li>Execute the TF graph to sample from the posterior</li>
</ul></li>
</ul></li>
<li>Exercises</li>
<li>References ___</li>
</ul></li>
</ul>
<p>本章介绍了更多的TFP语法和变量，以及如何从贝叶斯的角度思考如何建模系统。它还包含用于评估贝叶斯模型拟合优度的提示和数据可视化技术。</p>
<h3 id="dependencies-prerequisites">Dependencies &amp;
Prerequisites</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@title Imports and Global Variables (run this cell first)  &#123; display-mode: &quot;form&quot; &#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">The book uses a custom matplotlibrc file, which provides the unique styles for</span></span><br><span class="line"><span class="string">matplotlib plots. If executing this book, and you wish to use the book&#x27;s</span></span><br><span class="line"><span class="string">styling, provided are two options:</span></span><br><span class="line"><span class="string">    1. Overwrite your own matplotlibrc file with the rc-file provided in the</span></span><br><span class="line"><span class="string">       book&#x27;s styles/ dir. See http://matplotlib.org/users/customizing.html</span></span><br><span class="line"><span class="string">    2. Also in the styles is  bmh_matplotlibrc.json file. This can be used to</span></span><br><span class="line"><span class="string">       update the styles in only this notebook. Try running the following code:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        import json</span></span><br><span class="line"><span class="string">        s = json.load(open(&quot;../styles/bmh_matplotlibrc.json&quot;))</span></span><br><span class="line"><span class="string">        matplotlib.rcParams.update(s)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"><span class="comment">#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)</span></span><br><span class="line">warning_status = <span class="string">&quot;ignore&quot;</span> <span class="comment">#@param [&quot;ignore&quot;, &quot;always&quot;, &quot;module&quot;, &quot;once&quot;, &quot;default&quot;, &quot;error&quot;]</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(warning_status)</span><br><span class="line"><span class="keyword">with</span> warnings.catch_warnings():</span><br><span class="line">    warnings.filterwarnings(warning_status, category=DeprecationWarning)</span><br><span class="line">    warnings.filterwarnings(warning_status, category=UserWarning)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment">#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/)</span></span><br><span class="line">matplotlib_style = <span class="string">&#x27;fivethirtyeight&#x27;</span> <span class="comment">#@param [&#x27;fivethirtyeight&#x27;, &#x27;bmh&#x27;, &#x27;ggplot&#x27;, &#x27;seaborn&#x27;, &#x27;default&#x27;, &#x27;Solarize_Light2&#x27;, &#x27;classic&#x27;, &#x27;dark_background&#x27;, &#x27;seaborn-colorblind&#x27;, &#x27;seaborn-notebook&#x27;]</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt; plt.style.use(matplotlib_style)</span><br><span class="line"><span class="keyword">import</span> matplotlib.axes <span class="keyword">as</span> axes;</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Ellipse</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns; sns.set_context(<span class="string">&#x27;notebook&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> IPython.core.pylabtools <span class="keyword">import</span> figsize</span><br><span class="line"><span class="comment">#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)</span></span><br><span class="line">notebook_screen_res = <span class="string">&#x27;retina&#x27;</span> <span class="comment">#@param [&#x27;retina&#x27;, &#x27;png&#x27;, &#x27;jpeg&#x27;, &#x27;svg&#x27;, &#x27;pdf&#x27;]</span></span><br><span class="line">%config InlineBackend.figure_format = notebook_screen_res</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tfe = tf.contrib.eager</span><br><span class="line"></span><br><span class="line"><span class="comment"># Eager Execution</span></span><br><span class="line"><span class="comment">#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)</span></span><br><span class="line"><span class="comment">#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)</span></span><br><span class="line">use_tf_eager = <span class="literal">False</span> <span class="comment">#@param &#123;type:&quot;boolean&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use try/except so we can easily re-execute the whole notebook.</span></span><br><span class="line"><span class="keyword">if</span> use_tf_eager:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tf.enable_eager_execution()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow_probability <span class="keyword">as</span> tfp</span><br><span class="line">tfd = tfp.distributions</span><br><span class="line">tfb = tfp.bijectors</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">tensors</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Evaluates Tensor or EagerTensor to Numpy `ndarray`s.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,</span></span><br><span class="line"><span class="string">        `namedtuple` or combinations thereof.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ndarrays: Object with same structure as `tensors` except with `Tensor` or</span></span><br><span class="line"><span class="string">          `EagerTensor`s replaced by Numpy `ndarray`s.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> tf.executing_eagerly():</span><br><span class="line">        <span class="keyword">return</span> tf.contrib.framework.nest.pack_sequence_as(</span><br><span class="line">            tensors,</span><br><span class="line">            [t.numpy() <span class="keyword">if</span> tf.contrib.framework.is_tensor(t) <span class="keyword">else</span> t</span><br><span class="line">             <span class="keyword">for</span> t <span class="keyword">in</span> tf.contrib.framework.nest.flatten(tensors)])</span><br><span class="line">    <span class="keyword">return</span> sess.run(tensors)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_TFColor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Enum of colors used in TF docs.&quot;&quot;&quot;</span></span><br><span class="line">    red = <span class="string">&#x27;#F15854&#x27;</span></span><br><span class="line">    blue = <span class="string">&#x27;#5DA5DA&#x27;</span></span><br><span class="line">    orange = <span class="string">&#x27;#FAA43A&#x27;</span></span><br><span class="line">    green = <span class="string">&#x27;#60BD68&#x27;</span></span><br><span class="line">    pink = <span class="string">&#x27;#F17CB0&#x27;</span></span><br><span class="line">    brown = <span class="string">&#x27;#B2912F&#x27;</span></span><br><span class="line">    purple = <span class="string">&#x27;#B276B2&#x27;</span></span><br><span class="line">    yellow = <span class="string">&#x27;#DECF3F&#x27;</span></span><br><span class="line">    gray = <span class="string">&#x27;#4D4D4D&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="keyword">return</span> [</span><br><span class="line">            self.red,</span><br><span class="line">            self.orange,</span><br><span class="line">            self.green,</span><br><span class="line">            self.blue,</span><br><span class="line">            self.pink,</span><br><span class="line">            self.brown,</span><br><span class="line">            self.purple,</span><br><span class="line">            self.yellow,</span><br><span class="line">            self.gray,</span><br><span class="line">        ][i % <span class="number">9</span>]</span><br><span class="line">TFColor = _TFColor()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">session_options</span>(<span class="params">enable_gpu_ram_resizing=<span class="literal">True</span>, enable_xla=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Allowing the notebook to make use of GPUs if they&#x27;re available.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear </span></span><br><span class="line"><span class="string">    algebra that optimizes TensorFlow computations.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    config = tf.ConfigProto()</span><br><span class="line">    config.log_device_placement = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> enable_gpu_ram_resizing:</span><br><span class="line">        <span class="comment"># `allow_growth=True` makes it possible to connect multiple colabs to your</span></span><br><span class="line">        <span class="comment"># GPU. Otherwise the colab malloc&#x27;s all GPU ram.</span></span><br><span class="line">        config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> enable_xla:</span><br><span class="line">        <span class="comment"># Enable on XLA. https://www.tensorflow.org/performance/xla/.</span></span><br><span class="line">        config.graph_options.optimizer_options.global_jit_level = (</span><br><span class="line">            tf.OptimizerOptions.ON_1)</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reset_sess</span>(<span class="params">config=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Convenience function to create the TF graph &amp; session or reset them.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        config = session_options()</span><br><span class="line">    <span class="keyword">global</span> sess</span><br><span class="line">    tf.reset_default_graph()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        sess.close()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    sess = tf.InteractiveSession(config=config)</span><br><span class="line"></span><br><span class="line">reset_sess()</span><br></pre></td></tr></table></figure>
<pre><code>WARNING: Logging before flag parsing goes to stderr.
W0726 19:11:25.497503 140237485999936 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.</code></pre>
<h2 id="a-little-more-on-tensorflow-and-tensorflow-probability">A little
more on TensorFlow and TensorFlow Probability</h2>
<p>为了解释TensorFlow概率，值得研究使用Tensorflow张量的各种方法。在这里，我们介绍了Tensorflow图的概念，以及我们如何使用某些编码模式使我们的张量处理工作流更加快速和优雅。</p>
<h3 id="tensorflow图和eager模式">TensorFlow图和Eager模式</h3>
<p>TFP通过主张量流库实现了大部分繁重的工作。张量流库还包含许多熟悉的NumPy计算元素，并使用类似的表示法。当NumPy直接执行计算时（例如，当您运行+
b时），图形模式中的张量流会构建一个“计算图形”，跟踪您要对元素a和b执行+运算。只有在评估张量流表达式时才会进行计算
-
tensorflow是惰性求值的。使用Tensorflow而不是NumPy的好处是图形可以实现数学优化（例如简化），通过自动微分进行梯度计算，将整个图形编译为C以机器速度运行，以及编译它以在GPU或TPU上运行。</p>
<p>从根本上说，TensorFlow使用图形进行计算，其中图形表示计算作为各个操作之间的依赖关系。在Tensorflow图的编程范例中，我们首先定义数据流图，然后创建TensorFlow会话以运行图的部分。
Tensorflow
tf.Session（）对象运行图形以获取我们想要建模的变量。在下面的示例中，我们使用了一个全局会话对象sess，我们在上面的“Imports
and Global Variables”部分中创建了它。</p>
<p>为了避免懒惰评估有时令人困惑的方面，Tensorflow的渴望模式会立即对结果进行评估，从而为使用NumPy提供更加相似的感觉。使用Tensorflow
eager模式，您可以立即评估操作，而无需显式构建图形：操作返回具体值，而不是构建计算图形以便稍后运行。如果我们处于急切模式，我们会看到可以立即转换为NumPy数组等效的张量。
Eager模式使您可以轻松开始使用TensorFlow和调试模型。</p>
<p>TFP is essentially:</p>
<ul>
<li><p>各种概率分布的张量流符号表达式的集合，它们被组合成一个大的计算图</p></li>
<li><p>一组推理算法，使用该图来计算概率和梯度。</p></li>
</ul>
<p>出于实际目的，这意味着为了构建某些模型，我们有时必须使用核心Tensorflow。泊松采样的这个简单示例是我们如何使用图形和急切模式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parameter = tfd.Exponential(rate=<span class="number">1.</span>, name=<span class="string">&quot;poisson_param&quot;</span>).sample() <span class="comment"># 构建一个指数分布并进行采样</span></span><br><span class="line">rv_data_generator = tfd.Poisson(parameter, name=<span class="string">&quot;data_generator&quot;</span>) <span class="comment"># 构建一个泊松分布</span></span><br><span class="line">data_generator = rv_data_generator.sample() <span class="comment"># 取得泊松分布的样本</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> tf.executing_eagerly():</span><br><span class="line">    data_generator_ = tf.contrib.framework.nest.pack_sequence_as(data_generator,[t.numpy() <span class="keyword">if</span> tf.contrib.framework.is_tensor(t) <span class="keyword">else</span> t <span class="keyword">for</span> t <span class="keyword">in</span> tf.contrib.framework.nest.flatten(data_generator)])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_generator_ = sess.run(data_generator)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Value of sample from data generator random variable:&quot;</span>, data_generator_)</span><br></pre></td></tr></table></figure>
<pre><code>Value of sample from data generator random variable: 1.0</code></pre>
<p>在图形模式下，Tensorflow会自动将任何变量分配给图形;然后可以在会话中对它们进行评估，也可以在急切模式下使用它们。如果在会话已关闭或处于最终状态时尝试定义变量，则会出现错误。在“导入和全局变量”部分中，我们定义了一种特定类型的会话，称为InteractiveSession。全局InteractiveSession的这个定义允许我们通过shell或笔记本以交互方式访问我们的会话变量。</p>
<p>使用全局会话的模式，我们可以递增地构建图形并运行它的子集以获得结果。</p>
<p>热切执行进一步简化了我们的代码，无需显式调用会话功能。实际上，如果您尝试在急切模式下运行图形模式语义，您将收到如下错误消息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AttributeError: Tensor.graph is meaningless when eager execution is enabled.</span><br></pre></td></tr></table></figure>
<p>As mentioned in the previous chapter, we have a nifty tool that
allows us to create code that's usable in both graph mode and eager
mode. The custom <code>evaluate()</code> function allows us to evaluate
tensors whether we are operating in TF graph or eager mode. A
generalization of our data generator example above, the function looks
like the following:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">tensors</span>):</span><br><span class="line">    <span class="keyword">if</span> tf.executing_eagerly():</span><br><span class="line">         <span class="keyword">return</span> tf.contrib.framework.nest.pack_sequence_as(</span><br><span class="line">             tensors,</span><br><span class="line">             [t.numpy() <span class="keyword">if</span> tf.contrib.framework.is_tensor(t) <span class="keyword">else</span> t</span><br><span class="line">             <span class="keyword">for</span> t <span class="keyword">in</span> tf.contrib.framework.nest.flatten(tensors)])</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="keyword">return</span> sess.run(tensors)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Each of the tensors corresponds to a NumPy-like output. To
distinguish the tensors from their NumPy-like counterparts, we will use
the convention of appending an underscore to the version of the tensor
that one can use NumPy-like arrays on. In other words, the output of
<code>evaluate()</code> gets named as <code>variable</code> +
<code>_</code> = <code>variable_</code> . Now, we can do our Poisson
sampling using both the <code>evaluate()</code> function and this new
convention for naming Python variables in TFP.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义我们的假设</span></span><br><span class="line">parameter = tfd.Exponential(rate=<span class="number">1.</span>, name=<span class="string">&quot;poisson_param&quot;</span>).sample()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为numpy</span></span><br><span class="line">[ parameter_ ] = evaluate([ parameter ])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sample from exponential distribution before evaluation: &quot;</span>, parameter)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Evaluated sample from exponential distribution: &quot;</span>, parameter_)</span><br></pre></td></tr></table></figure>
<pre><code>Sample from exponential distribution before evaluation:  Tensor(&quot;poisson_param_1/sample/Reshape:0&quot;, shape=(), dtype=float32)
Evaluated sample from exponential distribution:  0.34844726</code></pre>
<p>更一般地说，我们可以使用我们的<code>evaluate()</code>函数在Tensorflow
<code>tensor</code>数据类型和我们可以运行操作的数据类型之间进行转换：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[ </span><br><span class="line">    parameter_,</span><br><span class="line">    data_generator_,</span><br><span class="line">] = evaluate([ </span><br><span class="line">    parameter, </span><br><span class="line">    data_generator,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#x27;parameter_&#x27; evaluated Tensor :&quot;</span>, parameter_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#x27;data_generator_&#x27; sample evaluated Tensor :&quot;</span>, data_generator_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>&#39;parameter_&#39; evaluated Tensor : 0.7298943
&#39;data_generator_&#39; sample evaluated Tensor : 0.0</code></pre>
<p>在TensorFlow中编程的一般经验法则是，如果您需要进行任何需要NumPy函数的类似数组的计算，则应在TensorFlow中使用它们的等价物。这种做法是必要的，因为NumPy只能产生常数值，但TensorFlow张量是计算图的动态部分。如果以错误的方式混合和匹配这些，通常会出现有关不兼容类型的错误。</p>
<h3 id="tfp-distributions">TFP Distributions</h3>
<p>让我们看看tfp.distributions如何工作。</p>
<p>TFP使用分布子类来表示随机随机变量。当满足以下条件时，变量是随机的：即使您知道变量的参数和组件的所有值，它仍然是随机的。此类别中包括Poisson，Uniform和Exponential类的实例。</p>
<p>您可以从随机变量中抽取随机样本。当您绘制样本时，这些样本将成为tensorflow.Tensors从该点开始具有确定性。快速的心理检查以确定某些东西是否具有确定性：如果我知道创建变量foo的所有输入，我可以计算foo的值。您可以通过下面讨论的各种方式添加，减去和操纵张量。这些操作几乎总是确定的。</p>
<h4 id="初始化分布">初始化分布</h4>
<p>初始化随机变量或随机变量需要一些特定于类的参数来描述分布的形状，例如位置和比例。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">some_distribution = tfd.Uniform(<span class="number">0.</span>, <span class="number">4.</span>)</span><br></pre></td></tr></table></figure>
<p>初始化随机或随机的均匀分布，其下限为0，上限为4.在分布上调用sample（）会返回一个张量，该张量将从该点开始确定性地表现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sampled_tensor = some_distribution.sample()</span><br></pre></td></tr></table></figure>
<p>下一个例子说明了当我们说分布是随机的但是张量是确定性时我们的意思：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">derived_tensor_1 = 1 + sampled_tensor</span><br><span class="line">derived_tensor_2 = 1 + sampled_tensor  # equal to 1</span><br><span class="line"></span><br><span class="line">derived_tensor_3 = 1 + some_distribution.sample()</span><br><span class="line">derived_tensor_4 = 1 + some_distribution.sample()  # different from 3</span><br></pre></td></tr></table></figure>
<p>前两行产生相同的值，因为它们引用相同的采样张量。最后两行可能产生不同的值，因为它们指的是从相同分布中提取的独立样本。</p>
<p>要定义多变量分布，只需传入具有您希望输出在创建分布时的形状的参数。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">betas = tfd.Uniform([<span class="number">0.</span>, <span class="number">0.</span>], [<span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<p>使用batch_shape（2，）创建分布。现在，当您调用betas.sample（）时，将返回两个值而不是一个。您可以在TFP文档中阅读有关TFP形状语义的更多信息，但本书中的大多数用法应该是不言自明的。</p>
<h4 id="确定变量">确定变量</h4>
<p>我们可以创建一个确定性分布，类似于我们如何创建随机分布。我们只是从Tensorflow
Distributions中调用Deterministic类，并传递我们想要的确定性值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">deterministic_variable = tfd.Deterministic(name=<span class="string">&quot;deterministic_variable&quot;</span>, loc=some_function_of_variables)</span><br></pre></td></tr></table></figure>
<p>调用tfd.Deterministic对于创建始终具有相同值的分布非常有用。但是，在TFP中使用确定性变量的更常见模式是从分布中创建张量或样本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lambda_1 = tfd.Exponential(rate=<span class="number">1.</span>, name=<span class="string">&quot;lambda_1&quot;</span>) <span class="comment">#随机变量</span></span><br><span class="line">lambda_2 = tfd.Exponential(rate=<span class="number">1.</span>, name=<span class="string">&quot;lambda_2&quot;</span>) <span class="comment">#随机变量</span></span><br><span class="line">tau = tfd.Uniform(name=<span class="string">&quot;tau&quot;</span>, low=<span class="number">0.</span>, high=<span class="number">10.</span>) <span class="comment">#随机变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为我们在采样后得到lambda的结果，所以确定性变量</span></span><br><span class="line">new_deterministic_variable = tfd.Deterministic(name=<span class="string">&quot;deterministic_variable&quot;</span>, loc=(lambda_1.sample() + lambda_2.sample()))</span><br><span class="line">new_deterministic_variable</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tfp.distributions.Deterministic &#39;deterministic_variable/&#39; batch_shape=[] event_shape=[] dtype=float32&gt;</code></pre>
<p>在前一章的文本消息示例中可以看到确定性变量的使用。回想一下λ的模型看起来像</p>
<p><span class="math display">\[
\lambda =
\begin{cases}\lambda_1  &amp; \text{if } t \lt \tau \cr
\lambda_2 &amp; \text{if } t \ge \tau
\end{cases}
\]</span></p>
<p>And in TFP code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 日子</span></span><br><span class="line">n_data_points = <span class="number">5</span>  <span class="comment"># in CH1 we had ~70 data points</span></span><br><span class="line">idx = np.arange(n_data_points)</span><br><span class="line"><span class="comment"># 对于n_data_points样本，如果采样tau&gt; = day值，则从lambda_2中选择，否则为lambda_1</span></span><br><span class="line">rv_lambda_deterministic = tfd.Deterministic(tf.gather([lambda_1.sample(), lambda_2.sample()],</span><br><span class="line">                    indices=tf.cast(</span><br><span class="line">                        tau.sample() &gt;= idx,tf.int32)))</span><br><span class="line">lambda_deterministic = rv_lambda_deterministic.sample()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Execute graph</span></span><br><span class="line">[lambda_deterministic_] = evaluate([lambda_deterministic])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show results</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; samples from our deterministic lambda model: \n&quot;</span>.<span class="built_in">format</span>(n_data_points), lambda_deterministic_ )</span><br></pre></td></tr></table></figure>
<pre><code>5 samples from our deterministic lambda model: 
 [0.24393924 0.24393924 0.24393924 0.24393924 0.24393924]</code></pre>
<p>显然，如果已知τ，<span
class="math inline">\(\lambda_1\)</span>，λ1和λ2，那么λ是完全已知的，因此它是一个确定性变量。我们在这里使用索引在适当的时间从λ1切换到λ2</p>
<h3 id="包括模型中的观察">包括模型中的观察</h3>
<p>在这一点上，它可能看起来不像，但我们已经完全指定了我们的先验。例如，我们可以提出并回答诸如“我之前分配<span
class="math inline">\(\lambda_1\)</span>的内容是什么样的问题？”之类的问题。</p>
<p>为此，我们将从分发中进行抽样。方法<code>.sample()</code>有一个非常简单的作用：从给定的分布中获取数据点。然后我们可以评估生成的张量以获得类似NumPy数组的对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义观测变量为指数分布</span></span><br><span class="line">rv_lambda_1 = tfd.Exponential(rate=<span class="number">1.</span>, name=<span class="string">&quot;lambda_1&quot;</span>)</span><br><span class="line">lambda_1 = rv_lambda_1.sample(sample_shape=<span class="number">20000</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 执行图</span></span><br><span class="line">[ lambda_1_ ] = evaluate([ lambda_1 ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化先验分布</span></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">5</span>))</span><br><span class="line">plt.hist(lambda_1_, bins=<span class="number">70</span>, normed=<span class="literal">True</span>, histtype=<span class="string">&quot;stepfilled&quot;</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;Prior distribution for$\lambda_1$&quot;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_20_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义观测变量为正态分布</span></span><br><span class="line">rv_lambda_1 = tfd.Normal(loc=<span class="number">0</span>,scale=<span class="number">1</span>,name=<span class="string">&quot;lambda_1&quot;</span>)</span><br><span class="line">lambda_1 = rv_lambda_1.sample(sample_shape=<span class="number">20000</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 执行图</span></span><br><span class="line">[ lambda_1_ ] = evaluate([ lambda_1 ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化先验分布</span></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">5</span>))</span><br><span class="line">plt.hist(lambda_1_, bins=<span class="number">70</span>, normed=<span class="literal">True</span>, histtype=<span class="string">&quot;stepfilled&quot;</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;Prior distribution for$\lambda_1$&quot;</span>)</span><br><span class="line">plt.xlim(-<span class="number">8</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_21_0.png" /></p>
<p>为了在第一章的符号中描述这一点，虽然这是对符号的轻微滥用，但我们已经指定了<span
class="math inline">\(P(A)\)</span>。我们的下一个目标是将数据/证据/观察结果<span
class="math inline">\(X\)</span>包含在我们的模型中。</p>
<p>有时我们可能希望将分布的属性与观察数据的属性相匹配。为此，我们从数据本身获取分布参数。在此示例中，泊松率（平均事件数）在数据平均值上显式设置为1：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建图</span></span><br><span class="line">data = tf.constant([<span class="number">10.</span>, <span class="number">5.</span>], dtype=tf.float32)</span><br><span class="line">rv_poisson = tfd.Poisson(rate=<span class="number">1.</span>/tf.reduce_mean(data))</span><br><span class="line">poisson = rv_poisson.sample()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Execute graph</span></span><br><span class="line">[ data_, poisson_, ] = evaluate([ data, poisson ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;two predetermined data points: &quot;</span>, data_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n mean of our data: &quot;</span>, np.mean(data_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n random sample from poisson distribution \n with the mean as the poisson&#x27;s rate: \n&quot;</span>, poisson_)</span><br></pre></td></tr></table></figure>
<pre><code>two predetermined data points:  [10.  5.]

 mean of our data:  7.5

 random sample from poisson distribution 
 with the mean as the poisson&#39;s rate: 
 1.0</code></pre>
<h2 id="建模方法">建模方法</h2>
<p>对贝叶斯建模的良好开端思考是考虑如何生成数据。将自己置于无所不知的位置，并尝试想象如何重新创建数据集。</p>
<p>在上一章中，我们研究了文本消息数据。我们首先询问我们的观察结果如何产生：</p>
<ol type="1">
<li><p>我们首先想到“描述这个计数数据的最佳随机变量是什么？”泊松随机变量是一个很好的候选变量，因为它可以表示计数数据。因此，我们模拟从泊松分布中采样的短信的数量。</p></li>
<li><p>接下来，我们认为，“好吧，假设短信是泊松分布的，那么泊松分布需要什么？”那么，泊松分布有一个参数<span
class="math inline">\(\lambda\)</span>。</p></li>
<li><p>我们知道<span
class="math inline">\(\lambda\)</span>吗？不。实际上，我们怀疑有<em>两个</em><span
class="math inline">\(\lambda\)</span>值，一个用于早期行为，一个用于后面的行为。我们不知道行为何时切换，但称切换点为<span
class="math inline">\(\tau\)</span>。</p></li>
<li><p>这两个<span
class="math inline">\(\lambda\)</span>的好分布是什么？指数是好的，因为它将概率分配给正实数。那么指数分布也有一个参数，称之为<span
class="math inline">\(\alpha\)</span>。</p></li>
<li><p>我们知道参数<span
class="math inline">\(\alpha\)</span>可能是什么吗？没有。此时，我们可以继续并将分配分配给<span
class="math inline">\(\alpha\)</span>，但是一旦达到设定的无知水平，最好停止：而我们先前有关于<span
class="math inline">\(\lambda\)</span>的信念，（“它可能会改变随着时间的推移，“它可能在10到30”之间，等等，我们对<span
class="math inline">\(\alpha\)</span>没有任何强烈的信念。所以最好停在这里。</p>
<p>那么<span
class="math inline">\(\alpha\)</span>有什么好处呢？我们认为<span
class="math inline">\(\lambda\)</span>s在10-30之间，所以如果我们将<span
class="math inline">\(\alpha\)</span>设置得非常低（相当于较高值的较大概率），我们就不会反映我们之前的好。类似的，太高的阿尔法也错过了我们先前的信念。反映我们信念的<span
class="math inline">\(\alpha\)</span>的一个好主意是设置值，以便<span
class="math inline">\(\alpha\)</span>的<span
class="math inline">\(\lambda\)</span>的平均值等于我们观察到的平均值。这在最后一章中有所体现。</p></li>
</ol>
<p>我们对<span
class="math inline">\(\tau\)</span>可能发生的时间没有专家意见。所以我们假设<span
class="math inline">\(\tau\)</span>来自整个时间跨度的离散均匀分布。</p>
<p>下面我们给出了这个的图形可视化，其中箭头表示父子关系。 （由Daft
Python库提供）</p>
<p><img src="http://i.imgur.com/7J30oCG.png" /></p>
<p>TFP和其他概率编程语言旨在告诉这些数据生成故事。更一般地说，B。Cronin写道[2]：</p>
<blockquote>
<p>概率编程将解读数据的叙述性解释，这是商业分析的圣杯之一，也是科学说服的无名英雄。人们从故事的角度思考
-
因此轶事的不合理的力量推动决策，有充分根据或没有。但现有的分析很大程度上无法提供这种故事;相反，数字似乎凭空出现，人们在权衡他们的选择时几乎没有因果关系。</p>
</blockquote>
<h3 id="相同的故事不同的结局">相同的故事;不同的结局。</h3>
<p>有趣的是，我们可以通过重述故事来创建<em>新数据集</em>。</p>
<p>例如，如果我们颠倒上述步骤，我们可以模拟数据集的可能实现。</p>
<p>1 通过从<span class="math inline">\(\text
{DiscreteUniform}（0,80）\)</span>中抽样来指定用户行为的切换时间：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tau = tf.random_uniform(shape=[<span class="number">1</span>], minval=<span class="number">0</span>, maxval=<span class="number">80</span>, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">[ tau_ ] = evaluate([ tau ])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Value of Tau (randomly taken from DiscreteUniform(0, 80)):&quot;</span>, tau_)</span><br></pre></td></tr></table></figure>
<pre><code>Value of Tau (randomly taken from DiscreteUniform(0, 80)): [58]</code></pre>
<p>2 绘制出<span class="math inline">\(\lambda_1\)</span>和<span
class="math inline">\(\lambda_2\)</span>从<span
class="math inline">\(\text{Gamma}(\alpha)\)</span>分布:</p>
<p>注意：伽玛分布是指数分布的推广。形状参数<span
class="math inline">\(α= 1\)</span>和尺度参数<span
class="math inline">\(β\)</span>的伽玛分布是指数（<span
class="math inline">\(β\)</span>）分布。在这里，我们使用伽玛分布比我们用指数建模时具有更大的灵活性。我们可以返回远大于<span
class="math inline">\(1\)</span>的值（即，在每日短信计数中会出现的数字种类），而不是返回<span
class="math inline">\(0\)</span>和<span
class="math inline">\(1\)</span>之间的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alpha = <span class="number">1.</span>/<span class="number">8.</span></span><br><span class="line"></span><br><span class="line">lambdas  = tfd.Gamma(concentration=<span class="number">1</span>/alpha, rate=<span class="number">0.3</span>).sample(sample_shape=[<span class="number">2</span>])  </span><br><span class="line">[ lambda_1_, lambda_2_ ] = evaluate( lambdas )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Lambda 1 (randomly taken from Gamma(α) distribution): &quot;</span>, lambda_1_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Lambda 2 (randomly taken from Gamma(α) distribution): &quot;</span>, lambda_2_)</span><br></pre></td></tr></table></figure>
<pre><code>Lambda 1 (randomly taken from Gamma(α) distribution):  57.477856
Lambda 2 (randomly taken from Gamma(α) distribution):  23.423761</code></pre>
<p>3
在τ之前的几天，通过从Poi（λ1）采样来表示用户接收的短信计数，并且在τ之后的几天表示来自Poi（λ2）的样本。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = tf.concat([tfd.Poisson(rate=lambda_1_).sample(sample_shape=tau_),</span><br><span class="line">                      tfd.Poisson(rate=lambda_2_).sample(sample_shape= (<span class="number">80</span> - tau_))], axis=<span class="number">0</span>)</span><br><span class="line">days_range = tf.<span class="built_in">range</span>(<span class="number">80</span>)</span><br><span class="line">[ data_, days_range_ ] = evaluate([ data, days_range ])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Artificial day-by-day user SMS count created by sampling: \n&quot;</span>, data_)</span><br></pre></td></tr></table></figure>
<pre><code>Artificial day-by-day user SMS count created by sampling: 
 [62. 65. 63. 63. 61. 70. 61. 49. 58. 78. 60. 51. 54. 61. 54. 59. 64. 54.
 68. 54. 45. 59. 71. 55. 51. 37. 52. 51. 55. 71. 60. 47. 59. 58. 61. 53.
 49. 44. 51. 56. 67. 64. 71. 52. 72. 48. 55. 62. 58. 49. 52. 62. 57. 56.
 52. 61. 69. 46. 29. 23. 20. 27. 26. 25. 20. 27. 23. 27. 30. 25. 21. 29.
 24. 17. 39. 25. 30. 22. 22. 22.]</code></pre>
<p>4 Plot the artificial dataset:</p>
<p>4 画出人造的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.bar(days_range_, data_, color=TFColor[<span class="number">3</span>])</span><br><span class="line">plt.bar(tau_ - <span class="number">1</span>, data_[tau_ - <span class="number">1</span>], color=<span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;user behaviour changed&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Time (days)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;count of text-msgs received&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Artificial dataset&quot;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">80</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_32_0.png" /></p>
<p>我们的虚构数据集看起来不像我们观察到的数据集是正常的：它的概率确实很小。
TFP的引擎旨在找到最大化此概率的良好参数<span
class="math inline">\(\lambda_i, \tau\)</span></p>
<p>生成人工数据集的能力是我们建模的一个有趣的副作用，我们将看到这种能力是贝叶斯推理的一个非常重要的方法。我们在下面生成一些数据集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_artificial_sms_dataset</span>():   </span><br><span class="line">    tau = tf.random_uniform(shape=[<span class="number">1</span>], </span><br><span class="line">                            minval=<span class="number">0</span>, </span><br><span class="line">                            maxval=<span class="number">80</span>,</span><br><span class="line">                            dtype=tf.int32)</span><br><span class="line">    alpha = <span class="number">1.</span>/<span class="number">8.</span></span><br><span class="line">    lambdas  = tfd.Gamma(concentration=<span class="number">1</span>/alpha, rate=<span class="number">0.3</span>).sample(sample_shape=[<span class="number">2</span>]) </span><br><span class="line">    [ lambda_1_, lambda_2_ ] = evaluate( lambdas )</span><br><span class="line">    data = tf.concat([tfd.Poisson(rate=lambda_1_).sample(sample_shape=tau),</span><br><span class="line">                      tfd.Poisson(rate=lambda_2_).sample(sample_shape= (<span class="number">80</span> - tau))], axis=<span class="number">0</span>)</span><br><span class="line">    days_range = tf.<span class="built_in">range</span>(<span class="number">80</span>)</span><br><span class="line">    </span><br><span class="line">    [ </span><br><span class="line">        tau_,</span><br><span class="line">        data_,</span><br><span class="line">        days_range_,</span><br><span class="line">    ] = evaluate([ </span><br><span class="line">        tau,</span><br><span class="line">        data,</span><br><span class="line">        days_range,</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    plt.bar(days_range_, data_, color=TFColor[<span class="number">3</span>])</span><br><span class="line">    plt.bar(tau_ - <span class="number">1</span>, data_[tau_ - <span class="number">1</span>], color=<span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;user behaviour changed&quot;</span>)</span><br><span class="line">    plt.xlim(<span class="number">0</span>, <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    plt.subplot(<span class="number">4</span>, <span class="number">1</span>, i+<span class="number">1</span>)</span><br><span class="line">    plot_artificial_sms_dataset()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_34_0.png" /></p>
<p>稍后我们将看到我们如何使用它来进行预测并测试模型的适当性。</p>
<h3 id="示例贝叶斯a-b测试">示例：贝叶斯A / B测试</h3>
<p>A /
B测试是用于确定两种不同处理之间的有效性差异的统计设计模式。例如，一家制药公司对药物A与药物B的有效性感兴趣。该公司将在其试验的某些部分测试药物A，在另一部分测试药物B（该部分通常是1/2，但我们将放松这个假设）。在进行了足够的试验后，内部统计人员筛选数据以确定哪种药物产生了更好的结果。</p>
<p>同样，前端Web开发人员对他们的网站设计产生更多销售额或其他一些感兴趣的指标感兴趣。他们将一部分访问者路由到站点A，将另一部分路由到站点B，并记录访问是否产生了销售。记录数据（实时），然后进行分析。</p>
<p>通常，实验后分析使用称为假设检验的方法进行，例如<em>平均值测试</em>或<em>比例差异测试</em>。这通常会误解为“Z分数”，甚至更令人困惑的“p值”（请不要问）。如果您已经学过统计学课程，那么您可能已经学过这种技术（尽管不一定<em>学习</em>这种技术）。如果你像我一样，你可能会对他们的推导感到不舒服
- 好的：贝叶斯方法解决这个问题要自然得多。</p>
<h3 id="一个简单的例子">一个简单的例子</h3>
<p>由于这是一本黑客书，我们将继续使用web-dev示例。目前，我们只关注网站A的分析。假设在显示站点A时最终从站点购买的用户有一些真正的<span
class="math inline">\(0 \lt p_A \lt
1\)</span>概率。这是网站A的真正有效性。目前，我们不知道这个数量</p>
<p>假设站点A显示为<span
class="math inline">\(N\)</span>人，并且从站点购买<span
class="math inline">\(n\)</span>人。有人可能会急忙得出结论：<span
class="math inline">\(p_A =
\frac{n}{N}\)</span>。不幸的是，<em>观察频率</em><span
class="math inline">\(\frac{n}{N}\)</span>不一定等于<span
class="math inline">\(p_A\)</span>-
<em>观察频率</em>与事件的<em>真实频率</em>之间存在差异。真实频率可以解释为事件发生的概率。例如，在6面骰子上滚动1的真实频率是<span
class="math inline">\(\frac{1}{6}\)</span>。了解事件的真实频率，例如：</p>
<ul>
<li>购买用户的比例</li>
<li>社会属性的频率</li>
<li>有猫等互联网用户的百分比</li>
</ul>
<p>是我们对大自然提出的常见要求。不幸的是，通常现实中充满了噪音和干扰隐藏了真实频率，我们必须从观察到的数据中推断它。</p>
<p>然后<em>观察到的频率</em>是我们观察到的频率：比如摇动色子100次，你可以观察到20个1.观察到的频率0.2，与真实频率不同，<span
class="math inline">\(\frac{1}{6}\)</span>。我们可以使用贝叶斯统计来使用适当的先验和观测数据来推断真实频率的可能值。</p>
<p>关于我们的A / B示例，我们有兴趣使用我们所知的<span
class="math inline">\(N\)</span>（管理的总试验次数）和<span
class="math inline">\(n\)</span>（转换次数）来估算<span
class="math inline">\(p_A\)</span>，买家的真实频率， 可能。</p>
<p>要设置贝叶斯模型，我们需要为我们的未知量分配先验分布<em>先验</em>，我们认为<span
class="math inline">\(p_A\)</span>可能是什么？对于这个例子，我们对<span
class="math inline">\(p_A\)</span>没有强烈的信念，所以现在，让我们假设<span
class="math inline">\(p_A\)</span>统一超过<span
class="math inline">\([0,1]\)</span>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_sess()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定一个平均分布</span></span><br><span class="line">rv_p = tfd.Uniform(low=<span class="number">0.</span>, high=<span class="number">1.</span>, name=<span class="string">&#x27;p&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果我们有更强烈的置信度，我们可以在上面的内容中表达它们。</p>
<p>对于此示例，请考虑<span class="math inline">\(p_A =
0.05\)</span>，<span class="math inline">\(N =
1500\)</span>用户访问了站点A，我们将模拟用户是否进行了购买。为了从<span
class="math inline">\(N\)</span>试验中模拟这个，我们将使用<em>伯努利</em>分布：如果<span
class="math inline">\(X \ \sim \text{Ber}（p）\)</span>，则<span
class="math inline">\(X\)</span>为1，概率为<span
class="math inline">\(p\)</span>，0为概率<span class="math inline">\(1 -
p\)</span>。当然，在实践中我们不知道<span
class="math inline">\(p_A\)</span>，但我们将在此处使用它来模拟数据。我们可以假设我们可以使用以下生成模型：</p>
<p><span class="math display">\[
\begin{aligned}
p &amp;\sim \text{Uniform}[\text{low}=0,\text{high}=1) \\
X\ &amp;\sim \text{Bernoulli}(\text{prob}=p) \\
\text{for }  i &amp;= 1\ldots N:\text{ Users} \\
X_i\ &amp;\sim \text{Bernoulli}(p_i)
\end{aligned}
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_sess()</span><br><span class="line"></span><br><span class="line"><span class="comment">#set constants</span></span><br><span class="line">prob_true = <span class="number">0.05</span>  <span class="comment"># 假设P_a是0.05</span></span><br><span class="line">N = <span class="number">1500</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本N来自 Ber(0.05).</span></span><br><span class="line"><span class="comment"># 每个变量有0.05的概率为1</span></span><br><span class="line"><span class="comment"># 这是数据生成的步骤</span></span><br><span class="line"></span><br><span class="line">occurrences = tfd.Bernoulli(probs=prob_true).sample(sample_shape=N, seed=<span class="number">10</span>) <span class="comment"># 生成1500的样本</span></span><br><span class="line">occurrences_sum = tf.reduce_sum(occurrences) <span class="comment"># 求和</span></span><br><span class="line">occurrences_mean = tf.reduce_mean(tf.cast(occurrences,tf.float32)) <span class="comment"># 求平均</span></span><br><span class="line"></span><br><span class="line">[ </span><br><span class="line">    occurrences_,</span><br><span class="line">    occurrences_sum_,</span><br><span class="line">    occurrences_mean_,</span><br><span class="line">] = evaluate([ </span><br><span class="line">    occurrences, </span><br><span class="line">    occurrences_sum,</span><br><span class="line">    occurrences_mean,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Array of &#123;&#125; Occurences:&quot;</span>.<span class="built_in">format</span>(N), occurrences_) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;(Remember: Python treats True == 1, and False == 0)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sum of (True == 1) Occurences:&quot;</span>, occurrences_sum_)</span><br></pre></td></tr></table></figure>
<pre><code>Array of 1500 Occurences: [0 0 0 ... 0 1 0]
(Remember: Python treats True == 1, and False == 0)
Sum of (True == 1) Occurences: 76</code></pre>
<p>观测频率如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Occurrences.mean is equal to n/N.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A组中观察到的频率是多少? %.4f&quot;</span> % occurrences_mean_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;他是否等于正式的频率? %s&quot;</span> % (occurrences_mean_ == prob_true))</span><br></pre></td></tr></table></figure>
<pre><code>A组中观察到的频率是多少? 0.0507
他是否等于正式的频率? False</code></pre>
<p>我们可以将我们的伯努利分布和我们观察到的事件组合成基于二者的对数概率函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">joint_log_prob</span>(<span class="params">occurrences, prob_A</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    联合对数概率优化函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      事件: 一个二进制数组 (0 &amp; 1), 表现观测频率</span></span><br><span class="line"><span class="string">      prob_A: 标量估计出现1的概率</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      来自所有先验和条件分布的联合对数概率之和</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>  </span><br><span class="line">    rv_prob_A = tfd.Uniform(low=<span class="number">0.</span>, high=<span class="number">1.</span>) <span class="comment"># 这里直接把概率P_A的分布设置为均匀分布</span></span><br><span class="line">    rv_occurrences = tfd.Bernoulli(probs=prob_A) <span class="comment"># 概率为P_A的二项分布</span></span><br><span class="line">    <span class="keyword">return</span> (rv_prob_A.log_prob(prob_A)+ tf.reduce_sum(rv_occurrences.log_prob(occurrences)))</span><br></pre></td></tr></table></figure>
<p>概率推断的目标是找到可以解释您观察到的数据的模型参数。
TFP通过使用<code>joint_log_prob</code>函数评估模型参数来执行概率推断。
<code>joint_log_prob</code>的参数是数据和模型参数 -
用于在<code>joint_log_prob</code>函数本身中定义的模型。该函数返回参数化模型的联合概率的对数，该模型按照输入参数生成观察数据。</p>
<p>所有的 <code>joint_log_prob</code> 函数都有共同的结构:</p>
<ol type="1">
<li><p>该函数需要一组<strong>输入</strong>来评估。每个输入都是观察值或模型参数。</p></li>
<li><p><code>joint_log_prob</code>函数使用概率分布来定义用于评估输入的<strong>模型</strong>。这些分布测量输入值的可能性。
（按照惯例，测量变量<code>foo</code>的可能性的分布将被命名为<code>rv_foo</code>以注意它是一个随机变量。）我们在<code>joint_log_prob</code>函数中使用两种类型的分布：</p>
<ol type="a">
<li><p><strong>先前的分布</strong>测量输入值的可能性。先前的分配决不依赖于输入值。每个先前分布都测量单个输入值的可能性。每个未知变量
- 一个未被直接观察到的变量 -
需要相应的先验变量。关于哪些值可能合理的信念决定了先前的分布。选择先验可能很棘手，因此我们将在第6章深入介绍。</p></li>
<li><p><strong>条件分布</strong>测量给定其他输入值的输入值的可能性。通常，条件分布返回给定模型中参数的当前猜测的观察数据的可能性，p（observed_data
| model_parameters）。</p></li>
</ol></li>
<li><p>最后，我们计算并返回输入的<strong>联合对数概率</strong>。联合对数概率是来自所有先验分布和条件分布的对数概率的总和。
（由于数值稳定性的原因，我们采用对数概率之和而不是直接乘以概率：计算机中的浮点数不能表示计算联合对数概率所需的非常小的值，除非它们在对数空间中。）概率之和实际上是一个非标准化的密度;虽然所有可能输入的概率总和可能不等于1，但概率之和与真实概率密度成正比。这种比例分布足以估计可能输入的分布。</p></li>
</ol>
<p>让我们将这些术语映射到上面的代码中。在这个例子中，输入值是<code>occurrence</code>的观察值和<code>prob_A</code>的未知值。
<code>joint_log_prob</code>获取<code>prob_A</code>的当前猜测并回答，如果<code>prob_A</code>是<code>occurrence</code>的概率，数据的可能性有多大。答案取决于两个分布：</p>
<ol type="1">
<li><p>先前的分布<code>rv_prob_A</code>表示<code>prob_A</code>的当前值本身的可能性。</p></li>
<li><p>如果<code>prob_A</code>是伯努利分布的概率，则条件分布<code>rv_occurrences</code>表示“发生”的可能性。</p></li>
</ol>
<p>这些概率的对数之和是 联合对数概率。</p>
<p>joint_log_prob与tfp.mcmc模块一起使用时特别有用。马尔可夫链蒙特卡罗（MCMC）算法通过对未知输入值进行有根据的猜测并计算这组参数的可能性来进行。
（我们将在第3章中讨论它是如何进行这些猜测的。）通过多次重复此过程，MCMC构建了可能参数的分布。构建此分布是概率推理的目标。</p>
<p>Then we run our inference algorithm:</p>
<p>让我运行推理算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number_of_steps = <span class="number">48000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:2000, max:50000, step:100&#125; #@markdown (Default is 18000).</span></span><br><span class="line">burnin = <span class="number">25000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:0, max:30000, step:100&#125; #@markdown (Default is 1000).</span></span><br><span class="line">leapfrog_steps=<span class="number">2</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:1, max:9, step:1&#125; #@markdown (Default is 6).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置链的开始状态</span></span><br><span class="line">initial_chain_state = [tf.reduce_mean(tf.to_float(occurrences)) * tf.ones([], dtype=tf.float32, name=<span class="string">&quot;init_prob_A&quot;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于HMC在无约束空间上运行，我们需要对样本进行变换，使它们存在于真实空间中。</span></span><br><span class="line">unconstraining_bijectors = [</span><br><span class="line">    tfp.bijectors.Identity()   <span class="comment"># Maps R to R.  </span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在我们的joint_log_prob上定义一个闭包</span></span><br><span class="line"><span class="comment"># 闭包使得HMC不会尝试改变“出现次数”，而是确定可能产生我们观察到的“出现次数”的其他参数的分布。</span></span><br><span class="line">unnormalized_posterior_log_prob = <span class="keyword">lambda</span> *args: joint_log_prob(occurrences, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化step_size。 （它将自动调整。）</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;foo&quot;</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    step_size = tf.get_variable(name=<span class="string">&#x27;step_size&#x27;</span>,initializer=tf.constant(<span class="number">0.5</span>, dtype=tf.float32),trainable=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 定义 HMC</span></span><br><span class="line">    hmc = tfp.mcmc.TransformedTransitionKernel(</span><br><span class="line">    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(</span><br><span class="line">        target_log_prob_fn=unnormalized_posterior_log_prob,</span><br><span class="line">        num_leapfrog_steps=leapfrog_steps,</span><br><span class="line">        step_size=step_size,</span><br><span class="line">        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=<span class="built_in">int</span>(burnin * <span class="number">0.8</span>)),</span><br><span class="line">        state_gradients_are_stopped=<span class="literal">True</span>),</span><br><span class="line">        bijector=unconstraining_bijectors)</span><br></pre></td></tr></table></figure>
<pre><code>W0726 19:11:30.352059 140237485999936 deprecation.py:323] From &lt;ipython-input-19-e4e347c50353&gt;:6: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0726 19:11:30.359435 140237485999936 deprecation.py:323] From &lt;ipython-input-19-e4e347c50353&gt;:26: make_simple_step_size_update_policy (from tensorflow_probability.python.mcmc.hmc) is deprecated and will be removed after 2019-05-22.
Instructions for updating:
Use tfp.mcmc.SimpleStepSizeAdaptation instead.
W0726 19:11:30.363615 140237485999936 deprecation.py:506] From &lt;ipython-input-19-e4e347c50353&gt;:27: calling HamiltonianMonteCarlo.__init__ (from tensorflow_probability.python.mcmc.hmc) with step_size_update_fn is deprecated and will be removed after 2019-05-22.
Instructions for updating:
The `step_size_update_fn` argument is deprecated. Use `tfp.mcmc.SimpleStepSizeAdaptation` instead.</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从链里面采样</span></span><br><span class="line">[posterior_prob_A], kernel_results = tfp.mcmc.sample_chain(</span><br><span class="line">    num_results=number_of_steps,</span><br><span class="line">    num_burnin_steps=burnin,</span><br><span class="line">    current_state=initial_chain_state,</span><br><span class="line">    kernel=hmc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line">init_g = tf.global_variables_initializer()</span><br><span class="line">init_l = tf.local_variables_initializer()</span><br></pre></td></tr></table></figure>
<pre><code>W0726 19:11:30.403772 140237485999936 deprecation.py:323] From /home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/uniform.py:182: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where</code></pre>
<h4 id="执行tf图以从后验采样">执行TF图以从后验采样</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluate(init_g)</span><br><span class="line">evaluate(init_l)</span><br><span class="line">[posterior_prob_A_,kernel_results_,] = evaluate([posterior_prob_A,kernel_results,])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;acceptance rate: <span class="subst">&#123;kernel_results_.inner_results.is_accepted.mean()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">burned_prob_A_trace_ = posterior_prob_A_[burnin:]</span><br></pre></td></tr></table></figure>
<pre><code>acceptance rate: 0.7248958333333333</code></pre>
<p>我们绘制下面未知<span
class="math inline">\(p_A\)</span>的后验分布：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">4</span>))</span><br><span class="line">plt.title(<span class="string">&quot;Posterior distribution of$p_A$, the true effectiveness of site A&quot;</span>)</span><br><span class="line">plt.vlines(prob_true, <span class="number">0</span>, <span class="number">90</span>, linestyle=<span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;true$p_A$(unknown)&quot;</span>)</span><br><span class="line">plt.hist(burned_prob_A_trace_, bins=<span class="number">25</span>, histtype=<span class="string">&quot;stepfilled&quot;</span>, normed=<span class="literal">True</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_52_0.png" /></p>
<p>我们的后验分布使得大部分权重接近<span
class="math inline">\(p_A\)</span>的真实值，但尾部也有一些权重。根据我们的观察，这可以衡量我们应该多么不确定。尝试改变观察数<code>N</code>，并观察后验分布如何变化</p>
<h3 id="a-和-b-一起">A 和 B 一起</h3>
<p>可以对站点B的响应数据进行类似的分析，以确定类似的<span
class="math inline">\(p_B\)</span>。但我们真正感兴趣的是<span
class="math inline">\(p_A\)</span>和<span
class="math inline">\(p_B\)</span>之间的<em>差异</em>。我们一下子推断<span
class="math inline">\(p_A\)</span>，<span
class="math inline">\(p_B\)</span>，<em>和</em><span
class="math inline">\(\text{delta} = p_A -
p_B\)</span>。我们可以使用TFP的确定性变量来做到这一点。
（我们假设这个练习<span class="math inline">\(p_B =
0.04\)</span>，所以<span class="math inline">\(\text{delta} =
0.01\)</span>，<span class="math inline">\(N_B =
750\)</span>（显著低于<span
class="math inline">\(N_A\)</span>）我们将像我们一样模拟站点B的数据网站A的数据）。我们的模型现在如下所示：</p>
<p><span class="math display">\[\begin{align*}
p_A &amp;\sim \text{Uniform}[\text{low}=0,\text{high}=1) \\
p_B &amp;\sim \text{Uniform}[\text{low}=0,\text{high}=1) \\
X\ &amp;\sim \text{Bernoulli}(\text{prob}=p) \\
\text{for }  i &amp;= 1\ldots N: \\
X_i\ &amp;\sim \text{Bernoulli}(p_i)
\end{align*}\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_sess()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设两个概率</span></span><br><span class="line">true_prob_A_ = <span class="number">0.05</span></span><br><span class="line">true_prob_B_ = <span class="number">0.04</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意不相等的样本大小 - 贝叶斯分析没有问题。</span></span><br><span class="line">N_A_ = <span class="number">1500</span></span><br><span class="line">N_B_ = <span class="number">750</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成观测值</span></span><br><span class="line">observations_A = tfd.Bernoulli(name=<span class="string">&quot;obs_A&quot;</span>, </span><br><span class="line">                          probs=true_prob_A_).sample(sample_shape=N_A_, seed=<span class="number">6.45</span>)</span><br><span class="line">observations_B = tfd.Bernoulli(name=<span class="string">&quot;obs_B&quot;</span>, </span><br><span class="line">                          probs=true_prob_B_).sample(sample_shape=N_B_, seed=<span class="number">6.45</span>)</span><br><span class="line">[   observations_A_,</span><br><span class="line">    observations_B_,</span><br><span class="line">] = evaluate([ </span><br><span class="line">    observations_A, </span><br><span class="line">    observations_B, </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;站点A观测值: &quot;</span>, observations_A_[:<span class="number">30</span>], <span class="string">&quot;...&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Prob_A观测值: &quot;</span>, np.mean(observations_A_), <span class="string">&quot;...&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;站点B观测值: &quot;</span>, observations_B_[:<span class="number">30</span>], <span class="string">&quot;...&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Prob_B观测值: &quot;</span>, np.mean(observations_B_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;发现观测值的均值收敛于概率&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>站点A观测值:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ...
Prob_A观测值:  0.050666666666666665 ...
站点B观测值:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ...
Prob_B观测值:  0.04
发现观测值的均值收敛于概率</code></pre>
<p>下面我们推理新的模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">delta</span>(<span class="params">prob_A, prob_B</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义确定性delta函数。这是我们未知的兴趣。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      prob_A: 标量1出现在观测集A中的估计概率</span></span><br><span class="line"><span class="string">      prob_B: 标量1出现在观测集B中的估计概率</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      prob_A 和 prob_B 之间的差值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> prob_A - prob_B</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">double_joint_log_prob</span>(<span class="params">observations_A, observations_B, </span></span><br><span class="line"><span class="params">                   prob_A, prob_B</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义新的联合对数概率优化函数,我个人感觉这里有点像最大似然</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      observations_A: 表示站点A的观察集的二进制值数组</span></span><br><span class="line"><span class="string">      observations_B: 表示站点B的观察集的二进制值数组</span></span><br><span class="line"><span class="string">      prob_A: 标量1出现在观测集A中的估计概率</span></span><br><span class="line"><span class="string">      prob_B: 标量1出现在观测集B中的估计概率</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      联合概率优化函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tfd = tfp.distributions</span><br><span class="line">  </span><br><span class="line">    rv_prob_A = tfd.Uniform(low=<span class="number">0.</span>, high=<span class="number">1.</span>) <span class="comment"># 假设P_A的分布</span></span><br><span class="line">    rv_prob_B = tfd.Uniform(low=<span class="number">0.</span>, high=<span class="number">1.</span>) <span class="comment"># 假设P_B的分布</span></span><br><span class="line">  </span><br><span class="line">    rv_obs_A = tfd.Bernoulli(probs=prob_A) <span class="comment"># 生成数据A</span></span><br><span class="line">    rv_obs_B = tfd.Bernoulli(probs=prob_B) <span class="comment"># 生成数据B</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> (rv_prob_A.log_prob(prob_A)+ <span class="comment"># P_A 分布的对数概率</span></span><br><span class="line">            rv_prob_B.log_prob(prob_B)+ <span class="comment"># P_B 分布的对数概率</span></span><br><span class="line">            tf.reduce_sum(rv_obs_A.log_prob(observations_A))+  <span class="comment"># 所有观测值的对数概率</span></span><br><span class="line">            tf.reduce_sum(rv_obs_B.log_prob(observations_B)))</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number_of_steps = <span class="number">37200</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:2000, max:50000, step:100&#125;</span></span><br><span class="line"><span class="comment">#@markdown (Default is 18000).</span></span><br><span class="line">burnin = <span class="number">1000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:0, max:30000, step:100&#125;</span></span><br><span class="line"><span class="comment">#@markdown (Default is 1000).</span></span><br><span class="line">leapfrog_steps=<span class="number">3</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:1, max:9, step:1&#125;</span></span><br><span class="line"><span class="comment">#@markdown (Default is 6).</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置初始状态</span></span><br><span class="line">initial_chain_state = [    </span><br><span class="line">    tf.reduce_mean(tf.cast(observations_A,tf.float32)) * tf.ones([], dtype=tf.float32, name=<span class="string">&quot;init_prob_A&quot;</span>),</span><br><span class="line">    tf.reduce_mean(tf.cast(observations_B,tf.float32)) * tf.ones([], dtype=tf.float32, name=<span class="string">&quot;init_prob_B&quot;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于HMC在无约束空间上运行，我们需要对样本进行变换，使它们存在于真实空间中。</span></span><br><span class="line">unconstraining_bijectors = [</span><br><span class="line">    tfp.bijectors.Identity(),   <span class="comment"># Maps R to R.</span></span><br><span class="line">    tfp.bijectors.Identity()    <span class="comment"># Maps R to R.</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将joint_log_prob.闭包</span></span><br><span class="line">unnormalized_posterior_log_prob = <span class="keyword">lambda</span> *args: double_joint_log_prob(observations_A, observations_B, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化step</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):</span><br><span class="line">    step_size = tf.get_variable(</span><br><span class="line">        name=<span class="string">&#x27;step_size&#x27;</span>,</span><br><span class="line">        initializer=tf.constant(<span class="number">0.5</span>, dtype=tf.float32),</span><br><span class="line">        trainable=<span class="literal">False</span>,</span><br><span class="line">        use_resource=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 HMC</span></span><br><span class="line">    hmc=tfp.mcmc.TransformedTransitionKernel(</span><br><span class="line">    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(</span><br><span class="line">        target_log_prob_fn=unnormalized_posterior_log_prob,</span><br><span class="line">        num_leapfrog_steps=<span class="number">3</span>,</span><br><span class="line">        step_size=step_size,</span><br><span class="line">        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=<span class="built_in">int</span>(burnin * <span class="number">0.8</span>)),</span><br><span class="line">        state_gradients_are_stopped=<span class="literal">True</span>),</span><br><span class="line">    bijector=unconstraining_bijectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from the chain.</span></span><br><span class="line">[posterior_prob_A,posterior_prob_B], kernel_results = tfp.mcmc.sample_chain(</span><br><span class="line">    num_results=number_of_steps,</span><br><span class="line">    num_burnin_steps=burnin,</span><br><span class="line">    current_state=initial_chain_state,</span><br><span class="line">    kernel=hmc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize any created variables.</span></span><br><span class="line">init_g = tf.global_variables_initializer()</span><br><span class="line">init_l = tf.local_variables_initializer()</span><br></pre></td></tr></table></figure>
<h4 id="执行tf图以从后验采样-1">执行TF图以从后验采样</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluate(init_g)</span><br><span class="line">evaluate(init_l)</span><br><span class="line">[</span><br><span class="line">    posterior_prob_A_,</span><br><span class="line">    posterior_prob_B_,</span><br><span class="line">    kernel_results_</span><br><span class="line">] = evaluate([</span><br><span class="line">    posterior_prob_A,</span><br><span class="line">    posterior_prob_B,</span><br><span class="line">    kernel_results</span><br><span class="line">])</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;接受率: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    kernel_results_.inner_results.is_accepted.mean()))</span><br><span class="line"></span><br><span class="line">burned_prob_A_trace_ = posterior_prob_A_[burnin:]</span><br><span class="line">burned_prob_B_trace_ = posterior_prob_B_[burnin:]</span><br><span class="line">burned_delta_trace_ = (posterior_prob_A_ - posterior_prob_B_)[burnin:]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>接受率: 0.6146505376344086</code></pre>
<p>下面我们绘制三个未知数的后验分布：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">12.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#histogram of posteriors</span></span><br><span class="line"></span><br><span class="line">ax = plt.subplot(<span class="number">311</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">.1</span>)</span><br><span class="line">plt.hist(burned_prob_A_trace_, histtype=<span class="string">&#x27;stepfilled&#x27;</span>, bins=<span class="number">25</span>, alpha=<span class="number">0.85</span>,</span><br><span class="line">         label=<span class="string">&quot;posterior of$p_A$&quot;</span>, color=TFColor[<span class="number">0</span>], normed=<span class="literal">True</span>)</span><br><span class="line">plt.vlines(true_prob_A_, <span class="number">0</span>, <span class="number">80</span>, linestyle=<span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;true$p_A$(unknown)&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;upper right&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Posterior distributions of$p_A$,$p_B$, and delta unknowns&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax = plt.subplot(<span class="number">312</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">.1</span>)</span><br><span class="line">plt.hist(burned_prob_B_trace_, histtype=<span class="string">&#x27;stepfilled&#x27;</span>, bins=<span class="number">25</span>, alpha=<span class="number">0.85</span>,</span><br><span class="line">         label=<span class="string">&quot;posterior of$p_B$&quot;</span>, color=TFColor[<span class="number">2</span>], normed=<span class="literal">True</span>)</span><br><span class="line">plt.vlines(true_prob_B_, <span class="number">0</span>, <span class="number">80</span>, linestyle=<span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;true$p_B$(unknown)&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;upper right&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax = plt.subplot(<span class="number">313</span>)</span><br><span class="line">plt.hist(burned_delta_trace_, histtype=<span class="string">&#x27;stepfilled&#x27;</span>, bins=<span class="number">30</span>, alpha=<span class="number">0.85</span>,</span><br><span class="line">         label=<span class="string">&quot;posterior of delta&quot;</span>, color=TFColor[<span class="number">6</span>], normed=<span class="literal">True</span>)</span><br><span class="line">plt.vlines(true_prob_A_ - true_prob_B_, <span class="number">0</span>, <span class="number">60</span>, linestyle=<span class="string">&quot;--&quot;</span>,</span><br><span class="line">           label=<span class="string">&quot;true delta (unknown)&quot;</span>)</span><br><span class="line">plt.vlines(<span class="number">0</span>, <span class="number">0</span>, <span class="number">60</span>, color=<span class="string">&quot;black&quot;</span>, alpha=<span class="number">0.2</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;upper right&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_61_0.png" /></p>
<p>请注意由于<code>N_B &lt; N_A</code>.
我们从网站B获得的数据较少，我们后来的<span
class="math inline">\(p_B\)</span>分布比较宽，这意味着我们相比于<span
class="math inline">\(p_A\)</span>不太确定<span
class="math inline">\(p_B\)</span>的真实值.</p>
<p>关于<span
class="math inline">\(\text{delta}\)</span>的后验分布，我们可以看到大部分分布都高于<span
class="math inline">\(\text{delta} =
0\)</span>，这意味着网站A的响应可能比网站B的响应更好。这种推断不正确的概率很容易计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算小于0的样本数，即曲线下面积</span></span><br><span class="line"><span class="comment"># 在0之前，表示站点A比站点B更差的概率。</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; site A 比 site B 差的概率: %.3f&quot;</span> % \</span><br><span class="line">    np.mean(burned_delta_trace_ &lt; <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; site A 比 site B 好的概率: %.3f&quot;</span> % \</span><br><span class="line">    np.mean(burned_delta_trace_ &gt; <span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<pre><code> site A 比 site B 差的概率: 0.298
 site A 比 site B 好的概率: 0.702</code></pre>
<p>如果这个概率对于舒适的决策来说太高了，我们可以在站点B上进行更多的试验（因为站点B开始时的样本较少，站点B的每个附加数据点比每个附加数据点比网站A提供更多的推理<code>功率</code>）。</p>
<p>尝试使用参数<code>true_prob_A</code>，<code>true_prob_B</code>，<code>N_A</code>和<code>N_B</code>进行测试，看看<span
class="math inline">\(\text{delta}\)</span>的后验是什么样的。请注意，在所有这些中，从未提及站点A和站点B之间的样本大小差异：它自然适合贝叶斯分析。</p>
<p>我希望读者觉得这种A /
B测试方式比假设测试更自然，假设测试可能比帮助从业者更困惑。在本书的后面，我们将看到这个模型的两个扩展：第一个帮助动态调整不良站点，第二个将通过将分析减少到单个方程来提高计算的速度。</p>
<h2 id="一种人为欺骗的算法">一种人为欺骗的算法</h2>
<p>社交数据还有一层额外的兴趣，因为人们并不总是诚实地回应，这进一步增加了推理的复杂性。例如，简单地询问个人“你有没有在考试中作弊？”肯定会包含一些不诚实的行为。你可以肯定的是，真实的比率低于你观察到的比率（假设个人谎言<em>只是</em>关于<em>不作弊</em>;我无法想象一个人会承认<strong>是</strong>作弊，而事实上他们没有作弊）。</p>
<p>为了提出一个优雅的解决方案来规避这个不诚实的问题，并演示贝叶斯模型，我们首先需要介绍二项分布。</p>
<h2 id="二项分布">二项分布</h2>
<p><code>二项分布</code>是最受欢迎的分布之一，主要是因为它的简单性和实用性。与本书迄今为止遇到的其他分布不同，二项分布有2个参数：<span
class="math inline">\(N\)</span>，表示<span
class="math inline">\(N\)</span>试验的正整数或潜在事件的实例数，以及<span
class="math inline">\(p\)</span>，事件的概率发生在一次试验中。像泊松分布一样，它是一个离散分布，但与泊松分布不同，它只能权衡从<span
class="math inline">\(0\)</span>到<span
class="math inline">\(N\)</span>的整数。质量分布如下：</p>
<p><span class="math display">\[P( X = k ) =  { {N}\choose{k}
}  p^k(1-p)^{N-k}\]</span></p>
<p>如果<span class="math inline">\(X\)</span>是一个带有参数<span
class="math inline">\(p\)</span>和<span
class="math inline">\(N\)</span>的二项式随机变量，表示为<span
class="math inline">\(X \sim \text{Bin}（N，p）\)</span>，则<span
class="math inline">\(X\)</span>是<span
class="math inline">\(中发生的事件数N\)</span>次试验（显然是<span
class="math inline">\(0 \le X \le N\)</span>）。较大的<span
class="math inline">\(p\)</span>（仍然保持在0和1之间），可能发生的事件越多。二项式的期望值等于<span
class="math inline">\(Np\)</span>。下面我们绘制不同参数的质量概率分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">N = <span class="number">10.</span></span><br><span class="line">k_values = tf.<span class="built_in">range</span>(start=<span class="number">0</span>, limit=(N + <span class="number">1</span>), dtype=tf.float32)</span><br><span class="line">rv_probs_1 = tfd.Binomial(total_count=N, probs=<span class="number">.4</span>).prob(k_values) <span class="comment"># 计算样本对应概率</span></span><br><span class="line">rv_probs_2 = tfd.Binomial(total_count=N, probs=<span class="number">.9</span>).prob(k_values) <span class="comment"># 计算样本对应概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行图</span></span><br><span class="line">[   k_values_,</span><br><span class="line">    rv_probs_1_,</span><br><span class="line">    rv_probs_2_,</span><br><span class="line">] = evaluate([</span><br><span class="line">    k_values,</span><br><span class="line">    rv_probs_1,</span><br><span class="line">    rv_probs_2,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12.5</span>, <span class="number">4</span>))</span><br><span class="line">colors = [TFColor[<span class="number">3</span>], TFColor[<span class="number">0</span>]] </span><br><span class="line"></span><br><span class="line">plt.bar(k_values_ - <span class="number">0.5</span>, rv_probs_1_, color=colors[<span class="number">0</span>],</span><br><span class="line">        edgecolor=colors[<span class="number">0</span>],</span><br><span class="line">        alpha=<span class="number">0.6</span>,</span><br><span class="line">        label=<span class="string">&quot;$N$: %d,$p$: %.1f&quot;</span> % (<span class="number">10.</span>, <span class="number">.4</span>),</span><br><span class="line">        linewidth=<span class="number">3</span>)</span><br><span class="line">plt.bar(k_values_ - <span class="number">0.5</span>, rv_probs_2_, color=colors[<span class="number">1</span>],</span><br><span class="line">        edgecolor=colors[<span class="number">1</span>],</span><br><span class="line">        alpha=<span class="number">0.6</span>,</span><br><span class="line">        label=<span class="string">&quot;$N$: %d,$p$: %.1f&quot;</span> % (<span class="number">10.</span>, <span class="number">.9</span>),</span><br><span class="line">        linewidth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.legend(loc=<span class="string">&quot;upper left&quot;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">10.5</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;$k$&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;$P(X = k)$&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Probability mass distributions of binomial random variables&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_67_0.png" /></p>
<p><span class="math inline">\(N =
1\)</span>的特殊情况对应于伯努利分布。伯努利和二项式随机变量之间存在另一种联系。如果我们有<span
class="math inline">\(X_1，X_2，...，X_N\)</span>Bernoulli随机变量具有相同的<span
class="math inline">\(p\)</span>，那么<span class="math inline">\(Z =
X_1 + X_2 + ... + X_N \sim \text{Binomial}（N，p）\)</span>。</p>
<p>伯努利随机变量的期望值是<span
class="math inline">\(p\)</span>。通过注意更一般的二项式随机变量具有预期值<span
class="math inline">\(Np\)</span>并设置<span class="math inline">\(N =
1\)</span>可以看出这一点</p>
<h2 id="例子-在学生之间作弊">例子: 在学生之间作弊</h2>
<p>我们将使用二项分布来确定学生在考试期间作弊的频率。如果我们让<span
class="math inline">\(N\)</span>成为参加考试的学生总数，并假设每个学生在考试后接受面试（回答无后果），我们将收到整数<span
class="math inline">\(X\)</span>“是的，我做了作弊”的答案。然后我们找到<span
class="math inline">\(p\)</span>的后验分布，给定<span
class="math inline">\(N\)</span>，一些在<span
class="math inline">\(p\)</span>之前指定，观察数据<span
class="math inline">\(X\)</span>。</p>
<p>这是一个完全荒谬的模型。没有学生，即使有免费通过惩罚，也会承认作弊。我们需要的是一个更好的<em>算法</em>来询问学生是否有欺骗行为。理想情况下，该算法应鼓励个人在保护隐私的同时保持诚实。以下提出的算法是我非常钦佩的解决方案，因为它的独创性和有效性：</p>
<blockquote>
<p>在每个学生的面试过程中，学生翻转一个隐藏在面试官面前的硬币。如果硬币正面，学生同意诚实地回答。否则，如果硬币反面，学生（秘密地）再次翻转硬币，如果硬币翻转落地为正面，则回答“是的，我做了作弊”，如果硬币翻转落地为反面，则回答“不，我没有作弊”。这样，面试官不知道“是”是认罪的结果，还是第二次掷硬币的正面。因此保护了隐私，研究人员得到了诚实的答案。</p>
</blockquote>
<p>我称之为隐私算法。人们当然可以争辩说，采访者仍在接收错误的数据，因为有些<em>Yes</em>不是供词而是随机性，但另一种观点是研究人员丢弃其原始数据集的大约一半，因为一半的回复将是噪声。但他们已经获得了可以建模的系统数据生成过程。此外，他们没有必要（或许有点天真）加入欺骗性答案的可能性。我们可以使用TFP来挖掘这个嘈杂的模型，并找到一个关于作弊真实频率的后验分布。</p>
<p>假设有100名学生正在接受作弊调查，我们希望找到<span
class="math inline">\(p\)</span>，作弊者的比例。我们可以通过几种方式在TFP中对此进行建模。我将演示最明确的方式，稍后会显示简化版本。两个版本都得出相同的推论。在我们的数据生成模型中，我们从之前的样本中抽取了<span
class="math inline">\(p\)</span>，这是作弊者的真实比例。由于我们对<span
class="math inline">\(p\)</span>一无所知，我们将先分配一个<span
class="math inline">\(\text{Uniform}（0,1）\)</span>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_sess()</span><br><span class="line"></span><br><span class="line">N = <span class="number">100</span></span><br><span class="line">rv_p = tfd.Uniform(name=<span class="string">&quot;freq_cheating&quot;</span>, low=<span class="number">0.</span>, high=<span class="number">1.</span>)</span><br></pre></td></tr></table></figure>
<p>再次，考虑到我们的数据生成模型，我们将伯努利随机变量分配给100名学生：1表示他们作弊，0表示他们没有。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">N = <span class="number">100</span></span><br><span class="line">reset_sess()</span><br><span class="line">rv_p = tfd.Uniform(name=<span class="string">&quot;freq_cheating&quot;</span>, low=<span class="number">0.</span>, high=<span class="number">1.</span>)</span><br><span class="line">true_answers = tfd.Bernoulli(name=<span class="string">&quot;truths&quot;</span>, probs=rv_p.sample()).sample(sample_shape=N, seed=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 执行图</span></span><br><span class="line">[true_answers_,] = evaluate([true_answers,])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(true_answers_)</span><br><span class="line"><span class="built_in">print</span>(true_answers_.<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure>
<pre><code>[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]
98</code></pre>
<p>如果我们执行算法，下一步发生的是每个学生做的第一次硬币翻转。这可以通过采样100个伯努利随机变量再次建模，其中𝑝=
1/2 表示1为头部，0表示尾部。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">N = <span class="number">100</span></span><br><span class="line">first_coin_flips = tfd.Bernoulli(name=<span class="string">&quot;first_flips&quot;</span>, probs=<span class="number">0.5</span>).sample(sample_shape=N, seed=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># Execute graph</span></span><br><span class="line">[first_coin_flips_,] = evaluate([first_coin_flips,])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(first_coin_flips_)</span><br></pre></td></tr></table></figure>
<pre><code>[1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1
 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0
 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1]</code></pre>
<p>虽然<em>不是每个人</em>第二次翻转，但我们仍然可以模拟第二次翻转的可能实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">N = <span class="number">100</span></span><br><span class="line">second_coin_flips = tfd.Bernoulli(name=<span class="string">&quot;second_flips&quot;</span>, probs=<span class="number">0.5</span>).sample(sample_shape=N, seed=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">[second_coin_flips_,] = evaluate([second_coin_flips,])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(second_coin_flips_)</span><br></pre></td></tr></table></figure>
<pre><code>[1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1
 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0
 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1]</code></pre>
<p>使用这些变量，我们可以返回观察到的“是”响应比例的概率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">observed_proportion_calc</span>(<span class="params">t_a = true_answers, </span></span><br><span class="line"><span class="params">                             fc = first_coin_flips,</span></span><br><span class="line"><span class="params">                             sc = second_coin_flips</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    非标准化的log后验分布函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      t_a: 表示真实答案的二进制变量数组</span></span><br><span class="line"><span class="string">      fc:  表示模拟的第一次翻转的二进制变量数组</span></span><br><span class="line"><span class="string">      sc:  表示模拟的第二次翻转的二进制变量数组</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      观察到硬币翻转的比例</span></span><br><span class="line"><span class="string">    Closure over: N</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    observed = fc * t_a + (<span class="number">1</span> - fc) * sc</span><br><span class="line">    observed_proportion = tf.cast(tf.reduce_sum(observed),tf.float32) / tf.cast(N,tf.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tf.cast(observed_proportion,tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>线fc * t_a +（1-fc）* sc包含隐私算法的核心。当且仅当 1.
第一次投掷是头并且学生被欺骗或 2.
第一次投掷是尾巴，第二次投掷是正面.并且是0否则时，该数组中的元素是1。</p>
<p>最后，最后一行将此向量相加并除以浮点数（N），产生一个比例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">observed_proportion_val = observed_proportion_calc(t_a=true_answers_,fc=first_coin_flips_,sc=second_coin_flips_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Execute graph</span></span><br><span class="line">[observed_proportion_val_,] = evaluate([observed_proportion_val,])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(observed_proportion_val_)</span><br></pre></td></tr></table></figure>
<pre><code>0.48</code></pre>
<p>接下来我们需要一个数据集。在进行了硬币翻转访谈后，研究人员收到了35条“是”回复。从相对的角度来看，如果确实没有作弊者，我们应该期望平均看到所有回答中的1/4是“是”（第一次投入硬币土地的一半机会，以及另外一次获得第二次硬币的机会）硬币土地负责人），在一个无欺诈的世界中大约有25个回应。另一方面，如果所有学生都作弊了，我们应该会看到大约3/4的答案都是“是”。</p>
<p>研究人员观察到二项式随机变量，其中“N = 100”和“total_yes = 35”：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total_count = <span class="number">100</span></span><br><span class="line">total_yes = <span class="number">35</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">coin_joint_log_prob</span>(<span class="params">total_yes, total_count, lies_prob</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    联合对数概率优化函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      headsflips：观察到的头部翻转总数的整数</span></span><br><span class="line"><span class="string">      N: 观察的整数</span></span><br><span class="line"><span class="string">      lies_prob: 测试二项分布的头翻转（1）的概率</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      联合对数概率优化函数。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">  </span><br><span class="line">    rv_lies_prob = tfd.Uniform(name=<span class="string">&quot;rv_lies_prob&quot;</span>,low=<span class="number">0.</span>, high=<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">    cheated = tfd.Bernoulli(probs=tf.to_float(lies_prob)).sample(total_count)</span><br><span class="line">    first_flips = tfd.Bernoulli(probs=<span class="number">0.5</span>).sample(total_count)</span><br><span class="line">    second_flips = tfd.Bernoulli(probs=<span class="number">0.5</span>).sample(total_count)</span><br><span class="line">    observed_probability = tf.reduce_sum(tf.to_float(</span><br><span class="line">        cheated * first_flips + (<span class="number">1</span> - first_flips) * second_flips)) / total_count</span><br><span class="line"></span><br><span class="line">    rv_yeses = tfd.Binomial(name=<span class="string">&quot;rv_yeses&quot;</span>,</span><br><span class="line">                total_count=<span class="built_in">float</span>(total_count),</span><br><span class="line">                probs=observed_probability)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        rv_lies_prob.log_prob(lies_prob)</span><br><span class="line">        + tf.reduce_sum(rv_yeses.log_prob(tf.to_float(total_yes)))</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>下面我们将所有感兴趣的变量添加到我们的Metropolis-Hastings采样器中，并在模型上运行我们的黑盒算法。值得注意的是，我们正在使用Metropolis-Hastings
MCMC而不是汉密尔顿主义者，因为我们正在内部采样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">burnin = <span class="number">15000</span></span><br><span class="line">num_of_steps = <span class="number">40000</span></span><br><span class="line">total_count=<span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置链的开始状态。</span></span><br><span class="line">initial_chain_state = [<span class="number">0.4</span> * tf.ones([], dtype=tf.float32, name=<span class="string">&quot;init_prob&quot;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 闭包</span></span><br><span class="line">unnormalized_posterior_log_prob = <span class="keyword">lambda</span> *args: coin_joint_log_prob(total_yes, total_count,  *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the Metropolis-Hastings</span></span><br><span class="line"><span class="comment"># 我们在这里使用Metropolis-Hastings方法而不是哈密顿方法，因为上面例子中的硬币翻转是不可微分的，不能与HMC一起使用。</span></span><br><span class="line">metropolis=tfp.mcmc.RandomWalkMetropolis(target_log_prob_fn=unnormalized_posterior_log_prob,seed=<span class="number">54</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from the chain.</span></span><br><span class="line">[posterior_p], kernel_results = tfp.mcmc.sample_chain(</span><br><span class="line">    num_results=num_of_steps,</span><br><span class="line">    num_burnin_steps=burnin,</span><br><span class="line">    current_state=initial_chain_state,</span><br><span class="line">    kernel=metropolis,</span><br><span class="line">    parallel_iterations=<span class="number">1</span>,</span><br><span class="line">    name=<span class="string">&#x27;Metropolis-Hastings_coin-flips&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="执行tf图以从后验采样-2">执行TF图以从后验采样</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Content Warning: This cell can take up to 5 minutes in Graph Mode</span></span><br><span class="line">[posterior_p_,kernel_results_] = evaluate([posterior_p,kernel_results,])</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;接受率: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    kernel_results_.is_accepted.mean()))</span><br><span class="line"><span class="comment"># print(&quot;prob_p trace: &quot;, posterior_p_)</span></span><br><span class="line"><span class="comment"># print(&quot;prob_p burned trace: &quot;, posterior_p_[burnin:])</span></span><br><span class="line">burned_cheating_freq_samples_ = posterior_p_[burnin:]</span><br></pre></td></tr></table></figure>
<pre><code>接受率: 0.1058</code></pre>
<p>最后我们可以绘制结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">6</span>))</span><br><span class="line">p_trace_ = burned_cheating_freq_samples_</span><br><span class="line">plt.hist(p_trace_, histtype=<span class="string">&quot;stepfilled&quot;</span>, density=<span class="literal">True</span>, alpha=<span class="number">0.85</span>, bins=<span class="number">30</span>, </span><br><span class="line">         label=<span class="string">&quot;posterior distribution&quot;</span>, color=TFColor[<span class="number">3</span>])</span><br><span class="line">plt.vlines([<span class="number">.1</span>, <span class="number">.40</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">5</span>], alpha=<span class="number">0.3</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_90_0.png" /></p>
<p>关于上面的情节，我们仍然非常不确定作弊者的真实频率，但我们已将其缩小到0.1到0.4之间的范围（用实线标出）。这是非常好的，因为<em>先验</em>我们不知道有多少学生可能被欺骗（因此我们之前的统一分布）。另一方面，这也是非常糟糕的，因为有一个.3长度窗口可能存在的真实价值。我们甚至获得了什么，或者我们是否仍然对真实频率不确定？</p>
<p>我会说，是的，我们发现了一些东西。根据我们的后验，它是不可信的，即<em>没有欺骗者</em>，即后验分配给<span
class="math inline">\(p=0\)</span>的概率很低。由于我们从一个统一的先验开始，将<span
class="math inline">\(p\)</span>的所有值视为同样合理，但数据排除了<span
class="math inline">\(p =
0\)</span>作为一种可能性，我们可以确信有欺骗者。</p>
<p>这种算法可用于从用户收集私人信息，并且<em>合理地</em>确信数据虽然有噪声但是是真实的。</p>
<h3 id="替代tfp模型">替代TFP模型</h3>
<p>给定<span
class="math inline">\(p\)</span>的值（我们知道我们的置信度），我们可以找到学生回答的概率是：
<span class="math display">\[
\begin{align}
P(\text{&quot;Yes&quot;}) &amp;= P( \text{Heads on first coin} )P(
\text{cheater} ) + P( \text{Tails on first coin} )P( \text{Heads on
second coin} ) \\
&amp;= \frac{1}{2}p + \frac{1}{2}\frac{1}{2}\\
&amp;= \frac{p}{2} + \frac{1}{4}
\end{align}
\]</span> 因此，知道<span
class="math inline">\(p\)</span>我们知道学生回答“是”的概率。</p>
<p>如果我们知道受访者说“是”的概率，即p_skewed，并且我们有𝑁=
100名学生，则“是”回答的数量是具有参数N和p_skewed的二项式随机变量。
这是我们在总共100个中包含我们观察到的35个“是”响应的地方，然后将其传递给下面进一步的代码部分中的joint_log_prob，在此我们通过thejoint_log_prob定义我们的闭包。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">N = <span class="number">100.</span></span><br><span class="line">total_yes = <span class="number">35.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">alt_joint_log_prob</span>(<span class="params">yes_responses, N, prob_cheating</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Alternative joint log probability optimization function.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      yes_responses: Integer for total number of affirmative responses</span></span><br><span class="line"><span class="string">      N: Integer for number of total observation</span></span><br><span class="line"><span class="string">      prob_cheating: Test probability of a student actually cheating</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      Joint log probability optimization function.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tfd = tfp.distributions</span><br><span class="line">  </span><br><span class="line">    rv_prob = tfd.Uniform(name=<span class="string">&quot;rv_prob&quot;</span>, low=<span class="number">0.</span>, high=<span class="number">1.</span>)</span><br><span class="line">    p_skewed = <span class="number">0.5</span> * prob_cheating + <span class="number">0.25</span></span><br><span class="line">    rv_yes_responses = tfd.Binomial(name=<span class="string">&quot;rv_yes_responses&quot;</span>,</span><br><span class="line">                                     total_count=tf.to_float(N), </span><br><span class="line">                                     probs=p_skewed)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        rv_prob.log_prob(prob_cheating)</span><br><span class="line">        + tf.reduce_sum(rv_yes_responses.log_prob(tf.to_float(yes_responses)))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>下面我们将所有感兴趣的变量添加到我们的HMC组件定义单元格中，并在模型上运行我们的黑盒算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number_of_steps = <span class="number">25000</span></span><br><span class="line">burnin = <span class="number">2500</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the chain&#x27;s start state.</span></span><br><span class="line">initial_chain_state = [</span><br><span class="line">    <span class="number">0.2</span> * tf.ones([], dtype=tf.float32, name=<span class="string">&quot;init_skewed_p&quot;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Since HMC operates over unconstrained space, we need to transform the</span></span><br><span class="line"><span class="comment"># samples so they live in real-space.</span></span><br><span class="line">unconstraining_bijectors = [</span><br><span class="line">    tfp.bijectors.Sigmoid(),   <span class="comment"># Maps [0,1] to R.</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a closure over our joint_log_prob.</span></span><br><span class="line"><span class="comment"># unnormalized_posterior_log_prob = lambda *args: alt_joint_log_prob(headsflips, total_yes, N, *args)</span></span><br><span class="line">unnormalized_posterior_log_prob = <span class="keyword">lambda</span> *args: alt_joint_log_prob(total_yes, N, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the step_size. (It will be automatically adapted.)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):</span><br><span class="line">    step_size = tf.get_variable(</span><br><span class="line">        name=<span class="string">&#x27;skewed_step_size&#x27;</span>,</span><br><span class="line">        initializer=tf.constant(<span class="number">0.5</span>, dtype=tf.float32),</span><br><span class="line">        trainable=<span class="literal">False</span>,</span><br><span class="line">        use_resource=<span class="literal">True</span></span><br><span class="line">    ) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the HMC</span></span><br><span class="line">hmc=tfp.mcmc.TransformedTransitionKernel(</span><br><span class="line">    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(</span><br><span class="line">        target_log_prob_fn=unnormalized_posterior_log_prob,</span><br><span class="line">        num_leapfrog_steps=<span class="number">2</span>,</span><br><span class="line">        step_size=step_size,</span><br><span class="line">        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=<span class="built_in">int</span>(burnin * <span class="number">0.8</span>)),</span><br><span class="line">        state_gradients_are_stopped=<span class="literal">True</span>),</span><br><span class="line">    bijector=unconstraining_bijectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from the chain.</span></span><br><span class="line">[</span><br><span class="line">    posterior_skewed_p</span><br><span class="line">], kernel_results = tfp.mcmc.sample_chain(</span><br><span class="line">    num_results=number_of_steps,</span><br><span class="line">    num_burnin_steps=burnin,</span><br><span class="line">    current_state=initial_chain_state,</span><br><span class="line">    kernel=hmc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize any created variables.</span></span><br><span class="line"><span class="comment"># This prevents a FailedPreconditionError</span></span><br><span class="line">init_g = tf.global_variables_initializer()</span><br><span class="line">init_l = tf.local_variables_initializer()</span><br></pre></td></tr></table></figure>
<h4 id="执行tf图以从后验采样-3">执行TF图以从后验采样</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># This cell may take 5 minutes in Graph Mode</span></span><br><span class="line">evaluate(init_g)</span><br><span class="line">evaluate(init_l)</span><br><span class="line">[posterior_skewed_p_,kernel_results_] = evaluate([posterior_skewed_p,kernel_results])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;acceptance rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    kernel_results_.inner_results.is_accepted.mean()))</span><br><span class="line"><span class="comment"># print(&quot;final step size: &#123;&#125;&quot;.format(</span></span><br><span class="line"><span class="comment">#     kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(&quot;p_skewed trace: &quot;, posterior_skewed_p_)</span></span><br><span class="line"><span class="comment"># print(&quot;p_skewed burned trace: &quot;, posterior_skewed_p_[burnin:])</span></span><br><span class="line">freq_cheating_samples_ = posterior_skewed_p_[burnin:]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>acceptance rate: 0.6818</code></pre>
<p>Now we can plot our results</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">6</span>))</span><br><span class="line">p_trace_ = freq_cheating_samples_</span><br><span class="line">plt.hist(p_trace_, histtype=<span class="string">&quot;stepfilled&quot;</span>, normed=<span class="literal">True</span>, alpha=<span class="number">0.85</span>, bins=<span class="number">30</span>, </span><br><span class="line">         label=<span class="string">&quot;posterior distribution&quot;</span>, color=TFColor[<span class="number">3</span>])</span><br><span class="line">plt.vlines([<span class="number">.1</span>, <span class="number">.40</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">5</span>], alpha=<span class="number">0.2</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_100_0.png" /></p>
<p>本章的其余部分将介绍TFP和TFP建模的一些实际示例：</p>
<h2 id="example-挑战者航天飞机灾难">Example: 挑战者航天飞机灾难</h2>
<p>1986年1月28日，美国航天飞机计划的第二十五次飞行结束，当一架航天飞机挑战者的火箭助推器在升空后不久爆炸，造成所有七名机组人员死亡。事故总统委员会得出结论认为，这是由于火箭助推器上的现场接头中的O形环失效造成的，而这种失败是由于设计错误导致O形圈对一些人不可接受的敏感。因素包括室外温度。在之前的24个航班中，有关23个O形圈故障的数据（其中一个在海上丢失），这些数据在挑战者发射前的晚上进行了讨论，但不幸的是只有与7个航班相对应的数据。其中有一个损坏事件被认为是重要的，这些被认为没有明显的趋势。数据如下所示（见[1]）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_sess()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> wget</span><br><span class="line">url = <span class="string">&#x27;https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv&#x27;</span></span><br><span class="line">filename = wget.download(url)</span><br><span class="line">filename</span><br></pre></td></tr></table></figure>
<pre><code>&#39;challenger_data.csv&#39;</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">3.5</span>))</span><br><span class="line">np.set_printoptions(precision=<span class="number">3</span>, suppress=<span class="literal">True</span>)</span><br><span class="line">challenger_data_ = np.genfromtxt(<span class="string">&quot;challenger_data.csv&quot;</span>, skip_header=<span class="number">1</span>,</span><br><span class="line">                                usecols=[<span class="number">1</span>, <span class="number">2</span>], missing_values=<span class="string">&quot;NA&quot;</span>,</span><br><span class="line">                                delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line"><span class="comment">#drop the NA values</span></span><br><span class="line">challenger_data_ = challenger_data_[~np.isnan(challenger_data_[:, <span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot it, as a function of tempature (the first column)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;温度 (F), O型环是否损坏?&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(challenger_data_)</span><br><span class="line"></span><br><span class="line">plt.scatter(challenger_data_[:, <span class="number">0</span>], challenger_data_[:, <span class="number">1</span>], s=<span class="number">75</span>, color=<span class="string">&quot;k&quot;</span>,</span><br><span class="line">            alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.yticks([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.ylabel(<span class="string">&quot;Damage Incident?&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Outside temperature (Fahrenheit)&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Defects of the Space Shuttle O-Rings vs temperature&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>温度 (F), O型环是否损坏?
[[66.  0.]
 [70.  1.]
 [69.  0.]
 [68.  0.]
 [67.  0.]
 [72.  0.]
 [73.  0.]
 [70.  0.]
 [57.  1.]
 [63.  1.]
 [70.  1.]
 [78.  0.]
 [67.  0.]
 [53.  1.]
 [67.  0.]
 [75.  0.]
 [70.  0.]
 [81.  0.]
 [76.  0.]
 [79.  0.]
 [75.  1.]
 [76.  0.]
 [58.  1.]]</code></pre>
<p><img src="/2019/07/26/tfp-ch2/output_104_1.png" /></p>
<p>很明显<em>随着室外温度的降低，发生的损害事故的概率</em>会增加。我们对这里的概率建模感兴趣，因为它看起来不像温度和损坏事件之间存在严格的截止点。我们能做的最好的事情就是“在温度<span
class="math inline">\(t\)</span>时，发生损坏事故的概率是多少？”。这个例子的目标是回答这个问题。</p>
<p>我们需要一个温度函数，称为<span
class="math inline">\(p（t）\)</span>，它在0和1之间（以便模拟概率），并随着温度的升高从1变为0。实际上有很多这样的功能，但最受欢迎的选择是<em>logits
函数</em></p>
<p><span class="math display">\[p(t) = \frac{1}{ 1 + e^{ \;\beta t }
}\]</span></p>
<p>在这个模型,<span
class="math inline">\(\beta\)</span>是我们不确定的变量. 下面是为<span
class="math inline">\(\beta = 1,3，-5\)</span>绘制的函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logistic</span>(<span class="params">x, beta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Logistic Function</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      x: independent variable</span></span><br><span class="line"><span class="string">      beta: beta term</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      Logistic function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + tf.exp(beta * x))</span><br><span class="line"></span><br><span class="line">x_vals = tf.linspace(start=-<span class="number">4.</span>, stop=<span class="number">4.</span>, num=<span class="number">100</span>)</span><br><span class="line">log_beta_1 = logistic(x_vals, <span class="number">1.</span>)</span><br><span class="line">log_beta_3 = logistic(x_vals, <span class="number">3.</span>)</span><br><span class="line">log_beta_m5 = logistic(x_vals, -<span class="number">5.</span>)</span><br><span class="line">log_beta_m1 = logistic(x_vals, -<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">[</span><br><span class="line">    x_vals_,</span><br><span class="line">    log_beta_1_,</span><br><span class="line">    log_beta_3_,</span><br><span class="line">    log_beta_m5_,</span><br><span class="line">    log_beta_m1_,</span><br><span class="line">] = evaluate([</span><br><span class="line">    x_vals,</span><br><span class="line">    log_beta_1,</span><br><span class="line">    log_beta_3,</span><br><span class="line">    log_beta_m5,</span><br><span class="line">    log_beta_m1,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(x_vals_, log_beta_1_, label=<span class="string">r&quot;$\beta = 1$&quot;</span>, color=TFColor[<span class="number">0</span>])</span><br><span class="line">plt.plot(x_vals_, log_beta_3_, label=<span class="string">r&quot;$\beta = 3$&quot;</span>, color=TFColor[<span class="number">3</span>])</span><br><span class="line">plt.plot(x_vals_, log_beta_m5_, label=<span class="string">r&quot;$\beta = -5$&quot;</span>, color=TFColor[<span class="number">6</span>])</span><br><span class="line">plt.plot(x_vals_, log_beta_m5_, label=<span class="string">r&quot;$\beta = -1$&quot;</span>, color=TFColor[<span class="number">9</span>])</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_106_0.png" /></p>
<p>但缺少一些东西。在逻辑函数的图中，概率仅在零附近变化，但在我们的数据中，概率在65到70左右变化。我们需要在逻辑函数中添加一个偏差项：</p>
<p><span class="math display">\[p(t) = \frac{1}{ 1 + e^{ \;\beta t +
\alpha } }\]</span></p>
<p>下面有一些情节，不同的<span
class="math inline">\(\alpha\)</span>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logistic</span>(<span class="params">x, beta, alpha=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    带偏移的Logistic函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: 独立变量</span></span><br><span class="line"><span class="string">        beta: beta term </span></span><br><span class="line"><span class="string">        alpha: alpha term</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        Logistic function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + tf.exp((beta * x) + alpha))</span><br><span class="line"></span><br><span class="line">x_vals = tf.linspace(start=-<span class="number">4.</span>, stop=<span class="number">4.</span>, num=<span class="number">100</span>)</span><br><span class="line">log_beta_1_alpha_1 = logistic(x_vals, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">log_beta_3_alpha_m2 = logistic(x_vals, <span class="number">3</span>, -<span class="number">2</span>)</span><br><span class="line">log_beta_m5_alpha_7 = logistic(x_vals, -<span class="number">5</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">[</span><br><span class="line">    x_vals_,</span><br><span class="line">    log_beta_1_alpha_1_,</span><br><span class="line">    log_beta_3_alpha_m2_,</span><br><span class="line">    log_beta_m5_alpha_7_,</span><br><span class="line">] = evaluate([</span><br><span class="line">    x_vals,</span><br><span class="line">    log_beta_1_alpha_1,</span><br><span class="line">    log_beta_3_alpha_m2,</span><br><span class="line">    log_beta_m5_alpha_7,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(x_vals_, log_beta_1_, label=<span class="string">r&quot;$\beta = 1$&quot;</span>, ls=<span class="string">&quot;--&quot;</span>, lw=<span class="number">1</span>, color=TFColor[<span class="number">0</span>])</span><br><span class="line">plt.plot(x_vals_, log_beta_3_, label=<span class="string">r&quot;$\beta = 3$&quot;</span>, ls=<span class="string">&quot;--&quot;</span>, lw=<span class="number">1</span>, color=TFColor[<span class="number">3</span>])</span><br><span class="line">plt.plot(x_vals_, log_beta_m5_, label=<span class="string">r&quot;$\beta = -5$&quot;</span>, ls=<span class="string">&quot;--&quot;</span>, lw=<span class="number">1</span>, color=TFColor[<span class="number">6</span>])</span><br><span class="line">plt.plot(x_vals_, log_beta_1_alpha_1_, label=<span class="string">r&quot;$\beta = 1, \alpha = 1$&quot;</span>, color=TFColor[<span class="number">0</span>])</span><br><span class="line">plt.plot(x_vals_, log_beta_3_alpha_m2_, label=<span class="string">r&quot;$\beta = 3, \alpha = -2$&quot;</span>, color=TFColor[<span class="number">3</span>])</span><br><span class="line">plt.plot(x_vals_, log_beta_m5_alpha_7_, label=<span class="string">r&quot;$\beta = -5, \alpha = 7$&quot;</span>, color=TFColor[<span class="number">6</span>])</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower left&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_108_0.png" /></p>
<p>添加一个常数项<span
class="math inline">\(\alpha\)</span>相当于向左或向右移动曲线（因此它被称为<em>偏差</em>）。</p>
<p>让我们开始在TFP中对此进行建模。<span
class="math inline">\(\beta，\alpha\)</span>参数没有理由为正，有界或相对较大，所以它们最好用<em>正态随机变量</em>建模，接下来介绍。</p>
<h3 id="正态分布">正态分布</h3>
<p>一个普通的随机变量，表示为<span class="math inline">\(X \sim
N（\mu，1/\tau）\)</span>，有一个包含两个参数的分布：均值，<span
class="math inline">\(\mu\)</span>和* precision*，<span
class="math inline">\(\tau\)</span>。那些熟悉Normal分布的人可能已经看到<span
class="math inline">\(\sigma^2\)</span>而不是<span
class="math inline">\(\tau^{-1}\)</span>。它们实际上是彼此的倒数。这种变化的动机是简单的数学分析，是旧贝叶斯方法的工件。请记住：<span
class="math inline">\(\tau\)</span>越小，分布的分布越大（即我们更不确定）;<span
class="math inline">\(\tau\)</span>越大，分布越紧（即我们更确定）。无论如何，<span
class="math inline">\(\tau\)</span>总是正的的。</p>
<p><span
class="math inline">\(N（\mu，1/\tau）\)</span>随机变量的概率密度函数是：</p>
<p><span class="math display">\[f(x ; \mu, \tau) =
\sqrt{\frac{\tau}{2\pi} } \exp\left( -\frac{\tau}{2} (x-\mu)^2
\right)\]</span></p>
<p>我们在下面绘制一些不同的密度函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rand_x_vals = tf.linspace(start=-<span class="number">8.</span>, stop=<span class="number">7.</span>, num=<span class="number">150</span>)</span><br><span class="line"></span><br><span class="line">density_func_1 = tfd.Normal(loc=<span class="built_in">float</span>(-<span class="number">2.</span>), scale=<span class="built_in">float</span>(<span class="number">1.</span>/<span class="number">.7</span>)).prob(rand_x_vals)</span><br><span class="line">density_func_2 = tfd.Normal(loc=<span class="built_in">float</span>(<span class="number">0.</span>), scale=<span class="built_in">float</span>(<span class="number">1.</span>/<span class="number">1</span>)).prob(rand_x_vals)</span><br><span class="line">density_func_3 = tfd.Normal(loc=<span class="built_in">float</span>(<span class="number">3.</span>), scale=<span class="built_in">float</span>(<span class="number">1.</span>/<span class="number">2.8</span>)).prob(rand_x_vals)</span><br><span class="line"></span><br><span class="line">[</span><br><span class="line">    rand_x_vals_,</span><br><span class="line">    density_func_1_,</span><br><span class="line">    density_func_2_,</span><br><span class="line">    density_func_3_,</span><br><span class="line">] = evaluate([</span><br><span class="line">    rand_x_vals,</span><br><span class="line">    density_func_1,</span><br><span class="line">    density_func_2,</span><br><span class="line">    density_func_3,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">colors = [TFColor[<span class="number">3</span>], TFColor[<span class="number">0</span>], TFColor[<span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(rand_x_vals_, density_func_1_,</span><br><span class="line">         label=<span class="string">r&quot;$\mu = %d, \tau = %.1f$&quot;</span> % (-<span class="number">2.</span>, <span class="number">.7</span>), color=TFColor[<span class="number">3</span>])</span><br><span class="line">plt.fill_between(rand_x_vals_, density_func_1_, color=TFColor[<span class="number">3</span>], alpha=<span class="number">.33</span>)</span><br><span class="line">plt.plot(rand_x_vals_, density_func_2_, </span><br><span class="line">         label=<span class="string">r&quot;$\mu = %d, \tau = %.1f$&quot;</span> % (<span class="number">0.</span>, <span class="number">1</span>), color=TFColor[<span class="number">0</span>])</span><br><span class="line">plt.fill_between(rand_x_vals_, density_func_2_, color=TFColor[<span class="number">0</span>], alpha=<span class="number">.33</span>)</span><br><span class="line">plt.plot(rand_x_vals_, density_func_3_,</span><br><span class="line">         label=<span class="string">r&quot;$\mu = %d, \tau = %.1f$&quot;</span> % (<span class="number">3.</span>, <span class="number">2.8</span>), color=TFColor[<span class="number">6</span>])</span><br><span class="line">plt.fill_between(rand_x_vals_, density_func_3_, color=TFColor[<span class="number">6</span>], alpha=<span class="number">.33</span>)</span><br><span class="line"></span><br><span class="line">plt.legend(loc=<span class="string">r&quot;upper right&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">r&quot;$x$&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">r&quot;density function at$x$&quot;</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;Probability distribution of three different Normal random variables&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_111_0.png" /></p>
<p>普通随机变量可以取任何实数，但变量很可能相对接近<span
class="math inline">\(\mu\)</span>。实际上，Normal的预期值等于其<span
class="math inline">\(\mu\)</span>参数：</p>
<p><span class="math display">\[E[ X ; \mu, \tau] = \mu\]</span></p>
<p>并且它的方差等于<span class="math inline">\(\tau\)</span>的倒数：</p>
<p><span class="math display">\[\text{Var}( X ; \mu, \tau ) =
\frac{1}{\tau}\]</span></p>
<p>下面我们继续我们对挑战者太空飞船的建模：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_sess()</span><br><span class="line"></span><br><span class="line">temperature_ = challenger_data_[:, <span class="number">0</span>]</span><br><span class="line">temperature = tf.convert_to_tensor(temperature_, dtype=tf.float32)</span><br><span class="line">D_ = challenger_data_[:, <span class="number">1</span>]                <span class="comment"># defect or not?</span></span><br><span class="line">D = tf.convert_to_tensor(D_, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">beta = tfd.Normal(name=<span class="string">&quot;beta&quot;</span>, loc=<span class="number">0.3</span>, scale=<span class="number">1000.</span>).sample()</span><br><span class="line">alpha = tfd.Normal(name=<span class="string">&quot;alpha&quot;</span>, loc=-<span class="number">15.</span>, scale=<span class="number">1000.</span>).sample()</span><br><span class="line">p_deterministic = tfd.Deterministic(name=<span class="string">&quot;p&quot;</span>, loc=<span class="number">1.0</span>/(<span class="number">1.</span> + tf.exp(beta * temperature_ + alpha))).sample()</span><br><span class="line"></span><br><span class="line">[</span><br><span class="line">    prior_alpha_,</span><br><span class="line">    prior_beta_,</span><br><span class="line">    p_deterministic_,</span><br><span class="line">    D_,</span><br><span class="line">] = evaluate([</span><br><span class="line">    alpha,</span><br><span class="line">    beta,</span><br><span class="line">    p_deterministic,</span><br><span class="line">    D,</span><br><span class="line">])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们有自己的概率，但我们如何将它们与我们观察到的数据联系起来？带参数<span
class="math inline">\(p\)</span>的A
<em>Bernoulli</em>随机变量，表示为<span
class="math inline">\(\text{Ber}（p）\)</span>，是一个随机变量，取值为1，概率为<span
class="math inline">\(p\)</span>，0为else。因此，我们的模型看起来像：</p>
<p><span class="math display">\[\text{Defect Incident, }D_i \sim
\text{Ber}( \;p(t_i)\; ), \;\; i=1..N\]</span></p>
<p>其中<span
class="math inline">\(p（t）\)</span>是我们的logistic函数，<span
class="math inline">\(t_i\)</span>是我们观察到的温度。请注意，在下面的代码中，我们在<code>initial_chain_state</code>中将<code>beta</code>和<code>alpha</code>的值设置为0。这样做的原因是，如果<code>beta</code>和<code>alpha</code>非常大，它们会使<code>p</code>等于1或0.不幸的是，<code>tfd.Bernoulli</code>不喜欢0或1的概率，尽管它们是数学上的定义明确的概率。因此，通过将系数值设置为“0”，我们将变量“p”设置为合理的起始值。这对我们的结果没有影响，也不意味着我们在之前的内容中包含任何其他信息。这只是TFP中的计算警告。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">challenger_joint_log_prob</span>(<span class="params">D, temperature_, alpha, beta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    联合对数概率优化函数。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      D: 来自挑战者灾难的数据表示存在或不存在缺陷</span></span><br><span class="line"><span class="string">      temperature_: 来自挑战者灾难的数据，特别是观察是否存在缺陷的温度</span></span><br><span class="line"><span class="string">      alpha: one of the inputs of the HMC</span></span><br><span class="line"><span class="string">      beta: one of the inputs of the HMC</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      Joint log probability optimization function.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    rv_alpha = tfd.Normal(loc=<span class="number">0.</span>, scale=<span class="number">1000.</span>)</span><br><span class="line">    rv_beta = tfd.Normal(loc=<span class="number">0.</span>, scale=<span class="number">1000.</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make this into a logit</span></span><br><span class="line">    logistic_p = <span class="number">1.0</span>/(<span class="number">1.</span> + tf.exp(beta * tf.to_float(temperature_) + alpha))</span><br><span class="line">    rv_observed = tfd.Bernoulli(probs=logistic_p)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        rv_alpha.log_prob(alpha)</span><br><span class="line">        + rv_beta.log_prob(beta)</span><br><span class="line">        + tf.reduce_sum(rv_observed.log_prob(D))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number_of_steps = <span class="number">10000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:2500, max:120000, step:100&#125;</span></span><br><span class="line">burnin = <span class="number">2000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:2000, max:100000, step:100&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数都是0</span></span><br><span class="line">initial_chain_state = [</span><br><span class="line">    <span class="number">1.</span> * tf.ones([], dtype=tf.float32, name=<span class="string">&quot;init_alpha&quot;</span>),</span><br><span class="line">    <span class="number">2.</span> * tf.ones([], dtype=tf.float32, name=<span class="string">&quot;init_beta&quot;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Since HMC operates over unconstrained space, we need to transform the</span></span><br><span class="line"><span class="comment"># samples so they live in real-space.</span></span><br><span class="line"><span class="comment"># Alpha is 100x of beta approximately, so apply Affine scalar bijector</span></span><br><span class="line"><span class="comment"># to multiply the unconstrained alpha by 100 to get back to </span></span><br><span class="line"><span class="comment"># the Challenger problem space</span></span><br><span class="line">unconstraining_bijectors = [</span><br><span class="line">    tfp.bijectors.AffineScalar(<span class="number">100.</span>),</span><br><span class="line">    tfp.bijectors.Identity()</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a closure over our joint_log_prob.</span></span><br><span class="line">unnormalized_posterior_log_prob = <span class="keyword">lambda</span> *args: challenger_joint_log_prob(D, temperature_, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the step_size. (It will be automatically adapted.)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):</span><br><span class="line">    step_size = tf.get_variable(</span><br><span class="line">        name=<span class="string">&#x27;step_size&#x27;</span>,</span><br><span class="line">        initializer=tf.constant(<span class="number">0.01</span>, dtype=tf.float32),</span><br><span class="line">        trainable=<span class="literal">False</span>,</span><br><span class="line">        use_resource=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the HMC</span></span><br><span class="line">hmc=tfp.mcmc.TransformedTransitionKernel(</span><br><span class="line">    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(</span><br><span class="line">        target_log_prob_fn=unnormalized_posterior_log_prob,</span><br><span class="line">        num_leapfrog_steps=<span class="number">40</span>, <span class="comment">#to improve convergence</span></span><br><span class="line">        step_size=step_size,</span><br><span class="line">        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(</span><br><span class="line">            num_adaptation_steps=<span class="built_in">int</span>(burnin * <span class="number">0.8</span>)),</span><br><span class="line">        state_gradients_are_stopped=<span class="literal">True</span>),</span><br><span class="line">    bijector=unconstraining_bijectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sampling from the chain.</span></span><br><span class="line">[posterior_alpha,posterior_beta], kernel_results = tfp.mcmc.sample_chain(</span><br><span class="line">    num_results = number_of_steps,</span><br><span class="line">    num_burnin_steps = burnin,</span><br><span class="line">    current_state=initial_chain_state,</span><br><span class="line">    kernel=hmc)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Initialize any created variables for preconditions</span></span><br><span class="line">init_g = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<h4 id="执行tf图以从后验采样-4">执行TF图以从后验采样</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time</span><br><span class="line"><span class="comment"># 在图形模式下，此单元格最多可能需要15分钟</span></span><br><span class="line">evaluate(init_g)</span><br><span class="line">[posterior_alpha_, posterior_beta_, kernel_results_] = evaluate([</span><br><span class="line">    posterior_alpha,</span><br><span class="line">    posterior_beta,</span><br><span class="line">    kernel_results</span><br><span class="line">])</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;接受率: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    kernel_results_.inner_results.is_accepted.mean()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;结束步: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    kernel_results_.inner_results.extra.step_size_assign[-<span class="number">100</span>:].mean()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>接受率: 0.388
结束步: 0.016446322202682495
CPU times: user 10min 51s, sys: 1min 2s, total: 11min 53s
Wall time: 5min 5s</code></pre>
<p>我们已经在观察到的数据上训练了我们的模型，所以让我们看看<span
class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>的后验分布：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">posterior_alpha_, posterior_beta_=evaluate([</span><br><span class="line">    posterior_alpha,</span><br><span class="line">    posterior_beta,</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本的直方图:</span></span><br><span class="line">plt.subplot(<span class="number">211</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;Posterior distributions of the variables$\alpha, \beta$&quot;</span>)</span><br><span class="line">plt.hist(posterior_beta_, histtype=<span class="string">&#x27;stepfilled&#x27;</span>, bins=<span class="number">35</span>, alpha=<span class="number">0.85</span>,</span><br><span class="line">         label=<span class="string">r&quot;posterior of$\beta$&quot;</span>, color=TFColor[<span class="number">6</span>], density=<span class="literal">True</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">212</span>)</span><br><span class="line">plt.hist(posterior_alpha_, histtype=<span class="string">&#x27;stepfilled&#x27;</span>, bins=<span class="number">35</span>, alpha=<span class="number">0.85</span>,</span><br><span class="line">         label=<span class="string">r&quot;posterior of$\alpha$&quot;</span>, color=TFColor[<span class="number">0</span>], density=<span class="literal">True</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_121_0.png" /></p>
<p><span
class="math inline">\(\beta\)</span>的所有样本都大于0.如果后验以0为中心，我们可能会怀疑<span
class="math inline">\(\beta =
0\)</span>，这意味着温度对缺陷概率没有影响。</p>
<p>同样，所有<span
class="math inline">\(\alpha\)</span>后验值都是负数且远离0，这意味着认为<span
class="math inline">\(\alpha\)</span>明显小于0是正确的。</p>
<p>关于数据的传播，我们非常不确定真实参数可能是什么（尽管考虑到样本量较小以及缺陷与非缺陷的大量重叠，这种行为可能是预期的）。</p>
<p>接下来，让我们看一下特定温度值的<em>预期概率</em>。也就是说，我们对来自后验的所有样本求平均值，得到<span
class="math inline">\(p（t_i）\)</span>的可能值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alpha_samples_1d_ = posterior_alpha_[:, <span class="literal">None</span>]  <span class="comment"># best to make them 1d</span></span><br><span class="line">beta_samples_1d_ = posterior_beta_[:, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">beta_mean = tf.reduce_mean(beta_samples_1d_.T[<span class="number">0</span>])</span><br><span class="line">alpha_mean = tf.reduce_mean(alpha_samples_1d_.T[<span class="number">0</span>])</span><br><span class="line">[ beta_mean_, alpha_mean_ ] = evaluate([ beta_mean, alpha_mean ])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;beta mean:&quot;</span>, beta_mean_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;alpha mean:&quot;</span>, alpha_mean_)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic</span>(<span class="params">x, beta, alpha=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Logistic function with alpha and beta.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      x: independent variable</span></span><br><span class="line"><span class="string">      beta: beta term </span></span><br><span class="line"><span class="string">      alpha: alpha term</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      Logistic function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + tf.exp((beta * x) + alpha))</span><br><span class="line"></span><br><span class="line">t_ = np.linspace(temperature_.<span class="built_in">min</span>() - <span class="number">5</span>, temperature_.<span class="built_in">max</span>() + <span class="number">5</span>, <span class="number">2500</span>)[:, <span class="literal">None</span>]</span><br><span class="line">p_t = logistic(t_.T, beta_samples_1d_, alpha_samples_1d_)</span><br><span class="line">mean_prob_t = logistic(t_.T, beta_mean_, alpha_mean_)</span><br><span class="line">[ </span><br><span class="line">    p_t_, mean_prob_t_</span><br><span class="line">] = evaluate([ </span><br><span class="line">    p_t, mean_prob_t</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<pre><code>beta mean: 0.034827046
alpha mean: -1.5132124</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">plt.plot(t_, mean_prob_t_.T, lw=<span class="number">3</span>, label=<span class="string">&quot;average posterior \nprobability \</span></span><br><span class="line"><span class="string">of defect&quot;</span>)</span><br><span class="line">plt.plot(t_, p_t_.T[:, <span class="number">0</span>], ls=<span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;realization from posterior&quot;</span>)</span><br><span class="line">plt.plot(t_, p_t_.T[:, -<span class="number">8</span>], ls=<span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;realization from posterior&quot;</span>)</span><br><span class="line">plt.scatter(temperature_, D_, color=<span class="string">&quot;k&quot;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Posterior expected value of probability of defect; \</span></span><br><span class="line"><span class="string">plus realizations&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower left&quot;</span>)</span><br><span class="line">plt.ylim(-<span class="number">0.1</span>, <span class="number">1.1</span>)</span><br><span class="line">plt.xlim(t_.<span class="built_in">min</span>(), t_.<span class="built_in">max</span>())</span><br><span class="line">plt.ylabel(<span class="string">&quot;probability&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;temperature&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_124_0.png" /></p>
<p>上面我们还绘制了实际底层系统可能实现的两种可能的实现。两者都与其他任何平局一样可能。当我们将所有20000条可能的虚线平均在一起时，会出现蓝线。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats.mstats <span class="keyword">import</span> mquantiles</span><br><span class="line"></span><br><span class="line"><span class="comment"># “置信区间”的矢量化底部和顶部2.5％分位数</span></span><br><span class="line">qs = mquantiles(p_t_, [<span class="number">0.025</span>, <span class="number">0.975</span>], axis=<span class="number">0</span>)</span><br><span class="line">plt.fill_between(t_[:, <span class="number">0</span>], *qs, alpha=<span class="number">0.7</span>,</span><br><span class="line">                 color=<span class="string">&quot;#7A68A6&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(t_[:, <span class="number">0</span>], qs[<span class="number">0</span>], label=<span class="string">&quot;95% CI&quot;</span>, color=<span class="string">&quot;#7A68A6&quot;</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(t_[:, <span class="number">0</span>], mean_prob_t_[<span class="number">0</span>,:], lw=<span class="number">1</span>, ls=<span class="string">&quot;--&quot;</span>, color=<span class="string">&quot;k&quot;</span>,</span><br><span class="line">         label=<span class="string">&quot;average posterior \nprobability of defect&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(t_.<span class="built_in">min</span>(), t_.<span class="built_in">max</span>())</span><br><span class="line">plt.ylim(-<span class="number">0.02</span>, <span class="number">1.02</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower left&quot;</span>)</span><br><span class="line">plt.scatter(temperature_, D_, color=<span class="string">&quot;k&quot;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;temp,$t$&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">&quot;probability estimate&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Posterior probability estimates given temp.$t$&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_126_0.png" /></p>
<p>95％可信区间，或95％CI，涂成紫色，表示每个温度的间隔，包含95％的分布。例如，在65度时，我们可以95％确定缺陷的概率介于0.25和0.85之间。</p>
<p>更一般地说，我们可以看到，当温度接近60度时，CI迅速扩散到<span
class="math inline">\([0,1]\)</span>以上。当我们通过70度时，CI再次收紧。这可以让我们深入了解如何继续下一步：我们应该在60-65温度附近测试更多的O形环，以更好地估计该范围内的概率。同样地，在向科学家报告您的估计时，您应该非常谨慎地告诉他们预期的概率，因为我们可以看到这并不能反映后验分布的<em>宽</em>。</p>
<h3 id="挑战者灾难的那天怎么样">挑战者灾难的那天怎么样？</h3>
<p>在挑战者灾难当天，室外温度为31华氏度。在这个温度下，出现缺陷的后验分布是什么？分布如下图所示。看起来几乎可以保证挑战者将受到有缺陷的O形圈的影响。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">prob_31 = logistic(<span class="number">31</span>, posterior_beta_, posterior_alpha_)</span><br><span class="line"></span><br><span class="line">[ prob_31_ ] = evaluate([ prob_31 ])</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0.98</span>, <span class="number">1</span>)</span><br><span class="line">plt.hist(prob_31_, bins=<span class="number">10</span>, density=<span class="literal">True</span>, histtype=<span class="string">&#x27;stepfilled&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Posterior distribution of probability of defect, given$t = 31$&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;probability of defect occurring in O-ring&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/26/tfp-ch2/output_129_0.png" /></p>
<h3 id="我们的模型是否合适">我们的模型是否合适?</h3>
<p>持怀疑态度的读者会说：“你故意为<span
class="math inline">\(p（t）\)</span>和特定的先验选择了logistic函数。或许其他函数或先验会给出不同的结果。我怎么知道我选择了一个好模特？”这绝对是真的。考虑一个极端情况，如果我选择了函数<span
class="math inline">\(p（t）= 1，\; \forall
t\)</span>，它保证了一直存在的缺陷：我将在1月28日再次预测灾难。然而，这显然是一个选择不当的模型。另一方面，如果我确实为<span
class="math inline">\(p（t）\)</span>选择了逻辑函数，但指定我的所有先验在0附近非常紧，可能我们会有非常不同的后验分布。我们怎么知道我们的模型是数据的表达？这鼓励我们衡量模型的<strong>适合度</strong>。</p>
<p>我们可以想：<em>我们如何测试我们的模型是否合适？</em>一个想法是将观测数据与我们可以模拟的人工数据集进行比较。基本原理是，如果模拟数据集在统计上与观察到的数据集不相似，则可能我们的模型未准确地表示观察到的数据。</p>
<p>在本章的前面，我们为SMS示例模拟了一个人工数据集。为此，我们从先验中采样值。我们看到了生成的数据集看起来多么多样，而且它们很少模仿我们观察到的数据集。在当前示例中，我们应该从<em>后验</em>分布中进行采样，以创建<em>非常合理的数据集</em>。幸运的是，我们的贝叶斯框架使这很容易。我们只需要从选择的分布中收集样本，并指定样本的数量，样本的形状（我们在原始数据集中有21个观测量，因此我们将使每个样本的形状为21），以及概率我们想用来确定1个观测值与0个观测值的比率。</p>
<p>因此我们创造了以下内容:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">simulated_data = tfd.Bernoulli(name=<span class="string">&quot;simulation_data&quot;</span>, probs=p).sample(sample_shape=N)</span><br></pre></td></tr></table></figure>
<p>模拟 10 000:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alpha = alpha_mean_ <span class="comment"># 我们将这些值基于上述模型的输出</span></span><br><span class="line">beta = beta_mean_</span><br><span class="line">p_deterministic = tfd.Deterministic(name=<span class="string">&quot;p&quot;</span>, loc=<span class="number">1.0</span>/(<span class="number">1.</span> + tf.exp(beta * temperature_ + alpha))).sample()<span class="comment">#seed=6.45)</span></span><br><span class="line">simulated_data = tfd.Bernoulli(name=<span class="string">&quot;bernoulli_sim&quot;</span>, </span><br><span class="line">                               probs=p_deterministic_).sample(sample_shape=<span class="number">10000</span>)</span><br><span class="line">[ </span><br><span class="line">    bernoulli_sim_samples_,</span><br><span class="line">    p_deterministic_</span><br><span class="line">] =evaluate([</span><br><span class="line">    simulated_data,</span><br><span class="line">    p_deterministic</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">simulations_ = bernoulli_sim_samples_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of simulations:             &quot;</span>, simulations_.shape[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number data points per simulation: &quot;</span>, simulations_.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">12</span>))</span><br><span class="line">plt.title(<span class="string">&quot;Simulated dataset using posterior parameters&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    ax = plt.subplot(<span class="number">4</span>, <span class="number">1</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.scatter(temperature_, simulations_[<span class="number">1000</span>*i, :], color=<span class="string">&quot;k&quot;</span>,</span><br><span class="line">                s=<span class="number">50</span>, alpha=<span class="number">0.6</span>)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<pre><code>Number of simulations:              10000
Number data points per simulation:  23</code></pre>
<p><img src="Ch2_MorePyMC_TFP/output_132_1.png" /></p>
<p>请注意，上面的图表是不同的（如果您能想到一个更清晰的方式来呈现此，请发送拉取请求并回答<a
target="_blank" rel="noopener" href="http://stats.stackexchange.com/questions/53078/how-to-visualize-bayesian-goodness-of-fit-for-logistic-regression">此处</a>!).</p>
<p>我们希望评估我们的模型有多好。
“好”当然是一个主观的术语，因此结果必须与其他模型相关。</p>
<p>我们也将以图形方式进行此操作，这似乎是一种更不客观的方法。另一种方法是使用<em>贝叶斯p值</em>。这些仍然是主观的，因为好与坏之间的恰当截止是任意的。格尔曼强调，图形测试比p值测试更有启发性[3]。我们同意。</p>
<p>以下图形测试是一种新颖的数据 -
逻辑回归方法。这些图称为<em>分离图</em>
[4]。对于我们希望比较的一组模型，每个模型都绘制在单独的分离图上。我将关于分离图的大部分技术细节留给了非常容易获得的<a
target="_blank" rel="noopener" href="http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf">原始论文</a>,
但我会在这里总结一下它们的用途。</p>
<p>对于每个模型，我们计算后验模拟为特定温度提出值1的次数，即通过平均计算<span
class="math inline">\(P（\; \text{缺陷} = 1 |
t，\alpha，\beta）\)</span>。这为我们提供了数据集中每个数据点的缺陷后验概率。例如，对于我们上面使用的模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">posterior_probability_ = simulations_.mean(axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;posterior prob of defect | realized defect &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(D_)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%.2f                     |   %d&quot;</span> % (posterior_probability_[i], D_[i]))</span><br></pre></td></tr></table></figure>
<pre><code>posterior prob of defect | realized defect 
0.00                     |   0
0.00                     |   1
0.00                     |   0
0.00                     |   0
0.00                     |   0
0.00                     |   0
0.00                     |   0
0.00                     |   0
0.00                     |   1
0.00                     |   1
0.00                     |   1
0.00                     |   0
0.00                     |   0
0.00                     |   1
0.00                     |   0
0.00                     |   0
0.00                     |   0
0.00                     |   0
0.00                     |   0
0.00                     |   0
0.00                     |   1
0.00                     |   0
0.00                     |   1</code></pre>
<p>接下来，我们按后验概率对每列进行排序:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ix_ = np.argsort(posterior_probability_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;probb | defect &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(D_)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%.2f  |   %d&quot;</span> % (posterior_probability_[ix_[i]], D_[ix_[i]]))</span><br></pre></td></tr></table></figure>
<pre><code>probb | defect 
0.00  |   0
0.00  |   1
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   1
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   1
0.00  |   1
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   0
0.00  |   1
0.00  |   1
0.00  |   1</code></pre>
<p>我们可以在图中更好地呈现上述数据：我们创建了一个<code>separation_plot</code>函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">separation_plot</span>(<span class="params"> p, y, **kwargs </span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function creates a separation plot for logistic and probit classification. </span></span><br><span class="line"><span class="string">    See http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    p: 比例/概率可以是表示M个模型的n×M矩阵。</span></span><br><span class="line"><span class="string">    y: 0-1响应变量。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>    </span><br><span class="line">    <span class="keyword">assert</span> p.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>], <span class="string">&quot;p.shape[0] != y.shape[0]&quot;</span></span><br><span class="line">    n = p.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        M = p.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        p = p.reshape( n, <span class="number">1</span> )</span><br><span class="line">        M = p.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    colors_bmh = np.array( [<span class="string">&quot;#eeeeee&quot;</span>, <span class="string">&quot;#348ABD&quot;</span>] )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    fig = plt.figure( )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        ax = fig.add_subplot(M, <span class="number">1</span>, i+<span class="number">1</span>)</span><br><span class="line">        ix = np.argsort( p[:,i] )</span><br><span class="line">        <span class="comment">#plot the different bars</span></span><br><span class="line">        bars = ax.bar( np.arange(n), np.ones(n), width=<span class="number">1.</span>,</span><br><span class="line">                color = colors_bmh[ y[ix].astype(<span class="built_in">int</span>) ], </span><br><span class="line">                edgecolor = <span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        ax.plot( np.arange(n+<span class="number">1</span>), np.append(p[ix,i], p[ix,i][-<span class="number">1</span>]), <span class="string">&quot;k&quot;</span>,</span><br><span class="line">                 linewidth = <span class="number">1.</span>,drawstyle=<span class="string">&quot;steps-post&quot;</span> )</span><br><span class="line">        <span class="comment">#create expected value bar.</span></span><br><span class="line">        ax.vlines( [(<span class="number">1</span>-p[ix,i]).<span class="built_in">sum</span>()], [<span class="number">0</span>], [<span class="number">1</span>] )</span><br><span class="line">        plt.xlim( <span class="number">0</span>, n)</span><br><span class="line">        </span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">11.</span>, <span class="number">3</span>))</span><br><span class="line">separation_plot(posterior_probability_, D_)</span><br></pre></td></tr></table></figure>
<p><img src="Ch2_MorePyMC_TFP/output_138_1.png" /></p>
<p>蛇形线是排序的概率，蓝色条表示缺陷，空白空间（或乐观读者的灰色条）表示非缺陷。随着概率的上升，我们发现越来越多的缺陷发生。在右侧，该图表明，随着后验概率很大（线接近1），则实现了更多的缺陷。这是一种很好的行为。理想情况下，所有蓝条<em>应该</em>靠近右侧，与此相反的偏差反映了错过的预测。</p>
<p>在给定此模型的情况下，黑色垂直线是我们应该观察到的预期缺陷数。这允许用户查看模型预测的事件总数与数据中的实际事件数量的比较。</p>
<p>将其与其他模型的分离图进行比较会提供更多信息。下面我们比较我们的模型（顶部）与其他三个模型：</p>
<ol type="1">
<li>完美模型，如果确实发生缺陷，则预测后验概率等于1。</li>
<li>一个完全随机的模型，它可以预测随机概率，而不管温度如何。</li>
<li>常数模型：其中<span class="math inline">\(P（D = 1 \; | \; t）=
c，\; \; \forall t\)</span>。<span
class="math inline">\(c\)</span>的最佳选择是观察到的缺陷频率，在这种情况下为7/23。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">11.</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Our temperature-dependent model</span></span><br><span class="line">separation_plot(posterior_probability_, D_)</span><br><span class="line">plt.title(<span class="string">&quot;Temperature-dependent model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完美模型</span></span><br><span class="line"><span class="comment"># i.e. the probability of defect is equal to if a defect occurred or not.</span></span><br><span class="line">p_ = D_</span><br><span class="line">separation_plot(p_, D_)</span><br><span class="line">plt.title(<span class="string">&quot;Perfect model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机模型</span></span><br><span class="line">p_ = np.random.rand(<span class="number">23</span>)</span><br><span class="line">separation_plot(p_, D_)</span><br><span class="line">plt.title(<span class="string">&quot;Random model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常量模型</span></span><br><span class="line">constant_prob_ = <span class="number">7.</span>/<span class="number">23</span> * np.ones(<span class="number">23</span>)</span><br><span class="line">separation_plot(constant_prob_, D_)</span><br><span class="line">plt.title(<span class="string">&quot;Constant-prediction model&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Ch2_MorePyMC_TFP/output_140_1.png" /></p>
<p><img src="Ch2_MorePyMC_TFP/output_140_2.png" /></p>
<p><img src="Ch2_MorePyMC_TFP/output_140_3.png" /></p>
<p><img src="Ch2_MorePyMC_TFP/output_140_4.png" /></p>
<p>In the random model, we can see that as the probability increases
there is no clustering of defects to the right-hand side. Similarly for
the constant model.</p>
<p>In the perfect model, the probability line is not well shown, as it
is stuck to the bottom and top of the figure. Of course the perfect
model is only for demonstration, and we cannot infer any scientific
inference from it.</p>
<h2 id="exercises">Exercises</h2>
<p>1 Try putting in extreme values for our observations in the cheating
example. What happens if we observe 25 affirmative responses? 10?
50?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#type your code here.</span></span><br></pre></td></tr></table></figure>
<p>2 Try plotting<span class="math inline">\(\alpha\)</span>samples
versus<span class="math inline">\(\beta\)</span>samples. Why might the
resulting plot look like this?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#type your code here.</span></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">plt.scatter(alpha_samples_, beta_samples_, alpha=<span class="number">0.1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Why does the plot look like this?&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">r&quot;$\alpha$&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">r&quot;$\beta$&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="references">References</h2>
<p>[1] Dalal, Fowlkes and Hoadley (1989),JASA, 84, 945-957.</p>
<p>[2] Cronin, Beau. "Why Probabilistic Programming Matters." 24 Mar
2013. Google, Online Posting to Google . Web. 24 Mar. 2013. <a
target="_blank" rel="noopener" href="https://plus.google.com/u/0/+BeauCronin/posts/KpeRdJKR6Z1"
class="uri">https://plus.google.com/u/0/+BeauCronin/posts/KpeRdJKR6Z1</a>.</p>
<p>[3] Gelman, Andrew. "Philosophy and the practice of Bayesian
statistics." British Journal of Mathematical and Statistical Psychology.
(2012): n. page. Web. 2 Apr. 2013.</p>
<p>[4] Greenhill, Brian, Michael D. Ward, and Audrey Sacks. "The
Separation Plot: A New Visual Method for Evaluating the Fit of Binary
Models." American Journal of Political Science. 55.No.4 (2011): n. page.
Web. 2 Apr. 2013.</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" rel="tag">概率论</a></li></ul></div><div class="post-nav"><a class="pre" href="/2019/07/27/tfp-ch3/">概率模型第三章 ： MCMC</a><a class="next" href="/2019/07/25/use-lan/">设置路由器使用LAN口</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/01/11/affine-fusion/">Affine Fusion Pass浅析</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>