<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>EM算法与EM路由 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">EM算法与EM路由</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">EM算法与EM路由</h1><div class="post-meta">2019-07-11<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 10.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 51</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>搞定了yolo，终于可以学习点新的东西了。今天就学习一波胶囊网络中的EM路由。
先推荐一个课程资料，杜克大学的<a
target="_blank" rel="noopener" href="https://people.duke.edu/~ccc14/sta-663">统计学课程</a>，从python讲到c++，从矩阵计算讲到概率统计，从jit讲到cuda编程。看看人家本科生学的东西。。。</p>
<span id="more"></span>
<h1 id="em算法">EM算法</h1>
<h2 id="詹森不等式">詹森不等式</h2>
<p>对于一个凸函数<span class="math inline">\(f\)</span>,<span
class="math inline">\(E[f(x)]\geq
f(E[x])\)</span>,将其反转获得一个凹函数。 如果一个函数<span
class="math inline">\(f(x)\)</span>在区间内都有<span
class="math inline">\(f&#39;&#39;(x)\geq
0\)</span>，那么它是一个凸函数。例如<span
class="math inline">\(f(x)=log\ x\)</span>,<span
class="math inline">\(f&#39;&#39;(x)=-\frac{1}{x^2}\)</span>,说明他在<span
class="math inline">\(x\in (0,+
\infty]\)</span>是一个凸函数，詹森不等式的直观说明如下。</p>
<p><img src="/2019/07/11/em-algm/EMAlgorithm_5_0.png" /></p>
<p>其实只有当<span
class="math inline">\(f(x)\)</span>为常数时，詹森不等式才会相等。</p>
<p>这里使用概率论表述的<span
class="math inline">\(E\)</span>，其实际就是积分。也就是说先经过凸函数的期望值必然大于等于期望的凸函数值。</p>
<h2 id="完整信息的最大似然">完整信息的最大似然</h2>
<p>设置一个实验，硬币A向上的概率为<span
class="math inline">\(\theta_A\)</span>的，硬币B向上的概率为<span
class="math inline">\(\theta_B\)</span>,接着一共做m此实验，每次实验随机选择一个硬币，投掷n次，并记录向下和向下的次数。如果我们记录了每个样本所使用的硬币，那我们就有完整的信息可以估计<span
class="math inline">\(\theta_A\)</span>和<span
class="math inline">\(\theta_B\)</span>。</p>
<p>假设我们做了5次实验，每次向上的次数记录成向量<span
class="math inline">\(x\)</span>，并且使用的硬币顺序为<span
class="math inline">\(A,A,B,A,B\)</span>。他的似然函数为： <span
class="math display">\[
\begin{aligned}
    L(\theta_A,\theta_B)&amp;= p(x_1; \theta_A)\cdot p(x_2;
\theta_A)\cdot p(x_3; \theta_B) \cdot  p(x_4; \theta_A) \cdot p(x_5;
\theta_B) \\
    &amp;=B(n,\theta_A).pmf(x_1)\cdot B(n,\theta_A).pmf(x_2)\cdot
B(n,\theta_B).pmf(x_3)\cdot B(n,\theta_A).pmf(x_4)\cdot
B(n,\theta_B).pmf(x_5)
\end{aligned}
\]</span> 其中二项分布的概率计算方式为： <span class="math display">\[
\begin{aligned}
    \because X &amp; \sim B(n,p)\\
    \therefore  P(X=k) &amp;= \binom{n}{k} p^k (1-p)^{n-k} \\
        &amp;=  C_n^k p^{k}(1-p)^{n-k}
\end{aligned}
\]</span></p>
<p>将似然函数对数化：</p>
<p><span class="math display">\[
\begin{aligned}
    L(\theta_A,\theta_B)=\log p(x_1; \theta_A) + \log p(x_2; \theta_A)
+\log p(x_3; \theta_B) + \log p(x_4; \theta_A) +\log p(x_5; \theta_B)
\end{aligned}
\]</span></p>
<p><span class="math inline">\(p(x_i;
\theta)\)</span>是二项式分布的概率质量函数，其中<span
class="math inline">\(n=m,p=\theta\)</span>。我们会使用<span
class="math inline">\(z_i\)</span>来表示第<span
class="math inline">\(i\)</span>个硬币的标签。</p>
<h3 id="使用最小化函数求解似然">使用最小化函数求解似然</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core.umath_tests <span class="keyword">import</span> matrix_multiply <span class="keyword">as</span> mm</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> bernoulli, binom</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">np.random.seed(<span class="number">1234</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; 做五次实验 &quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">10</span>  <span class="comment"># 每次实验投掷10次</span></span><br><span class="line">theta_A = <span class="number">0.8</span></span><br><span class="line">theta_B = <span class="number">0.3</span></span><br><span class="line">theta_0 = [theta_A, theta_B]</span><br><span class="line"><span class="comment"># 两个硬币对应两个不同thta的二项分布</span></span><br><span class="line">coin_A = bernoulli(theta_A)</span><br><span class="line">coin_B = bernoulli(theta_B)</span><br><span class="line"></span><br><span class="line">zs = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 代表使用的硬币为哪个</span></span><br><span class="line"><span class="comment"># 得到实验结果</span></span><br><span class="line">xs = np.array([np.<span class="built_in">sum</span>(coin.rvs(n)) <span class="keyword">for</span> coin <span class="keyword">in</span> [coin_A, coin_A, coin_B, coin_A, coin_B]])</span><br><span class="line"><span class="built_in">print</span>(xs) <span class="comment"># [7 9 2 6 0]</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; 精确求解 &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 计算所有的硬币A朝上的比例作为概率</span></span><br><span class="line">ml_A = np.<span class="built_in">sum</span>(xs[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>]]) / (<span class="number">3.0</span> * n)</span><br><span class="line"><span class="comment"># 计算所有的硬币B朝上的比例作为概率</span></span><br><span class="line">ml_B = np.<span class="built_in">sum</span>(xs[[<span class="number">2</span>, <span class="number">4</span>]]) / (<span class="number">2.0</span> * n)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ml_A, ml_B)  <span class="comment"># 0.7333333333333333 0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; 数值估计 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">neg_loglik</span>(<span class="params">thetas, n, xs, zs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 对数似然计算函数 </span></span><br><span class="line"><span class="string">        这里的的二项分布是次数为n，概率可能为 theta_A 或 theta_B。</span></span><br><span class="line"><span class="string">        将每个二项分布对应x的对数概率密度函数求和后取相反数，接下来就是最小化此函数即可。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    logpmf = -np.<span class="built_in">sum</span>([binom(n, thetas[z]).logpmf(x) <span class="keyword">for</span> (x, z) <span class="keyword">in</span> <span class="built_in">zip</span>(xs, zs)])</span><br><span class="line">    <span class="keyword">return</span> logpmf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bnds = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)]</span><br><span class="line"><span class="comment"># 使用优化策略进行最小化求解</span></span><br><span class="line">res = minimize(neg_loglik, [<span class="number">0.5</span>, <span class="number">0.5</span>], args=(n, xs, zs),</span><br><span class="line">               bounds=bnds, method=<span class="string">&#x27;tnc&#x27;</span>, options=&#123;<span class="string">&#x27;maxiter&#x27;</span>: <span class="number">100</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br><span class="line"><span class="string">&quot;&quot;&quot; fun: 7.655267754139319</span></span><br><span class="line"><span class="string">     jac: array([-7.31859018e-05, -7.58504370e-05])</span></span><br><span class="line"><span class="string"> message: &#x27;Converged (|f_n-f_(n-1)| ~= 0)&#x27;</span></span><br><span class="line"><span class="string">    nfev: 17</span></span><br><span class="line"><span class="string">     nit: 6</span></span><br><span class="line"><span class="string">  status: 1</span></span><br><span class="line"><span class="string"> success: True</span></span><br><span class="line"><span class="string">       x: array([0.73333285, 0.09999965]) &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>可以发现似然估计的估计值相当近似与精确计算所得到的结论。</p>
<h2 id="当信息缺失时的最大似然">当信息缺失时的最大似然</h2>
<p>当我们没有记录实验所使用的硬币种类时，这个问题的解决就开始困难起来了。有一种解决问题的方式就是我们根据这个样本是由<span
class="math inline">\(A\)</span>或<span
class="math inline">\(B\)</span>产生的来给每个样本假设权重<span
class="math inline">\(\boldsymbol{w}_i\)</span>。直觉上来想，权重应该是<span
class="math inline">\(\boldsymbol{z}_i\)</span>的后验分布： <span
class="math display">\[
\begin{aligned}
    w_i = p(z_i \ | \ x_i; \theta)
\end{aligned}
\]</span></p>
<p>假设我们有<span
class="math inline">\(\theta\)</span>的一些估计值，如果我们知道<span
class="math inline">\(z_i\)</span>，那我们就可以估计出<span
class="math inline">\(\theta\)</span>，因为我们相当于拥有了全部的似然信息。EM算法的基本思想就是先猜测<span
class="math inline">\(\theta\)</span>，然后计算出<span
class="math inline">\(z_i\)</span>，接着继续更新<span
class="math inline">\(\theta\)</span>再计算<span
class="math inline">\(z_i\)</span>，重复数次直到收敛。</p>
<p>非书面的表述：首先考虑一个对数似然函数作为曲线(曲面)，他的<span
class="math inline">\(x\)</span>轴表示<span
class="math inline">\(\theta\)</span>。找到另一个<span
class="math inline">\(\theta\)</span>的函数<span
class="math inline">\(\boldsymbol{Q}\)</span>，他是对数似然函数的下界，在特定的<span
class="math inline">\(\theta\)</span>值时对数似然函数与函数<span
class="math inline">\(\boldsymbol{Q}\)</span>接触。接下来找到这个<span
class="math inline">\(\theta\)</span>的值，并且使函数<span
class="math inline">\(\boldsymbol{Q}\)</span>最大化，重复这个过程，是下界函数<span
class="math inline">\(\boldsymbol{Q}\)</span>和对数似然函数的最大值相同，那么我们就得到了最大对数似然～</p>
<p>从下图可以理解，每次迭代都能找到新的下界函数<span
class="math inline">\(\boldsymbol{Q}\)</span>和当前最大的对数似然，等到迭代到一定程度时，就找到了全局的最大的对数似然。</p>
<p><img
src="/2019/07/11/em-algm/14_ExpectationMaximization_21_0.png" /></p>
<p>当然，还有个问题就是如何找到这个对数似然函数的下界函数<span
class="math inline">\(\boldsymbol{Q}\)</span>，这就需要使用詹森不等式来进行数学推理。</p>
<h3 id="推导">推导</h3>
<p>在EM算法的<code>E-step</code>中，我们确定一个函数，他是对数似然的下界。
<span class="math display">\[
\begin{align}
l &amp;= \sum_i{\log p(x_i; \theta)} &amp;&amp; \text{定义对数似然函数}
\\
&amp;= \sum_i \log \sum_{z_i}{p(x_i, z_i; \theta)} &amp;&amp;
\text{在函数中加入隐变量$z$} \\
&amp;= \sum_i \log \sum_{z_i} Q_i(z_i) \frac{p(x_i, z_i;
\theta)}{Q_i(z_i)} &amp;&amp; \text{贝叶斯定理得$Q_i$为$z_i$分布} \\
&amp;= \sum_i \log E_{z_i}[\frac{p(x_i, z_i; \theta)}{Q_i(z_i)}]
&amp;&amp; \text{得到期望 - 就是EM中的E} \\
&amp;\geq \sum E_{z_i}[\log \frac{p(x_i, z_i; \theta)}{Q_i(z_i)}]
&amp;&amp; \text{使用詹森不等式计算凹的对数似然函数} \\
&amp;\geq \sum_i \sum_{z_i} Q_i(z_i) \log \frac{p(x_i, z_i;
\theta)}{Q_i(z_i)} &amp;&amp; \text{得到期望的定义}
\end{align}
\]</span></p>
<p>我们如何确定分布<span
class="math inline">\(\boldsymbol{Q_i}\)</span>？我们想要<span
class="math inline">\(\boldsymbol{Q}\)</span>函数与对数似然函数进行接触，并且也知道了詹森不等式相等的充要条件就是这个函数为常数，因此：</p>
<p><span class="math display">\[
\begin{align}
\frac{p(x_i, z_i; \theta)}{Q_i(z_i)} =&amp; c \\
\implies Q_i(z_i) &amp;\propto p(x_i, z_i; \theta)\\
\implies Q_i(z_i) &amp;= \frac{p(x_i, z_i; \theta) }{\sum_{z_i}{p(x_i,
z_i; \theta) } } &amp;&amp;\text{因为 $\boldsymbol{Q}$ 是个分布且和为1}
\\
\implies Q_i(z_i) &amp;= \frac{p(x_i, z_i; \theta) }{ {p(x_i, \theta) }
} &amp;&amp; \text{边缘化 $z_i$}\\
\implies Q_i(z_i) &amp;= p(z_i | x_i; \theta) &amp;&amp;
\text{根据条件概率定义}
\end{align}
\]</span></p>
<p>因此得到<span
class="math inline">\(\boldsymbol{Q_i}\)</span>就是<span
class="math inline">\(z_i\)</span>的后验概率，这就完成了<code>E-step</code>。</p>
<p>在<code>M-step</code>中，我找到最大化对数似然函数下界时的<span
class="math inline">\(\theta\)</span>值，然后我们迭代<code>E</code>和<code>M</code>即可。</p>
<p>所以EM算法是在缺少信息的情况下最大化似然的优化算法，或者说他可以很方便的添加隐变量来简化最大似然计算。</p>
<h3 id="例子">例子</h3>
<p>现在如果我们忘记记录了投掷硬币的顺序，那我们来求解一波A、B硬币向上的概率。
对于<code>E-step</code>,我们：</p>
<p><span class="math display">\[
\begin{align}
w_j &amp;= Q_i(z_i = j) \\
&amp;= p(z_i = j \mid x_i; \theta) \\
&amp;= \frac{p(x_i \mid z_i = j; \theta) p(z_i = j;
\phi)}  {\sum_{l=1}^k{p(x_i \mid z_i = l; \theta) p(z_i = l; \phi) }
}  &amp;&amp; \text{贝叶斯准则} \\
&amp;= \frac{\theta_j^h(1-\theta_j)^{n-h} \phi_j}{\sum_{l=1}^k
\theta_l^h(1-\theta_l)^{n-h} \phi_l} &amp;&amp; \text{代入二项分布公式}
\\
&amp;= \frac{\theta_j^h(1-\theta_j)^{n-h} }{\sum_{l=1}^k
\theta_l^h(1-\theta_l)^{n-h} } &amp;&amp; \text{为了简单起见使 $\phi$
为常数}
\end{align}
\]</span></p>
<p>对于<code>M-step</code>,我们需要找到最大时的<span
class="math inline">\(\theta\)</span>值。</p>
<p><span class="math display">\[
\begin{align}
&amp; \sum_i \sum_{z_i} Q_i(z_i) \log \frac{p(x_i, z_i;
\theta)}{Q_i(z_i)} \\
&amp;= \sum_{i=1}^m \sum_{j=1}^k w_j \log \frac{p(x_i \mid z_i=j;
\theta) \, p(z_i = j; \phi)}{w_j} \\
&amp;= \sum_{i=1}^m \sum_{j=1}^k w_j \log
\frac{\theta_j^h(1-\theta_j)^{n-h} \phi_j}{w_j} \\
&amp;= \sum_{i=1}^m \sum_{j=1}^k w_j \left( h \log \theta_j + (n-h) \log
(1-\theta_j) + \log \phi_j - \log w_j \right)
\end{align}
\]</span></p>
<p>我们使用区间的方式解决<span
class="math inline">\(\theta_s\)</span>导数消失的问题： <span
class="math display">\[
\begin{align}
\sum_{i=1}^m w_s \left( \frac{h}{\theta_s} - \frac{n-h}{1-\theta_s}
\right) &amp;= 0  \\
\implies \theta_s &amp;= \frac {\sum_{i=1}^m w_s h}{\sum_{i=1}^m w_s n}
\end{align}
\]</span></p>
<h3 id="第一种直接的代码">第一种直接的代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xs = np.array([(<span class="number">5</span>, <span class="number">5</span>), (<span class="number">9</span>, <span class="number">1</span>), (<span class="number">8</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">6</span>), (<span class="number">7</span>, <span class="number">3</span>)])</span><br><span class="line">thetas = np.array([[<span class="number">0.6</span>, <span class="number">0.4</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>]])  <span class="comment"># 初始化参数A B (向上概率，向下概率)</span></span><br><span class="line"></span><br><span class="line">tol = <span class="number">0.01</span>  <span class="comment"># 变化容忍度</span></span><br><span class="line">max_iter = <span class="number">100</span>  <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line">ll_old = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">    exp_A = []</span><br><span class="line">    exp_B = []</span><br><span class="line"></span><br><span class="line">    ll_new = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ! E-step: 计算可能的概率分布</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xs:</span><br><span class="line">        <span class="comment"># 求解当前theta下两个分布的对数似然</span></span><br><span class="line">        ll_A = np.<span class="built_in">sum</span>(x * np.log(thetas[<span class="number">0</span>]))  <span class="comment"># 多项式分布的对数似然函数 (忽略常数).</span></span><br><span class="line">        ll_B = np.<span class="built_in">sum</span>(x * np.log(thetas[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        w_A = np.exp(ll_A) / (np.exp(ll_A) + np.exp(ll_B))  <span class="comment"># 求出概率A的权重</span></span><br><span class="line">        w_B = np.exp(ll_B) / (np.exp(ll_A) + np.exp(ll_B))  <span class="comment"># 求出概率B的权重</span></span><br><span class="line"></span><br><span class="line">        exp_A.append(w_A * x)  <span class="comment"># 概率A权重乘上样本</span></span><br><span class="line">        exp_B.append(w_B * x)  <span class="comment"># 概率B权重乘上样本</span></span><br><span class="line"></span><br><span class="line">        ll_new += w_A * ll_A + w_B * ll_B  <span class="comment"># 计算当前的theta值对应的似然值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ! M-step: 为给定的分布更新当前参数</span></span><br><span class="line">    thetas[<span class="number">0</span>] = np.<span class="built_in">sum</span>(exp_A, <span class="number">0</span>) / np.<span class="built_in">sum</span>(exp_A)  <span class="comment"># 利用更新之后的样本值计算出当前A的theta值</span></span><br><span class="line">    thetas[<span class="number">1</span>] = np.<span class="built_in">sum</span>(exp_B, <span class="number">0</span>) / np.<span class="built_in">sum</span>(exp_B)  <span class="comment"># 利用更新之后的样本值计算出当前B的theta值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出每个x和当前参数估计z的分布</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Iteration: %d&quot;</span> % (i + <span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;theta_A = %.2f, theta_B = %.2f, ll = %.2f&quot;</span> % (thetas[<span class="number">0</span>, <span class="number">0</span>], thetas[<span class="number">1</span>, <span class="number">0</span>], ll_new))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">abs</span>(ll_new - ll_old) &lt; tol:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    ll_old = ll_new</span><br></pre></td></tr></table></figure>
<h3 id="结果">结果</h3>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Iteration: 1</span><br><span class="line">theta_A = 0.71, theta_B = 0.58, ll = -32.69</span><br><span class="line">Iteration: 2</span><br><span class="line">theta_A = 0.75, theta_B = 0.57, ll = -31.26</span><br><span class="line">Iteration: 3</span><br><span class="line">theta_A = 0.77, theta_B = 0.55, ll = -30.76</span><br><span class="line">Iteration: 4</span><br><span class="line">theta_A = 0.78, theta_B = 0.53, ll = -30.33</span><br><span class="line">Iteration: 5</span><br><span class="line">theta_A = 0.79, theta_B = 0.53, ll = -30.07</span><br><span class="line">Iteration: 6</span><br><span class="line">theta_A = 0.79, theta_B = 0.52, ll = -29.95</span><br><span class="line">Iteration: 7</span><br><span class="line">theta_A = 0.80, theta_B = 0.52, ll = -29.90</span><br><span class="line">Iteration: 8</span><br><span class="line">theta_A = 0.80, theta_B = 0.52, ll = -29.88</span><br><span class="line">Iteration: 9</span><br><span class="line">theta_A = 0.80, theta_B = 0.52, ll = -29.87</span><br></pre></td></tr></table></figure>
<p><strong>NOTE：</strong>
他这里的对数似然函数是根据上面的公式推导之后简化而来的，所以直接对<span
class="math inline">\(\theta\)</span>做对数即可，下面我按最基本的思路来写了一个例程。</p>
<h3 id="我的代码">我的代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = <span class="number">10</span>  <span class="comment"># 实验次数</span></span><br><span class="line">m_xs = np.array([<span class="number">5</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">7</span>])  <span class="comment"># 向上次数</span></span><br><span class="line">theta_A = <span class="number">0.6</span></span><br><span class="line">theta_B = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tol = <span class="number">0.01</span>  <span class="comment"># 变化容忍度</span></span><br><span class="line">max_iter = <span class="number">100</span>  <span class="comment"># 迭代次数</span></span><br><span class="line">loglike_old = <span class="number">0</span>  <span class="comment"># 初始对数似然值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">    cnt_A = []</span><br><span class="line">    cnt_B = []</span><br><span class="line">    loglike_new = <span class="number">0</span>  <span class="comment"># 新的对数似然值</span></span><br><span class="line">    <span class="comment"># ! E-step</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> m_xs:</span><br><span class="line">        pmf_A = binom(n, theta_A).pmf(x)  <span class="comment"># 当前theta下 A的概率</span></span><br><span class="line">        pmf_B = binom(n, theta_B).pmf(x)  <span class="comment"># 当前theta下 B的概率</span></span><br><span class="line"></span><br><span class="line">        logpmf_A = binom(n, theta_A).logpmf(x)  <span class="comment"># 当前theta下 A的对数概率</span></span><br><span class="line">        logpmf_B = binom(n, theta_B).logpmf(x)  <span class="comment"># 当前theta下 B的对数概率</span></span><br><span class="line"></span><br><span class="line">        weight_A = pmf_A / (pmf_A + pmf_B)  <span class="comment"># 求得权重</span></span><br><span class="line">        weight_B = pmf_B / (pmf_A + pmf_B)  <span class="comment"># 求得权重</span></span><br><span class="line"></span><br><span class="line">        cnt_A.append(weight_A * np.array([x, n - x]))  <span class="comment"># 概率A权重乘上样本，得到新的硬币次数统计</span></span><br><span class="line">        cnt_B.append(weight_B * np.array([x, n - x]))  <span class="comment"># 概率B权重乘上样本，得到新的硬币次数统计</span></span><br><span class="line"></span><br><span class="line">        loglike_new += weight_A * logpmf_A + weight_B * logpmf_B  <span class="comment"># 计算当前的theta值对应的似然值</span></span><br><span class="line">    <span class="comment"># ! M-step</span></span><br><span class="line">    theta_A = np.<span class="built_in">sum</span>(cnt_A, <span class="number">0</span>)[<span class="number">0</span>] / np.<span class="built_in">sum</span>(cnt_A)  <span class="comment"># 硬币A向上的次数除以总次数即为theta_A</span></span><br><span class="line">    theta_B = np.<span class="built_in">sum</span>(cnt_B, <span class="number">0</span>)[<span class="number">0</span>] / np.<span class="built_in">sum</span>(cnt_B)  <span class="comment"># 硬币B向上的次数除以总次数即为theta_B</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出每个x和当前参数估计z的分布</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Iteration: %d&quot;</span> % (i + <span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;theta_A = %.2f, theta_B = %.2f, ll = %.2f&quot;</span> % (theta_A, theta_B, loglike_new))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">abs</span>(loglike_new - loglike_old) &lt; tol:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    loglike_old = loglike_new</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="结果-1">结果</h3>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Iteration: 1</span><br><span class="line">theta_A = 0.71, theta_B = 0.58, ll = -10.91</span><br><span class="line">Iteration: 2</span><br><span class="line">theta_A = 0.75, theta_B = 0.57, ll = -9.49</span><br><span class="line">Iteration: 3</span><br><span class="line">theta_A = 0.77, theta_B = 0.55, ll = -8.99</span><br><span class="line">Iteration: 4</span><br><span class="line">theta_A = 0.78, theta_B = 0.53, ll = -8.56</span><br><span class="line">Iteration: 5</span><br><span class="line">theta_A = 0.79, theta_B = 0.53, ll = -8.30</span><br><span class="line">Iteration: 6</span><br><span class="line">theta_A = 0.79, theta_B = 0.52, ll = -8.18</span><br><span class="line">Iteration: 7</span><br><span class="line">theta_A = 0.80, theta_B = 0.52, ll = -8.13</span><br><span class="line">Iteration: 8</span><br><span class="line">theta_A = 0.80, theta_B = 0.52, ll = -8.11</span><br><span class="line">Iteration: 9</span><br><span class="line">theta_A = 0.80, theta_B = 0.52, ll = -8.10</span><br></pre></td></tr></table></figure>
<p><strong>NOTE：</strong> 就是对数似然值和例程计算的不一样。。</p>
<h2 id="混合模型">混合模型</h2>
<p>从一个简单的混合模型开始，即<code>k-means</code>，<code>k-means</code>不使用<code>EM</code>算法，但是可以结合<code>EM</code>算法帮助理解<code>EM</code>算法如何用于高斯混合模型。</p>
<h3 id="k-means">K-means</h3>
<p>这个算法比较简单，初始化选择<span
class="math inline">\(k\)</span>个中心点，然后做如下：</p>
<ol type="1">
<li>找到每个点与中心点的距离</li>
<li>给每个点分配最近的中心点</li>
<li>根据所分配的点来更新中心点位置</li>
</ol>
<p>下面给出一段程序示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core.umath_tests <span class="keyword">import</span> inner1d</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kmeans</span>(<span class="params">xs, k, max_iter=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;K-means 算法.&quot;&quot;&quot;</span></span><br><span class="line">    idx = np.random.choice(<span class="built_in">len</span>(xs), k, replace=<span class="literal">False</span>)</span><br><span class="line">    cs = xs[idx]</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        ds = np.array([inner1d(xs - c, xs - c) <span class="keyword">for</span> c <span class="keyword">in</span> cs])</span><br><span class="line">        zs = np.argmin(ds, axis=<span class="number">0</span>)</span><br><span class="line">        cs = np.array([xs[zs == i].mean(axis=<span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k)])</span><br><span class="line">    <span class="keyword">return</span> (cs, zs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iris = sns.load_dataset(<span class="string">&#x27;iris&#x27;</span>)</span><br><span class="line">data = iris.iloc[:, :<span class="number">4</span>].values</span><br><span class="line">cs, zs = kmeans(data, <span class="number">3</span>)</span><br><span class="line">iris[<span class="string">&#x27;cluster&#x27;</span>] = zs</span><br><span class="line">sns.pairplot(iris, hue=<span class="string">&#x27;cluster&#x27;</span>, diag_kind=<span class="string">&#x27;kde&#x27;</span>, <span class="built_in">vars</span>=iris.columns[:<span class="number">4</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-2">结果</h3>
<p><img src="/2019/07/11/em-algm/kmeans.png" /></p>
<h2 id="高斯混合模型">高斯混合模型</h2>
<p><span
class="math inline">\(k\)</span>个高斯分布混合具有以下的概率密度函数：
<span class="math display">\[
\begin{align}
p(x) = \sum_{j=1}^k \pi_j \phi(x; \mu_j, \sigma_j)
\end{align}
\]</span></p>
<p><span class="math inline">\(\pi_j\)</span>是第<span
class="math inline">\(j\)</span>个高斯分布的权重，且：</p>
<p><span class="math display">\[
\begin{align}
\phi(x; \mu, \sigma) = \frac{1}{(2 \pi)^{d/2}|\sigma|^{1/2 } } \exp
\left( -\frac{1}{2}(x-\mu)^T\sigma^{-1}(x-\mu) \right)
\end{align}
\]</span></p>
<p>假设我们观察<span class="math inline">\(y_1, y2, \ldots,
y_n\)</span>作为来自高斯混合模型的样本，那么对数似然为： <span
class="math display">\[
\begin{align}
l(\theta) = \sum_{i=1}^n \log \left( \sum_{j=1}^k \pi_j \phi(y_i; \mu_j,
\sigma_j) \right)
\end{align}
\]</span></p>
<p>其中<span class="math inline">\(\theta = (\pi, \mu,
\sigma)\)</span>，很难拟合这种对数似然最大值的参数，因为他们要在对数函数中求和。</p>
<h3 id="使用em算法">使用EM算法</h3>
<p>假设我们增加潜在变量<span
class="math inline">\(z\)</span>，它表明我们观察到的<span
class="math inline">\(y\)</span>来自于第<span
class="math inline">\(k\)</span>个高斯分布。其中<code>E-step</code>和<code>M-step</code>的推导与之前的例子类似，只是变量更多。</p>
<p>在<code>E-step</code>我们想要计算样本<span
class="math inline">\(x_i\)</span>输入第<span
class="math inline">\(j\)</span>个类别的后验概率，给定参数<span
class="math inline">\(\theta =(\pi,\mu,\sigma)\)</span></p>
<p><strong>NOTE：</strong> 这里后验概率有的地方使用公式<span
class="math inline">\(p(j|x_i)\)</span>,这里使用<span
class="math inline">\(w_j^i\)</span>来表示。</p>
<p><span class="math display">\[
\begin{align}
w_j^i &amp;= Q_i(z^i = j) \\
&amp;= p(z^i = j \mid y^i; \theta) \\
&amp;= \frac{p(x^i \mid z^i = j; \mu, \sigma) p(z^i = j;
\pi)}  {\sum_{l=1}^k{p(y^i \mid z^i = l; \mu, \sigma) p(z^i = l; \pi) }
}  &amp;&amp; \text{贝叶斯定理} \\
&amp;= \frac{\phi(x^i; \mu_j, \sigma_j) \pi_j}{\sum_{l=1}^k \phi(x^i;
\mu_l, \sigma_l) \pi_l}
\end{align}
\]</span></p>
<p>在<code>M-step</code>中，我们要找到<span class="math inline">\(\theta
= (w, \mu, \sigma)\)</span>来最大化<span
class="math inline">\(\boldsymbol{Q}\)</span>函数，对应与真实对数似然函数的下界。</p>
<p><span class="math display">\[
\begin{align}
\sum_{i=1}^{m}\sum_{j=1}^{k} Q(z^i=j) \log \frac{p(x^i \mid z^i= j; \mu,
\sigma) p(z^i=j; \pi)}{Q(z^i=j)}
\end{align}
\]</span></p>
<p>通过分别取<span class="math inline">\((w, \mu,
\sigma)\)</span>的导数并求解（使用拉格朗日乘数构造约束<span
class="math inline">\(\sum_{j=1}^k w_j =
1\)</span>来求解），我们得到：</p>
<p><span class="math display">\[
\begin{align}
\pi_j &amp;= \frac{1}{m} \sum_{i=1}^{m} w_j^i \\
\mu_j &amp;= \frac{\sum_{i=1}^{m} w_j^i x^i}{\sum_{i=1}^{m} w_j^i} \\
\sigma_j &amp;= \frac{\sum_{i=1}^{m} w_j^i (x^i - \mu)(x^i -
\mu)^T}{\sum_{i1}^{m} w_j^i}
\end{align}
\]</span></p>
<h3 id="代码">代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize</span>(<span class="params">xs, axis=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return normalized marirx so that sum of row or column (default) entries = 1.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> axis <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> xs / xs.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">elif</span> axis == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> xs / xs.<span class="built_in">sum</span>(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> xs / xs.<span class="built_in">sum</span>(<span class="number">1</span>)[:, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mix_mvn_pdf</span>(<span class="params">xs, pis, mus, sigmas</span>):</span><br><span class="line">    <span class="keyword">return</span> np.array([pi * multivariate_normal(mu, sigma).pdf(xs) <span class="keyword">for</span> (pi, mu, sigma) <span class="keyword">in</span> <span class="built_in">zip</span>(pis, mus, sigmas)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">em_gmm_orig</span>(<span class="params">xs, pis, mus, sigmas, tol=<span class="number">0.01</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line"></span><br><span class="line">    n, p = xs.shape</span><br><span class="line">    k = <span class="built_in">len</span>(pis)</span><br><span class="line"></span><br><span class="line">    ll_old = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        exp_A = []</span><br><span class="line">        exp_B = []</span><br><span class="line">        ll_new = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ！ E-step</span></span><br><span class="line">        ws = np.zeros((k, n))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(mus)):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="comment"># 遍历所有的 mu，sigma，pi 来计算概率密度</span></span><br><span class="line">                ws[j, i] = pis[j] * multivariate_normal(mus[j], sigmas[j]).pdf(xs[i])</span><br><span class="line">        ws /= ws.<span class="built_in">sum</span>(<span class="number">0</span>)  <span class="comment"># 根据概率密度求权值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># M-step</span></span><br><span class="line">        <span class="comment"># NOTE 下面的更新过程是根据公式来计算的！</span></span><br><span class="line">        pis = np.zeros(k)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(mus)):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                pis[j] += ws[j, i]  <span class="comment"># 使用权值更新pi</span></span><br><span class="line">        pis /= n</span><br><span class="line"></span><br><span class="line">        mus = np.zeros((k, p))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                mus[j] += ws[j, i] * xs[i]  <span class="comment"># 使用权值更新mu</span></span><br><span class="line">            mus[j] /= ws[j, :].<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        sigmas = np.zeros((k, p, p))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                ys = np.reshape(xs[i] - mus[j], (<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">                sigmas[j] += ws[j, i] * np.dot(ys, ys.T)  <span class="comment"># 使用权值更新sigma</span></span><br><span class="line">            sigmas[j] /= ws[j, :].<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新对数似然函数</span></span><br><span class="line">        ll_new = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            s = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                s += pis[j] * multivariate_normal(mus[j], sigmas[j]).pdf(xs[i])</span><br><span class="line">            ll_new += np.log(s)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">abs</span>(ll_new - ll_old) &lt; tol:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        ll_old = ll_new</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ll_new, pis, mus, sigmas</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建数据集</span></span><br><span class="line">n = <span class="number">1000</span></span><br><span class="line">_mus = np.array([[<span class="number">0</span>, <span class="number">4</span>], [-<span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line">_sigmas = np.array([[[<span class="number">3</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.5</span>]], [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>]]])</span><br><span class="line">_pis = np.array([<span class="number">0.6</span>, <span class="number">0.4</span>])</span><br><span class="line">xs = np.concatenate([np.random.multivariate_normal(mu, sigma, <span class="built_in">int</span>(pi * n))</span><br><span class="line">                     <span class="keyword">for</span> pi, mu, sigma <span class="keyword">in</span> <span class="built_in">zip</span>(_pis, _mus, _sigmas)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化预测值</span></span><br><span class="line">pis = normalize(np.random.random(<span class="number">2</span>))  <span class="comment"># pi 要经过归一化</span></span><br><span class="line">mus = np.random.random((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">sigmas = np.array([np.eye(<span class="number">2</span>)] * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用EM算法拟合</span></span><br><span class="line">ll1, pis1, mus1, sigmas1 = em_gmm_orig(xs, pis, mus, sigmas)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">intervals = <span class="number">101</span></span><br><span class="line">ys = np.linspace(-<span class="number">8</span>, <span class="number">8</span>, intervals)</span><br><span class="line">X, Y = np.meshgrid(ys, ys)</span><br><span class="line">_ys = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line"></span><br><span class="line">z = np.zeros(<span class="built_in">len</span>(_ys))</span><br><span class="line"><span class="keyword">for</span> pi, mu, sigma <span class="keyword">in</span> <span class="built_in">zip</span>(pis1, mus1, sigmas1):</span><br><span class="line">    z += pi * multivariate_normal(mu, sigma).pdf(_ys)</span><br><span class="line">z = z.reshape((intervals, intervals))</span><br><span class="line"></span><br><span class="line">ax = plt.subplot(<span class="number">111</span>)</span><br><span class="line">plt.scatter(xs[:, <span class="number">0</span>], xs[:, <span class="number">1</span>], alpha=<span class="number">0.2</span>)</span><br><span class="line">plt.contour(X, Y, z, N=<span class="number">10</span>)</span><br><span class="line">plt.axis([-<span class="number">8</span>, <span class="number">6</span>, -<span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line">ax.axes.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-3">结果</h3>
<p><img src="/2019/07/11/em-algm/mxiguass.png" /></p>
<h1 id="胶囊网络">胶囊网络</h1>
<h2 id="矩阵胶囊">矩阵胶囊</h2>
<p>现在的矩阵胶囊与之前的胶囊不是很一样了，因为胶囊是矩阵，就不能把模长作为激活了。所以多加一个标量<span
class="math inline">\(a\)</span>来表示激活值。</p>
<p>矩阵胶囊的主体，名字也变了一下，叫做姿态矩阵，这里是<span
class="math inline">\(4\times4\)</span>的矩阵。</p>
<p><img src="/2019/07/11/em-algm/capp.png" /></p>
<h2 id="胶囊投票机制">胶囊投票机制</h2>
<p>在胶囊网络中，通过寻找底层胶囊投票之间的一致性来检测一个高层特征(比如一张脸)，对于来自胶囊<span
class="math inline">\(i\)</span>对父胶囊<span
class="math inline">\(j\)</span>的投票<span
class="math inline">\(\boldsymbol{V}_{ij}\)</span>，通过将胶囊<span
class="math inline">\(i\)</span>的姿态矩阵<span
class="math inline">\(\boldsymbol{P}_i\)</span>和视角不变矩阵<span
class="math inline">\(W_{ij}\)</span>相乘得到。</p>
<p><span class="math display">\[
\begin{split}
&amp;\boldsymbol{V}_{ij} =\boldsymbol{P}_i W_{ij} \quad
\end{split}
\]</span></p>
<p>基于投票<span
class="math inline">\(\boldsymbol{V}_{ij}\)</span>和其他投票<span
class="math inline">\((\boldsymbol{V}_{o_{1}j} \ldots
\boldsymbol{V}_{o_{k}j})\)</span>相似度来计算胶囊<span
class="math inline">\(i\)</span>被分组到胶囊<span
class="math inline">\(j\)</span>中的概率。</p>
<h2 id="胶囊分配">胶囊分配</h2>
<p>EM算法对低层的胶囊进行聚类，也计算底层胶囊与上层胶囊间分配概率<span
class="math inline">\(r_{ij}\)</span>。例如一个表示手的胶囊不属于脸的胶囊，他们的分配概率就被抑止到0，<span
class="math inline">\(r_{ij}\)</span>也受到胶囊激活的影响。</p>
<p><img src="/2019/07/11/em-algm/c2.jpg" /></p>
<h2 id="计算胶囊激活和姿态矩阵">计算胶囊激活和姿态矩阵</h2>
<p>在EM聚类中，我们将数据使用正态分布来表现。在EM路由中，我们也对父胶囊使用正态分布来建模，姿态矩阵<span
class="math inline">\(M\)</span>是<span
class="math inline">\(4\times4\)</span>的矩阵，即16个分量。我们对姿态矩阵建立16个<span
class="math inline">\(\mu\)</span>,16个<span
class="math inline">\(\sigma\)</span>的高斯分布，每个<span
class="math inline">\(\mu\)</span>就表征了姿态矩阵的每个分量。注意为了避免在概率密度函数中求逆，胶囊网络中的<span
class="math inline">\(\sigma\)</span>是被固定成一个对角矩阵的。</p>
<p>让<span class="math inline">\(v_{ij}\)</span>作为底层胶囊<span
class="math inline">\(i\)</span>对父胶囊<span
class="math inline">\(j\)</span>的投票，<span
class="math inline">\(v^{n}_{ij}\)</span>代表着第<code>n</code>个分量。我们使用高斯概率密度函数</p>
<p><span class="math display">\[
\begin{split}
P(x) &amp; = \frac{1}{\sigma \sqrt{2\pi } }e^{-(x - \mu)^{2}/2\sigma^{2}
} \\
\end{split}
\]</span></p>
<p>来计算属于胶囊<span
class="math inline">\(j\)</span>正态分布的投票<span
class="math inline">\(v^{n}_{ij}\)</span>概率。 <span
class="math display">\[
\begin{split}
p^n_{i \vert j} &amp; = \frac{1}{\sqrt{2 \pi ({\sigma^n_j})^2 } }
\exp{(- \frac{(v^n_{ij}-\mu^n_j)^2}{2 ({\sigma^n_j})^2})} \\
\end{split}
\]</span></p>
<p>取其对数：</p>
<p><span class="math display">\[
\begin{split}
\ln(p^n_{i \vert j}) &amp;= \ln \frac{1}{\sqrt{2 \pi ({\sigma^n_j})^2 }
} \exp{(- \frac{(v^n_{ij}-\mu^n_j)^2}{2 ({\sigma^n_j})^2})} \\
\\
&amp;= - \ln(\sigma^n_j) - \frac{\ln(2 \pi)}{2} -
\frac{(v^n_{ij}-\mu^n_j)^2}{2 ({\sigma^n_j})^2}\\
\end{split}
\]</span></p>
<p>接下来估计胶囊激活的损失值，越低的损失值，代表胶囊越有可能被激活。如果损失值较大，就代表投票与父胶囊的高斯分布不匹配，因此激活概率较小。</p>
<p>让<span class="math inline">\(cost_{ij}\)</span>作为胶囊<span
class="math inline">\(i\)</span>激活胶囊<span
class="math inline">\(j\)</span>的损失，是负的对数似然也可以称之为<strong>熵</strong>(熵越小，那就意味<strong>似然估计越准</strong>)：
<span class="math display">\[
cost^n_{ij} = - \ln(P^n_{i \vert j})
\]</span></p>
<p>由于胶囊<span class="math inline">\(i\)</span>与胶囊<span
class="math inline">\(j\)</span>之间的联系并不相同，因此我们将在运行时按比例<span
class="math inline">\(r_{ij}\)</span>来分配<span
class="math inline">\(cost\)</span>，那下层胶囊的<span
class="math inline">\(cost\)</span>计算为：</p>
<p><span class="math display">\[
\begin{split}
cost^n_j &amp;= \sum_i  r_{ij} cost^n_{ij} \\
&amp;= \sum_i - r_{ij} \ln(p^n_{i \vert j}) \\
&amp;= \sum_i r_{ij}  \big( \frac{(v^n_{ij}-\mu^n_j)^2}{2
({\sigma^n_j})^2} + \ln(\sigma^n_j) + \frac{\ln(2 \pi)}{2} \big)\\
&amp;= \frac{\sum_i r_{ij} (\sigma^n_j)^2}{2 (\sigma^n_j)^2} +
(\ln(\sigma^n_j) + \frac{\ln(2 \pi)}{2}) \sum_i r_{ij} \\
&amp;= \big(\ln(\sigma^n_j) + \beta_v \big) \sum_i r_{ij}  \quad
\beta_v\text{是待优化参数}
\end{split}
\]</span></p>
<p>有了<span class="math inline">\(cost\)</span>值，使用<span
class="math inline">\(a_j\)</span>来决定胶囊<span
class="math inline">\(j\)</span>是否被激活： <span
class="math display">\[
a_j = sigmoid(\lambda(\beta_a - \sum_h cost^n_j))
\]</span></p>
<p>这里面的<span
class="math inline">\(\beta_a，\beta_v\)</span>是根据反向传播进行优化的，所以并不需要直接计算。</p>
<p>其中的<span
class="math inline">\(r_{ij},\mu,\sigma,a_j\)</span>是根据EM路由来计算的。<span
class="math inline">\(\lambda\)</span>是根据温度参数<span
class="math inline">\(\frac{1}{temperature}\)</span>来计算(退火策略)，随着<span
class="math inline">\(r_{ij}\)</span>越来越好，就降低<code>temperature</code>以获得更大的<span
class="math inline">\(\lambda\)</span>，也就是增加<span
class="math inline">\(sigmoid\)</span>的陡度，这样就更加在更加小的范围内微调<span
class="math inline">\(r_{ij}\)</span>。</p>
<h2 id="em路由">EM路由</h2>
<p>这一块是重点，所以我要仔细看每个公式细节。并且要一步一步的渐进的看。</p>
<h3 id="新动态路由1">新动态路由1</h3>
<p>首先用一个矩阵<span class="math inline">\(\boldsymbol{P}_{i}，
i=1,\ldots,n\)</span>来表示第<span
class="math inline">\(l\)</span>层的胶囊，用矩阵<span
class="math inline">\(\boldsymbol{M}_j，
j=1,\ldots,k\)</span>来表示第<span
class="math inline">\(l+1\)</span>层的胶囊。在做EM路由的过程中，他将<span
class="math inline">\(P_i\)</span>变成长16的向量，且协方差矩阵为对角阵。</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow
N(\boldsymbol{P}_i;\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j)
&amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{\pi_j p_{ij} }{\sum\limits_{j=1}^k\pi_j
p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{R_{ij } }{\sum\limits_{i=1}^n R_{ij } }
&amp;&amp; \text{计算归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n
r_{ij}\boldsymbol{P}_i &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n
r_{ij}(\boldsymbol{P}_i-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma
\\
    &amp;\pi_j \leftarrow \frac{1}{n}\sum\limits_{i=1}^n R_{ij}
&amp;&amp; \text{更新聚类中心}\pi
\end{aligned}
\]</span></p>
<p>这里的<span
class="math inline">\(R_{ij}\)</span>就是后验概率，也就是在<code>EM</code>算法中使用的<span
class="math inline">\(w_j\)</span>。</p>
<h3 id="新动态路由2">新动态路由2</h3>
<p>前面讲道理有一个激活标量<span
class="math inline">\(a_j\)</span>来衡量胶囊单元的显著程度，根据EM算法计算我们可以得到<span
class="math inline">\(\pi_j\)</span>为第<span
class="math inline">\(l+1\)</span>层胶囊的聚类中心，但我们不能选择<span
class="math inline">\(\pi\)</span>因为：</p>
<ol type="1">
<li><span
class="math inline">\(\pi_j\)</span>是归一化的，就是我们只想得到他的显著程度，而不是显著程度的概率。</li>
<li><span
class="math inline">\(\pi_j\)</span>不能反映出类内的元素特征是否相似。</li>
</ol>
<p>现在再看前面所使用的激活标量<span
class="math inline">\(a_j\)</span>，他既考虑了似然估计值<span
class="math inline">\(cost_j^h\)</span>,又考虑了显著程度<span
class="math inline">\(\pi_j\)</span>。所以作者直接将<span
class="math inline">\(a_j\)</span>替换了<span
class="math inline">\(\pi_j\)</span>，这样虽然不完全与原始EM算法相同，但是也能收敛，得到新的动态路由：</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow
N(\boldsymbol{P}_i;\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j)
&amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{a_j p_{ij} }{\sum\limits_{j=1}^k a_j
p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{R_{ij } }{\sum\limits_{i=1}^n R_{ij } }
&amp;&amp; \text{归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n
r_{ij}\boldsymbol{P}_i &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n
r_{ij}(\boldsymbol{P}_i-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma
\\
   &amp; cost_j \leftarrow \left(\beta_v+\sum\limits_{l=1}^d \ln
\boldsymbol{\sigma}_j^l \right)\sum\limits_i r_{ij} &amp;&amp;
\text{计算熵} \\
   &amp; a_j \leftarrow sigmoid(\lambda(\beta_a - \sum_h cost^h_j))
&amp;&amp; \text{更新}a_j
\end{aligned}
\]</span></p>
<h3 id="新动态路由3">新动态路由3</h3>
<p>上面好像没什么问题。但是没有使用底层胶囊的激活值<span
class="math inline">\(a^{last}_i\)</span>，作者在计算归一化后验概率<span
class="math inline">\(r_{ij}\leftarrow \frac{R_{ij }
}{\sum\limits_{i=1}^n R_{ij } }\)</span>的时候加入：</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow
N(\boldsymbol{P}_i;\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j)
&amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{a_j p_{ij} }{\sum\limits_{j=1}^k a_j
p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{a_i^{last}R_{ij } }{\sum\limits_{i=1}^n
a_i^{last} R_{ij } } &amp;&amp; \text{计算归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n
r_{ij}\boldsymbol{P}_i &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n
r_{ij}(\boldsymbol{P}_i-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma
\\
   &amp; cost_j \leftarrow \left(\beta_v+\sum\limits_{l=1}^d \ln
\boldsymbol{\sigma}_j^l \right)\sum\limits_i r_{ij} &amp;&amp;
\text{计算熵} \\
   &amp; a_j \leftarrow sigmoid(\lambda(\beta_a - \sum_h cost^h_j))
&amp;&amp; \text{更新}a_j
\end{aligned}
\]</span></p>
<h2 id="新动态路由4">新动态路由4</h2>
<p>因为<span class="math inline">\(\sum\limits_i
r_{ij}=1\)</span>，所以下面还可以化简。</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow
N(\boldsymbol{P}_i;\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j)
&amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{a_j p_{ij} }{\sum\limits_{j=1}^k a_j
p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{a_i^{last}R_{ij } }{\sum\limits_{i=1}^n
a_i^{last} R_{ij } } &amp;&amp; \text{计算归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n
r_{ij}\boldsymbol{P}_i &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n
r_{ij}(\boldsymbol{P}_i-\boldsymbol{M}_j)^2 &amp;&amp; \text{更新}\sigma
\\
   &amp; cost_j \leftarrow \left(\beta_v+\sum\limits_{l=1}^d \ln
\boldsymbol{\sigma}_j^l \right)\sum\limits_i a_i^{last} R_{ij}
&amp;&amp; \text{计算熵} \\
   &amp; a_j \leftarrow sigmoid(\lambda(\beta_a - \sum_h cost^h_j))
&amp;&amp; \text{更新}a_j
\end{aligned}
\]</span></p>
<h2 id="最终的动态路由">最终的动态路由</h2>
<p>现在再把投票机制与视角不变矩阵加入进来,投票矩阵与上面描述一样<span
class="math inline">\(\boldsymbol{V}_{ij}=\boldsymbol{P}_{i}\boldsymbol{W}_{ij}\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;p_{ij} \leftarrow
N(\boldsymbol{V}_{ij};\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j)
&amp;&amp; \text{计算概率密度} \\  
    &amp;R_{ij} \leftarrow \frac{a_j p_{ij} }{\sum\limits_{j=1}^k a_j
p_{ij} } &amp;&amp; \text{计算后验概率} \\
    &amp;r_{ij}\leftarrow \frac{a_i^{last}R_{ij } }{\sum\limits_{i=1}^n
a_i^{last} R_{ij } } &amp;&amp; \text{计算归一化后验概率} \\
    &amp;\boldsymbol{M}_j \leftarrow \sum\limits_{i=1}^n
r_{ij}\boldsymbol{V}_{ij} &amp;&amp; \text{计算新样本} \\
    &amp;\boldsymbol{\sigma}^2_j \leftarrow \sum\limits_{i=1}^n
r_{ij}(\boldsymbol{V}_{ij}-\boldsymbol{M}_j)^2 &amp;&amp;
\text{更新}\sigma \\
   &amp; cost_j \leftarrow \left(\beta_v+\sum\limits_{l=1}^d \ln
\boldsymbol{\sigma}_j^l \right)\sum\limits_i a_i^{last} R_{ij}
&amp;&amp; \text{计算熵} \\
   &amp; a_j \leftarrow sigmoid(\lambda(\beta_a - \sum_h cost^h_j))
&amp;&amp; \text{更新}a_j
\end{aligned}
\]</span></p>
<h2 id="代码-1">代码</h2>
<h3 id="论文中的伪代码">论文中的伪代码</h3>
<p>官方的<code>EM路由</code>的顺序是和之前使用的<code>EM</code>算法不一样的，他先做<code>M-step</code>再做<code>E-step</code>，因为一开始我们是不知道父胶囊的分布，所以我们没有办法通过概率密度函数来计算后验分布，他这里比较简单粗暴，直接先把后验概率<span
class="math inline">\(R_{ij}\)</span>分配为1，再来算分配权重矩阵<span
class="math inline">\(r_{ij}\)</span>。 <img
src="/2019/07/11/em-algm/em.png" /></p>
<p>在<code>M-step</code>中计算出父胶囊正态分布的<span
class="math inline">\(\mu,\sigma\)</span>。然后计算熵<span
class="math inline">\(cost\)</span>，再计算激活。</p>
<p><strong>NOTE：</strong> 在论文或者<code>tensorflow</code>里面，<span
class="math inline">\(log\)</span>默认都是以<span
class="math inline">\(e\)</span>为底的。并且这里的<code>h</code>就是我上面用的<code>n</code></p>
<p><img src="/2019/07/11/em-algm/em-m.png" /></p>
<p>在<code>E-step</code>中，我们有了之前的父胶囊的<span
class="math inline">\(\mu,\sigma\)</span>，那我们根据正态分布公式计算概率密度<span
class="math inline">\(p_{ij}\)</span>，然后贝叶斯公式更新后验概率<span
class="math inline">\(R_{ij}\)</span>。这样一个循环就形成了。 <img
src="/2019/07/11/em-algm/em-e.png" /></p>
<h3 id="python实现">python实现</h3>
<p>使用<code>Tensorflow 1.14</code>直接运行即可。主要就是看<code>EM</code>算法的实现部分，当然这里在计算<span
class="math inline">\(a_j\)</span>的时候，用的是迂回的方式来计算，避免值溢出。我再尝试过这个算法之后，感觉真的不咋地，思路很高深，但是效果没有那些简单粗暴的算法好，我还是喜欢谷歌提出一些网络，虽然没那么多数学原理，但是很直接有效。不过这个笔记对我拿来写论文还是很有帮助的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow.python <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.layers <span class="keyword">import</span> xavier_initializer</span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">1e-9</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">inputs, kernel, out_channels, stride, padding, name, is_train=<span class="literal">True</span>, activation_fn=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d], trainable=is_train):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">            output = slim.conv2d(inputs,</span><br><span class="line">                                 num_outputs=out_channels,</span><br><span class="line">                                 kernel_size=[kernel, kernel], stride=stride, padding=padding,</span><br><span class="line">                                 scope=scope, activation_fn=activation_fn)</span><br><span class="line">            tf.logging.info(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> output shape: <span class="subst">&#123;output.get_shape()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">primary_caps</span>(<span class="params">inputs, kernel_size, out_capsules, stride, padding, pose_shape, name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;This constructs a primary capsule layer using regular convolution layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs: shape (N, H, W, C) (?, 14, 14, 32)</span></span><br><span class="line"><span class="string">    :param kernel_size: Apply a filter of [kernel, kernel] [5x5]</span></span><br><span class="line"><span class="string">    :param out_capsules: # of output capsule (32)</span></span><br><span class="line"><span class="string">    :param stride: 1, 2, or ... (1)</span></span><br><span class="line"><span class="string">    :param padding: padding: SAME or VALID.</span></span><br><span class="line"><span class="string">    :param pose_shape: (4, 4)</span></span><br><span class="line"><span class="string">    :param name: scope name</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: (P, a), (P (?, 14, 14, 32, 4, 4), a (?, 14, 14, 32))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment"># Generate the P matrics for the 32 output capsules</span></span><br><span class="line">        P = conv2d(</span><br><span class="line">            inputs,</span><br><span class="line">            kernel_size,</span><br><span class="line">            out_capsules * pose_shape[<span class="number">0</span>] * pose_shape[<span class="number">1</span>],</span><br><span class="line">            stride,</span><br><span class="line">            padding=padding,</span><br><span class="line">            name=<span class="string">&#x27;pose_stacked&#x27;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        input_shape = inputs.get_shape()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Reshape 16 scalar values into a 4x4 matrix</span></span><br><span class="line">        P = tf.reshape(</span><br><span class="line">            P, shape=[-<span class="number">1</span>, input_shape[-<span class="number">3</span>], input_shape[-<span class="number">2</span>], out_capsules, pose_shape[<span class="number">0</span>], pose_shape[<span class="number">1</span>]],</span><br><span class="line">            name=<span class="string">&#x27;P&#x27;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate the activation for the 32 output capsules</span></span><br><span class="line">        a = conv2d(</span><br><span class="line">            inputs,</span><br><span class="line">            kernel_size,</span><br><span class="line">            out_capsules,</span><br><span class="line">            stride,</span><br><span class="line">            padding=padding,</span><br><span class="line">            activation_fn=tf.sigmoid,</span><br><span class="line">            name=<span class="string">&#x27;activation&#x27;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        tf.summary.histogram(</span><br><span class="line">            <span class="string">&#x27;a&#x27;</span>, a</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># P (?, 14, 14, 32, 4, 4), a (?, 14, 14, 32)</span></span><br><span class="line">    <span class="keyword">return</span> P, a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernel_tile</span>(<span class="params"><span class="built_in">input</span>, kernel, stride</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;This constructs a primary capsule layer using regular convolution layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs: shape (?, 14, 14, 32, 4, 4)</span></span><br><span class="line"><span class="string">    :param kernel: 3</span></span><br><span class="line"><span class="string">    :param stride: 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return output: (?, 5, 5, 3x3=9, 136)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># (?, 14, 14, 32x(16)=512)</span></span><br><span class="line">    input_shape = <span class="built_in">input</span>.get_shape()</span><br><span class="line">    size = input_shape[<span class="number">4</span>] * input_shape[<span class="number">5</span>] <span class="keyword">if</span> <span class="built_in">len</span>(input_shape) &gt; <span class="number">5</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">    <span class="built_in">input</span> = tf.reshape(<span class="built_in">input</span>, shape=[-<span class="number">1</span>, input_shape[<span class="number">1</span>], input_shape[<span class="number">2</span>], input_shape[<span class="number">3</span>] * size])</span><br><span class="line"></span><br><span class="line">    input_shape = <span class="built_in">input</span>.get_shape()</span><br><span class="line">    <span class="comment"># (3, 3, 512, 9)</span></span><br><span class="line">    tile_filter = np.zeros(shape=[kernel, kernel, input_shape[<span class="number">3</span>],</span><br><span class="line">                                  kernel * kernel], dtype=np.float32)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(kernel):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(kernel):</span><br><span class="line">            tile_filter[i, j, :, i * kernel + j] = <span class="number">1.0</span>  <span class="comment"># (3, 3, 512, 9)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># (3, 3, 512, 9)</span></span><br><span class="line">    tile_filter_op = tf.constant(tile_filter, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (?, 6, 6, 4608)</span></span><br><span class="line">    output = tf.nn.depthwise_conv2d(<span class="built_in">input</span>, tile_filter_op, strides=[</span><br><span class="line">                                    <span class="number">1</span>, stride, stride, <span class="number">1</span>], padding=<span class="string">&#x27;VALID&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    output_shape = output.get_shape()</span><br><span class="line">    output = tf.reshape(output, shape=[-<span class="number">1</span>, output_shape[<span class="number">1</span>], output_shape[<span class="number">2</span>], input_shape[<span class="number">3</span>], kernel * kernel])</span><br><span class="line">    output = tf.transpose(output, perm=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (?, 6, 6, 9, 512)</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># import tensorflow.python as tf</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mat_transform</span>(<span class="params">inputs, output_cap_size, size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the vote.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs: shape (size, 288, 16)</span></span><br><span class="line"><span class="string">    :param output_cap_size: 32</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return V: (24, 5, 5, 3x3=9, 136)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    caps_num_i = <span class="built_in">int</span>(inputs.get_shape()[<span class="number">1</span>])  <span class="comment"># 288</span></span><br><span class="line">    P = tf.reshape(inputs, shape=[size, caps_num_i, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>])  <span class="comment"># (size, 288, 1, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    W = slim.variable(<span class="string">&#x27;W&#x27;</span>, shape=[<span class="number">1</span>, caps_num_i, output_cap_size, <span class="number">4</span>, <span class="number">4</span>], dtype=tf.float32,</span><br><span class="line">                      initializer=tf.truncated_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>))  <span class="comment"># (1, 288, 32, 4, 4)</span></span><br><span class="line">    W = tf.tile(W, [size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># (24, 288, 32, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    P = tf.tile(P, [<span class="number">1</span>, <span class="number">1</span>, output_cap_size, <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># (size, 288, 32, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    V = tf.matmul(P, W)  <span class="comment"># (24, 288, 32, 4, 4)</span></span><br><span class="line">    V = tf.reshape(V, [size, caps_num_i, output_cap_size, <span class="number">16</span>])  <span class="comment"># (size, 288, 32, 16)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> V</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">coord_addition</span>(<span class="params">V, H, W</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Coordinate addition.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param V: (24, 4, 4, 32, 10, 16)</span></span><br><span class="line"><span class="string">    :param H, W: spaital height and width 4</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return V: (24, 4, 4, 32, 10, 16)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    coordinate_offset_hh = tf.reshape(</span><br><span class="line">        (tf.<span class="built_in">range</span>(H, dtype=tf.float32) + <span class="number">0.50</span>) / H, [<span class="number">1</span>, H, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    )</span><br><span class="line">    coordinate_offset_h0 = tf.constant(</span><br><span class="line">        <span class="number">0.0</span>, shape=[<span class="number">1</span>, H, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=tf.float32</span><br><span class="line">    )</span><br><span class="line">    coordinate_offset_h = tf.stack(</span><br><span class="line">        [coordinate_offset_hh, coordinate_offset_h0] + [coordinate_offset_h0 <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">14</span>)], axis=-<span class="number">1</span></span><br><span class="line">    )  <span class="comment"># (1, 4, 1, 1, 1, 16)</span></span><br><span class="line"></span><br><span class="line">    coordinate_offset_ww = tf.reshape(</span><br><span class="line">        (tf.<span class="built_in">range</span>(W, dtype=tf.float32) + <span class="number">0.50</span>) / W, [<span class="number">1</span>, <span class="number">1</span>, W, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    )</span><br><span class="line">    coordinate_offset_w0 = tf.constant(</span><br><span class="line">        <span class="number">0.0</span>, shape=[<span class="number">1</span>, <span class="number">1</span>, W, <span class="number">1</span>, <span class="number">1</span>], dtype=tf.float32</span><br><span class="line">    )</span><br><span class="line">    coordinate_offset_w = tf.stack(</span><br><span class="line">        [coordinate_offset_w0, coordinate_offset_ww] + [coordinate_offset_w0 <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">14</span>)], axis=-<span class="number">1</span></span><br><span class="line">    )  <span class="comment"># (1, 1, 4, 1, 1, 16)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># (24, 4, 4, 32, 10, 16)</span></span><br><span class="line">    V = V + coordinate_offset_h + coordinate_offset_w</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> V</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_capsule</span>(<span class="params">inputs, shape, strides, iterations, batch_size, name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; This constructs a convolution capsule layer from a primary or convolution capsule layer.</span></span><br><span class="line"><span class="string">        i: input capsules (32)</span></span><br><span class="line"><span class="string">        o: output capsules (32)</span></span><br><span class="line"><span class="string">        batch size: 24</span></span><br><span class="line"><span class="string">        spatial dimension: 14x14</span></span><br><span class="line"><span class="string">        kernel: 3x3</span></span><br><span class="line"><span class="string">    :param inputs: a primary or convolution capsule layer with poses and a_j</span></span><br><span class="line"><span class="string">           pose: (24, 14, 14, 32, 4, 4)</span></span><br><span class="line"><span class="string">           activation: (24, 14, 14, 32)</span></span><br><span class="line"><span class="string">    :param shape: the shape of convolution operation kernel, [kh, kw, i, o] = (3, 3, 32, 32)</span></span><br><span class="line"><span class="string">    :param strides: often [1, 2, 2, 1] (stride 2), or [1, 1, 1, 1] (stride 1).</span></span><br><span class="line"><span class="string">    :param iterations: number of iterations in EM routing. 3</span></span><br><span class="line"><span class="string">    :param name: name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: (poses, a_j).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    P, a_last = inputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line"></span><br><span class="line">        stride = strides[<span class="number">1</span>]  <span class="comment"># 2</span></span><br><span class="line">        i_size = shape[-<span class="number">2</span>]  <span class="comment"># 32</span></span><br><span class="line">        o_size = shape[-<span class="number">1</span>]  <span class="comment"># 32</span></span><br><span class="line">        pose_size = P.get_shape()[-<span class="number">1</span>]  <span class="comment"># 4</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Tile the input capusles&#x27; pose matrices to the spatial dimension of the output capsules</span></span><br><span class="line">        <span class="comment"># Such that we can later multiple with the transformation matrices to generate the V.</span></span><br><span class="line">        P = kernel_tile(P, <span class="number">3</span>, stride)  <span class="comment"># (?, 14, 14, 32, 4, 4) -&gt; (?, 6, 6, 3x3=9, 32x16=512)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Tile the a_j needed for the EM routing</span></span><br><span class="line">        a_last = kernel_tile(a_last, <span class="number">3</span>, stride)  <span class="comment"># (?, 14, 14, 32) -&gt; (?, 6, 6, 9, 32)</span></span><br><span class="line">        spatial_size = <span class="built_in">int</span>(a_last.get_shape()[<span class="number">1</span>])  <span class="comment"># 6</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Reshape it for later operations</span></span><br><span class="line">        P = tf.reshape(P, shape=[-<span class="number">1</span>, <span class="number">3</span> * <span class="number">3</span> * i_size, <span class="number">16</span>])  <span class="comment"># (?, 9x32=288, 16)</span></span><br><span class="line">        a_last = tf.reshape(a_last, shape=[-<span class="number">1</span>, spatial_size, spatial_size, <span class="number">3</span> * <span class="number">3</span> * i_size])  <span class="comment"># (?, 6, 6, 9x32=288)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;V&#x27;</span>) <span class="keyword">as</span> scope:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Generate the V by multiply it with the transformation matrices</span></span><br><span class="line">            V = mat_transform(P, o_size, size=batch_size * spatial_size * spatial_size)  <span class="comment"># (864, 288, 32, 16)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Reshape the vote for EM routing</span></span><br><span class="line">            V_shape = V.get_shape()</span><br><span class="line">            V = tf.reshape(</span><br><span class="line">                V,</span><br><span class="line">                shape=[batch_size, spatial_size,</span><br><span class="line">                       spatial_size, V_shape[-<span class="number">3</span>],</span><br><span class="line">                       V_shape[-<span class="number">2</span>], V_shape[-<span class="number">1</span>]])  <span class="comment"># (24, 6, 6, 288, 32, 16)</span></span><br><span class="line">            tf.logging.info(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> V shape: <span class="subst">&#123;V.get_shape()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;routing&#x27;</span>) <span class="keyword">as</span> scope:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># beta_v and beta_a one for each output capsule: (1, 1, 1, 32)</span></span><br><span class="line">            beta_v = tf.get_variable(</span><br><span class="line">                name=<span class="string">&#x27;beta_v&#x27;</span>, shape=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, o_size], dtype=tf.float32,</span><br><span class="line">                initializer=xavier_initializer()</span><br><span class="line">            )</span><br><span class="line">            beta_a = tf.get_variable(</span><br><span class="line">                name=<span class="string">&#x27;beta_a&#x27;</span>, shape=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, o_size], dtype=tf.float32,</span><br><span class="line">                initializer=xavier_initializer()</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Use EM routing to compute the pose and activation</span></span><br><span class="line">            <span class="comment"># V (24, 6, 6, 3x3x32=288, 32, 16), a_last (?, 6, 6, 288)</span></span><br><span class="line">            <span class="comment"># poses (24, 6, 6, 32, 16), activation (24, 6, 6, 32)</span></span><br><span class="line">            M, a_j = matrix_capsules_em_routing(</span><br><span class="line">                V, a_last, beta_v, beta_a, iterations, name=<span class="string">&#x27;em_routing&#x27;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Reshape it back to 4x4 pose matrix</span></span><br><span class="line">            poses_shape = M.get_shape()</span><br><span class="line">            <span class="comment"># (24, 6, 6, 32, 4, 4)</span></span><br><span class="line">            M = tf.reshape(</span><br><span class="line">                M, [</span><br><span class="line">                    poses_shape[<span class="number">0</span>], poses_shape[<span class="number">1</span>], poses_shape[<span class="number">2</span>], poses_shape[<span class="number">3</span>], pose_size, pose_size</span><br><span class="line">                ]</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        tf.logging.info(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> pose shape: <span class="subst">&#123;M.get_shape()&#125;</span>&quot;</span>)</span><br><span class="line">        tf.logging.info(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> a_j shape: <span class="subst">&#123;a_j.get_shape()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> M, a_j</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">class_capsules</span>(<span class="params">inputs, num_classes, iterations, batch_size, name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param inputs: ((24, 4, 4, 32, 4, 4), (24, 4, 4, 32))</span></span><br><span class="line"><span class="string">    :param num_classes: 10</span></span><br><span class="line"><span class="string">    :param iterations: 3</span></span><br><span class="line"><span class="string">    :param batch_size: 24</span></span><br><span class="line"><span class="string">    :param name:</span></span><br><span class="line"><span class="string">    :return poses, a_j: poses (24, 10, 4, 4), activation (24, 10).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    P, a_last = inputs  <span class="comment"># (24, 4, 4, 32, 4, 4), (24, 4, 4, 32)</span></span><br><span class="line"></span><br><span class="line">    P_shape = P.get_shape()</span><br><span class="line">    spatial_size = <span class="built_in">int</span>(P_shape[<span class="number">1</span>])  <span class="comment"># 4</span></span><br><span class="line">    pose_size = <span class="built_in">int</span>(P_shape[-<span class="number">1</span>])    <span class="comment"># 4</span></span><br><span class="line">    i_size = <span class="built_in">int</span>(P_shape[<span class="number">3</span>])        <span class="comment"># 32</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># P (24*4*4=384, 32, 16)</span></span><br><span class="line">    P = tf.reshape(P, shape=[batch_size * spatial_size * spatial_size, P_shape[-<span class="number">3</span>], P_shape[-<span class="number">2</span>] * P_shape[-<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;V&#x27;</span>) <span class="keyword">as</span> scope:</span><br><span class="line">            <span class="comment"># P (384, 32, 16)</span></span><br><span class="line">            <span class="comment"># V: (384, 32, 10, 16)</span></span><br><span class="line">            V = mat_transform(P, num_classes, size=batch_size * spatial_size * spatial_size)</span><br><span class="line">            tf.logging.info(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> V shape: <span class="subst">&#123;V.get_shape()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># V (24, 4, 4, 32, 10, 16)</span></span><br><span class="line">            V = tf.reshape(V, shape=[batch_size, spatial_size, spatial_size, i_size, num_classes, pose_size * pose_size])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (24, 4, 4, 32, 10, 16)</span></span><br><span class="line">            V = coord_addition(V, spatial_size, spatial_size)</span><br><span class="line"></span><br><span class="line">            tf.logging.info(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> V shape with coord addition: <span class="subst">&#123;V.get_shape()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;routing&#x27;</span>) <span class="keyword">as</span> scope:</span><br><span class="line">            <span class="comment"># beta_v and beta_a one for each output capsule: (1, 10)</span></span><br><span class="line">            beta_v = tf.get_variable(</span><br><span class="line">                name=<span class="string">&#x27;beta_v&#x27;</span>, shape=[<span class="number">1</span>, num_classes], dtype=tf.float32,</span><br><span class="line">                initializer=xavier_initializer()</span><br><span class="line">            )</span><br><span class="line">            beta_a = tf.get_variable(</span><br><span class="line">                name=<span class="string">&#x27;beta_a&#x27;</span>, shape=[<span class="number">1</span>, num_classes], dtype=tf.float32,</span><br><span class="line">                initializer=xavier_initializer()</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># V (24, 4, 4, 32, 10, 16) -&gt; (24, 512, 10, 16)</span></span><br><span class="line">            V_shape = V.get_shape()</span><br><span class="line">            V = tf.reshape(V, shape=[batch_size, V_shape[<span class="number">1</span>] * V_shape[<span class="number">2</span>] * V_shape[<span class="number">3</span>], V_shape[<span class="number">4</span>], V_shape[<span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># a_last (24, 4, 4, 32) -&gt; (24, 512)</span></span><br><span class="line">            a_last = tf.reshape(a_last, shape=[batch_size,</span><br><span class="line">                                               V_shape[<span class="number">1</span>] * V_shape[<span class="number">2</span>] * V_shape[<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># V (24, 512, 10, 16), a_last (24, 512)</span></span><br><span class="line">            <span class="comment"># poses (24, 10, 16), activation (24, 10)</span></span><br><span class="line">            M, a_j = matrix_capsules_em_routing(</span><br><span class="line">                V, a_last, beta_v, beta_a, iterations, name=<span class="string">&#x27;em_routing&#x27;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># M (24, 10, 16) -&gt; (24, 10, 4, 4)</span></span><br><span class="line">        M = tf.reshape(M, shape=[batch_size, num_classes, pose_size, pose_size])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># M (24, 10, 4, 4), activation (24, 10)</span></span><br><span class="line">        <span class="keyword">return</span> M, a_j</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_capsules_em_routing</span>(<span class="params">V, a_last, beta_v, beta_a, iterations, name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;The EM routing between input capsules (i) and output capsules (j).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param V: (N, OH, OW, kh x kw x i, o, 4 x 4) = (24, 6, 6, 3x3*32=288, 32, 16)</span></span><br><span class="line"><span class="string">    :param i_activation: activation from Level L (24, 6, 6, 288)</span></span><br><span class="line"><span class="string">    :param beta_v: (1, 1, 1, 32)</span></span><br><span class="line"><span class="string">    :param beta_a: (1, 1, 1, 32)</span></span><br><span class="line"><span class="string">    :param iterations: number of iterations in EM routing, often 3.</span></span><br><span class="line"><span class="string">    :param name: name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: (pose, activation) of output capsules.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    V_shape = V.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Match R_ij (routing assignment) shape, a_last shape with V shape for broadcasting in EM routing</span></span><br><span class="line">        <span class="comment"># R_ij 就是后验概率</span></span><br><span class="line">        <span class="comment"># R_ij: [3x3x32=288, 32, 1]</span></span><br><span class="line">        <span class="comment"># R_ij: routing matrix from each input capsule (i) to each output capsule (o)</span></span><br><span class="line">        R_ij = tf.constant(</span><br><span class="line">            <span class="number">1.0</span> / V_shape[-<span class="number">2</span>], shape=V_shape[-<span class="number">3</span>:-<span class="number">1</span>] + [<span class="number">1</span>], dtype=tf.float32</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># a_last: expand_dims to (24, 6, 6, 288, 1, 1)</span></span><br><span class="line">        a_last = a_last[..., tf.newaxis, tf.newaxis]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># beta_v and beta_a: expand_dims to (1, 1, 1, 1, 32, 1]</span></span><br><span class="line">        beta_v = beta_v[..., tf.newaxis, :, tf.newaxis]</span><br><span class="line">        beta_a = beta_a[..., tf.newaxis, :, tf.newaxis]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">m_step</span>(<span class="params">R_ij, V, a_last, beta_v, beta_a, inverse_temperature</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;The M-Step in EM Routing from input capsules i to output capsule j.</span></span><br><span class="line"><span class="string">            i: input capsules (32)</span></span><br><span class="line"><span class="string">            o: output capsules (32)</span></span><br><span class="line"><span class="string">            h: 4x4 = 16</span></span><br><span class="line"><span class="string">            output spatial dimension: 6x6</span></span><br><span class="line"><span class="string">            :param R_ij: routing assignments. shape = (kh x kw x i, o, 1) =(3x3x32, 32, 1) = (288, 32, 1)</span></span><br><span class="line"><span class="string">            :param V. shape = (N, OH, OW, kh x kw x i, o, 4x4) = (24, 6, 6, 288, 32, 16)</span></span><br><span class="line"><span class="string">            :param a_last: input capsule activation (at Level L). (N, OH, OW, kh x kw x i, 1, 1) = (24, 6, 6, 288, 1, 1)</span></span><br><span class="line"><span class="string">               with dimensions expanded to match V for broadcasting.</span></span><br><span class="line"><span class="string">            :param beta_v: Trainable parameters in computing cost (1, 1, 1, 1, 32, 1)</span></span><br><span class="line"><span class="string">            :param beta_a: Trainable parameters in computing next level activation (1, 1, 1, 1, 32, 1)</span></span><br><span class="line"><span class="string">            :param inverse_temperature: lambda, increase over each iteration by the caller.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            :return: (M, sigma, o_activation)</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 用于计算归一化后验概率的零时变量</span></span><br><span class="line">            R_ij_a_last = R_ij * a_last</span><br><span class="line">            <span class="comment"># R_ij_a_last_sum: sum over all input capsule i</span></span><br><span class="line">            R_ij_a_last_sum = tf.reduce_sum(R_ij_a_last, axis=-<span class="number">3</span>, keepdims=<span class="literal">True</span>, name=<span class="string">&#x27;R_ij_a_last_sum&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算聚类中心,也就是新样本</span></span><br><span class="line">            M = tf.reduce_sum(R_ij_a_last * V, axis=-<span class="number">3</span>, keepdims=<span class="literal">True</span>) / R_ij_a_last_sum</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算输出方差sigma:  sigma (24, 6, 6, 1, 32, 16)</span></span><br><span class="line">            sigma = tf.sqrt(</span><br><span class="line">                tf.reduce_sum(</span><br><span class="line">                    R_ij_a_last * tf.square(V - M), axis=-<span class="number">3</span>, keepdims=<span class="literal">True</span></span><br><span class="line">                ) / R_ij_a_last_sum</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算每component的差异 cost_n: (24, 6, 6, 1, 32, 16)</span></span><br><span class="line">            cost_n = (beta_v + tf.log(sigma + epsilon)) * R_ij_a_last_sum</span><br><span class="line"></span><br><span class="line">            <span class="comment"># cost: (24, 6, 6, 1, 32, 1)</span></span><br><span class="line">            <span class="comment"># 计算求和得到熵</span></span><br><span class="line">            cost = tf.reduce_sum(cost_n, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># NOTE 为了数值上的稳定性 计算输出激活值a_j的时候,利用分布之间的差异来算</span></span><br><span class="line">            cost_mu = tf.reduce_mean(cost, axis=-<span class="number">2</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># 这是熵的sigma</span></span><br><span class="line">            cost_sigma = tf.sqrt(</span><br><span class="line">                tf.reduce_sum(</span><br><span class="line">                    tf.square(cost - cost_mu), axis=-<span class="number">2</span>, keepdims=<span class="literal">True</span></span><br><span class="line">                ) / cost.get_shape().as_list()[-<span class="number">2</span>]</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算激活值之间的差异性  a_cost = (24, 6, 6, 1, 32, 1)</span></span><br><span class="line">            a_cost = beta_a + (cost_mu - cost) / (cost_sigma + epsilon)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 归一化激活值 (24, 6, 6, 1, 32, 1)</span></span><br><span class="line">            a_j = tf.sigmoid(inverse_temperature * a_cost)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> M, sigma, a_j</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">e_step</span>(<span class="params">M, sigma, a_j, V</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;The E-Step in EM Routing.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            :param M: (24, 6, 6, 1, 32, 16)</span></span><br><span class="line"><span class="string">            :param sigma: (24, 6, 6, 1, 32, 16)</span></span><br><span class="line"><span class="string">            :param a_j: (24, 6, 6, 1, 32, 1)</span></span><br><span class="line"><span class="string">            :param V: (24, 6, 6, 288, 32, 16)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            :return: R_ij</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">            o_p_unit0 = - tf.reduce_sum(tf.square(V - M) / (<span class="number">2</span> * tf.square(sigma)), axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">            o_p_unit2 = - tf.reduce_sum(tf.log(sigma + epsilon), axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 求出概率 p_ij</span></span><br><span class="line">            <span class="comment"># (24, 6, 6, 1, 32, 16)</span></span><br><span class="line">            p_ij = o_p_unit0 + o_p_unit2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># R_ij: (24, 6, 6, 288, 32, 1)</span></span><br><span class="line">            zz = tf.log(a_j + epsilon) + p_ij</span><br><span class="line">            <span class="comment"># 求出后验概率</span></span><br><span class="line">            R_ij = tf.nn.softmax(zz, dim=<span class="built_in">len</span>(zz.get_shape().as_list()) - <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> R_ij</span><br><span class="line"></span><br><span class="line">        <span class="comment"># inverse_temperature schedule (min, max)</span></span><br><span class="line">        it_min = <span class="number">1.0</span></span><br><span class="line">        it_max = <span class="built_in">min</span>(iterations, <span class="number">3.0</span>)</span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">            inverse_temperature = it_min + (it_max - it_min) * it / <span class="built_in">max</span>(<span class="number">1.0</span>, iterations - <span class="number">1.0</span>)</span><br><span class="line">            M, sigma, a_j = m_step(</span><br><span class="line">                R_ij, V, a_last, beta_v, beta_a, inverse_temperature=inverse_temperature</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> it &lt; iterations - <span class="number">1</span>:</span><br><span class="line">                R_ij = e_step(</span><br><span class="line">                    M, sigma, a_j, V</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pose: (N, OH, OW, o 4 x 4) via squeeze M (24, 6, 6, 32, 16)</span></span><br><span class="line">        M = tf.squeeze(M, axis=-<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># activation: (N, OH, OW, o) via squeeze o_activationis [24, 6, 6, 32]</span></span><br><span class="line">        a_j = tf.squeeze(a_j, axis=[-<span class="number">3</span>, -<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M, a_j</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">capsules_net</span>(<span class="params">inputs, num_classes, iterations, batch_size, name=<span class="string">&#x27;capsule_em&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Define the Capsule Network model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment"># ReLU Conv1</span></span><br><span class="line">        <span class="comment"># Images shape (24, 28, 28, 1) -&gt; conv 5x5 filters, 32 output channels, strides 2 with padding, ReLU</span></span><br><span class="line">        <span class="comment"># nets -&gt; (?, 14, 14, 32)</span></span><br><span class="line">        nets = conv2d(</span><br><span class="line">            inputs,</span><br><span class="line">            kernel=<span class="number">5</span>, out_channels=<span class="number">32</span>, stride=<span class="number">2</span>, padding=<span class="string">&#x27;SAME&#x27;</span>,</span><br><span class="line">            activation_fn=tf.nn.relu, name=<span class="string">&#x27;relu_conv1&#x27;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># PrimaryCaps</span></span><br><span class="line">        <span class="comment"># (?, 14, 14, 32) -&gt; capsule 1x1 filter, 32 output capsule, strides 1 without padding</span></span><br><span class="line">        <span class="comment"># nets -&gt; (poses (?, 14, 14, 32, 4, 4), activations (?, 14, 14, 32))</span></span><br><span class="line">        nets = primary_caps(</span><br><span class="line">            nets,</span><br><span class="line">            kernel_size=<span class="number">1</span>, out_capsules=<span class="number">32</span>, stride=<span class="number">1</span>, padding=<span class="string">&#x27;VALID&#x27;</span>,</span><br><span class="line">            pose_shape=[<span class="number">4</span>, <span class="number">4</span>], name=<span class="string">&#x27;primary_caps&#x27;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ConvCaps1</span></span><br><span class="line">        <span class="comment"># (poses, activations) -&gt; conv capsule, 3x3 kernels, strides 2, no padding</span></span><br><span class="line">        <span class="comment"># nets -&gt; (poses (24, 6, 6, 32, 4, 4), activations (24, 6, 6, 32))</span></span><br><span class="line">        nets = conv_capsule(</span><br><span class="line">            nets, shape=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], iterations=iterations,</span><br><span class="line">            batch_size=batch_size, name=<span class="string">&#x27;conv_caps1&#x27;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ConvCaps2</span></span><br><span class="line">        <span class="comment"># (poses, activations) -&gt; conv capsule, 3x3 kernels, strides 1, no padding</span></span><br><span class="line">        <span class="comment"># nets -&gt; (poses (24, 4, 4, 32, 4, 4), activations (24, 4, 4, 32))</span></span><br><span class="line">        nets = conv_capsule(</span><br><span class="line">            nets, shape=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], iterations=iterations,</span><br><span class="line">            batch_size=batch_size, name=<span class="string">&#x27;conv_caps2&#x27;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Class capsules</span></span><br><span class="line">        <span class="comment"># (poses, activations) -&gt; 1x1 convolution, 10 output capsules</span></span><br><span class="line">        <span class="comment"># nets -&gt; (poses (24, 10, 4, 4), activations (24, 10))</span></span><br><span class="line">        nets = class_capsules(nets, num_classes, iterations=iterations,</span><br><span class="line">                              batch_size=batch_size, name=<span class="string">&#x27;class_capsules&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># poses (24, 10, 4, 4), activations (24, 10)</span></span><br><span class="line">        poses, activations = nets</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> poses, activations</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">spread_loss</span>(<span class="params">labels, activations, iterations_per_epoch, global_step, name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Spread loss</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param labels: (24, 10] in one-hot vector</span></span><br><span class="line"><span class="string">    :param activations: [24, 10], activation for each class</span></span><br><span class="line"><span class="string">    :param margin: increment from 0.2 to 0.9 during training</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: spread loss</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Margin schedule</span></span><br><span class="line">    <span class="comment"># Margin increase from 0.2 to 0.9 by an increment of 0.1 for every epoch</span></span><br><span class="line">    margin = tf.train.piecewise_constant(</span><br><span class="line">        tf.cast(global_step, dtype=tf.int32),</span><br><span class="line">        boundaries=[</span><br><span class="line">            (iterations_per_epoch * x) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">8</span>)</span><br><span class="line">        ],</span><br><span class="line">        values=[</span><br><span class="line">            x / <span class="number">10.0</span> <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    activations_shape = activations.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment"># mask_t, mask_f Tensor (?, 10)</span></span><br><span class="line">        mask_t = tf.equal(labels, <span class="number">1</span>)      <span class="comment"># Mask for the true label</span></span><br><span class="line">        mask_i = tf.equal(labels, <span class="number">0</span>)      <span class="comment"># Mask for the non-true label</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Activation for the true label</span></span><br><span class="line">        <span class="comment"># activations_t (?, 1)</span></span><br><span class="line">        activations_t = tf.reshape(tf.boolean_mask(activations, mask_t), shape=(tf.shape(activations)[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Activation for the other classes</span></span><br><span class="line">        <span class="comment"># activations_i (?, 9)</span></span><br><span class="line">        activations_i = tf.reshape(</span><br><span class="line">            tf.boolean_mask(activations, mask_i), [tf.shape(activations)[<span class="number">0</span>], activations_shape[<span class="number">1</span>] - <span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        l = tf.reduce_sum(tf.square(tf.maximum(<span class="number">0.0</span>, margin - (activations_t - activations_i))))</span><br><span class="line">        tf.losses.add_loss(l)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_image</span>(<span class="params">image, label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Scale the image value between -1 and 1.</span></span><br><span class="line"><span class="string">      :param image: An image in Tensor.</span></span><br><span class="line"><span class="string">      :return A scaled image in Tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    image = tf.to_float(image)</span><br><span class="line">    image = tf.subtract(image, <span class="number">128.0</span>)</span><br><span class="line">    image = tf.div(image, <span class="number">128.0</span>)</span><br><span class="line">    image = tf.reshape(image, (<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">    label = tf.one_hot(label, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_batch</span>(<span class="params">batch_size=<span class="number">32</span></span>) -&gt; [tf.Tensor, tf.Tensor]:</span><br><span class="line">    (x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">    dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train))</span><br><span class="line">               .shuffle(<span class="number">10000</span>, <span class="number">66</span>)</span><br><span class="line">               .repeat()</span><br><span class="line">               .<span class="built_in">map</span>(preprocess_image)</span><br><span class="line">               .batch(batch_size))  <span class="comment"># type: tf.data.Dataset</span></span><br><span class="line">    <span class="keyword">return</span> dataset._make_one_shot_iterator().get_next()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">    seed = <span class="number">66</span></span><br><span class="line">    batch_size = <span class="number">8</span></span><br><span class="line">    log_dir = <span class="string">&#x27;./log/train&#x27;</span></span><br><span class="line"></span><br><span class="line">    config = tf.ConfigProto()</span><br><span class="line">    config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.set_random_seed(seed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Slim dataset contains data sources, decoder, reader and other meta-information</span></span><br><span class="line">    iterations_per_epoch = <span class="number">60000</span> // batch_size  <span class="comment"># 60,000/24 = 2500</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># images: Tensor (?, 28, 28, 1)</span></span><br><span class="line">    <span class="comment"># labels: Tensor (?)</span></span><br><span class="line">    next_images, next_labels = load_batch(batch_size)</span><br><span class="line"></span><br><span class="line">    images = tf.placeholder_with_default(next_images, (batch_size, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">    labels = tf.placeholder_with_default(next_labels, (batch_size, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># poses: Tensor(?, 10, 4, 4) activations: (?, 10)</span></span><br><span class="line">    poses, activations = capsules_net(images, num_classes=<span class="number">10</span>, iterations=<span class="number">3</span>, batch_size=batch_size, name=<span class="string">&#x27;capsules_em&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    global_step = tf.train.get_or_create_global_step()</span><br><span class="line">    loss = spread_loss(labels, activations, iterations_per_epoch, global_step, name=<span class="string">&#x27;spread_loss&#x27;</span>)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/spread_loss&#x27;</span>, loss)</span><br><span class="line"></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">    train_tensor = slim.learning.create_train_op(loss, optimizer, global_step=global_step, clip_gradient_norm=<span class="number">4.0</span>)</span><br><span class="line"></span><br><span class="line">    slim.learning.train(</span><br><span class="line">        train_tensor,</span><br><span class="line">        logdir=log_dir,</span><br><span class="line">        log_every_n_steps=<span class="number">10</span>,</span><br><span class="line">        save_summaries_secs=<span class="number">60</span>,</span><br><span class="line">        saver=tf.train.Saver(max_to_keep=<span class="number">2</span>),</span><br><span class="line">        save_interval_secs=<span class="number">600</span>,</span><br><span class="line">        session_config=config</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()                                       </span><br></pre></td></tr></table></figure>
<h1 id="参考">参考</h1>
<ol type="1">
<li><p><a
target="_blank" rel="noopener" href="https://kexue.fm/archives/5155">苏建林的博客</a></p></li>
<li><p><a
target="_blank" rel="noopener" href="https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-Capsule-Network/">理解胶囊网络</a></p></li>
</ol>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" rel="tag">聚类方法</a></li></ul></div><div class="post-nav"><a class="pre" href="/2019/07/25/use-lan/">设置路由器使用LAN口</a><a class="next" href="/2019/07/10/yolo-error/">实现yolo时踩过的坑！</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>