<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>概率模型第三章 ： MCMC | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">概率模型第三章 ： MCMC</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">概率模型第三章 ： MCMC</h1><div class="post-meta">2019-07-27<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 10.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 48</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#probabilistic-programming-and-bayesian-methods-for-hackers-chapter-3"><span class="toc-number">1.</span> <span class="toc-text">Probabilistic
Programming and Bayesian Methods for Hackers Chapter 3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#table-of-contents"><span class="toc-number">1.0.1.</span> <span class="toc-text">Table of Contents</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%93%E5%BC%80mcmc%E7%9A%84%E9%BB%91%E5%8C%A3%E5%AD%90"><span class="toc-number">1.1.</span> <span class="toc-text">打开MCMC的黑匣子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9B%BE%E5%83%8F"><span class="toc-number">1.1.1.</span> <span class="toc-text">贝叶斯图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8mcmc%E6%8E%A2%E7%B4%A2%E5%9B%BE%E5%83%8F"><span class="toc-number">1.1.2.</span> <span class="toc-text">使用MCMC探索图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E4%BB%A5%E5%8D%83%E8%AE%A1%E7%9A%84%E6%A0%B7%E6%9C%AC"><span class="toc-number">1.1.3.</span> <span class="toc-text">为什么数以千计的样本？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8Cmcmc%E7%9A%84%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.4.</span> <span class="toc-text">执行MCMC的算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8E%E9%AA%8C%E7%9A%84%E5%85%B6%E4%BB%96%E8%BF%91%E4%BC%BC%E8%A7%A3"><span class="toc-number">1.1.5.</span> <span class="toc-text">后验的其他近似解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#example-%E4%BD%BF%E7%94%A8%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E8%81%9A%E7%B1%BB"><span class="toc-number">1.2.</span> <span class="toc-text">Example:
使用混合模型的无监督聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E8%B0%83%E6%9F%A5"><span class="toc-number">1.2.1.</span> <span class="toc-text">聚类调查</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%8F%90%E7%A4%BA%E4%B8%8D%E8%A6%81%E6%B7%B7%E5%90%88%E5%90%8E%E9%AA%8C%E6%A0%B7%E6%9C%AC"><span class="toc-number">1.2.2.</span> <span class="toc-text">重要提示：不要混合后验样本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%88%B0%E8%81%9A%E7%B1%BB%E9%A2%84%E6%B5%8B"><span class="toc-number">1.2.3.</span> <span class="toc-text">回到聚类：预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#diagnosing-convergence"><span class="toc-number">1.3.</span> <span class="toc-text">Diagnosing Convergence</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%9B%B8%E5%85%B3"><span class="toc-number">1.3.1.</span> <span class="toc-text">自相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%99%E4%B8%8Emcmc%E6%94%B6%E6%95%9B%E6%9C%89%E4%BD%95%E5%85%B3%E7%B3%BB"><span class="toc-number">1.3.2.</span> <span class="toc-text">这与MCMC收敛有何关系?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%86%E5%8C%96"><span class="toc-number">1.3.3.</span> <span class="toc-text">细化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mcmc%E7%9A%84%E6%9C%89%E7%94%A8%E6%8F%90%E7%A4%BA"><span class="toc-number">1.4.</span> <span class="toc-text">MCMC的有用提示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E5%90%AF%E5%8A%A8%E5%80%BC"><span class="toc-number">1.4.1.</span> <span class="toc-text">智能启动值</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#priors-%E5%85%88%E9%AA%8C"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">Priors 先验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E8%AE%A1%E7%AE%97%E7%9A%84%E6%B0%91%E9%97%B4%E5%AE%9A%E7%90%86"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">统计计算的民间定理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.5.</span> <span class="toc-text">结论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#references"><span class="toc-number">1.5.1.</span> <span class="toc-text">References</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><p>Tensorflow
概率模型学习，代码运行于<code>Tensorflow 1.14</code>，文字半机器翻译。</p>
<span id="more"></span>
<h1
id="probabilistic-programming-and-bayesian-methods-for-hackers-chapter-3">Probabilistic
Programming and Bayesian Methods for Hackers Chapter 3</h1>
<hr />
<h3 id="table-of-contents">Table of Contents</h3>
<ul>
<li>Opening the black box of MCMC
<ul>
<li>The Bayesian landscape</li>
<li>Exploring the landscape using the MCMC</li>
<li>Why Thousands of Samples?</li>
<li>Algorithms to perform MCMC</li>
<li>Other aproximation solutions to the posterior</li>
</ul></li>
<li>Example: Unsupervised Clustering Using a Mixture Model
<ul>
<li>Cluster Investigation</li>
<li>Important: Don't mix posterior samples</li>
<li>Returning to Clustering: Prediction</li>
</ul></li>
<li>Diagnosing Convergence
<ul>
<li>Autocorrelation</li>
<li>How does this relate to MCMC convergence?</li>
<li>Thinning</li>
<li>Additional Plotting Options</li>
</ul></li>
<li>Useful tips for MCMC
<ul>
<li>Intelligent starting values
<ul>
<li>Priors</li>
<li>Covariance matrices and eliminating parameters</li>
<li>The Folk Theorem of Statistical Computing</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>References</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@title Imports and Global Variables  &#123; display-mode: &quot;form&quot; &#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">The book uses a custom matplotlibrc file, which provides the unique styles for</span></span><br><span class="line"><span class="string">matplotlib plots. If executing this book, and you wish to use the book&#x27;s</span></span><br><span class="line"><span class="string">styling, provided are two options:</span></span><br><span class="line"><span class="string">    1. Overwrite your own matplotlibrc file with the rc-file provided in the</span></span><br><span class="line"><span class="string">       book&#x27;s styles/ dir. See http://matplotlib.org/users/customizing.html</span></span><br><span class="line"><span class="string">    2. Also in the styles is  bmh_matplotlibrc.json file. This can be used to</span></span><br><span class="line"><span class="string">       update the styles in only this notebook. Try running the following code:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        import json</span></span><br><span class="line"><span class="string">        s = json.load(open(&quot;../styles/bmh_matplotlibrc.json&quot;))</span></span><br><span class="line"><span class="string">        matplotlib.rcParams.update(s)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)</span></span><br><span class="line">warning_status = <span class="string">&quot;ignore&quot;</span> <span class="comment">#@param [&quot;ignore&quot;, &quot;always&quot;, &quot;module&quot;, &quot;once&quot;, &quot;default&quot;, &quot;error&quot;]</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(warning_status)</span><br><span class="line"><span class="keyword">with</span> warnings.catch_warnings():</span><br><span class="line">    warnings.filterwarnings(warning_status, category=DeprecationWarning)</span><br><span class="line">    warnings.filterwarnings(warning_status, category=UserWarning)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment">#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))</span></span><br><span class="line">matplotlib_style = <span class="string">&#x27;fivethirtyeight&#x27;</span> <span class="comment">#@param [&#x27;fivethirtyeight&#x27;, &#x27;bmh&#x27;, &#x27;ggplot&#x27;, &#x27;seaborn&#x27;, &#x27;default&#x27;, &#x27;Solarize_Light2&#x27;, &#x27;classic&#x27;, &#x27;dark_background&#x27;, &#x27;seaborn-colorblind&#x27;, &#x27;seaborn-notebook&#x27;]</span></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt; plt.style.use(matplotlib_style)</span><br><span class="line"><span class="keyword">import</span> matplotlib.axes <span class="keyword">as</span> axes;</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Ellipse</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> pandas_datareader.data <span class="keyword">as</span> web</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;YaHei Consolas Hybrid&#x27;</span>]</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns; sns.set_context(<span class="string">&#x27;notebook&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> IPython.core.pylabtools <span class="keyword">import</span> figsize</span><br><span class="line"><span class="comment">#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)</span></span><br><span class="line">notebook_screen_res = <span class="string">&#x27;retina&#x27;</span> <span class="comment">#@param [&#x27;retina&#x27;, &#x27;png&#x27;, &#x27;jpeg&#x27;, &#x27;svg&#x27;, &#x27;pdf&#x27;]</span></span><br><span class="line">%config InlineBackend.figure_format = notebook_screen_res</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tfe = tf.contrib.eager</span><br><span class="line"></span><br><span class="line"><span class="comment"># Eager Execution</span></span><br><span class="line"><span class="comment">#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)</span></span><br><span class="line"><span class="comment">#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)</span></span><br><span class="line">use_tf_eager = <span class="literal">False</span> <span class="comment">#@param &#123;type:&quot;boolean&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use try/except so we can easily re-execute the whole notebook.</span></span><br><span class="line"><span class="keyword">if</span> use_tf_eager:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tf.enable_eager_execution()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow_probability <span class="keyword">as</span> tfp</span><br><span class="line">tfd = tfp.distributions</span><br><span class="line">tfb = tfp.bijectors</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">tensors</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Evaluates Tensor or EagerTensor to Numpy `ndarray`s.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,</span></span><br><span class="line"><span class="string">      `namedtuple` or combinations thereof.</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      ndarrays: Object with same structure as `tensors` except with `Tensor` or</span></span><br><span class="line"><span class="string">        `EagerTensor`s replaced by Numpy `ndarray`s.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> tf.executing_eagerly():</span><br><span class="line">        <span class="keyword">return</span> tf.contrib.framework.nest.pack_sequence_as(</span><br><span class="line">            tensors,</span><br><span class="line">            [t.numpy() <span class="keyword">if</span> tf.contrib.framework.is_tensor(t) <span class="keyword">else</span> t</span><br><span class="line">             <span class="keyword">for</span> t <span class="keyword">in</span> tf.contrib.framework.nest.flatten(tensors)])</span><br><span class="line">    <span class="keyword">return</span> sess.run(tensors)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_TFColor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Enum of colors used in TF docs.&quot;&quot;&quot;</span></span><br><span class="line">    red = <span class="string">&#x27;#F15854&#x27;</span></span><br><span class="line">    blue = <span class="string">&#x27;#5DA5DA&#x27;</span></span><br><span class="line">    orange = <span class="string">&#x27;#FAA43A&#x27;</span></span><br><span class="line">    green = <span class="string">&#x27;#60BD68&#x27;</span></span><br><span class="line">    pink = <span class="string">&#x27;#F17CB0&#x27;</span></span><br><span class="line">    brown = <span class="string">&#x27;#B2912F&#x27;</span></span><br><span class="line">    purple = <span class="string">&#x27;#B276B2&#x27;</span></span><br><span class="line">    yellow = <span class="string">&#x27;#DECF3F&#x27;</span></span><br><span class="line">    gray = <span class="string">&#x27;#4D4D4D&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="keyword">return</span> [</span><br><span class="line">            self.red,</span><br><span class="line">            self.orange,</span><br><span class="line">            self.green,</span><br><span class="line">            self.blue,</span><br><span class="line">            self.pink,</span><br><span class="line">            self.brown,</span><br><span class="line">            self.purple,</span><br><span class="line">            self.yellow,</span><br><span class="line">            self.gray,</span><br><span class="line">        ][i % <span class="number">9</span>]</span><br><span class="line">TFColor = _TFColor()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">session_options</span>(<span class="params">enable_gpu_ram_resizing=<span class="literal">True</span>, enable_xla=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Allowing the notebook to make use of GPUs if they&#x27;re available.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear </span></span><br><span class="line"><span class="string">    algebra that optimizes TensorFlow computations.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    config = tf.ConfigProto()</span><br><span class="line">    config.log_device_placement = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> enable_gpu_ram_resizing:</span><br><span class="line">        <span class="comment"># `allow_growth=True` makes it possible to connect multiple colabs to your</span></span><br><span class="line">        <span class="comment"># GPU. Otherwise the colab malloc&#x27;s all GPU ram.</span></span><br><span class="line">        config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> enable_xla:</span><br><span class="line">        <span class="comment"># Enable on XLA. https://www.tensorflow.org/performance/xla/.</span></span><br><span class="line">        config.graph_options.optimizer_options.global_jit_level = (</span><br><span class="line">            tf.OptimizerOptions.ON_1)</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reset_sess</span>(<span class="params">config=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Convenience function to create the TF graph &amp; session or reset them.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        config = session_options()</span><br><span class="line">    <span class="keyword">global</span> sess</span><br><span class="line">    tf.reset_default_graph()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        sess.close()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    sess = tf.InteractiveSession(config=config)</span><br><span class="line"></span><br><span class="line">reset_sess()</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="打开mcmc的黑匣子">打开MCMC的黑匣子</h2>
<p>前两章隐藏了TFP的内部机制，更常见的是来自读者的Markov Chain Monte
Carlo（MCMC）。包含本章的原因有三个方面。首先，任何关于贝叶斯推理的书都必须讨论MCMC。我不能打这个。责备统计学家。其次，了解MCMC的过程可以让您深入了解您的算法是否已融合（融合到什么？我们将达到这个目标）。第三，我们将理解<em>为什么</em>我们从后验返回了数千个样本作为解决方案，起初认为这可能是奇怪的。</p>
<h3 id="贝叶斯图像">贝叶斯图像</h3>
<p>当我们用<span
class="math inline">\(N\)</span>未知数设置贝叶斯推理问题时，我们隐含地为先前的分布创建<span
class="math inline">\(N\)</span>维空间。与空间相关联是另一个维度，我们可以将其描述为<em>曲面</em>，或<em>曲线</em>，位于空间的顶部，反映特定点的<em>先验概率</em>。空间的表面由我们先前的分布定义。例如，如果我们有两个未知数<span
class="math inline">\(p_1\)</span>和<span
class="math inline">\(p_2\)</span>，并且两者的先验都是<span
class="math inline">\(\text{Uniform}(0,5)\)</span>，则创建的空间是长度为5的正方形，表面是平面位于正方形顶部（表示每个点都有可能）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_ = y_ = np.linspace(<span class="number">0.</span>, <span class="number">5.</span>, <span class="number">100.</span>, dtype=np.float32)</span><br><span class="line">X_, Y_ = evaluate(tf.meshgrid(x_, y_))</span><br><span class="line"></span><br><span class="line">uni_x_ = evaluate(tfd.Uniform(low=<span class="number">0.</span>, high=<span class="number">5.</span>).prob(x_))</span><br><span class="line">m_ = np.median(uni_x_)</span><br><span class="line"></span><br><span class="line">uni_y_ = evaluate(tfd.Uniform(low=<span class="number">0.</span>, high=<span class="number">5.</span>).prob(y_))</span><br><span class="line">M_ = evaluate(tf.matmul(tf.expand_dims(uni_x_, <span class="number">1</span>), tf.expand_dims(uni_y_, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">6</span>))</span><br><span class="line">jet = plt.cm.jet</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line"> </span><br><span class="line">im = plt.imshow(M_, interpolation=<span class="string">&#x27;none&#x27;</span>, origin=<span class="string">&#x27;lower&#x27;</span>,</span><br><span class="line">                cmap=jet, vmax=<span class="number">1</span>, vmin=-<span class="number">.15</span>, extent=(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.title(<span class="string">&quot;平均分布先验图像.&quot;</span>)</span><br><span class="line"> </span><br><span class="line">ax = fig.add_subplot(<span class="number">122</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(X_, Y_, M_, cmap=plt.cm.jet, vmax=<span class="number">1</span>, vmin=-<span class="number">.15</span>)</span><br><span class="line">ax.view_init(azim=<span class="number">390</span>)</span><br><span class="line">plt.title(<span class="string">&quot;平均分布先验图像; 备用视图&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>&lt;Figure size 900x432 with 0 Axes&gt;</code></pre>
<p><img src="/2019/07/27/tfp-ch3/output_4_1.png" /></p>
<p>或者，如果两个先验是$ （3）<span class="math inline">\(和\)</span>
（10）$，则该空间是2-D平面上的所有正数，并且由先验引起的表面看起来像一个从<code>（0,0）</code>点开始的水落，并流过正数。</p>
<p>下面的图表显示了这一点。越
<font color="#8b0000">红</font>的颜色,表明越大的先验概率被分配到这个位置，反过来越
<font color="#00008B">蓝</font> 的地方表示我们分配的先验概率越小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">exp_x_ = evaluate(tfd.Exponential(rate=(<span class="number">1.</span>/<span class="number">3.</span>)).prob(x_))</span><br><span class="line">exp_y_ = evaluate(tfd.Exponential(rate=(<span class="number">1.</span>/<span class="number">10.</span>)).prob(y_))</span><br><span class="line"></span><br><span class="line">M_ = evaluate(tf.matmul(tf.expand_dims(exp_x_, <span class="number">1</span>), tf.expand_dims(exp_y_, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">6</span>))</span><br><span class="line">jet = plt.cm.jet</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">CS = plt.contour(X_, Y_, M_)</span><br><span class="line">im = plt.imshow(M_, interpolation=<span class="string">&#x27;none&#x27;</span>, origin=<span class="string">&#x27;lower&#x27;</span>,</span><br><span class="line">                cmap=jet, extent=(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;$Exp(3), Exp(10)$ 先验图像&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax = fig.add_subplot(<span class="number">122</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(X_, Y_, M_, cmap=plt.cm.jet)</span><br><span class="line">ax.view_init(azim=<span class="number">30</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;$Exp(3), Exp(10)$ 先验图像; 备用视角&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>&lt;Figure size 900x432 with 0 Axes&gt;</code></pre>
<p><img src="/2019/07/27/tfp-ch3/output_6_1.png" /></p>
<p>这些是2D空间中的简单示例，我们的大脑可以很好地理解表面。在实践中，由我们的先验可以生成的空间或表面等更高维度。</p>
<p>如果这些表面描述了未知数的<em>先前分布</em>，那么在我们合并观察到的数据<span
class="math inline">\(X\)</span>后，我们的空间会发生什么？数据<span
class="math inline">\(X\)</span>不会改变空间，但它通过<em>拉动和拉伸先前曲面</em>的结构来改变空间的表面，以反映真实参数可能存在的位置。更多的数据意味着更多的拉伸和拉伸，与新形成的形状相比，我们的原始形状变得严重或微不足道。数据越少，我们的原始形状就越多。无论如何，得到的表面描述了<em>后验分布</em>。</p>
<p>我必须再次强调，遗憾的是，不可能在大尺寸上将其可视化。对于两个维度，数据基本上<em>向上推</em>原始表面以形成<em>高山</em>。通过先验概率分布检查观测数据<em>推高</em>某些区域中的后验概率的趋势，因此较少的先验概率意味着更大的<em>阻力</em>。因此，在上面的双指数先验案例中，可能在<code>（0,0）</code>角附近爆发的山（或多个山）将比远离<code>（5,5）</code>的山脉高得多，因为那里在<code>（5,5）</code>附近有更大的阻力（低先验概率）。峰值反映了可能找到真实参数的后验概率。重要的是，如果先验已经指定概率为<code>0</code>，则不会在那里分配后验概率。</p>
<p>假设上面提到的先验代表两个泊松分布的不同参数$
$。我们观察了一些数据点并可视化新的图像：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 创建观察变量</span></span><br><span class="line"><span class="comment"># 尝试改变采样数观察结果</span></span><br><span class="line">N = <span class="number">2</span> <span class="comment">#param &#123;type:&quot;slider&quot;, min:1, max:15, step:1&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 真正的参数，但当然我们没有看到这些值......</span></span><br><span class="line">lambda_1_true = <span class="built_in">float</span>(<span class="number">1.</span>)</span><br><span class="line">lambda_2_true = <span class="built_in">float</span>(<span class="number">3.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#...依据真实参数生成对应的分布</span></span><br><span class="line">data = tf.concat([</span><br><span class="line">    tfd.Poisson(rate=lambda_1_true).sample(sample_shape=(N, <span class="number">1</span>), seed=<span class="number">4</span>),</span><br><span class="line">    tfd.Poisson(rate=lambda_2_true).sample(sample_shape=(N, <span class="number">1</span>), seed=<span class="number">8</span>)</span><br><span class="line">], axis=<span class="number">1</span>)</span><br><span class="line">data_ = evaluate(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;observed (2-dimensional,sample size = %d): \n&quot;</span> % N, data_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plotting details.</span></span><br><span class="line">x_ = y_ = np.linspace(<span class="number">.01</span>, <span class="number">5</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">likelihood_x = tf.math.reduce_prod(tfd.Poisson(rate=x_).prob(data_[:,<span class="number">0</span>][:,tf.newaxis]),axis=<span class="number">0</span>)</span><br><span class="line">likelihood_y = tf.math.reduce_prod(tfd.Poisson(rate=y_).prob(data_[:,<span class="number">1</span>][:,tf.newaxis]),axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">L_ = evaluate(tf.matmul(likelihood_x[:,tf.newaxis],likelihood_y[tf.newaxis,:]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>observed (2-dimensional,sample size = 2): 
 [[3. 4.]
 [1. 3.]]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">15.0</span>))</span><br><span class="line"><span class="comment"># matplotlib heavy lifting below, beware!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SUBPLOT for regular Uniform</span></span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line"></span><br><span class="line">uni_x_ = evaluate(tfd.Uniform(low=<span class="number">0.</span>, high=<span class="number">5.</span>).prob(tf.cast(x_,dtype=tf.float32)))</span><br><span class="line">m = np.median(uni_x_[uni_x_ &gt; <span class="number">0</span>])</span><br><span class="line">uni_x_[uni_x_ == <span class="number">0</span>] = m</span><br><span class="line">uni_y_ = evaluate(tfd.Uniform(low=<span class="number">0.</span>, high=<span class="number">5.</span>).prob(tf.cast(y_,dtype=tf.float32)))</span><br><span class="line">m = np.median(uni_y_[uni_y_ &gt; <span class="number">0</span>])</span><br><span class="line">uni_y_[uni_y_ == <span class="number">0</span>] = m</span><br><span class="line"></span><br><span class="line">M_ = evaluate(tf.matmul(tf.expand_dims(uni_x_, <span class="number">1</span>), tf.expand_dims(uni_y_, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">im = plt.imshow(M_, interpolation=<span class="string">&#x27;none&#x27;</span>, origin=<span class="string">&#x27;lower&#x27;</span>,</span><br><span class="line">                cmap=jet, vmax=<span class="number">1</span>, vmin=-<span class="number">.15</span>, extent=(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">plt.scatter(lambda_2_true, lambda_1_true, c=<span class="string">&quot;k&quot;</span>, s=<span class="number">50</span>, edgecolor=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;$p_1, p_2$平均先验图像&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SUBPLOT for Uniform + Data point</span></span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.contour(x_, y_, M_ * L_)</span><br><span class="line">im = plt.imshow(M_ * L_, interpolation=<span class="string">&#x27;none&#x27;</span>, origin=<span class="string">&#x27;lower&#x27;</span>,</span><br><span class="line">                cmap=jet, extent=(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">plt.title(<span class="string">&quot;图像被%d个数据而扭曲;\n $p_1, p_2$.的平均先验&quot;</span> % N)</span><br><span class="line">plt.scatter(lambda_2_true, lambda_1_true, c=<span class="string">&quot;k&quot;</span>, s=<span class="number">50</span>, edgecolor=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SUBPLOT for regular Exponential</span></span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">exp_x_ = evaluate(tfd.Exponential(rate=<span class="number">.3</span>).prob(tf.to_float(x_)))</span><br><span class="line">exp_x_[np.isnan(exp_x_)] = exp_x_[<span class="number">1</span>]</span><br><span class="line">exp_y_ = evaluate(tfd.Exponential(rate=<span class="number">.10</span>).prob(tf.to_float(y_)))</span><br><span class="line">exp_y_[np.isnan(exp_y_)] = exp_y_[<span class="number">1</span>]</span><br><span class="line">M_ = evaluate(tf.matmul(tf.expand_dims(exp_x_, <span class="number">1</span>), tf.expand_dims(exp_y_, <span class="number">0</span>)))</span><br><span class="line">plt.contour(x_, y_, M_)</span><br><span class="line">im = plt.imshow(M_, interpolation=<span class="string">&#x27;none&#x27;</span>, origin=<span class="string">&#x27;lower&#x27;</span>,</span><br><span class="line">                cmap=jet, extent=(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">plt.scatter(lambda_2_true, lambda_1_true, c=<span class="string">&quot;k&quot;</span>, s=<span class="number">50</span>, edgecolor=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.title(<span class="string">&quot;$p_1, p_2$的指数先验图像&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SUBPLOT for Exponential + Data point</span></span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line"><span class="comment"># This is the likelihood times prior, that results in the posterior.</span></span><br><span class="line">plt.contour(x_, y_, M_ * L_)</span><br><span class="line">im = plt.imshow(M_ * L_, interpolation=<span class="string">&#x27;none&#x27;</span>, origin=<span class="string">&#x27;lower&#x27;</span>,</span><br><span class="line">                cmap=jet, extent=(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">plt.scatter(lambda_2_true, lambda_1_true, c=<span class="string">&quot;k&quot;</span>, s=<span class="number">50</span>, edgecolor=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;图像被%d个数据扭曲;\n $p_1, p_2$的指数先验&quot;</span> % N)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">5</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_9_0.png" /></p>
<p>左边的图是带有$ （0,5）$
先验的变形图像，右边的图是带有指数先验的变形景观。请注意，后方景观看起来彼此不同，尽管观察到的数据在两种情况下都是相同的。原因如下。注意指数先验的图像，右上图，在图的右上角的值上放置非常小的<em>后验</em>权重：这是因为<em>先验在那里没有太大的权重</em>。另一方面，均匀先验的图像很乐意将后部权重放在右上角，因为之前的重量更重。</p>
<p>另请注意，对应最暗的红色的最高点在指数情况下偏向于<code>（0,0）</code>，这是指数先验在<code>（0,0）</code>角落放置更多先前权重的结果。</p>
<p>黑点代表真实的参数。即使有1个样本点，山也会尝试包含真实参数。当然，样本大小为“1”的推断非常幼稚，选择如此小的样本量只是说明性的。</p>
<p>尝试将样本大小更改为其他值（尝试<code>2</code>,<code>5</code>,<code>10</code>,<code>100</code>？...）并观察我们的<code>山峰</code>后验如何变化是一个很好的练习。</p>
<h3 id="使用mcmc探索图像">使用MCMC探索图像</h3>
<p>我们应该探索我们先验表面和产生的变形后图像和观测数据，以找到后验。但是，我们不能天真地搜索空间：任何计算机科学家都会告诉你，在<span
class="math inline">\(N\)</span>中遍历<span
class="math inline">\(N\)</span>维空间是指数级的难度：当我们增加<span
class="math inline">\(N\)</span>时，空间的大小很快就会爆炸（<a
target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">the curse of
dimensionality</a>）。我们有什么希望找到这些隐藏的山脉？
MCMC背后的想法是对空间进行智能搜索。说“搜索”意味着我们正在寻找一个特定的点，这可能不准确，因为我们真的在寻找一座宽阔的山峰。</p>
<p>回想一下，MCMC从后验分布返回<em>样本</em>，而不是分布本身。将我们的山区类比延伸至极限，MCMC执行类似于反复询问“我发现这块鹅卵石有多大可能来自我正在寻找的山脉？”的任务，并通过返回数千个可接受的鹅卵石来完成其任务，以期重建原始的山。</p>
<p>当我说MCMC智能搜索时，我真的在说MCMC将<em>希望</em>会聚到高后验概率区域。
MCMC通过探索附近的位置并进入更高概率的区域来做到这一点。同样，也许“收敛”并不是描述MCMC进展的准确术语。收敛通常意味着朝向空间中的一个点移动，但是MCMC向空间中的<em>更宽的区域</em>移动并随机地在该区域中行走，从该区域拾取样本。</p>
<h3 id="为什么数以千计的样本">为什么数以千计的样本？</h3>
<p>首先，向用户返回数千个样本可能听起来像是描述后验分布的低效方式。我认为这非常有效。考虑其他可能性：</p>
<ol type="1">
<li>返回“山脉”的数学公式将涉及描述具有任意峰和谷的N维表面。</li>
<li>返回图像的“峰值”，虽然在数学上可能并且作为最高点的合理事情对应于未知数的最可能估计，但忽略了景观的形状，我们之前认为这对于确定后验置信度非常重要在未知数。</li>
</ol>
<p>除了计算原因，返回样本的最有力理由可能是我们可以轻松使用<em>大数定律</em>来解决其他棘手的问题。我推迟了下一章的讨论。通过数千个样本，我们可以通过在直方图中组织它们来重建后表面。</p>
<h3 id="执行mcmc的算法">执行MCMC的算法</h3>
<p>有一大系列算法可以执行MCMC。这些算法中的大多数可以高级表达如下:(数学细节可以在附录中找到。）</p>
<ol type="1">
<li>从当前位置开始。</li>
<li>建议搬到新的位置（调查你附近的鹅卵石）。</li>
<li>根据位置对数据和先前分布的遵守情况接受/拒绝新位置（询问卵石是否可能来自山区）。</li>
<li><ol type="1">
<li>如果您接受：移动到新位置。返回第1步。</li>
<li>否则：不要搬到新的位置。返回第1步。</li>
</ol></li>
<li>经过大量迭代后，返回所有接受的位置。</li>
</ol>
<p>这样，我们朝向存在后验分布的区域的大致方向移动，并且在旅程中节省地收集样本。一旦我们到达后验分布，我们就可以轻松地收集样本，因为它们可能都属于后验分布。</p>
<p>如果MCMC算法的当前位置处于极低概率的区域（这通常是算法开始时的情况（通常在空间中的随机位置），则算法将移动到可能不是来自<em>后验</em>但比附近的其他所有更好。因此算法的第一次移动不反映后验。</p>
<p>在上述算法的伪代码中，请注意只有当前位置很重要（仅在当前位置附近调查新位置）。我们可以将这个属性描述为<em>无记忆</em>，即算法不关心它<em>如何</em>到达当前位置，只是它在那里。</p>
<h3 id="后验的其他近似解">后验的其他近似解</h3>
<p>除MCMC外，还有其他程序可用于确定后验分布。拉普拉斯近似是使用简单函数的后验近似。更先进的方法是<a
target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Variational_Bayesian_methods">变分贝叶斯</a>.</p>
<p>所有三种方法，拉普拉斯近似，变分贝叶斯和经典MCMC都有其优缺点。我们在本书中只关注MCMC。</p>
<h2 id="example-使用混合模型的无监督聚类">Example:
使用混合模型的无监督聚类</h2>
<p>假设我们得到以下数据集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_sess()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> wget</span><br><span class="line">url = <span class="string">&#x27;https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/data/mixture_data.csv&#x27;</span></span><br><span class="line">filename = wget.download(url)</span><br><span class="line">filename</span><br></pre></td></tr></table></figure>
<pre><code>&#39;mixture_data.csv&#39;</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">6</span>))</span><br><span class="line">data_ = np.loadtxt(<span class="string">&quot;mixture_data.csv&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.hist(data_, bins=<span class="number">20</span>, color=<span class="string">&quot;k&quot;</span>, histtype=<span class="string">&quot;stepfilled&quot;</span>, alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.title(<span class="string">&quot;数据集的直方图&quot;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="literal">None</span>]);</span><br><span class="line"><span class="built_in">print</span>(data_[:<span class="number">10</span>], <span class="string">&quot;...&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>[115.85679142 152.26153716 178.87449059 162.93500815 107.02820697
 105.19141146 118.38288501 125.3769803  102.88054011 206.71326136] ...</code></pre>
<p><img src="/2019/07/27/tfp-ch3/output_17_1.png" /></p>
<p>数据表明了什么？看起来数据具有双峰形式，也就是说，它似乎有两个峰值，一个接近<code>120</code>，另一个接近<code>200</code>。也许在这个数据集中有<em>两个簇</em>。</p>
<p>该数据集是上一章中数据生成建模技术的一个很好的例子。我们可以提出<em>如何创建数据</em>。我建议使用以下数据生成算法：</p>
<ol type="1">
<li>对于每个数据点，选择概率为<span
class="math inline">\(p\)</span>的集群1，否则选择集群2。</li>
<li>使用参数$ _i <span class="math inline">\(和\)</span> _i <span
class="math inline">\(从Normal分布中绘制随机变量，其中\)</span> i
$在步骤1中选择。</li>
<li>重复.</li>
</ol>
<p>该算法将产生与观察数据集类似的效果，因此我们选择此作为我们的模型。当然，我们不知道<span
class="math inline">\(p\)</span>或Normal分布的参数。因此，我们必须推断或<em>学习</em>这些未知数。</p>
<p>表示正态分布$ _0 <span class="math inline">\(和\)</span> _1 <span
class="math inline">\(。两者目前都有未知的均值和标准差，表示为\)</span>
_i <span class="math inline">\(和\)</span> _i，; i = 0,1 <span
class="math inline">\(。特定数据点可以来自\)</span> _0 <span
class="math inline">\(或\)</span> _1 <span
class="math inline">\(，我们假设数据点以\)</span> p <span
class="math inline">\(的概率分配给\)</span> _0 $。</p>
<p>将数据点分配给集群的适当方法是使用<a
target="_blank" rel="noopener" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Categorical">TF<code>分类</code>变量</a>它的参数是一个长为$
k <span
class="math inline">\(概率数组，必须总和为1，它的`value`属性是一个在0和\)</span>k-1<span
class="math inline">\(之间的整数，根据精心设计的概率数据随机选择（在我们的例子中为\)</span>
K = 2 <span class="math inline">\(）。
对于*先验*，我们不知道分配给集群1的概率是多少，所以我们在\)</span>（0,1）<span
class="math inline">\(上形成一个统一变量。我们称之为\)</span>p_1<span
class="math inline">\(，因此属于群集2的概率为\)</span>p_2 = 1 -
p_1$。</p>
<p>幸运的是，我们可以将<code>[p1，p2]</code>赋予我们的<code>Categorical</code>变量。如果需要，我们也可以使用<code>tf.stack（）</code>将$
p_1 <span class="math inline">\(和\)</span> p_2
$组合成一个它能理解的向量。我们将此向量传递给<code>Categorical</code>变量，以便了解从多个分布中进行选择的可能性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p1 = tfd.Uniform(name=<span class="string">&#x27;p&#x27;</span>, low=<span class="number">0.</span>, high=<span class="number">1.</span>).sample()  <span class="comment"># p1 概率</span></span><br><span class="line">p2 = <span class="number">1</span> - p1 <span class="comment"># # p1 概率</span></span><br><span class="line">p = tf.stack([p1, p2]) <span class="comment"># 串联</span></span><br><span class="line"></span><br><span class="line">rv_assignment = tfd.Categorical(name=<span class="string">&quot;assignment&quot;</span>,probs=p)  <span class="comment">#</span></span><br><span class="line">assignment = rv_assignment.sample(sample_shape=data_.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">[p_,assignment_] = evaluate([p,assignment])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;先验分配, with p = %.2f:&quot;</span> % p_[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span> (assignment_[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>先验分配, with p = 0.16: [0 1 0 0 1 0 1 0 1 1]</p>
</blockquote>
<p>看看上面的数据集，我猜想两个法线的标准偏差是不同的。为了保持对标准偏差的未知，我们最初将它们模型化为<code>0</code>到<code>100</code>。我们将使用单行TFP代码在我们的模型中包含两个标准偏差：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rv_sds = tfd.Uniform(name=<span class="string">&quot;rv_sds&quot;</span>, low=[<span class="number">0.</span>, <span class="number">0.</span>], high=[<span class="number">100.</span>, <span class="number">100.</span>])</span><br></pre></td></tr></table></figure>
<p>在这里，我们使用批量形状2，创建两个独立的分布，恰好具有相同的参数。查看
<a
target="_blank" rel="noopener" href="https://https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Understanding_TensorFlow_Distributions_Shapes.ipynb">colab
on TFP shapes</a> 得到更多信息.</p>
<p>我们还需要在集群的中心指定先验。这些正常分布中的中心实际上是<span
class="math inline">\(\mu\)</span>参数。他们的先验可以通过正态分布建模。看看这些数据，我知道这两个中心可能在哪里；虽然我对这些看起来的估计并不十分自信，但我猜的分别在120左右和190左右。因此我将设置<span
class="math inline">\(\mu_0 = 120,\mu_1 = 190\)</span>和<span
class="math inline">\(\sigma_0 = \sigma_1 = 10\)</span>。</p>
<p>最后，我们使用了<a
target="_blank" rel="noopener" href="https://https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily">MixtureSameFamily</a>
分布以实现我们的两个正态分布的混合, 使用我们的 <a
target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/distributions/Categorical">Categorical</a>
分布作为我们的选择功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rv_sds = tfd.Uniform(name=<span class="string">&quot;rv_sds&quot;</span>, low=[<span class="number">0.</span>, <span class="number">0.</span>], high=[<span class="number">100.</span>, <span class="number">100.</span>])</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">str</span>(rv_sds))</span><br><span class="line"></span><br><span class="line">rv_centers = tfd.Normal(name=<span class="string">&quot;rv_centers&quot;</span>, loc=[<span class="number">120.</span>, <span class="number">190.</span>], scale=[<span class="number">10.</span>, <span class="number">10.</span>])</span><br><span class="line">    </span><br><span class="line">sds = rv_sds.sample()</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;shape of sds sample:&quot;</span>,sds.shape)</span><br><span class="line">centers = rv_centers.sample()</span><br><span class="line"></span><br><span class="line">rv_assignments = tfd.Categorical(probs=tf.stack([<span class="number">0.4</span>, <span class="number">0.6</span>]))</span><br><span class="line">assignments = rv_assignments.sample(sample_shape=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and to combine it with the observations:</span></span><br><span class="line">rv_observations = tfd.MixtureSameFamily(</span><br><span class="line">    mixture_distribution=rv_assignments,</span><br><span class="line">    components_distribution=tfd.Normal(</span><br><span class="line">        loc=centers,</span><br><span class="line">        scale=sds))</span><br><span class="line"></span><br><span class="line">observations = rv_observations.sample(sample_shape=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">[    </span><br><span class="line">    assignments_,</span><br><span class="line">    observations_,</span><br><span class="line">    sds_,</span><br><span class="line">    centers_</span><br><span class="line">] = evaluate([</span><br><span class="line">    assignments,</span><br><span class="line">    observations,</span><br><span class="line">    sds,</span><br><span class="line">    centers</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;simulated data: &quot;</span>, observations_[:<span class="number">4</span>], <span class="string">&quot;...&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Random assignments: &quot;</span>, assignments_[:<span class="number">4</span>], <span class="string">&quot;...&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Assigned center: &quot;</span>, centers_[:<span class="number">4</span>], <span class="string">&quot;...&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Assigned standard deviation: &quot;</span>, sds_[:<span class="number">4</span>],<span class="string">&quot;...&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>tfp.distributions.Uniform(&quot;rv_sds/&quot;, batch_shape=[2], event_shape=[], dtype=float32)
shape of sds sample: (2,)
simulated data:  [143.32745 203.31703 138.44893 157.09035] ...
Random assignments:  [1 0 0 0] ...
Assigned center:  [140.52045 187.54768] ...
Assigned standard deviation:  [34.886158 39.296032] ...</code></pre>
<p>类似地，在下面的联合<code>log_prob</code>函数中，我们创建了两个集群，每个集群都有我们在中心和标准偏差上的先验。
然后，我们根据我们的<a
target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/distributions/Categorical"><code>Categorical</code></a>变量确定的权重按比例混合它们创造了高斯分布的混合体。最后，对于每个数据点，我们从该混合分布生成样本。</p>
<p>请注意，此模型将群集分配变量边缘化，这样所有剩余的随机变量都是连续的，这使得它特别适合简单的MCMC--
<a
target="_blank" rel="noopener" href="https://www.tensorflow.org/probability/api_docs/python/tfp/mcmc/HamiltonianMonteCarlo"><code>HamiltonianMonteCarlo</code></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">joint_log_prob</span>(<span class="params">data_, sample_prob_1, sample_centers, sample_sds</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Joint log probability optimization function.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      data: tensor array representation of original data</span></span><br><span class="line"><span class="string">      sample_prob_1: Scalar representing probability (out of 1.0) of assignment </span></span><br><span class="line"><span class="string">        being 0</span></span><br><span class="line"><span class="string">      sample_sds: 2d vector containing standard deviations for both normal dists</span></span><br><span class="line"><span class="string">        in model</span></span><br><span class="line"><span class="string">      sample_centers: 2d vector containing centers for both normal dists in model</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">      Joint log probability optimization function.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>  </span><br><span class="line">    <span class="comment">### Create a mixture of two scalar Gaussians:</span></span><br><span class="line">    rv_prob = tfd.Uniform(name=<span class="string">&#x27;rv_prob&#x27;</span>, low=<span class="number">0.</span>, high=<span class="number">1.</span>)</span><br><span class="line">    sample_prob_2 = <span class="number">1.</span> - sample_prob_1</span><br><span class="line">    rv_assignments = tfd.Categorical(probs=tf.stack([sample_prob_1, sample_prob_2]))</span><br><span class="line">    </span><br><span class="line">    rv_sds = tfd.Uniform(name=<span class="string">&quot;rv_sds&quot;</span>, low=[<span class="number">0.</span>, <span class="number">0.</span>], high=[<span class="number">100.</span>, <span class="number">100.</span>])</span><br><span class="line">    rv_centers = tfd.Normal(name=<span class="string">&quot;rv_centers&quot;</span>, loc=[<span class="number">120.</span>, <span class="number">190.</span>], scale=[<span class="number">10.</span>, <span class="number">10.</span>])</span><br><span class="line">    </span><br><span class="line">    rv_observations = tfd.MixtureSameFamily(</span><br><span class="line">        mixture_distribution=rv_assignments,</span><br><span class="line">        components_distribution=tfd.Normal(</span><br><span class="line">          loc=sample_centers,       <span class="comment"># One for each component.</span></span><br><span class="line">          scale=sample_sds))        <span class="comment"># And same here.</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        rv_prob.log_prob(sample_prob_1)</span><br><span class="line">        + rv_prob.log_prob(sample_prob_2)</span><br><span class="line">        + tf.reduce_sum(rv_observations.log_prob(data_))      <span class="comment"># Sum over samples.</span></span><br><span class="line">        + tf.reduce_sum(rv_centers.log_prob(sample_centers)) <span class="comment"># Sum over components.</span></span><br><span class="line">        + tf.reduce_sum(rv_sds.log_prob(sample_sds))         <span class="comment"># Sum over components.</span></span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们将使用我们的HMC采样方法，通过使用下面的25000个样本迭代来探索空间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number_of_steps=<span class="number">25000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:0, max:50000, step:1000&#125;</span></span><br><span class="line">burnin=<span class="number">1000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:0, max:2000, step:100&#125;</span></span><br><span class="line">num_leapfrog_steps=<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the chain&#x27;s start state.</span></span><br><span class="line">initial_chain_state = [</span><br><span class="line">    tf.constant(<span class="number">0.5</span>, name=<span class="string">&#x27;init_probs&#x27;</span>),</span><br><span class="line">    tf.constant([<span class="number">120.</span>, <span class="number">190.</span>], name=<span class="string">&#x27;init_centers&#x27;</span>),</span><br><span class="line">    tf.constant([<span class="number">10.</span>, <span class="number">10.</span>], name=<span class="string">&#x27;init_sds&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Since MCMC operates over unconstrained space, we need to transform the</span></span><br><span class="line"><span class="comment"># samples so they live in real-space.</span></span><br><span class="line">unconstraining_bijectors = [</span><br><span class="line">    tfp.bijectors.Identity(),       <span class="comment"># Maps R to R.</span></span><br><span class="line">    tfp.bijectors.Identity(),       <span class="comment"># Maps R to R.</span></span><br><span class="line">    tfp.bijectors.Identity(),       <span class="comment"># Maps R to R.</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a closure over our joint_log_prob.</span></span><br><span class="line">unnormalized_posterior_log_prob = <span class="keyword">lambda</span> *args: joint_log_prob(data_, *args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the step_size. (It will be automatically adapted.)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):</span><br><span class="line">    step_size = tf.get_variable(</span><br><span class="line">        name=<span class="string">&#x27;step_size&#x27;</span>,</span><br><span class="line">        initializer=tf.constant(<span class="number">0.5</span>, dtype=tf.float32),</span><br><span class="line">        trainable=<span class="literal">False</span>,</span><br><span class="line">        use_resource=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the HMC</span></span><br><span class="line">hmc=tfp.mcmc.TransformedTransitionKernel(</span><br><span class="line">    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(</span><br><span class="line">        target_log_prob_fn=unnormalized_posterior_log_prob,</span><br><span class="line">        num_leapfrog_steps=num_leapfrog_steps,</span><br><span class="line">        step_size=step_size,</span><br><span class="line">        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=<span class="built_in">int</span>(burnin * <span class="number">0.8</span>)),</span><br><span class="line">        state_gradients_are_stopped=<span class="literal">True</span>),</span><br><span class="line">    bijector=unconstraining_bijectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from the chain.</span></span><br><span class="line">[</span><br><span class="line">    posterior_prob,</span><br><span class="line">    posterior_centers,</span><br><span class="line">    posterior_sds</span><br><span class="line">], kernel_results = tfp.mcmc.sample_chain(</span><br><span class="line">    num_results=number_of_steps,</span><br><span class="line">    num_burnin_steps=burnin,</span><br><span class="line">    current_state=initial_chain_state,</span><br><span class="line">    kernel=hmc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize any created variables.</span></span><br><span class="line">init_g = tf.global_variables_initializer()</span><br><span class="line">init_l = tf.local_variables_initializer()</span><br></pre></td></tr></table></figure>
<pre><code>W0727 20:52:11.363048 140336188528448 deprecation.py:323] From &lt;ipython-input-31-144a4acba7c5&gt;:39: make_simple_step_size_update_policy (from tensorflow_probability.python.mcmc.hmc) is deprecated and will be removed after 2019-05-22.
Instructions for updating:
Use tfp.mcmc.SimpleStepSizeAdaptation instead.
W0727 20:52:11.368744 140336188528448 deprecation.py:506] From &lt;ipython-input-31-144a4acba7c5&gt;:40: calling HamiltonianMonteCarlo.__init__ (from tensorflow_probability.python.mcmc.hmc) with step_size_update_fn is deprecated and will be removed after 2019-05-22.
Instructions for updating:
The `step_size_update_fn` argument is deprecated. Use `tfp.mcmc.SimpleStepSizeAdaptation` instead.</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluate(init_g)</span><br><span class="line">evaluate(init_l)</span><br><span class="line">[</span><br><span class="line">    posterior_prob_,</span><br><span class="line">    posterior_centers_,</span><br><span class="line">    posterior_sds_,</span><br><span class="line">    kernel_results_</span><br><span class="line">] = evaluate([</span><br><span class="line">    posterior_prob,</span><br><span class="line">    posterior_centers,</span><br><span class="line">    posterior_sds,</span><br><span class="line">    kernel_results</span><br><span class="line">])</span><br><span class="line">    </span><br><span class="line">new_step_size_initializer_ = kernel_results_.inner_results.is_accepted.mean()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;接受率: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    new_step_size_initializer_))</span><br><span class="line">new_step_size_initializer_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;结束步: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    kernel_results_.inner_results.extra.step_size_assign[-<span class="number">100</span>:].mean()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>接受率: 0.7698
结束步: 0.052764225751161575</code></pre>
<p>让我们检查未知参数的迹线。换句话说，到目前为止，未知参数（中心，精度和p）的路线已经采用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">9</span>))</span><br><span class="line">plt.subplot(<span class="number">311</span>)</span><br><span class="line">lw = <span class="number">1</span></span><br><span class="line">center_trace = posterior_centers_</span><br><span class="line"></span><br><span class="line"><span class="comment"># for pretty colors later in the book.</span></span><br><span class="line">colors = [TFColor[<span class="number">3</span>], TFColor[<span class="number">0</span>]] <span class="keyword">if</span> center_trace[-<span class="number">1</span>, <span class="number">0</span>] &gt; center_trace[-<span class="number">1</span>, <span class="number">1</span>] \</span><br><span class="line">    <span class="keyword">else</span> [TFColor[<span class="number">0</span>], TFColor[<span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">plt.plot(center_trace[:, <span class="number">0</span>], label=<span class="string">&quot;中心0的轨迹&quot;</span>, c=colors[<span class="number">0</span>], lw=lw)</span><br><span class="line">plt.plot(center_trace[:, <span class="number">1</span>], label=<span class="string">&quot;中心1的轨迹&quot;</span>, c=colors[<span class="number">1</span>], lw=lw)</span><br><span class="line">plt.title(<span class="string">&quot;未知参数的轨迹&quot;</span>)</span><br><span class="line">leg = plt.legend(loc=<span class="string">&quot;upper right&quot;</span>)</span><br><span class="line">leg.get_frame().set_alpha(<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">312</span>)</span><br><span class="line">std_trace = posterior_sds_</span><br><span class="line">plt.plot(std_trace[:, <span class="number">0</span>], label=<span class="string">&quot;聚类0的方差轨迹&quot;</span>,</span><br><span class="line">     c=colors[<span class="number">0</span>], lw=lw)</span><br><span class="line">plt.plot(std_trace[:, <span class="number">1</span>], label=<span class="string">&quot;聚类1的方差估计&quot;</span>,</span><br><span class="line">     c=colors[<span class="number">1</span>], lw=lw)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;upper left&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">313</span>)</span><br><span class="line">p_trace = posterior_prob_</span><br><span class="line">plt.plot(p_trace, label=<span class="string">&quot;$p$: 分配给聚类0的概率&quot;</span>,</span><br><span class="line">     color=TFColor[<span class="number">2</span>], lw=lw)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Steps&quot;</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.legend();</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_28_0.png" /></p>
<p>请注意以下特征： 1.
迹线会聚，而不是单个点，而是概率点的<em>分布</em>。这是MCMC算法中的*
收敛 <em>。 2.
使用前几千个点的推断是一个坏主意，因为它们与我们感兴趣的最终分布无关。因此，在使用样本进行推断之前丢弃这些样本是个好主意。我们在收敛</em>老化期*之前称这个时期。
3.
迹线显示为围绕空间的随机“行走”，即，路径表现出与先前位置的相关性。这既好又坏。我们将始终在当前位置和之前的位置之间建立相关性，但是太多意味着我们没有很好地探索空间。这将在本章后面的“诊断”部分详细介绍。</p>
<p>为了实现进一步的融合，我们将执行更多的MCMC步骤。在上面MCMC的伪代码算法中，唯一重要的位置是当前位置（在当前位置附近调查新位置）。为了继续我们离开的地方，我们将未知参数的当前值传递给<code>initial_chain_state（）</code>变量。我们已经计算过的值不会被覆盖。这样可以确保我们的采样在停止的地方继续进行。</p>
<p>我们将对MCMC进行五万次采样，并将以下进度可视化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number_of_steps=<span class="number">50000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:0, max:50000, step:1000&#125;</span></span><br><span class="line">burnin=<span class="number">10000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:0, max:2000, step:100&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the chain&#x27;s start state.</span></span><br><span class="line">initial_chain_state = [</span><br><span class="line">    tf.constant(posterior_prob_[-<span class="number">1</span>], name=<span class="string">&#x27;init_probs_2&#x27;</span>),</span><br><span class="line">    tf.constant(posterior_centers_[-<span class="number">1</span>], name=<span class="string">&#x27;init_centers_2&#x27;</span>),</span><br><span class="line">    tf.constant(posterior_sds_[-<span class="number">1</span>], name=<span class="string">&#x27;init_sds_2&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the step_size. (It will be automatically adapted.)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):</span><br><span class="line">    step_size = tf.get_variable(</span><br><span class="line">        name=<span class="string">&#x27;step_size_2&#x27;</span>,</span><br><span class="line">        <span class="comment">#initializer=tf.constant(new_step_size_initializer_, dtype=tf.float32),</span></span><br><span class="line">        initializer=tf.constant(<span class="number">0.5</span>, dtype=tf.float32),</span><br><span class="line">        trainable=<span class="literal">False</span>,</span><br><span class="line">        use_resource=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=<span class="built_in">int</span>(burnin * <span class="number">0.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the HMC</span></span><br><span class="line">hmc=tfp.mcmc.TransformedTransitionKernel(</span><br><span class="line">    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(</span><br><span class="line">        target_log_prob_fn=unnormalized_posterior_log_prob,</span><br><span class="line">        num_leapfrog_steps=num_leapfrog_steps,</span><br><span class="line">        step_size=step_size,</span><br><span class="line">        step_size_update_fn=step_size_update_fn,</span><br><span class="line">        state_gradients_are_stopped=<span class="literal">True</span>),</span><br><span class="line">    bijector=unconstraining_bijectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from the chain.</span></span><br><span class="line">[</span><br><span class="line">    posterior_prob_2,</span><br><span class="line">    posterior_centers_2,</span><br><span class="line">    posterior_sds_2</span><br><span class="line">], kernel_results = tfp.mcmc.sample_chain(</span><br><span class="line">    num_results=number_of_steps,</span><br><span class="line">    num_burnin_steps=burnin,</span><br><span class="line">    current_state=initial_chain_state,</span><br><span class="line">    kernel=hmc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize any created variables.</span></span><br><span class="line">init_g = tf.global_variables_initializer()</span><br><span class="line">init_l = tf.local_variables_initializer()</span><br><span class="line"></span><br><span class="line">evaluate(init_g)</span><br><span class="line">evaluate(init_l)</span><br><span class="line">[</span><br><span class="line">    posterior_prob_2_,</span><br><span class="line">    posterior_centers_2_,</span><br><span class="line">    posterior_sds_2_,</span><br><span class="line">    kernel_results_</span><br><span class="line">] = evaluate([</span><br><span class="line">    posterior_prob_2,</span><br><span class="line">    posterior_centers_2,</span><br><span class="line">    posterior_sds_2,</span><br><span class="line">    kernel_results</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;acceptance rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    kernel_results_.inner_results.is_accepted.mean()))</span><br><span class="line">new_step_size_initializer_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;final step size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    kernel_results_.inner_results.extra.step_size_assign[-<span class="number">100</span>:].mean()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>acceptance rate: 0.60624 final step size: 0.05713607743382454</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">4</span>))</span><br><span class="line">center_trace = posterior_centers_2_</span><br><span class="line">prev_center_trace = posterior_centers_</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">25000</span>)</span><br><span class="line">plt.plot(x, prev_center_trace[:, <span class="number">0</span>], label=<span class="string">&quot;之前的中心0轨迹&quot;</span>,</span><br><span class="line">      lw=lw, alpha=<span class="number">0.4</span>, c=colors[<span class="number">1</span>])</span><br><span class="line">plt.plot(x, prev_center_trace[:, <span class="number">1</span>], label=<span class="string">&quot;之前的中心1轨迹&quot;</span>,</span><br><span class="line">      lw=lw, alpha=<span class="number">0.4</span>, c=colors[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">25000</span>, <span class="number">75000</span>)</span><br><span class="line">plt.plot(x, center_trace[:, <span class="number">0</span>], label=<span class="string">&quot;中心0的新轨迹&quot;</span>, lw=lw, c=<span class="string">&quot;#5DA5DA&quot;</span>)</span><br><span class="line">plt.plot(x, center_trace[:, <span class="number">1</span>], label=<span class="string">&quot;中心1的新轨迹&quot;</span>, lw=lw, c=<span class="string">&quot;#F15854&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;未知中心参数的痕迹&quot;</span>)</span><br><span class="line">leg = plt.legend(loc=<span class="string">&quot;upper right&quot;</span>)</span><br><span class="line">leg.get_frame().set_alpha(<span class="number">0.8</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Steps&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_31_0.png" /></p>
<h3 id="聚类调查">聚类调查</h3>
<p>我们没有忘记我们的主要挑战：确定聚类。我们确定了未知数的后验分布。我们绘制下面的中心和标准差变量的后验分布：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">8</span>))</span><br><span class="line">std_trace = posterior_sds_2_</span><br><span class="line">prev_std_trace = posterior_sds_</span><br><span class="line"></span><br><span class="line">_i = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, _i[<span class="number">2</span> * i])</span><br><span class="line">    plt.title(<span class="string">&quot;聚类%d的后验&quot;</span> % i)</span><br><span class="line">    plt.hist(center_trace[:, i], color=colors[i], bins=<span class="number">30</span>,</span><br><span class="line">             histtype=<span class="string">&quot;stepfilled&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, _i[<span class="number">2</span> * i + <span class="number">1</span>])</span><br><span class="line">    plt.title(<span class="string">&quot;聚类%d方差的后验&quot;</span> % i)</span><br><span class="line">    plt.hist(std_trace[:, i], color=colors[i], bins=<span class="number">30</span>,</span><br><span class="line">             histtype=<span class="string">&quot;stepfilled&quot;</span>)</span><br><span class="line">    <span class="comment"># plt.autoscale(tight=True)</span></span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_33_0.png" /></p>
<p>MCMC算法提出两个聚类中最可能的中心分别接近120和200。类似的推断可以应用于标准偏差。</p>
<p>在本章的PyMC3版本中，我们描述了每个数据点的标签的后验分布。但是，在我们的TFP版本中，由于我们的模型边缘化了赋值变量，因此我们没有来自MCMC的该变量的迹线。</p>
<p>作为替代，下面我们可以在分配上生成后验预测分布，然后从中生成一些样本。</p>
<p>以下是对此的可视化。 y轴代表来自后验预测分布的样本。
x轴是原始数据点的排序值。红色方块是对簇0的赋值，蓝色方块是对簇1的赋值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将数据放入张量</span></span><br><span class="line">data = tf.constant(data_,dtype=tf.float32)</span><br><span class="line">data = data[:,tf.newaxis]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 他根据MCMC链生成一个聚类</span></span><br><span class="line">rv_clusters_1 = tfd.Normal(posterior_centers_2_[:, <span class="number">0</span>], posterior_sds_2_[:, <span class="number">0</span>])</span><br><span class="line">rv_clusters_2 = tfd.Normal(posterior_centers_2_[:, <span class="number">1</span>], posterior_sds_2_[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算每个群集的对数概率</span></span><br><span class="line">cluster_1_log_prob = rv_clusters_1.log_prob(data) + tf.math.log(posterior_prob_2_)</span><br><span class="line">cluster_2_log_prob = rv_clusters_2.log_prob(data) + tf.math.log(<span class="number">1.</span> - posterior_prob_2_)</span><br><span class="line"></span><br><span class="line">x = tf.stack([cluster_1_log_prob, cluster_2_log_prob],axis=-<span class="number">1</span>)</span><br><span class="line">y = tf.math.reduce_logsumexp(x,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 贝叶斯规则计算分配概率：P（cluster = 1 | data）αP（data | cluster = 1）P（cluster = 1）</span></span><br><span class="line">log_p_assign_1 = cluster_1_log_prob - tf.math.reduce_logsumexp(tf.stack([cluster_1_log_prob, cluster_2_log_prob], axis=-<span class="number">1</span>), -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 整个MCMC链的平均值</span></span><br><span class="line">log_p_assign_1 = tf.math.reduce_logsumexp(log_p_assign_1, -<span class="number">1</span>) - tf.math.log(tf.cast(log_p_assign_1.shape[-<span class="number">1</span>], tf.float32))</span><br><span class="line"> </span><br><span class="line">p_assign_1 = tf.exp(log_p_assign_1)</span><br><span class="line">p_assign = tf.stack([p_assign_1,<span class="number">1</span>-p_assign_1],axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for charting </span></span><br><span class="line">probs_assignments = p_assign_1 </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">burned_assignment_trace_ = evaluate(tfd.Categorical(probs=p_assign).sample(sample_shape=<span class="number">200</span>))</span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">5</span>))</span><br><span class="line">plt.cmap = mpl.colors.ListedColormap(colors)</span><br><span class="line">plt.imshow(burned_assignment_trace_[:, np.argsort(data_)],</span><br><span class="line">       cmap=plt.cmap, aspect=<span class="number">.4</span>, alpha=<span class="number">.9</span>)</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>, data_.shape[<span class="number">0</span>], <span class="number">40</span>),</span><br><span class="line">       [<span class="string">&quot;%.2f&quot;</span> % s <span class="keyword">for</span> s <span class="keyword">in</span> np.sort(data_)[::<span class="number">40</span>]])</span><br><span class="line">plt.ylabel(<span class="string">&quot;后验样本&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;第$i$个数据点的值&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;数据点的后验标签&quot;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_36_0.png" /></p>
<p>看看上面的绘图，似乎最不确定性在150到170之间。上面的绘图略微歪曲事物，因为x轴不是真正的比例（它显示第<span
class="math inline">\(i\)</span>排序数据点的值。
）下面是一个更清晰的图表，我们估算了属于标签0和1的每个数据点的<em>频率</em>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">cmap = mpl.colors.LinearSegmentedColormap.from_list(<span class="string">&quot;BMH&quot;</span>, colors)</span><br><span class="line">assign_trace = evaluate(probs_assignments)[np.argsort(data_)]</span><br><span class="line">plt.scatter(data_[np.argsort(data_)], assign_trace, cmap=cmap,</span><br><span class="line">        c=(<span class="number">1</span> - assign_trace), s=<span class="number">50</span>)</span><br><span class="line">plt.ylim(-<span class="number">0.05</span>, <span class="number">1.05</span>)</span><br><span class="line">plt.xlim(<span class="number">35</span>, <span class="number">300</span>)</span><br><span class="line">plt.title(<span class="string">&quot;属于聚类0的数据点的概率&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;概率&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;数据点的值&quot;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_38_0.png" /></p>
<p>即使我们使用正态分布对集群进行建模，我们也没有得到<em>最佳</em>适合数据的正态分布（无论我们的最佳定义是什么），而是正态分布的值的分布。我们如何才能为均值和方差选择一对值并确定<em>八九不离十</em>的高斯分布？</p>
<p>一种快速而肮脏的方式（我们将在第5章中看到它具有很好的理论属性），就是使用后验分布的<em>mean</em>。下面我们使用我们观察到的数据覆盖正态密度函数，使用后验分布的平均值作为所选参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_ = np.linspace(<span class="number">20</span>, <span class="number">300</span>, <span class="number">500</span>)</span><br><span class="line">posterior_center_means_ = evaluate(tf.reduce_mean(posterior_centers_2_, axis=<span class="number">0</span>))</span><br><span class="line">posterior_std_means_ = evaluate(tf.reduce_mean(posterior_sds_2_, axis=<span class="number">0</span>))</span><br><span class="line">posterior_prob_mean_ = evaluate(tf.reduce_mean(posterior_prob_2_, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.hist(data_, bins=<span class="number">20</span>, histtype=<span class="string">&quot;step&quot;</span>, density=<span class="literal">True</span>, color=<span class="string">&quot;k&quot;</span>,</span><br><span class="line">     lw=<span class="number">2</span>, label=<span class="string">&quot;数据直方图&quot;</span>)</span><br><span class="line">y_ = posterior_prob_mean_ * evaluate(tfd.Normal(loc=posterior_center_means_[<span class="number">0</span>],</span><br><span class="line">                                scale=posterior_std_means_[<span class="number">0</span>]).prob(x_))</span><br><span class="line">plt.plot(x_, y_, label=<span class="string">&quot;聚类 0 (使用后验平均参数)&quot;</span>, lw=<span class="number">3</span>)</span><br><span class="line">plt.fill_between(x_, y_, color=colors[<span class="number">1</span>], alpha=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">y_ = (<span class="number">1</span> - posterior_prob_mean_) * evaluate(tfd.Normal(loc=posterior_center_means_[<span class="number">1</span>],</span><br><span class="line">                                      scale=posterior_std_means_[<span class="number">1</span>]).prob(x_))</span><br><span class="line">plt.plot(x_, y_, label=<span class="string">&quot;聚类 1 (使用后验平均参数)&quot;</span>, lw=<span class="number">3</span>)</span><br><span class="line">plt.fill_between(x_, y_, color=colors[<span class="number">0</span>], alpha=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">plt.legend(loc=<span class="string">&quot;upper left&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;使用后验平均参数可视化聚类&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_40_0.png" /></p>
<h3 id="重要提示不要混合后验样本">重要提示：不要混合后验样本</h3>
<p>在上面的示例中，可能的（尽管不太可能）场景是聚类0具有非常大的标准偏差，聚类1具有小的标准偏差。这仍然可以满足证据，尽管不如我们原来的推论那么多。或者，由于数据根本不支持这一假设，因此<em>两个</em>分布都不太可能具有较小的标准偏差。因此，两个标准偏差相互<em>依赖</em>：如果一个很小，另一个必须很大。事实上，<em>所有</em>未知数都以类似的方式相关。例如，如果标准偏差很大，则均值具有更宽的可能实现空间。相反，较小的标准偏差将平均值限制在较小的区域。</p>
<p>在MCMC期间，我们返回的矢量代表来自未知后验的样本。不同矢量的元素不能一起使用，因为这会破坏上述逻辑：可能样本已经返回，聚类1具有小的标准偏差，因此该样本中的所有其他变量将合并并相应地进行调整。但是很容易避免这个问题，只需确保正确索引跟踪。</p>
<p>另一个小例子来说明这一点。假设两个变量<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>通过<span class="math inline">\(x + y =
10\)</span>相关联。我们将<span
class="math inline">\(x\)</span>建模为普通随机变量，均值为4，并探索500个样本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number_of_steps = <span class="number">10000</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:0, max:20000, step:1000&#125;</span></span><br><span class="line">burnin = <span class="number">500</span> <span class="comment">#@param &#123;type:&quot;slider&quot;, min:0, max:500, step:100&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the chain&#x27;s start state.</span></span><br><span class="line">initial_chain_state = [</span><br><span class="line">    tf.to_float(<span class="number">1.</span>) * tf.ones([], name=<span class="string">&#x27;init_x&#x27;</span>, dtype=tf.float32),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the step_size. (It will be automatically adapted.)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):</span><br><span class="line">    step_size = tf.get_variable(</span><br><span class="line">        name=<span class="string">&#x27;step_size&#x27;</span>,</span><br><span class="line">        initializer=tf.constant(<span class="number">0.5</span>, dtype=tf.float32),</span><br><span class="line">        trainable=<span class="literal">False</span>,</span><br><span class="line">        use_resource=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=<span class="built_in">int</span>(burnin * <span class="number">0.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the HMC</span></span><br><span class="line"><span class="comment"># Since we&#x27;re only using one distribution for our simplistic example, </span></span><br><span class="line"><span class="comment"># the use of the bijectors and unnormalized log_prob function is </span></span><br><span class="line"><span class="comment"># unneccesary</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># While not a good example of what to do if you have dependent </span></span><br><span class="line"><span class="comment"># priors, this IS a good example of how to set up just one variable </span></span><br><span class="line"><span class="comment"># with a simple distribution</span></span><br><span class="line">hmc=tfp.mcmc.HamiltonianMonteCarlo(</span><br><span class="line">        target_log_prob_fn=tfd.Normal(name=<span class="string">&quot;rv_x&quot;</span>, loc=tf.to_float(<span class="number">4.</span>), </span><br><span class="line">                                      scale=tf.to_float(<span class="number">1.</span>/np.sqrt(<span class="number">10.</span>))).log_prob,</span><br><span class="line">        num_leapfrog_steps=<span class="number">2</span>,</span><br><span class="line">        step_size=step_size,</span><br><span class="line">        step_size_update_fn=step_size_update_fn,</span><br><span class="line">        state_gradients_are_stopped=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sampling from the chain.</span></span><br><span class="line">[</span><br><span class="line">    x_samples,</span><br><span class="line">], kernel_results = tfp.mcmc.sample_chain(</span><br><span class="line">    num_results = number_of_steps,</span><br><span class="line">    num_burnin_steps = burnin,</span><br><span class="line">    current_state=initial_chain_state,</span><br><span class="line">    kernel=hmc,</span><br><span class="line">    name=<span class="string">&#x27;HMC_sampling&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">y_samples = <span class="number">10</span> - x_samples</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize any created variables for preconditions</span></span><br><span class="line">init_g = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Running</span></span><br><span class="line">evaluate(init_g)</span><br><span class="line">[</span><br><span class="line">    x_samples_,</span><br><span class="line">    y_samples_,</span><br><span class="line">] = evaluate([</span><br><span class="line">    x_samples,</span><br><span class="line">    y_samples,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(np.arange(number_of_steps), x_samples_, color=TFColor[<span class="number">3</span>], alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.plot(np.arange(number_of_steps), y_samples_, color=TFColor[<span class="number">0</span>], alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Displaying (extreme) case of dependence between unknowns&#x27;</span>, fontsize=<span class="number">14</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Text(0.5, 1.0, 'Displaying (extreme) case of dependence between
unknowns')</p>
</blockquote>
<p><img src="/2019/07/27/tfp-ch3/output_42_1.png" /></p>
<p>正如您所看到的，这两个变量并不相关，将<span
class="math inline">\(x\)</span>样本的<span
class="math inline">\(x\)</span>添加到<span
class="math inline">\(y\)</span>的<span
class="math inline">\(j\)</span>样本中是错误的，除非<span
class="math inline">\(i=j\)</span>。</p>
<h3 id="回到聚类预测">回到聚类：预测</h3>
<p>上面的聚类可以推广到<span class="math inline">\(k\)</span>
个聚类。选择<span class="math inline">\(k =
2\)</span>可以让我们更好地可视化MCMC，并检查一些非常有趣的图。</p>
<p>预测怎么样？假设我们观察到一个新的数据点，比如<span
class="math inline">\(x =
175\)</span>，我们希望将它标记为一个集群。简单地将其分配给<em>接近</em>集群中心是愚蠢的，因为这忽略了集群的标准偏差，我们从上面的图中看到，这种考虑非常重要。更正式地说：我们对向集群1分配<span
class="math inline">\(x =
175\)</span>的<em>概率</em>（因为我们不能确定标签）感兴趣。将$ x <span
class="math inline">\(的赋值表示为\)</span>L_x<span
class="math inline">\(，等于0或1 ，我们感兴趣的是\)</span>P(L_x = 1 ; |
; x = 175)$。</p>
<p>一种简单的计算方法是重新运行上面的MCMC并附加附加数据点。这种方法的缺点是推断每个新颖数据点的速度很慢。或者，我们可以尝试<em>不太精确</em>，但更快的方法。</p>
<p>我们将使用贝叶斯定理。如果你还记得，贝叶斯定理看起来像：</p>
<p><span class="math display">\[ P( A | X ) = \frac{ P( X  | A )P(A)
}{P(X) }\]</span></p>
<p>在我们的例子中，$ A <span class="math inline">\(代表\)</span> L_x = 1
<span class="math inline">\(和\)</span> X <span
class="math inline">\(是我们的证据：我们观察到\)</span> x = 175 <span
class="math inline">\(。对于我们后验分布的特定样本参数集\)</span>（_0，_0，_1，_1，p）<span
class="math inline">\(，我们有兴趣询问“\)</span>x$在群集1
<em>中的概率是否更大</em>比它在集群0中的概率？“，其中概率取决于所选择的参数。</p>
<p><span class="math display">\[
\begin{align}
&amp; P(L_x = 1| x = 175 ) \gt P(L_x = 0| x = 175 ) \\[5pt]
&amp; \frac{ P( x=175  | L_x = 1  )P( L_x = 1 ) }{P(x = 175) } \gt
\frac{ P( x=175  | L_x = 0  )P( L_x = 0 )}{P(x = 175) }
\end{align}
\]</span></p>
<p>由于分母是相等的，它们可以被忽略（并且很好地消除，因为计算数量$ P（x
= 175）$可能很困难）。</p>
<p><span class="math display">\[  P( x=175  | L_x = 1  )P( L_x = 1 )
\gt  P( x=175  | L_x = 0  )P( L_x = 0 ) \]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p_trace = posterior_prob_2_[<span class="number">25000</span>:]</span><br><span class="line"></span><br><span class="line">x = <span class="number">175</span></span><br><span class="line"></span><br><span class="line">v = (<span class="number">1</span> - p_trace) * evaluate(tfd.Normal(loc=center_trace[<span class="number">25000</span>:, <span class="number">1</span>], </span><br><span class="line">                                        scale=std_trace[<span class="number">25000</span>:, <span class="number">1</span>]).log_prob(x)) &gt; \</span><br><span class="line">                                        p_trace * evaluate(tfd.Normal(loc=center_trace[<span class="number">25000</span>:, <span class="number">0</span>], \</span><br><span class="line">                                        scale=std_trace[<span class="number">25000</span>:, <span class="number">0</span>]).log_prob(x))</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;属于聚类1的概率:&quot;</span>, (v.mean()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>属于聚类1的概率: 0.03192</p>
</blockquote>
<p>给我们一个概率而不是标签是一件非常有用的事情。而不是简单的：</p>
<blockquote>
<p>L = 1 if prob &gt; 0.5 else 0</p>
</blockquote>
<p>我们可以使用<em>损失函数</em>来优化我们的猜测，这是第五章的全部内容。</p>
<h2 id="diagnosing-convergence">Diagnosing Convergence</h2>
<h3 id="自相关">自相关</h3>
<p>自相关是衡量一系列数字与自身相关程度的指标。
1.0的测量是完全正自相关，0没有自相关，-1是完全负相关。如果你熟悉标准<em>相关</em>，那么自相关就是$
t_k <span class="math inline">\(时刻和时刻\)</span> t_k
$序列的相关性。</p>
<p><span class="math display">\[R(k) = \text{Corr}( x_t, x_{t-k} )
\]</span></p>
<p>例如，考虑两个序列：</p>
<p><span class="math display">\[x_t \sim \text{Normal}(0,1), \;\; x_0 =
0\]</span> <span class="math display">\[y_t \sim \text{Normal}(y_{t-1},
1 ), \;\; y_0 = 0\]</span></p>
<p>其中包含示例路径：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_t = evaluate(tfd.Normal(loc=<span class="number">0.</span>, scale=<span class="number">1.</span>).sample(sample_shape=<span class="number">200</span>))</span><br><span class="line">x_t[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">y_t = evaluate(tf.zeros(<span class="number">200</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">200</span>):</span><br><span class="line">    y_t[i] = evaluate(tfd.Normal(loc=y_t[i - <span class="number">1</span>], scale=<span class="number">1.</span>).sample())</span><br><span class="line"></span><br><span class="line">plt.figure(figsize(<span class="number">12.5</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(y_t, label=<span class="string">&quot;$y_t$&quot;</span>, lw=<span class="number">3</span>)</span><br><span class="line">plt.plot(x_t, label=<span class="string">&quot;$x_t$&quot;</span>, lw=<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;time, $t$&quot;</span>)</span><br><span class="line">plt.legend();</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_49_0.png" /></p>
<p>想到自相关的一种方法是“如果我知道系列在$ s <span
class="math inline">\(时的位置，它能帮助我知道我在时间\)</span> t <span
class="math inline">\(的位置吗？”在\)</span> x_t <span
class="math inline">\(系列中，答案是否定的。通过构造，\)</span> x_t
<span class="math inline">\(是随机变量。如果我告诉你\)</span> x_2 = 0.5
<span class="math inline">\(，你能给我一个更好的猜测\)</span> x_3
$吗？没有。</p>
<p>另一方面，$ y_t <span
class="math inline">\(是自相关的。通过构造，如果我知道\)</span> y_2 = 10
<span class="math inline">\(，我可以非常自信\)</span> y_3 <span
class="math inline">\(与10相差不多。同样，我甚至可以对\)</span> y_4
<span
class="math inline">\(进行（不太自信的猜测）：它可能会不要接近0或20，但值不是太小。我可以就\)</span>
y_5 <span
class="math inline">\(做出类似的争论，但同样，我不太自信。考虑到这个逻辑结论，我们必须承认，作为\)</span>
k $，时间点之间的滞后会增加自相关减少。我们可以想象这个：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">autocorr</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># from http://tinyurl.com/afz57c4</span></span><br><span class="line">    result = np.correlate(x, x, mode=<span class="string">&#x27;full&#x27;</span>)</span><br><span class="line">    result = result / np.<span class="built_in">max</span>(result)</span><br><span class="line">    <span class="keyword">return</span> result[result.size // <span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line">colors = [TFColor[<span class="number">3</span>], TFColor[<span class="number">0</span>], TFColor[<span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">1</span>, <span class="number">200</span>)</span><br><span class="line">plt.bar(x, autocorr(y_t)[<span class="number">1</span>:], width=<span class="number">1</span>, label=<span class="string">&quot;$y_t$&quot;</span>,</span><br><span class="line">        edgecolor=colors[<span class="number">0</span>], color=colors[<span class="number">0</span>])</span><br><span class="line">plt.bar(x, autocorr(x_t)[<span class="number">1</span>:], width=<span class="number">1</span>, label=<span class="string">&quot;$x_t$&quot;</span>,</span><br><span class="line">        color=colors[<span class="number">1</span>], edgecolor=colors[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.legend(title=<span class="string">&quot;自相关&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;$y_t$ 和 $y_&#123;t-k&#125;$ \n测量相关性 &quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;k (lag)&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;$y_t$和$x_t$不同$k$滞后的相关性图表&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_51_0.png" /></p>
<p>请注意，随着<span class="math inline">\(k\)</span>的增加，<span
class="math inline">\(y_t\)</span>的自相关从非常高的点开始减少。与<span
class="math inline">\(x_t\)</span>的自相关相比，它看起来像噪声（实际上是它），因此我们可以得出结论，在这个系列中不存在自相关。</p>
<h3 id="这与mcmc收敛有何关系">这与MCMC收敛有何关系?</h3>
<p>根据MCMC算法的性质，我们将始终返回显示自相关的样本（这是因为从您当前位置开始的步骤，移动到您附近的位置）。</p>
<p>没有很好地探索太空的链条将表现出非常高的自相关性。在视觉上，如果迹象似乎像河流一样蜿蜒而不能安定下来，那么链条将具有很高的自相关性。</p>
<p>这并不意味着收敛的MCMC具有低自相关性。因此，收敛不需要低自相关，但这已足够。
TFP也有内置的自相关工具。</p>
<h3 id="细化">细化</h3>
<p>如果后验样本之间存在高度自相关，则会出现另一个问题。许多后处理算法要求样本彼此<em>独立</em>。这可以通过仅每个$
n <span
class="math inline">\(样本返回给用户来解决或至少减少，从而消除一些自相关。下面我们使用不同的细化级别执行\)</span>
y_t $的自相关图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_x = <span class="number">200</span> // <span class="number">3</span> + <span class="number">1</span></span><br><span class="line">x = np.arange(<span class="number">1</span>, max_x)</span><br><span class="line"></span><br><span class="line">plt.bar(x, autocorr(y_t)[<span class="number">1</span>:max_x], edgecolor=colors[<span class="number">0</span>],</span><br><span class="line">        label=<span class="string">&quot;no thinning&quot;</span>, color=colors[<span class="number">0</span>], width=<span class="number">1</span>)</span><br><span class="line">plt.bar(x, autocorr(y_t[::<span class="number">2</span>])[<span class="number">1</span>:max_x], edgecolor=colors[<span class="number">1</span>],</span><br><span class="line">        label=<span class="string">&quot;keeping every 2nd sample&quot;</span>, color=colors[<span class="number">1</span>], width=<span class="number">1</span>)</span><br><span class="line">plt.bar(x, autocorr(y_t[::<span class="number">3</span>])[<span class="number">1</span>:max_x], width=<span class="number">1</span>, edgecolor=colors[<span class="number">2</span>],</span><br><span class="line">        label=<span class="string">&quot;keeping every 3rd sample&quot;</span>, color=colors[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">plt.autoscale(tight=<span class="literal">True</span>)</span><br><span class="line">plt.legend(title=<span class="string">&quot;$y_t$自相关图&quot;</span>, loc=<span class="string">&quot;upper right&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;$y_t$和$y_&#123;t-k&#125;$\n测量相关性 &quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;k (lag)&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;$y_t$和不同的$k$滞后的自相关 (没有细化 vs. 细化).&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/27/tfp-ch3/output_54_0.png" /></p>
<p>随着更薄，自相关性下降得更快。但是需要权衡：更高的细化需要更多的MCMC迭代才能获得相同数量的返回样本。例如，未填充的10
000个样本为10万，稀疏度为10（尽管后者具有较少的自相关性）。</p>
<p>什么是稀释量很大？无论进行多少细化，返回的样本将始终显示一些自相关。只要自相关趋于零，你就可以了。通常不需要超过10的减薄。</p>
<h2 id="mcmc的有用提示">MCMC的有用提示</h2>
<p>如果不是MCMC的计算困难，贝叶斯推断将是<em>事实上的</em>方法。事实上，MCMC是大多数用户拒绝实际贝叶斯推理的原因。下面我介绍一些很好的启发式方法来帮助收敛并加速MCMC引擎：</p>
<h3 id="智能启动值">智能启动值</h3>
<p>在后验分布附近启动MCMC算法会很棒，因此开始正确采样将花费很少的时间。通过在“随机”变量创建中指定<code>testval</code>参数，我们可以通过告诉我们<em>认为</em>后验分布将在何处来帮助算法。在许多情况下，我们可以对参数进行合理的猜测。例如，如果我们有来自Normal分布的数据，并且我们希望估计$
$参数，那么一个好的起始值将是数据的* mean *。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mu = tfd.Uniform(name=<span class="string">&quot;mu&quot;</span>, low=<span class="number">0.</span>, high=<span class="number">100.</span>).sample(seed=data.mean())</span><br></pre></td></tr></table></figure>
<p>对于模型中的大多数参数，有一个频繁的估计。这些估计值对于我们的MCMC算法来说是一个很好的起始值。当然，对于某些变量来说，这并不总是可行的，但包括尽可能多的适当初始值总是一个好主意。即使您的猜测是错误的，MCMC仍然会收敛到正确的分布，因此几乎没有损失。</p>
<h4 id="priors-先验">Priors 先验</h4>
<p>如果先验选择不当，MCMC算法可能不会收敛，或者至少难以收敛。考虑如果先前选择的甚至不包含真实参数可能发生的事情：先验为未知分配0概率，因此后验也将分配0概率。这可能导致病理结果。</p>
<p>因此，最好仔细选择先验。通常情况下，缺乏掩盖或样本拥挤到边界的证据意味着所选择的先验有些问题（参见下面的<em>统计计算的民间定理</em>）。</p>
<h4 id="统计计算的民间定理">统计计算的民间定理</h4>
<blockquote>
<p><em>如果您遇到计算问题，可能您的模型是错误的.</em></p>
</blockquote>
<h2 id="结论">结论</h2>
<p>TFP为执行贝叶斯推理提供了非常强大的后端，主要是因为它允许用户微调MCMC的内部工作。</p>
<h3 id="references">References</h3>
<p>[1] Tensorflow Probability API docs.
https://www.tensorflow.org/probability/api_docs/python/tfp</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" rel="tag">概率论</a></li></ul></div><div class="post-nav"><a class="pre" href="/2019/07/28/tfp-ch4/">概率模型第四章 ： 大数定理</a><a class="next" href="/2019/07/26/tfp-ch2/">概率模型第二章 ： A little more on TFP</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>