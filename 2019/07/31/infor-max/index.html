<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>互信息：无监督提取特征 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">互信息：无监督提取特征</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">互信息：无监督提取特征</h1><div class="post-meta">2019-07-31<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4.2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 19</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>本文是对苏剑林的<a
target="_blank" rel="noopener" href="https://kexue.fm/archives/6024">深度学习的互信息：无监督提取特征</a>的学习总结,主要是关于<code>Deep INFOMAX</code>的论文复现.</p>
<span id="more"></span>
<h1 id="互信息">互信息</h1>
<p>大概是需要有一个标准来衡量<strong>两个值统一出现的概率</strong>,如果这个概率很大,说明两个值倾向于<strong>共同出现</strong>,如果这个概率很小,就代表他们<strong>刻意回避对方</strong>。这个值定义为点互信息(Pointwise
Mutual Information，PMI): <span class="math display">\[
\begin{aligned}
    PMI(x,z)&amp;=p(x,z)\log\frac{p(x,z)}{p(x)p(z)}
\end{aligned}
\]</span></p>
<p>因为<span
class="math inline">\(\log\)</span>的原因，所以两个多元变量的相关度可以被累加：</p>
<p><span class="math display">\[
\begin{aligned}
    I(X,Z)&amp;=\sum^k_{i=1}\sum^l_{j=1}p(x_i,z_j)
\log\frac{p(x_i,z_j)}{p(x_i)p(z_j)} \ \ \ \ \text{离散型}\\
    &amp;=\int_Z\int_X p(x,z)\log\frac{p(x,z)}{p(x)p(z)}\ dx\ dz\ \ \ \
\text{连续型}
\end{aligned}
\]</span></p>
<p>细心的小伙伴可以看出，这个不就是<code>KL散度</code>么,如下： <span
class="math display">\[
\begin{aligned}
    I(X,Z)&amp;=KL(p(x,z)\|p(x)p(z))
\end{aligned}
\]</span> 且假设<span class="math inline">\(p(x)\)</span>和<span
class="math inline">\(p(z)\)</span>相互独立时，则： <span
class="math display">\[
\begin{aligned}
    \because p(x|z)&amp;=\frac{p(x,z)}{p(z)}\\
    \therefore I(X,Z)&amp;=\int_Z\int_X\ p(z)
p(x|z)\log\frac{p(x|z)}{p(x)}\ dx\ dz\\
    &amp;=\mathbb{E}[KL(\ p(x|z)\ \| \ p(z)\ )]\\
    \text{或}\\
    \because p(z|x)&amp;=\frac{p(x,z)}{p(x)}\\
    \therefore I(X,Z)&amp;=\int_Z\int_X\ p(x)
p(z|x)\log\frac{p(z|x)}{p(z)}\ dx\ dz\\
    &amp;=\mathbb{E}[KL(\ p(z|x)\ \| \ p(x)\ )]
\end{aligned}
\]</span></p>
<h1 id="自编码器">自编码器</h1>
<p>通常自编码器都配有一个解码器，使用自编码对原始数据进行解析并<strong>保留尽可能多的重要特征</strong>，那么如何验证自编码器所解析的特征是<strong>最有效</strong>的呢？</p>
<h2 id="关于重构">关于重构</h2>
<p>传统思想是使用解码器将去重构原始图像，但使用低维编码重构原图的结果通常是很模糊的。比如我们都能轻松辨认一个物体是什么，但是让我们将他完整地画出来是很难的，这就说明对于任务来说，<strong>最合理的特征并不能一定能完成图像重构。</strong></p>
<h1 id="最大化互信息">最大化互信息</h1>
<h2 id="互信息引出">互信息引出</h2>
<p>既然重构不是好特征的必要条件。那好特征的基本原则应当是<code>能够从整个数据集中辨别出该样本出来</code>，也就是说，提取出该样本（最）独特的信息。如何衡量提取出来的信息是该样本独特的呢？我们用<code>互信息</code>来衡量。</p>
<p>用<span class="math inline">\(X\)</span>表示原始图像的集合，<span
class="math inline">\(x\in X\)</span>表示其中一图像，<span
class="math inline">\(\tilde{p}(x)\)</span>表示采样所得<span
class="math inline">\(x\)</span>的分布，<span
class="math inline">\(Z\)</span>为编码向量集合，<span
class="math inline">\(z\in Z\)</span>表示其中一编码向量，<span
class="math inline">\(p(z|x)\)</span>表示<span
class="math inline">\(x\)</span>所产生的编码向量<span
class="math inline">\(z\)</span>的分布，我们假设它为正态分布且<span
class="math inline">\(\tilde{p}(x)\)</span>和<span
class="math inline">\(p(z)\)</span><strong>相互独立</strong>，即 <span
class="math display">\[
\begin{aligned}
    p(z) = \int p(z|x)\tilde{p}(x)\ dx
\end{aligned}
\]</span></p>
<p>接下来使用互信息来表示<span
class="math inline">\(X，Z\)</span>的相关性： <span
class="math display">\[
\begin{aligned}
I(X,Z) &amp;= \iint p(z,x)\log \frac{p(z,x)}{\tilde{p}(x)p(z)}\ dx\ dz
\\
&amp;= \iint p(z,x)\log \frac{p(z|x)}{p(z)}\ dx\ dz\\
&amp;= \iint p(z|x)\tilde{p}(x)\log \frac{p(z|x)}{p(z)}\ dx\ dz
\end{aligned}
\]</span></p>
<p>那么接下来我们就要使互信息尽可能的大，来得到一个好的特征编码器：
<span class="math display">\[
\begin{aligned}
p(z|x) = -\min_{p(z|x)} I(X,Z)
\end{aligned}
\]</span></p>
<p>互信息越大就代表<span class="math inline">\(\log
\frac{p(z|x)}{p(z)}\)</span>越大，说明<span
class="math inline">\(p(z|x)\gg p(x)\)</span>,则此时的解码出向量<span
class="math inline">\(p(z|x)\)</span>和<span
class="math inline">\(p(x)\)</span><strong>同时出现</strong>的概率越大，那就代表编码器找到了专属于<span
class="math inline">\(x\)</span>的那个<span
class="math inline">\(z\)</span>，这不就是我们所需要的独特信息么。</p>
<h2 id="先验分布">先验分布</h2>
<p>前面提到，相对于自编码器，变分自编码器同时还希望隐变量服从标准正态分布的先验分布，<strong>这有利于使得编码空间更加规整，甚至有利于解耦特征，便于后续学习</strong>。因此，在这里我们同样希望加上这个约束。</p>
<pre><code>关于服从于标准正态就可以有利于解耦特征，我暂时没有找到相关的资料，这里是摘录的原文。</code></pre>
<p>利用在<code>VAE</code>中思路(<a
href="https://zhen8838.github.io/2019/07/29/vae2/">参考VAE直观推导</a>)添加<code>KL散度</code>约束来实现，假设<span
class="math inline">\(q(z)\sim N(0,1)\)</span>,则： <span
class="math display">\[
    KL(q(z)\|p(z))=\int p(z)\log\frac{p(z)}{q(z)}\ dz
\]</span></p>
<h2 id="化简先验分布">化简先验分布</h2>
<p>将<code>KL散度</code>约束加权并与互信息约束混合得： <span
class="math display">\[
\begin{aligned}
    p(z|x) &amp;= \min_{p(z|x)}\left\{- I(X,Z) + \lambda KL(p(z)\Vert
q(z))\right\}\\
    =\min_{p(z|x)}&amp;\left\{- I(X,Z)+ \lambda\int p(z)\log
\frac{p(z)}{q(z)}\ dz\right\} \\
    =\min_{p(z|x)}&amp;\left\{- I(X,Z)+ \lambda\left(\int p(z)\log
\left(\frac{p(z|x)}{q(z)}\times\frac{p(z)}{p(z|x)}\right)\
dz\right)\right\} \\
    =\min_{p(z|x)}&amp;\left\{- I(X,Z)+ \lambda\left(\int
p(z)\log\frac{p(z|x)}{q(z)} \ dz-\int p(z)\log\frac{p(z|x)}{p(z)} \
dz\right)\right\} \\
    =\min_{p(z|x)}&amp;\left\{- \iint p(z)\log \frac{p(z|x)}{p(z)}\ dx\
dz+ \lambda\left(\int p(z)\log\frac{p(z|x)}{q(z)} \ dz-\int
p(z)\log\frac{p(z|x)}{p(z)} \ dz\right)\right\} \\
    =\min_{p(z|x)}&amp;\left\{- (1+\lambda)\iint p(z)\log
\frac{p(z|x)}{p(z)}\ dx\ dz+ \lambda\int p(z)\log\frac{p(z|x)}{q(z)} \
dz\right\} \\
    =\min_{p(z|x)}&amp;\left\{\iint p(z|x)\tilde{p}(x) \left[-
(1+\lambda)\log\frac{p(z|x)}{p(z)}+\lambda\log\frac{p(z|x)}{q(x)}\right]\
dx\ dz \right\} \\
    =\min_{p(z|x)}&amp;\left\{-(1+\lambda)\cdot I(X,Z)+\lambda\cdot
\mathbb{E}_{x\sim\tilde{p}(x)}\left[KL(p(z|x)||q(z))\right] \right\} \\
    =\min_{p(z|x)}&amp;\left\{-(1+\lambda)\cdot KL(p(z|x)\tilde{p}(x)\|
p(z)\tilde{p}(x)) +\lambda\cdot
\mathbb{E}_{x\sim\tilde{p}(x)}\left[KL(p(z|x)||q(z))\right] \right\}
\end{aligned}
\]</span></p>
<p>现在发现先验分布在化简后被去除了，并且约束函数中后面一部分就是<code>VAE</code>中的约束，比较容易实现，接下来考虑如何构建<code>互信息约束</code>。</p>
<h2 id="互信息转化">互信息转化</h2>
<p>在第一部分提到了互信息可以化为<code>KL散度</code>的形式：</p>
<p><span class="math display">\[
\begin{aligned}
    I(X,Z) =&amp; \iint p(z|x)\tilde{p}(x)\log
\frac{p(z|x)\tilde{p}(x)}{p(z)\tilde{p}(x)}dxdz\\
    =&amp; KL(p(z|x)\tilde{p}(x)\Vert p(z)\tilde{p}(x))
\end{aligned}
\]</span></p>
<p>这个形式展示了互信息的本质含义：<span
class="math inline">\(p(z|x)\tilde{p}(x)\)</span>表示变量<span
class="math inline">\(x,z\)</span>的联合分布，<span
class="math inline">\(p(z)\tilde{p}(x)\)</span>表示随机抽取一个<span
class="math inline">\(x\)</span>和一个<span
class="math inline">\(z\)</span>时的分布(假设不相关)，而互信息则是这两个分布的<code>KL散度</code>，那么最大化互信息意思就是拉大这两个分布间的距离。</p>
<p><strong>一个严重问题是<code>KL散度</code>理论上是无上界的，我们不能去最大化一个无上界的量</strong>。为了有效优化，抓住</p>
<blockquote>
<p>最大化互信息就是拉大<span
class="math inline">\(p(z|x)\tilde{p}{x}\)</span>和<span
class="math inline">\(p(z)\tilde{p}(x)\)</span>之间的距离</p>
</blockquote>
<p>这个核心要点，我们可以选择其他的度量函数来进行约束，可以选择<code>JS散度</code>和<code>Hellinger距离</code>。这里使用<code>JS散度</code>：</p>
<p><span class="math display">\[
\begin{aligned}
    JS(P,Q) =
\frac{1}{2}KL\left(P\left\Vert\frac{P+Q}{2}\right.\right)+\frac{1}{2}KL\left(Q\left\Vert\frac{P+Q}{2}\right.\right)
\end{aligned}   
\]</span></p>
<p><code>JS散度</code>同样衡量了两个分布的距离，但他具备上界<span
class="math inline">\(\frac{1}{2}\log2\)</span>，我们最大化他的时候，也可以起到最大化互信息的效果，下面将<code>JS散度</code>代入约束函数。</p>
<p><span class="math display">\[
\begin{aligned}
    p(z|x) =\min_{p(z|x)}&amp;\left\{-(1+\lambda)\cdot
JS(p(z|x)\tilde{p}(x)\| p(z)\tilde{p}(x)) +\lambda\cdot
\mathbb{E}_{x\sim\tilde{p}(x)}\left[KL(p(z|x)||q(z))\right] \right\}
\end{aligned}
\]</span></p>
<p>但是还是存在一个<span
class="math inline">\(p(z)\)</span>让我们无法解决，下面就要把他解决：</p>
<h2 id="变分推断">变分推断</h2>
<p>参考<a
target="_blank" rel="noopener" href="https://kexue.fm/archives/6016">f-GAN简介：GAN模型的生产车间</a>中的<code>f散度局部变分推断</code>中的距离定义为：</p>
<p><span class="math display">\[
\begin{aligned}\mathcal{D}_f(P\Vert Q) =&amp; \max_{T}\int q(x)
\left[\frac{p(x)}{q(x)}T\left(\frac{p(x)}{q(x)}\right)-g\left(T\left(\frac{p(x)}{q(x)}\right)\right)\right]\
dx\\
=&amp; \max_{T}\int\left[p(x)\cdot
T\left(\frac{p(x)}{q(x)}\right)-q(x)\cdot
g\left(T\left(\frac{p(x)}{q(x)}\right)\right)\right]\ dx \\
\text{记}T\left(\frac{p(x)}{q(x)}\right)\text{为}&amp;T(x)\text{得}\\
\mathcal{D}_f(P\Vert Q) =&amp; \max_{T}\Big(\mathbb{E}_{x\sim
p(x)}[T(x)]-\mathbb{E}_{x\sim q(x)}[g(T(x))]\Big)
\end{aligned}
\]</span></p>
<p>利用上面这个公式，我们可以进行<code>f散度估计</code>，意思是：分别从两个分布中采样，然后分别计算<span
class="math inline">\(T(x)\)</span>和<span
class="math inline">\(g(T(x))\)</span>的均值，优化函数<span
class="math inline">\(T(x)\)</span>使得他们的差值尽可能的大，最终的结果即为<code>f散度</code>的近似值了。显然<span
class="math inline">\(T(x)\)</span>可以用神经网络进行拟合，我们优化此函数就是优化神经网络参数。</p>
<p>将上述公式代入<code>JS散度</code>中得： <span class="math display">\[
\begin{aligned}
    &amp;JS(P,Q) \\
    =&amp; \max_{T}\Big(\mathbb{E}_{x\sim p(x)}[\log \sigma(T(x))] +
\mathbb{E}_{x\sim q(x)}[\log(1-\sigma(T(x))]\Big) \\
    \\
    &amp;JS\big(p(z|x)\tilde{p}(x), p(z)\tilde{p}(x)\big)\\
    =&amp; \max_{T}\Big(\mathbb{E}_{(x,z)\sim p(z|x)\tilde{p}(x)}[\log
\sigma(T(x,z))] + \mathbb{E}_{(x,z)\sim
p(z)\tilde{p}(x)}[\log(1-\sigma(T(x,z))]\Big)
\end{aligned}
\]</span></p>
<p><strong>注意</strong>:这里的<span
class="math inline">\(1-\sigma(T(x))\)</span>是根据此前文章中的变分推断所计算的激活函数<span
class="math inline">\(g\)</span>，具体请参考<a
target="_blank" rel="noopener" href="https://kexue.fm/archives/6016">文章</a></p>
<p>并且现在这个约束函数可以看成<code>负采样估计</code>：引入一个判别网络<span
class="math inline">\(\sigma(T(x,z))\)</span>，<span
class="math inline">\(x\)</span>和对应的<span
class="math inline">\(z\)</span>看成一个正样本对，<span
class="math inline">\(x\)</span>与其他一个随机抽取的<span
class="math inline">\(z\)</span>作为负样本对，然后最大化似然函数，等价于最小化交叉熵。终于通过负采样的方式，得到了一种估计<code>JS散度</code>的方案，从而解决了互信息的最大化问题，得到具体的<code>loss</code>为：</p>
<p><span class="math display">\[
\begin{aligned}
p(z|x),T(x,z)=\\
\min_{p(z|x),T(x,z)}\Big\{-&amp;(1+\lambda)\cdot\Big(\mathbb{E}_{(x,z)\sim
p(z|x)\tilde{p}(x)}[\log \sigma(T(x,z))] + \mathbb{E}_{(x,z)\sim
p(z)\tilde{p}(x)}[\log(1-\sigma(T(x,z))]\Big)\\
+&amp;\lambda\cdot \mathbb{E}_{x\sim\tilde{p}(x)}[KL(p(z|x)\Vert
q(z))]\Big\}
\end{aligned}
\]</span></p>
<p>下面开始实际实验。</p>
<h1 id="从全局到局部">从全局到局部</h1>
<p>在实际实验中，如何去践行<code>负采样</code>呢？</p>
<h2 id="batch内负采样">batch内负采样</h2>
<p>我们可以对这个<code>batch</code>中样本对顺序进行<code>shuffle</code>，<code>shuffle</code>前的样本对为正样本，<code>shuffle</code>之后的样本对为负样本。</p>
<h2 id="局部负采样">局部负采样</h2>
<p>上面的做法，实际上就是考虑了整张图片之间的关联，但是我们知道，图片的相关性更多体现在局部中（也就是因为这样所以我们才可以对图片使用CNN）。换言之，图片的识别、分类等应该是一个从局部到整体的过程。因此，有必要把“局部互信息”也考虑进来。</p>
<p>一般的CNN编码过程： <span class="math display">\[
\text{原始图片}x\xrightarrow{\text{多个卷积层}} h\times w\times
c\text{的特征} \xrightarrow{\text{卷积和全局池化}}
\text{固定长度的向量}z
\]</span></p>
<p>我们已经考虑到了<span class="math inline">\(x\)</span>和<span
class="math inline">\(z\)</span>的关联，那么中间层特征(feature
map)和<span
class="math inline">\(z\)</span>的关联呢？把中间层向量记为<span
class="math inline">\(\{C_{ij}(x)|i=1,2,\dots,h;j=1,2,\dots,w\}\)</span>,一共<span
class="math inline">\(h\times w\)</span>个向量与<span
class="math inline">\(z_x\)</span>的互信息，称为<code>局部互信息</code>。</p>
<p>那么我们要构建出一个局部的互信息估算网络，首先把每个通道的向量与<span
class="math inline">\(z_x\)</span>拼接得到<span
class="math inline">\([C_{ij}(x),z_x]\)</span>,相当于得到了一个更大的feature
map，然后对这个feature
map用多个1x1的卷积层来作为局部互信息的估算网络<span
class="math inline">\(T_{local}\)</span>。负样本的选取方法也是用在batch内随机打算的方案。</p>
<p>加入局部互信息的总<code>loss</code>为： <span class="math display">\[
\begin{aligned}
&amp;p(z|x),T_1(x,z),T_2(C_{ij}, z) \\
=&amp;\min_{p(z|x),T_1,T_2}\Big\{-\alpha\cdot\Big(\mathbb{E}_{(x,z)\sim
p(z|x)\tilde{p}(x)}[\log \sigma(T_1(x,z))] + \mathbb{E}_{(x,z)\sim
p(z)\tilde{p}(x)}[\log(1-\sigma(T_1(x,z))]\Big)\\
&amp;\qquad-\frac{\beta}{hw}\sum_{i,j}\Big(\mathbb{E}_{(x,z)\sim
p(z|x)\tilde{p}(x)}[\log \sigma(T_2(C_{ij},z))] + \mathbb{E}_{(x,z)\sim
p(z)\tilde{p}(x)}[\log(1-\sigma(T_2(C_{ij},z))]\Big)\\
&amp;\qquad+\gamma\cdot \mathbb{E}_{x\sim\tilde{p}(x)}[KL(p(z|x)\Vert
q(z))]\Big\}
\end{aligned}
\]</span></p>
<p><strong>注意</strong>：上面的几部分的损失权重分别设置为<span
class="math inline">\(\alpha,\beta,\gamma\)</span>。</p>
<h1 id="代码实现">代码实现</h1>
<p>在<code>Tensorflow 1.14</code>下运行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> tensorflow.python <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.datasets <span class="keyword">import</span> cifar10</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">K.set_session(tf.Session(config=config))</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br><span class="line">x_train = x_train.astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255</span> - <span class="number">0.5</span></span><br><span class="line">x_test = x_test.astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255</span> - <span class="number">0.5</span></span><br><span class="line">y_train = y_train.reshape(-<span class="number">1</span>)</span><br><span class="line">y_test = y_test.reshape(-<span class="number">1</span>)</span><br><span class="line">img_dim = x_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">z_dim = <span class="number">256</span>  <span class="comment"># 隐变量维度</span></span><br><span class="line">α = <span class="number">0.5</span>  <span class="comment"># 全局互信息的loss比重</span></span><br><span class="line">β = <span class="number">1.5</span>  <span class="comment"># 局部互信息的loss比重</span></span><br><span class="line">γ = <span class="number">0.01</span>  <span class="comment"># 先验分布的loss比重</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码器（卷积与最大池化）</span></span><br><span class="line">x_in = Input(shape=(img_dim, img_dim, <span class="number">3</span>))</span><br><span class="line">x = x_in</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    x = Conv2D(z_dim // <span class="number">2</span>**<span class="built_in">int</span>(<span class="number">2</span> - i), kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;SAME&#x27;</span>)(x)</span><br><span class="line">    x = BatchNormalization()(x)</span><br><span class="line">    x = LeakyReLU(<span class="number">0.2</span>)(x)</span><br><span class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line"></span><br><span class="line">feature_map = x  <span class="comment"># 截断到这里，认为到这里是feature_map（局部特征）</span></span><br><span class="line">feature_map_encoder = Model(x_in, x)</span><br><span class="line"></span><br><span class="line">plot_model(feature_map_encoder, to_file=<span class="string">&#x27;feature_map_encoder.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    x = Conv2D(z_dim,</span><br><span class="line">               kernel_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">               padding=<span class="string">&#x27;SAME&#x27;</span>)(x)</span><br><span class="line">    x = BatchNormalization()(x)</span><br><span class="line">    x = LeakyReLU(<span class="number">0.2</span>)(x)</span><br><span class="line"></span><br><span class="line">x = GlobalMaxPooling2D()(x)  <span class="comment"># 全局特征</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">z_μ = Dense(z_dim)(x)  <span class="comment"># 均值，也就是最终输出的编码</span></span><br><span class="line">z_log_σ = Dense(z_dim)(x)  <span class="comment"># 方差，这里都是模仿VAE的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">encoder = Model(x_in, z_μ)  <span class="comment"># 总的编码器就是输出z_μ</span></span><br><span class="line"></span><br><span class="line">plot_model(encoder, to_file=<span class="string">&#x27;encoder.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重参数技巧</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sampling</span>(<span class="params">args</span>):</span><br><span class="line">    z_μ, z_log_σ = args</span><br><span class="line">    u = K.random_normal(shape=K.shape(z_μ))</span><br><span class="line">    <span class="keyword">return</span> z_μ + K.exp(z_log_σ / <span class="number">2</span>) * u</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重参数层，相当于给输入加入噪声</span></span><br><span class="line">z_samples = Lambda(sampling)([z_μ, z_log_σ])</span><br><span class="line">prior_kl_loss = - <span class="number">0.5</span> * K.mean(<span class="number">1</span> + z_log_σ - K.square(z_μ) - K.exp(z_log_σ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle层，打乱第一个轴</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shuffling</span>(<span class="params">x</span>):</span><br><span class="line">    idxs = K.arange(<span class="number">0</span>, K.shape(x)[<span class="number">0</span>])</span><br><span class="line">    idxs = tf.random_shuffle(idxs)</span><br><span class="line">    <span class="keyword">return</span> K.gather(x, idxs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; 构建正负样本对 &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 与随机采样的特征拼接（全局）</span></span><br><span class="line">z_shuffle = Lambda(shuffling)(z_samples)</span><br><span class="line">z_z_true = Concatenate()([z_samples, z_samples])  <span class="comment"># 这就是正样本对</span></span><br><span class="line">z_z_false = Concatenate()([z_samples, z_shuffle])  <span class="comment"># 这就是负样本对</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 与随机采样的特征拼接（局部）</span></span><br><span class="line">feature_map_shuffle = Lambda(shuffling)(feature_map)</span><br><span class="line">z_samples_map = Reshape((<span class="number">4</span>, <span class="number">4</span>, z_dim))(RepeatVector(<span class="number">4</span> * <span class="number">4</span>)(z_samples))  <span class="comment"># 重复z样本到H×W次并reshape</span></span><br><span class="line">z_f_true = Concatenate()([z_samples_map, feature_map])  <span class="comment"># 拼接出局部正样本</span></span><br><span class="line">z_f_false = Concatenate()([z_samples_map, feature_map_shuffle])  <span class="comment"># 拼接出局部负样本</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局互信息判别器</span></span><br><span class="line">z_in = Input(shape=(z_dim * <span class="number">2</span>,))</span><br><span class="line">z = z_in</span><br><span class="line">z = Dense(z_dim, activation=<span class="string">&#x27;relu&#x27;</span>)(z)</span><br><span class="line">z = Dense(z_dim, activation=<span class="string">&#x27;relu&#x27;</span>)(z)</span><br><span class="line">z = Dense(z_dim, activation=<span class="string">&#x27;relu&#x27;</span>)(z)</span><br><span class="line">z = Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)(z)</span><br><span class="line"></span><br><span class="line">GlobalDiscriminator = Model(z_in, z)</span><br><span class="line">plot_model(GlobalDiscriminator, to_file=<span class="string">&#x27;GlobalDiscriminator.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">z_z_true_scores = GlobalDiscriminator(z_z_true)  <span class="comment"># 使用判别器判别正样本对</span></span><br><span class="line">z_z_flase_scores = GlobalDiscriminator(z_z_false)  <span class="comment"># 使用判别器判别负样本对</span></span><br><span class="line">global_info_loss = - K.mean(K.log(z_z_true_scores + <span class="number">1e-6</span>) + K.log(<span class="number">1</span> - z_z_flase_scores + <span class="number">1e-6</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 局部互信息判别器</span></span><br><span class="line">z_in = Input(shape=(<span class="literal">None</span>, <span class="literal">None</span>, z_dim * <span class="number">2</span>))</span><br><span class="line">z = z_in</span><br><span class="line">z = Dense(z_dim, activation=<span class="string">&#x27;relu&#x27;</span>)(z)</span><br><span class="line">z = Dense(z_dim, activation=<span class="string">&#x27;relu&#x27;</span>)(z)</span><br><span class="line">z = Dense(z_dim, activation=<span class="string">&#x27;relu&#x27;</span>)(z)</span><br><span class="line">z = Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)(z)</span><br><span class="line"></span><br><span class="line">LocalDiscriminator = Model(z_in, z)</span><br><span class="line">plot_model(LocalDiscriminator, to_file=<span class="string">&#x27;LocalDiscriminator.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">z_f_true_scores = LocalDiscriminator(z_f_true)  <span class="comment"># 使用判别器判别正样本对</span></span><br><span class="line">z_f_false_scores = LocalDiscriminator(z_f_false)  <span class="comment"># 使用判别器判别负样本对</span></span><br><span class="line">local_info_loss = - K.mean(K.log(z_f_true_scores + <span class="number">1e-6</span>) + K.log(<span class="number">1</span> - z_f_false_scores + <span class="number">1e-6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用来训练的模型</span></span><br><span class="line">model_train = Model(x_in, [z_z_true_scores, z_z_flase_scores, z_f_true_scores, z_f_false_scores])</span><br><span class="line">model_train.add_loss(α * global_info_loss + β * local_info_loss + γ * prior_kl_loss)</span><br><span class="line">model_train.<span class="built_in">compile</span>(optimizer=Adam(<span class="number">1e-3</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> Path(<span class="string">&#x27;cifar10.h5&#x27;</span>).exists():</span><br><span class="line">    model_train.fit(x_train, epochs=<span class="number">50</span>, batch_size=<span class="number">64</span>)</span><br><span class="line">    model_train.save_weights(<span class="string">&#x27;cifar10.h5&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model_train.load_weights(<span class="string">&#x27;cifar10.h5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出编码器的特征</span></span><br><span class="line">zs = encoder.predict(x_train, verbose=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(colored(<span class="string">&#x27;z mean&#x27;</span>, <span class="string">&#x27;green&#x27;</span>), zs.mean())  <span class="comment"># 查看均值（简单观察先验分布有没有达到效果）</span></span><br><span class="line"><span class="built_in">print</span>(colored(<span class="string">&#x27;z std&#x27;</span>, <span class="string">&#x27;green&#x27;</span>), zs.std())  <span class="comment"># 查看方差（简单观察先验分布有没有达到效果）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机选一张图片，输出最相近的图片</span></span><br><span class="line"><span class="comment"># 可以选用欧氏距离或者cos值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_knn</span>(<span class="params">path</span>):</span><br><span class="line">    n = <span class="number">10</span></span><br><span class="line">    topn = <span class="number">10</span></span><br><span class="line">    figure1 = np.zeros((img_dim * n, img_dim * topn, <span class="number">3</span>))</span><br><span class="line">    figure2 = np.zeros((img_dim * n, img_dim * topn, <span class="number">3</span>))</span><br><span class="line">    zs_ = zs / (zs**<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">1</span>, keepdims=<span class="literal">True</span>)**<span class="number">0.5</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        one = np.random.choice(<span class="built_in">len</span>(x_train))</span><br><span class="line">        idxs = ((zs**<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">1</span>) + (zs[one]**<span class="number">2</span>).<span class="built_in">sum</span>() - <span class="number">2</span> * np.dot(zs, zs[one])).argsort()[:topn]</span><br><span class="line">        <span class="keyword">for</span> j, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(idxs):</span><br><span class="line">            digit = x_train[k]</span><br><span class="line">            figure1[i * img_dim: (i + <span class="number">1</span>) * img_dim,</span><br><span class="line">                    j * img_dim: (j + <span class="number">1</span>) * img_dim] = digit</span><br><span class="line">        idxs = np.dot(zs_, zs_[one]).argsort()[-n:][::-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> j, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(idxs):</span><br><span class="line">            digit = x_train[k]</span><br><span class="line">            figure2[i * img_dim: (i + <span class="number">1</span>) * img_dim,</span><br><span class="line">                    j * img_dim: (j + <span class="number">1</span>) * img_dim] = digit</span><br><span class="line">    figure1 = (figure1 + <span class="number">1</span>) / <span class="number">2</span> * <span class="number">255</span></span><br><span class="line">    figure1 = np.clip(figure1, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    figure2 = (figure2 + <span class="number">1</span>) / <span class="number">2</span> * <span class="number">255</span></span><br><span class="line">    figure2 = np.clip(figure2, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    imageio.imwrite(path + <span class="string">&#x27;_l2.png&#x27;</span>, figure1)</span><br><span class="line">    imageio.imwrite(path + <span class="string">&#x27;_cos.png&#x27;</span>, figure2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sample_knn(<span class="string">&#x27;test&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="效果">效果</h1>
<h2 id="余弦距离knn采样">余弦距离KNN采样</h2>
<p><img src="/2019/07/31/infor-max/test_cos.png" /></p>
<h2 id="欧式距离knn采样">欧式距离KNN采样</h2>
<p><img src="/2019/07/31/infor-max/test_l2.png" /></p>
<h1 id="思考">思考</h1>
<p>我个人是想把这个使用在监督学习中，但是转念一想，其实在监督学习中不是已经在优化<span
class="math inline">\(y\)</span>与对应<span
class="math inline">\(x\)</span>之间的最大互信息了么？可以看到这里的这里构建正负样本对后所使用的损失函数，其实际就是交叉熵。</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">无监督学习</a></li></ul></div><div class="post-nav"><a class="pre" href="/2019/08/01/face-recognition/">人脸识别方法总结</a><a class="next" href="/2019/07/30/rasp-config/">树莓派修改配置使能串口登陆</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>