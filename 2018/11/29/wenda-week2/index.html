<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>机器学习作业第二周 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习作业第二周</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习作业第二周</h1><div class="post-meta">2018-11-29<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.6k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 15</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ex2.py"><span class="toc-number">1.</span> <span class="toc-text">ex2.py</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E6%95%88%E6%9E%9C"><span class="toc-number">1.1.</span> <span class="toc-text">执行效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ex2_reg.py"><span class="toc-number">2.</span> <span class="toc-text">ex2_reg.py</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">2.1.</span> <span class="toc-text">正则化逻辑回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">2.2.</span> <span class="toc-text">执行结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#func.py"><span class="toc-number">3.</span> <span class="toc-text">func.py</span></a></li></ol></div></div><div class="post-content"><p>这是第二周逻辑回归的作业，经过上一次的作业，我对<code>ocatve</code>和<code>numpy</code>的语法有了一个感觉，所以转换起来也比较方便了。我这次就像他的原版作业一样，两个执行文件，另外把所有需要自己实现的函数都放在<code>func.py</code>里面了。</p>
<p>这次没有什么需要特别记录的坑点。</p>
<span id="more"></span>
<h1 id="ex2.py">ex2.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.special</span><br><span class="line"><span class="keyword">from</span> fucs <span class="keyword">import</span> costFuc, costFunction, gradFuc, load, minimize, plotData, plotDecisionBoundary, predict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Machine Learning Online Class - Exercise 2: Logistic Regression</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Instructions</span></span><br><span class="line">    <span class="comment">#  ------------</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  This file contains code that helps you get started on the logistic</span></span><br><span class="line">    <span class="comment">#  regression exercise. You will need to complete the following functions</span></span><br><span class="line">    <span class="comment">#  in this exericse:</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#     sigmoid.m</span></span><br><span class="line">    <span class="comment">#     costFunction.m</span></span><br><span class="line">    <span class="comment">#     predict.m</span></span><br><span class="line">    <span class="comment">#     costFunctionReg.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  For this exercise, you will not need to change any code in this file,</span></span><br><span class="line">    <span class="comment">#  or any other files other than those mentioned above.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤一（替换sans-serif字体）</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;YaHei Consolas Hybrid&#x27;</span>]</span><br><span class="line">    <span class="comment"># 步骤二（解决坐标轴负数的负号显示问题）</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">    <span class="comment"># Load Data</span></span><br><span class="line">    <span class="comment">#  The first two columns contains the exam scores and the third column</span></span><br><span class="line">    <span class="comment">#  contains the label.</span></span><br><span class="line"></span><br><span class="line">    data = load(<span class="string">&#x27;machine_learning_exam/week2/ex2data1.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    X = data[:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    <span class="comment"># 一定要reshape不然会变成[0,100]的矩阵</span></span><br><span class="line">    y = data[:, <span class="number">2</span>].reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># ==================== Part 1: Plotting ====================</span></span><br><span class="line">    <span class="comment">#  We start the exercise by first plotting the data to understand the</span></span><br><span class="line">    <span class="comment">#  the problem we are working with.</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Plotting data with + indicating (y = 1)\</span></span><br><span class="line"><span class="string">     examples and o indicating (y = 0) examples.\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plotData(X, y)</span><br><span class="line">    <span class="comment"># Labels and Legend</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;测试 1 分数&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;测试 2 分数&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;接受&#x27;</span>, <span class="string">&#x27;不接受&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;原始数据集&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============ Part 2: Compute Cost and Gradient ============</span></span><br><span class="line">    <span class="comment">#  In this part of the exercise, you will implement the cost and gradient</span></span><br><span class="line">    <span class="comment">#  for logistic regression. You neeed to complete the code in</span></span><br><span class="line">    <span class="comment">#  costFunction.m</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Setup the data matrix appropriately, and add ones for the intercept term</span></span><br><span class="line">    [m, n] = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add intercept term to x and X_test</span></span><br><span class="line">    X = np.hstack([np.ones((m, <span class="number">1</span>), dtype=<span class="built_in">float</span>), X])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize fitting parameters</span></span><br><span class="line">    initial_theta = np.zeros((n + <span class="number">1</span>, <span class="number">1</span>), dtype=<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and display initial cost and gradient</span></span><br><span class="line">    [cost, grad] = costFunction(initial_theta, X, y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Cost at initial theta (zeros): &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected cost (approx): 0.693&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Gradient at initial theta (zeros):&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(grad)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected gradients (approx):\n -0.1000\n -12.0092\n -11.2628&#x27;</span>)</span><br><span class="line">    <span class="comment"># Compute and display cost and gradient with non-zero theta</span></span><br><span class="line">    test_theta = np.array([-<span class="number">24</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    [cost, grad] = costFunction(test_theta, X, y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nCost at test theta: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected cost (approx): 0.218\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Gradient at test theta: \n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(grad)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected gradients (approx):\n 0.043\n 2.566\n 2.647&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============= Part 3: Optimizing using fminunc  =============</span></span><br><span class="line">    <span class="comment">#  In this exercise, you will use a built-in function (fminunc) to find the</span></span><br><span class="line">    <span class="comment">#  optimal parameters theta.</span></span><br><span class="line">    <span class="comment">#  Set options for fminunc</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, 400)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Run fminunc to obtain the optimal theta</span></span><br><span class="line">    <span class="comment">#  This function will return theta and the cost</span></span><br><span class="line">    <span class="comment"># [theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 牛顿共轭法 x0必须是(n,)的向量 jac函数返回值要与x0相同维度</span></span><br><span class="line"></span><br><span class="line">    initial_theta = initial_theta.reshape(-<span class="number">1</span>,)  <span class="comment"># [n,1]=&gt;[n,]</span></span><br><span class="line">    Result = minimize(fun=costFuc, x0=initial_theta,</span><br><span class="line">                      args=(X, y), method=<span class="string">&#x27;TNC&#x27;</span>, jac=gradFuc)</span><br><span class="line">    theta = Result.x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    cost = Result.fun</span><br><span class="line">    <span class="comment"># Print theta to screen</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Cost at theta found by fminunc: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected cost (approx): 0.203\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;theta: \n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(theta)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected theta (approx):&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27; -25.161\n 0.206\n 0.201&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot Boundary</span></span><br><span class="line">    plotDecisionBoundary(theta, X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============== Part 4: Predict and Accuracies ==============</span></span><br><span class="line">    <span class="comment">#  After learning the parameters, you&#x27;ll like to use it to predict the outcomes</span></span><br><span class="line">    <span class="comment">#  on unseen data. In this part, you will use the logistic regression model</span></span><br><span class="line">    <span class="comment">#  to predict the probability that a student with score 45 on exam 1 and</span></span><br><span class="line">    <span class="comment">#  score 85 on exam 2 will be admitted.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Furthermore, you will compute the training and test set accuracies of</span></span><br><span class="line">    <span class="comment">#  our model.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Your task is to complete the code in predict.m</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Predict probability for a student with score 45 on exam 1</span></span><br><span class="line">    <span class="comment">#  and score 85 on exam 2</span></span><br><span class="line"></span><br><span class="line">    prob = scipy.special.expit(np.array([<span class="number">1</span>, <span class="number">45</span>, <span class="number">85</span>])@theta)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;For a student with scores 45 and 85, we predict an admission probability of &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(prob))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected value: 0.775 +/- 0.002&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute accuracy on our training set</span></span><br><span class="line">    p = predict(theta, X)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Train Accuracy: &#x27;</span>, np.mean(np.array(p == y)) * <span class="number">100</span>, <span class="string">&#x27;%&#x27;</span>, sep=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected accuracy (approx): 89.0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="执行效果">执行效果</h2>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week2/ex2.py</span><br><span class="line">Plotting data with + indicating (y = 1)     examples and o indicating (y = 0) examples.</span><br><span class="line"></span><br><span class="line">Cost at initial theta (zeros): 0.6931471805599453</span><br><span class="line">Expected cost (approx): 0.693</span><br><span class="line">Gradient at initial theta (zeros):</span><br><span class="line">[[ -0.1       ]</span><br><span class="line"> [-12.00921659]</span><br><span class="line"> [-11.26284221]]</span><br><span class="line">Expected gradients (approx):</span><br><span class="line"> -0.1000</span><br><span class="line"> -12.0092</span><br><span class="line"> -11.2628</span><br><span class="line"></span><br><span class="line">Cost at <span class="built_in">test</span> theta: 0.21833019382659774</span><br><span class="line"></span><br><span class="line">Expected cost (approx): 0.218</span><br><span class="line"></span><br><span class="line">Gradient at <span class="built_in">test</span> theta:</span><br><span class="line"></span><br><span class="line">[[0.04290299]</span><br><span class="line"> [2.56623412]</span><br><span class="line"> [2.64679737]]</span><br><span class="line">Expected gradients (approx):</span><br><span class="line"> 0.043</span><br><span class="line"> 2.566</span><br><span class="line"> 2.647</span><br><span class="line">Cost at theta found by fminunc: 0.20349770158947478</span><br><span class="line"></span><br><span class="line">Expected cost (approx): 0.203</span><br><span class="line"></span><br><span class="line">theta:</span><br><span class="line"></span><br><span class="line">[[-25.16131857]</span><br><span class="line"> [  0.20623159]</span><br><span class="line"> [  0.20147149]]</span><br><span class="line">Expected theta (approx):</span><br><span class="line"> -25.161</span><br><span class="line"> 0.206</span><br><span class="line"> 0.201</span><br><span class="line">For a student with scores 45 and 85, we predict an admission probability of [0.77629062]</span><br><span class="line">Expected value: 0.775 +/- 0.002</span><br><span class="line">Train Accuracy: 89.0%</span><br><span class="line">Expected accuracy (approx): 89.0</span><br></pre></td></tr></table></figure>
<p><img src="/2018/11/29/wenda-week2/11.png" alt="数据可视化" /> <img
src="/2018/11/29/wenda-week2/12.png" alt="直线决策线" /></p>
<h1 id="ex2_reg.py">ex2_reg.py</h1>
<h2 id="正则化逻辑回归">正则化逻辑回归</h2>
<p>这个程序不但使用了正则化，并且还将数据特征的维度进行了扩展，最终出现曲线</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> fucs <span class="keyword">import</span> costFuc, costFunction, gradFuc, load, minimize, plotData, plotDecisionBoundary, predict, mapFeature, costFunctionReg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Machine Learning Online Class - Exercise 2: Logistic Regression</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Instructions</span></span><br><span class="line">    <span class="comment">#  ------------</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  This file contains code that helps you get started on the second part</span></span><br><span class="line">    <span class="comment">#  of the exercise which covers regularization with logistic regression.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  You will need to complete the following functions in this exericse:</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#     sigmoid.m</span></span><br><span class="line">    <span class="comment">#     costFunction.m</span></span><br><span class="line">    <span class="comment">#     predict.m</span></span><br><span class="line">    <span class="comment">#     costFunctionReg.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  For this exercise, you will not need to change any code in this file,</span></span><br><span class="line">    <span class="comment">#  or any other files other than those mentioned above.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load Data</span></span><br><span class="line">    <span class="comment">#  The first two columns contains the X values and the third column</span></span><br><span class="line">    <span class="comment">#  contains the label (y).</span></span><br><span class="line"></span><br><span class="line">    data = load(<span class="string">&#x27;machine_learning_exam/week2/ex2data2.txt&#x27;</span>)</span><br><span class="line">    X = data[:, :<span class="number">2</span>]</span><br><span class="line">    y = data[:, <span class="number">2</span>].reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    plotData(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Labels and Legend</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Microchip Test 1&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Microchip Test 2&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Specified in plot order</span></span><br><span class="line">    plt.legend([<span class="string">&#x27;y = 1&#x27;</span>, <span class="string">&#x27;y = 0&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 1: Regularized Logistic Regression ============</span></span><br><span class="line">    <span class="comment">#  In this part, you are given a dataset with data points that are not</span></span><br><span class="line">    <span class="comment">#  linearly separable. However, you would still like to use logistic</span></span><br><span class="line">    <span class="comment">#  regression to classify the data points.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  To do so, you introduce more features to use -- in particular, you add</span></span><br><span class="line">    <span class="comment">#  polynomial features to our data matrix (similar to polynomial</span></span><br><span class="line">    <span class="comment">#  regression).</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add Polynomial Features</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note that mapFeature also adds a column of ones for us, so the intercept</span></span><br><span class="line">    <span class="comment"># term is handled</span></span><br><span class="line">    X = mapFeature(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(X.shape)</span><br><span class="line">    <span class="comment"># Initialize fitting parameters</span></span><br><span class="line">    initial_theta = zeros((size(X, <span class="number">1</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Set regularization parameter lamda to 1</span></span><br><span class="line">    lamda = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and display initial cost and gradient for regularized logistic</span></span><br><span class="line">    <span class="comment"># regression</span></span><br><span class="line">    [cost, grad] = costFunctionReg(initial_theta, X, y, lamda)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Cost at initial theta (zeros): &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected cost (approx): 0.693&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Gradient at initial theta (zeros) - first five values only:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(grad[<span class="number">0</span>: <span class="number">5</span>, <span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected gradients (approx) - first five values only:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27; 0.0085\n 0.0188\n 0.0001\n 0.0503\n 0.0115&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and display cost and gradient</span></span><br><span class="line">    <span class="comment"># with all-ones theta and lamda = 10</span></span><br><span class="line">    test_theta = ones((size(X, <span class="number">1</span>), <span class="number">1</span>))</span><br><span class="line">    [cost, grad] = costFunctionReg(test_theta, X, y, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Cost at test theta (with lamda = 10): &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected cost (approx): 3.16&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Gradient at test theta - first five values only:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(grad[<span class="number">0</span>: <span class="number">5</span>, <span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected gradients (approx) - first five values only:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27; 0.3460\n 0.1614\n 0.1948\n 0.2269\n 0.0922&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============= Part 2: Regularization and Accuracies =============</span></span><br><span class="line">    <span class="comment">#  Optional Exercise:</span></span><br><span class="line">    <span class="comment">#  In this part, you will get to try different values of lamda and</span></span><br><span class="line">    <span class="comment">#  see how regularization affects the decision coundart</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Try the following values of lamda (0, 1, 10, 100).</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  How does the decision boundary change when you vary lamda? How does</span></span><br><span class="line">    <span class="comment">#  the training set accuracy vary?</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize fitting parameters</span></span><br><span class="line">    initial_theta = zeros((X.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set regularization parameter lamda to 1 (you should vary this)</span></span><br><span class="line">    lamda = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set Options</span></span><br><span class="line">    Result = minimize(fun=costFuc, x0=initial_theta,</span><br><span class="line">                      args=(X, y), method=<span class="string">&#x27;TNC&#x27;</span>, jac=gradFuc)</span><br><span class="line">    theta = Result.x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    cost = Result.fun</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot Boundary</span></span><br><span class="line">    plotDecisionBoundary(theta, X, y)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">&#x27;lamda = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(lamda))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Labels and Legend</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Microchip Test 1&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Microchip Test 2&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;y = 1&#x27;</span>, <span class="string">&#x27;y = 0&#x27;</span>, <span class="string">&#x27;Decision boundary&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute accuracy on our training set</span></span><br><span class="line">    p = predict(theta, X)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Train Accuracy: &#123;&#125;%&#x27;</span>.<span class="built_in">format</span>(mean(array(p == y)) * <span class="number">100</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Expected accuracy (with lamda = 1): 83.1 (approx)&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="执行结果">执行结果</h2>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week2/ex2_reg.py</span><br><span class="line">(118, 28)</span><br><span class="line">Cost at initial theta (zeros): 0.6931471805599454</span><br><span class="line">Expected cost (approx): 0.693</span><br><span class="line">Gradient at initial theta (zeros) - first five values only:</span><br><span class="line">[8.47457627e-03 1.87880932e-02 7.77711864e-05 5.03446395e-02</span><br><span class="line"> 1.15013308e-02]</span><br><span class="line">Expected gradients (approx) - first five values only:</span><br><span class="line"> 0.0085</span><br><span class="line"> 0.0188</span><br><span class="line"> 0.0001</span><br><span class="line"> 0.0503</span><br><span class="line"> 0.0115</span><br><span class="line">Cost at <span class="built_in">test</span> theta (with lamda = 10): 3.2068822129709416</span><br><span class="line">Expected cost (approx): 3.16</span><br><span class="line">Gradient at <span class="built_in">test</span> theta - first five values only:</span><br><span class="line">[0.34604507 0.16135192 0.19479576 0.22686278 0.09218568]</span><br><span class="line">Expected gradients (approx) - first five values only:</span><br><span class="line"> 0.3460</span><br><span class="line"> 0.1614</span><br><span class="line"> 0.1948</span><br><span class="line"> 0.2269</span><br><span class="line"> 0.0922</span><br><span class="line">Train Accuracy: 87.28813559322035%</span><br><span class="line">Expected accuracy (with lamda = 1): 83.1 (approx)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/11/29/wenda-week2/21.png" alt="数据可视化" /> <img
src="/2018/11/29/wenda-week2/22.png" alt="曲线决策线" /></p>
<h1 id="func.py">func.py</h1>
<p>包含了所有要实现的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> expit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">filepath: <span class="built_in">str</span></span>)-&gt;np.ndarray:</span><br><span class="line">    dataset = []</span><br><span class="line">    f = <span class="built_in">open</span>(filepath)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        dataset.append(line.strip().split(<span class="string">&#x27;,&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> np.asfarray(dataset)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the postive and negative data</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotData</span>(<span class="params">X: np.ndarray, y: np.ndarray</span>):</span><br><span class="line">    pos = [it <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(y.shape[<span class="number">0</span>]) <span class="keyword">if</span> y[it, <span class="number">0</span>] == <span class="number">1</span>]</span><br><span class="line">    neg = [it <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(y.shape[<span class="number">0</span>]) <span class="keyword">if</span> y[it, <span class="number">0</span>] == <span class="number">0</span>]</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(X[pos, <span class="number">0</span>], X[pos, <span class="number">1</span>], c=<span class="string">&#x27;k&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">    plt.scatter(X[neg, <span class="number">0</span>], X[neg, <span class="number">1</span>], c=<span class="string">&#x27;y&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the boundary line</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotDecisionBoundary</span>(<span class="params">theta: np.ndarray, X: np.ndarray, y: np.ndarray</span>):</span><br><span class="line">    plotData(X[:, <span class="number">1</span>:], y)</span><br><span class="line">    <span class="comment"># 特征值小于3，那么theta只有2个参数，只需要画直线</span></span><br><span class="line">    <span class="keyword">if</span> X.shape[<span class="number">1</span>] &lt;= <span class="number">3</span>:</span><br><span class="line">        <span class="comment"># Only need 2 points to define a line, so choose two endpoints</span></span><br><span class="line">        plot_x = np.array([<span class="built_in">min</span>(X[:, <span class="number">1</span>])-<span class="number">2</span>,  <span class="built_in">max</span>(X[:, <span class="number">2</span>])+<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the decision boundary line</span></span><br><span class="line">        plot_y = np.zeros(plot_x.shape)</span><br><span class="line">        plot_y = (-<span class="number">1</span>/theta[<span class="number">2</span>, <span class="number">0</span>])*(theta[<span class="number">1</span>, <span class="number">0</span>]*plot_x + theta[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">        <span class="comment"># Plot, and adjust axes for better viewing</span></span><br><span class="line">        line = plt.plot(np.linspace(plot_x[<span class="number">0</span>], plot_x[<span class="number">1</span>]),</span><br><span class="line">                        np.linspace(plot_y[<span class="number">0</span>], plot_y[<span class="number">1</span>]), color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legend, specific for the exercise</span></span><br><span class="line">        plt.legend([<span class="string">&#x27;决策线&#x27;</span>, <span class="string">&#x27;接受&#x27;</span>, <span class="string">&#x27;不接受&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">        plt.axis([<span class="number">30</span>, <span class="number">100</span>, <span class="number">30</span>, <span class="number">100</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Here is the grid range</span></span><br><span class="line">        u = np.linspace(-<span class="number">1</span>, <span class="number">1.5</span>, <span class="number">50</span>)</span><br><span class="line">        v = np.linspace(-<span class="number">1</span>, <span class="number">1.5</span>, <span class="number">50</span>)</span><br><span class="line">        z = np.zeros((<span class="built_in">len</span>(u), <span class="built_in">len</span>(v)))</span><br><span class="line">        <span class="comment"># Evaluate z = theta*x over the grid</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(u)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(v)):</span><br><span class="line">                z[i, j] = mapFeature(np.mat(u[i]), np.mat(v[j]))@theta</span><br><span class="line">        <span class="comment"># important to transpose z before calling contour</span></span><br><span class="line">        z = z.T</span><br><span class="line">        plt.contour(u, v, z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 适配原题目中的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">costFunction</span>(<span class="params">theta: np.ndarray, X: np.ndarray, y: np.ndarray</span>):</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    J = <span class="number">0</span></span><br><span class="line">    grad = np.zeros(theta.shape)</span><br><span class="line">    <span class="comment"># x:[m,n]*theta:[n,1] =&gt; z:[m,1]</span></span><br><span class="line">    h = expit(X@theta)  <span class="comment"># h:[m,1]</span></span><br><span class="line">    J = np.<span class="built_in">sum</span>(-y*np.log(h)-(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-h)) / m</span><br><span class="line">    <span class="comment"># J 的导数</span></span><br><span class="line">    grad = X.T@(h-y)/m</span><br><span class="line">    <span class="keyword">return</span> J, grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只计算损失，用于适配scipy的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">costFuc</span>(<span class="params">theta: np.ndarray, X: np.ndarray, y: np.ndarray</span>):</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    theta = theta.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    h = expit(X@theta)  <span class="comment"># h:[m,1]</span></span><br><span class="line">    J = np.<span class="built_in">sum</span>(-y*np.log(h)-(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-h)) / m</span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只计算梯度，用于适配scipy的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradFuc</span>(<span class="params">theta: np.ndarray, X: np.ndarray, y: np.ndarray</span>):</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    theta = theta.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    grad = np.zeros(theta.shape)</span><br><span class="line">    h = expit(X@theta)  <span class="comment"># h:[m,1]</span></span><br><span class="line">    grad = X.T@(h-y)/m</span><br><span class="line">    <span class="keyword">return</span> grad.flatten()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检测结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">theta: np.array, X: np.array</span>):</span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    p = np.zeros((m, <span class="number">1</span>))</span><br><span class="line">    p = np.around(expit(X@theta))</span><br><span class="line">    <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 扩展特征维度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mapFeature</span>(<span class="params">X1: np.ndarray, X2: np.ndarray</span>)-&gt;np.ndarray:</span><br><span class="line">    degree = <span class="number">6</span></span><br><span class="line">    out = np.ones(</span><br><span class="line">        (X1.shape[<span class="number">0</span>], <span class="built_in">sum</span>([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, degree+<span class="number">2</span>)])), dtype=<span class="built_in">float</span>)</span><br><span class="line">    <span class="comment"># out=[X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..]</span></span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(degree+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>):</span><br><span class="line">            out[:, cnt] = np.power(X1, (i-j))*np.power(X2, j)</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正规化的损失函数计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">costFunctionReg</span>(<span class="params">theta: np.ndarray, X: np.ndarray, y: np.ndarray, lamda: np.ndarray</span>):</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    J = <span class="number">0</span></span><br><span class="line">    grad = np.zeros(theta.shape)  <span class="comment"># type:np.ndarray</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x:[m,n]*theta:[n,1] =&gt; z:[m,1]</span></span><br><span class="line">    h = expit(X@theta)  <span class="comment"># h:[m,1]</span></span><br><span class="line">    J = np.<span class="built_in">sum</span>(-y*np.log(h)-(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-h)) / m \</span><br><span class="line">        + lamda * np.<span class="built_in">sum</span>(np.power(theta, <span class="number">2</span>)) / (<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># J 的导数 theta [0,0] 需要忽略</span></span><br><span class="line">    temptheta = np.zeros(theta.shape)</span><br><span class="line">    temptheta[<span class="number">1</span>:, :] = theta[<span class="number">1</span>:, :]</span><br><span class="line">    <span class="comment"># print(theta)</span></span><br><span class="line">    <span class="comment"># print(temptheta)</span></span><br><span class="line">    grad = (X.T@(h-y)+lamda * temptheta) / m</span><br><span class="line">    <span class="keyword">return</span> J, grad</span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" rel="tag">吴恩达课程</a></li></ul></div><div class="post-nav"><a class="pre" href="/2018/12/01/libmpfrerr/">libmpfr错误</a><a class="next" href="/2018/11/26/fcitxconfig/">fcitx配置</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>