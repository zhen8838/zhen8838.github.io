<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>机器学习作业第一周 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习作业第一周</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习作业第一周</h1><div class="post-meta">2018-11-25<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 9</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.</span> <span class="toc-text">总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E5%85%AC%E5%BC%8F%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.1.</span> <span class="toc-text">重要公式的推导</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C"><span class="toc-number">2.</span> <span class="toc-text">执行：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%88%E6%9E%9C"><span class="toc-number">3.</span> <span class="toc-text">效果</span></a></li></ol></div></div><div class="post-content"><p>这是吴恩达老师的机器学习作业....并不是我们学校的 hh</p>
<p>因为机器学习这个课是老师在2014年讲的,那时候<code>Python</code>还不火,所以老师讲的时候用的是<code>ocatve</code>,现在我准备都用<code>Python</code>写.</p>
<span id="more"></span>
<h1 id="总结">总结</h1>
<p>这是第一周作业,单/多变量线性回归</p>
<ol type="1">
<li><p>注意损失函数求导出来的结果,更新方式是不一样的</p></li>
<li><p><code>python</code>中的矩阵和<code>list</code>类型非常蛋疼,不如<code>matlab</code>全是矩阵就好了</p></li>
<li><p>给X加列,可以利用矩阵乘法快速求解</p></li>
<li><p>绘3维图时要用<code>meshgrid</code>函数连接变量</p></li>
<li><p><code>numpy.std</code>函数要加上参数<code>ddof=1</code>，因为他的标准差默认除以<code>n</code></p></li>
</ol>
<h2 id="重要公式的推导">重要公式的推导</h2>
<p>这里有一个多变量的梯度更新，需要把原来的损失函数进行求导，因为现在是每一个变量都是向量，所以推导的时候我纠结了好一会。具体如下：</p>
<p><strong>注：上标为行数，下标为列数，设数据X=[m,n]</strong></p>
<p><span class="math display">\[ \begin{aligned}
    \theta_j &amp;= \theta_j -
\alpha\frac{\partial}{\partial\theta_j}J(\theta) = \theta_j -
\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})*x_j^{(i)}
\\
    \theta_j &amp;-=\frac{ \alpha}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) -
y^{(i)})*x_j^{(i)}
\end{aligned} \]</span> 现在我们分析此部分<span
class="math inline">\(\sum_{i=1}^{m}(h_\theta(x^{(i)}) -
y^{(i)})*x_j^{(i)}\)</span>：</p>
<p><strong>注：以下都省略了常数项<span
class="math inline">\(\alpha/m\)</span></strong></p>
<p><span class="math display">\[ \begin{aligned}
    \because &amp;x=[m,n]\ \ \ y=[m,1]\\
    \therefore &amp;x^{(i)} =[1,n]\ \ \ y^{(i)}=[1,1]\ \ \ \theta=[n,1]
\\
    \Rightarrow &amp;h_\theta(x^{(i)})=x^{(i)}*\theta=[1,1]\\
    \Rightarrow &amp;h_\theta(x^{(i)})-y^{(i)}=[1,1]=E_\theta^{(i)}\\
    \because &amp;\sum_{i=1}^{m}(h_\theta(x^{(i)}) -
y^{(i)})*x_j^{(i)}=\sum_{i=1}^{m}E_\theta^{(i)}*x_j^{(i)}\\
    &amp;=E_\theta^{(0)}*x_j^{(0)}+E_\theta^{(1)}*x_j^{(1)}+\ldots+E_\theta^{(m)}*x_j^{(m)}\\
    \therefore \Theta&amp;=
    \begin{bmatrix}
    x_1^1E_1 + x_2^1E_1 + \ldots + x_m^1E_1 \\
    x_1^2E_2 + x_2^2E_2 + \ldots + x_m^2E_2 \\
    x_1^nE_n + x_2^nE_n + \ldots + x_m^nE_n \\
    \end{bmatrix}
\end{aligned} \]</span></p>
<p>将其放到全局的矩阵中就是这样：</p>
<p><span class="math display">\[ \begin{aligned}
    X&amp;=\begin{bmatrix}
        x_1^1 &amp; x_2^1 &amp; \ldots &amp; x_n^1 \\
        \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
        x_1^m &amp; x_2^m &amp; \ldots &amp; x_n^1 \\
    \end{bmatrix}\\
    Y&amp;=\begin{bmatrix}
        y_1^1  \\
        \vdots \\
        y_1^m  \\
    \end{bmatrix}\\
    \theta&amp;=\begin{bmatrix}
        \theta_1^1  \\
        \vdots \\
        \theta_1^n  \\
    \end{bmatrix} \\
    \Theta-&amp;=X^T(h_\theta(X) -Y) = X^T(X*\Theta-Y) \\
    &amp;=
    \begin{bmatrix}
     x_1^1 &amp; x_1^2 &amp; \ldots &amp; x_1^m \\
    \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
    x_n^1 &amp; x_n^2 &amp; \ldots &amp; x_n^m \\    
    \end{bmatrix}*
    (\begin{bmatrix}
        x_1^1 &amp; x_2^1 &amp; \ldots &amp; x_n^1 \\
        \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
        x_1^m &amp; x_2^m &amp; \ldots &amp; x_n^1 \\
    \end{bmatrix}*
    \begin{bmatrix}
        \theta_1^1  \\
        \vdots \\
        \theta_1^n  \\
    \end{bmatrix}-
    \begin{bmatrix}
        y_1^1  \\
        \vdots \\
        y_1^m  \\
    \end{bmatrix}) \\
    &amp;= \begin{bmatrix}
     x_1^1 &amp; x_1^2 &amp; \ldots &amp; x_1^m \\
    \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
    x_n^1 &amp; x_n^2 &amp; \ldots &amp; x_n^m \\    
    \end{bmatrix}*
    \begin{bmatrix}
        h_1^1  - y_1^1  \\
        \vdots          \\
        h_1^m  - y_1^m  \\
    \end{bmatrix}\\
    &amp;=
    \begin{bmatrix}
    x_1^1E_1 + x_2^1E_1 + \ldots + x_m^1E_1 \\
    x_1^2E_2 + x_2^2E_2 + \ldots + x_m^2E_2 \\
    x_1^nE_n + x_2^nE_n + \ldots + x_m^nE_n \\
    \end{bmatrix}
\end{aligned} \]</span></p>
<p><strong>即推出：</strong></p>
<p><span
class="math display">\[\Theta-=\frac{\alpha}{m}X^T(X*\Theta-Y)\]</span>
# 代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">filepath: <span class="built_in">str</span></span>)-&gt;np.ndarray:</span><br><span class="line">    dataset = []</span><br><span class="line">    f = <span class="built_in">open</span>(filepath)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        dataset.append(line.strip().split(<span class="string">&#x27;,&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> np.asfarray(dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义推演函数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">h_fuc</span>(<span class="params">x: <span class="built_in">float</span>, theta: np.ndarray</span>)-&gt;<span class="built_in">float</span>:</span><br><span class="line">    <span class="keyword">return</span> theta[<span class="number">0</span>, <span class="number">0</span>]+theta[<span class="number">0</span>, <span class="number">1</span>]*x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># z定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeCost</span>(<span class="params">X: np.ndarray, y: np.ndarray, theta: np.ndarray</span>)-&gt;<span class="built_in">float</span>:</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.power(np.dot(X, theta)-y, <span class="number">2</span>))/(<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多参数损失函数计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeCostMulti</span>(<span class="params">X: np.ndarray, y: np.ndarray, theta: np.ndarray</span>)-&gt;<span class="built_in">float</span>:</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.power(np.dot(X, theta)-y, <span class="number">2</span>))/(<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent</span>(<span class="params">X: np.ndarray, y: np.ndarray, theta: np.ndarray, alpha: <span class="built_in">float</span>, num_iters: <span class="built_in">int</span></span>):</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    j_history = np.zeros((num_iters, <span class="number">1</span>))</span><br><span class="line">    theta_s = theta.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        theta[<span class="number">0</span>, <span class="number">0</span>] -= alpha / m * np.<span class="built_in">sum</span>(np.dot(X, theta_s) - y)</span><br><span class="line">        theta[<span class="number">1</span>, <span class="number">0</span>] -= alpha / m * np.<span class="built_in">sum</span>((np.dot(X, theta_s) - y)*X)</span><br><span class="line">        <span class="comment"># 必须同时更新theta(1)和theta(2)</span></span><br><span class="line">        theta_s = theta</span><br><span class="line">        j_history[i, <span class="number">0</span>] = computeCost(X, y, theta)</span><br><span class="line">    <span class="keyword">return</span> j_history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescentMulti</span>(<span class="params">X: np.ndarray, y: np.ndarray, theta: np.ndarray, alpha: <span class="built_in">float</span>, num_iters: <span class="built_in">int</span></span>):</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    j_history = np.zeros((num_iters, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        theta -= alpha*(X.T@(X@theta-y))/m</span><br><span class="line">        j_history[i, <span class="number">0</span>] = computeCostMulti(X, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, j_history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">featureNormalize</span>(<span class="params">X: np.ndarray</span>):</span><br><span class="line">    X_norm = X</span><br><span class="line">    mu = np.zeros((<span class="number">1</span>, X.shape[<span class="number">0</span>]))</span><br><span class="line">    sigma = np.zeros((<span class="number">1</span>, X.shape[<span class="number">0</span>]))</span><br><span class="line">    mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 加上ddof=1 因为matlab中默认除以 n-1 np 默认除以 n</span></span><br><span class="line">    sigma = np.std(X, axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">    X_norm = (X-mu)/sigma</span><br><span class="line">    <span class="keyword">return</span> X_norm, mu, sigma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正规方程求解</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalEqn</span>(<span class="params">X: np.ndarray, y: np.ndarray</span>):</span><br><span class="line">    theta = np.linalg.inv(X.T@X)@X.T@y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤一（替换sans-serif字体）</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;YaHei Consolas Hybrid&#x27;</span>]</span><br><span class="line">    <span class="comment"># 步骤二（解决坐标轴负数的负号显示问题）</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 以下为单变量回归 &quot;&quot;&quot;</span></span><br><span class="line">    dataset = load_data(<span class="string">&#x27;machine_learning_exam/week1/ex1data1.txt&#x27;</span>)</span><br><span class="line">    m = dataset.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># x 添加一列 便于矩阵计算 x=[m,2]</span></span><br><span class="line">    X = np.c_[np.ones((m, <span class="number">1</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">              np.array(dataset[:, <span class="number">0</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line">    Y = np.array(dataset[:, <span class="number">1</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># # theta 设置为列向量 theta=[2,1] !!!这里一定要设置数据类型!!!</span></span><br><span class="line">    theta = np.array([<span class="number">0</span>, <span class="number">0</span>], dtype=<span class="built_in">float</span>, ndmin=<span class="number">2</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    iterations = <span class="number">1500</span></span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    cost = computeCost(X, Y, theta)</span><br><span class="line">    j_history = gradientDescent(X, Y, theta, alpha, <span class="number">1500</span>)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(X[:, <span class="number">1</span>], Y, c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    plt.plot(X[:, <span class="number">1</span>], np.dot(X, theta))</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Profit in $10,000s&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Population of city in 10,1000s&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;单边量回归&#x27;</span>)</span><br><span class="line">    <span class="comment"># 可视化损失</span></span><br><span class="line">    theta0_vals = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100.0</span>)</span><br><span class="line">    theta1_vals = np.linspace(-<span class="number">1</span>, <span class="number">4</span>, <span class="number">100.0</span>)</span><br><span class="line">    <span class="comment"># 这里必须要加 不然画出来只用中间一条</span></span><br><span class="line">    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)</span><br><span class="line">    J_vals = np.zeros((<span class="built_in">len</span>(theta0_vals), <span class="built_in">len</span>(theta1_vals)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(theta0_vals)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(theta1_vals)):</span><br><span class="line">            t = np.array([theta0_vals[i, j], theta1_vals[i, j]]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            J_vals[i, j] = computeCost(X, Y, t)</span><br><span class="line">            <span class="comment"># print(&quot;J_vals[&#123;&#125;, &#123;&#125;]=&#123;&#125;&quot;.format(i,j,J_vals[i,j]))</span></span><br><span class="line"></span><br><span class="line">    fig1 = plt.figure()</span><br><span class="line">    ax = fig1.gca(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">    surf = ax.plot_surface(theta0_vals, theta1_vals, J_vals)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;theta0&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;theta1&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;可视化损失&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 以下为多变量回归 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># ===========数据标准化=============</span></span><br><span class="line"></span><br><span class="line">    dataset = load_data(<span class="string">&#x27;machine_learning_exam/week1/ex1data2.txt&#x27;</span>)</span><br><span class="line">    m = dataset.shape[<span class="number">0</span>]</span><br><span class="line">    X = np.array(dataset[:, :<span class="number">2</span>])  <span class="comment"># x=[m,2]</span></span><br><span class="line">    Y = np.array(dataset[:, <span class="number">2</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># y=[m,1]</span></span><br><span class="line">    <span class="comment"># 归一化特征值</span></span><br><span class="line">    X, mu, sigma = featureNormalize(X)</span><br><span class="line">    <span class="comment"># x add a cloum</span></span><br><span class="line">    X = np.c_[np.ones((m, <span class="number">1</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>), X]</span><br><span class="line">    <span class="comment"># ===========梯度下降=============</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择学习率</span></span><br><span class="line">    iterations = <span class="number">8500</span></span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># theta 列向量 theta=[3,1]</span></span><br><span class="line">    theta = np.zeros((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">    theta, j_history = gradientDescentMulti(X, Y, theta, alpha, iterations)</span><br><span class="line">    <span class="comment"># 绘画</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(j_history)), j_history, <span class="string">&#x27;-b&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Number of iterations&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Cost J&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;学习率为:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(alpha))</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Theta computed from gradient descent:&#x27;</span>, theta[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 估计</span></span><br><span class="line">    price = np.ones((<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">    price[<span class="number">0</span>, <span class="number">1</span>:] = (np.array([<span class="number">1650</span>, <span class="number">3</span>])-mu)/sigma</span><br><span class="line">    price = price@theta</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Predicted price of a 1650 sq-ft, 3 br house (using gradient descent):&#x27;</span>, price)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 以下使用正规方程求解 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># ===========数据读取=============</span></span><br><span class="line"></span><br><span class="line">    dataset = load_data(<span class="string">&#x27;machine_learning_exam/week1/ex1data2.txt&#x27;</span>)</span><br><span class="line">    m = dataset.shape[<span class="number">0</span>]</span><br><span class="line">    X = np.array(dataset[:, :<span class="number">2</span>])  <span class="comment"># x=[m,2]</span></span><br><span class="line">    Y = np.array(dataset[:, <span class="number">2</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># y=[m,1]</span></span><br><span class="line">    <span class="comment"># x add a cloum</span></span><br><span class="line">    X = np.c_[np.ones((m, <span class="number">1</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>), X]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ===========梯度下降=============</span></span><br><span class="line">    <span class="comment"># 选择学习率</span></span><br><span class="line">    iterations = <span class="number">400</span></span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># theta 列向量 theta=[3,1]</span></span><br><span class="line">    theta = normalEqn(X, Y)</span><br><span class="line">    <span class="comment"># 估计</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Theta computed from normal equations:&#x27;</span>, theta[:, <span class="number">0</span>])</span><br><span class="line">    price = np.array([<span class="number">1</span>, <span class="number">1650</span>, <span class="number">3</span>])@theta</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Predicted price of a 1650 sq-ft, 3 br house (using normal equations):&#x27;</span>, price)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="执行">执行：</h1>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week1/ex1.py</span><br><span class="line">Theta computed from gradient descent: [ 340412.65957447  110631.05027884   -6649.47427082]</span><br><span class="line">Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): [[ 293081.46433489]]</span><br><span class="line">Theta computed from normal equations: [ 89597.9095428     139.21067402  -8738.01911233]</span><br><span class="line">Predicted price of a 1650 sq-ft, 3 br house (using normal equations): [ 293081.4643349]</span><br></pre></td></tr></table></figure>
<h1 id="效果">效果</h1>
<p><img src="/2018/11/25/wenda-week1/1.png" alt="单变量回归" /> <img
src="/2018/11/25/wenda-week1/2.png" /> <img
src="/2018/11/25/wenda-week1/0.3.png" /> <img
src="/2018/11/25/wenda-week1/0.1.png" /> <img
src="/2018/11/25/wenda-week1/0.03.png" /> <img
src="/2018/11/25/wenda-week1/0.01.png" /></p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" rel="tag">吴恩达课程</a></li></ul></div><div class="post-nav"><a class="pre" href="/2018/11/26/fcitxconfig/">fcitx配置</a><a class="next" href="/2018/11/24/minmax/">最大最小聚类</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a> <a href="/tags/%E7%AE%97%E5%AD%90/" style="font-size: 15px;">算子</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/10/15/jax-reshard/">探究jax reshard优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/09/26/flashattn/">Flash Attention记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>