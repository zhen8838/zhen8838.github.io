<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>som算法 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">som算法</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">som算法</h1><div class="post-meta">2018-11-07<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 9</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>相比于bp神经网络算法，som相对来说比较容易理解。自组织神经网络，是一种用于<strong>聚类</strong>的神经网络算法，从名字便可以看出，这是一种无监督式的算法，意味着，它不需要任何训练样本，便可以直接对输入样本根据其<strong>特征</strong>分类，将具有相似特征的划分为一类。</p>
<span id="more"></span>
<h1 id="算法结构">算法结构</h1>
<p>som算法由两层网络组成，输入层与输出层(竞争层)，如图下所示：</p>
<ol type="1">
<li><p><strong>输入层</strong></p>
<p>负责接收外界信息，将输入模式向竞争层传递，起“观察”作用。</p></li>
<li><p><strong>竞争层</strong></p>
<p>负责对输入模式进行“分析比较”，寻找规律并归类。</p></li>
</ol>
<p><img src="/2018/11/07/som/2.png" /></p>
<h2 id="向量归一化">1.向量归一化</h2>
<p>对自组织网络中的当前输入模式向量<span
class="math inline">\(X_i(i=1,2,...n)\)</span>、随机生成输出层神经元<span
class="math inline">\(w_j(j=1,2,...m)\)</span>，全部进行归一化处理,得到<span
class="math inline">\(\hat{X_i}\)</span>和<span
class="math inline">\(\hat{W_j}\)</span>:</p>
<p><span class="math display">\[ \begin{aligned}
    \hat{X_i}=\frac{X_i}{||X_i||}\ \ ,\ \ \hat{W_j}=\frac{W_j}{||W_j||}
\end{aligned} \]</span></p>
<h2 id="寻找获胜神经元">2.寻找获胜神经元</h2>
<p>将<span class="math inline">\(\hat{X_i}\)</span>和<span
class="math inline">\(\hat{W_j}\)</span>进行相似性对比，设获胜神经元矩阵为<span
class="math inline">\(Win\)</span>：</p>
<ol type="1">
<li>基于距离的相似性对比
例如在二维情况下，利用<strong>欧式距离</strong>做相似性对比。 <span
class="math display">\[ \begin{aligned}
    Win_j=min\{||\hat{X}_{i(i=1,2,...n)}-\hat{W_j}||\}\ \ \ \
(j=1,2,...m)
\end{aligned} \]</span></li>
<li>基于方向的相似性对比
利用矩阵的空间性质，夹角越小，<strong>余弦</strong>越大做相似性对比。
<span class="math display">\[ \begin{aligned}
    Win_j=max\{\hat{X_i}*\hat{W_j}\}
\end{aligned} \]</span></li>
</ol>
<h2 id="网络输出与权调整">3.网络输出与权调整</h2>
<p>按照<code>Winner take all</code>的学习法则，获胜的神经元可以调整其权值:
<span class="math display">\[ \begin{aligned}
    Win_j(t+1)=\hat{Win_j}(t)+\eta(t)(\hat{X_i}-\hat{Win_j})\ \ \ \
0&lt;\eta(t)\leq1
\end{aligned} \]</span></p>
<h2 id="循环迭代">4.循环迭代</h2>
<p>先将上一步中的学习率按照梯度下降的缩减 <span
class="math display">\[  
\begin{aligned}
\eta(t)=\eta(t)e^{-N}
\end{aligned}
\]</span></p>
<p>接着进行循环进行第一步。当梯度小于某个<strong>临界点</strong>，或者<span
class="math inline">\(N\)</span>大于某个<strong>临界值</strong>时结束。</p>
<h1 id="优缺点分析">优缺点分析</h1>
<ol type="1">
<li>网络结构是固定的,不能动态改变</li>
<li>网络训练时,有些神经元始终不能获胜,成为“死神经元”</li>
<li>SOM 网络在没有经过完整的重新学习之前,不能加入新的类别</li>
<li>当输入数据较少时,训练的结果通常依赖于样本的输入顺序</li>
<li>网络连接权的初始状态、算法中的参数选择对网络的收敛性能有较大影响。</li>
</ol>
<h1 id="实例">实例</h1>
<h2 id="流程">流程</h2>
<p><img src="/2018/11/07/som/1.png" /></p>
<h2 id="代码">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SOM</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X, output, iteration, batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param X: 形状是N*D， 输入样本有N个,每个D维</span></span><br><span class="line"><span class="string">        :param output: (n,m)一个元组，为输出层的形状是一个n*m的二维矩阵</span></span><br><span class="line"><span class="string">        :param iteration:迭代次数</span></span><br><span class="line"><span class="string">        :param batch_size:每次迭代时的样本数量</span></span><br><span class="line"><span class="string">        初始化一个权值矩阵，形状为D*(n*m)，即有n*m权值向量，每个D维</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.X = X  <span class="comment"># 30 行 2 列 =&gt;  30个数据 2个参数</span></span><br><span class="line">        self.output = output  <span class="comment"># 输出 5x5 的矩阵</span></span><br><span class="line">        self.iteration = iteration  <span class="comment"># 迭代次数</span></span><br><span class="line">        self.batch_size = batch_size  <span class="comment"># 迭代时的样本数量 30</span></span><br><span class="line">        self.W = np.random.rand(</span><br><span class="line">            X.shape[<span class="number">1</span>], output[<span class="number">0</span>] * output[<span class="number">1</span>])  <span class="comment"># 权值矩阵 2行 25 列，</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;W mat shape is&quot;</span>, self.W.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetN</span>(<span class="params">self, t</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param t:时间t, 这里用迭代次数来表示时间</span></span><br><span class="line"><span class="string">        :return: 返回一个整数，表示拓扑距离，时间越大，拓扑邻域越小</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        a = <span class="built_in">min</span>(self.output)  <span class="comment"># 选取输出矩阵中最小的值</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(a-<span class="built_in">float</span>(a)*t/self.iteration)  <span class="comment"># a减去迭代次数的百分比</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Geteta</span>(<span class="params">self, t, n</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param t: 时间t, 这里用迭代次数来表示时间</span></span><br><span class="line"><span class="string">        :param n: 拓扑距离</span></span><br><span class="line"><span class="string">        :return: 返回学习率，</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> np.power(np.e, -n)/(t+<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">updata_W</span>(<span class="params">self, X, t, winner</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        用于更新权值矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N = self.GetN(t)  <span class="comment"># 设置邻域半径</span></span><br><span class="line">        <span class="keyword">for</span> x, i <span class="keyword">in</span> <span class="built_in">enumerate</span>(winner):  <span class="comment"># 获取winner的各个值</span></span><br><span class="line">            to_update = self.getneighbor(i[<span class="number">0</span>], N)  <span class="comment"># i(0)就是当前的winner元素</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(N+<span class="number">1</span>):</span><br><span class="line">                e = self.Geteta(t, j)</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> to_update[j]:</span><br><span class="line">                    self.W[:, w] = np.add(</span><br><span class="line">                        self.W[:, w], e*(X[x, :] - self.W[:, w]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getneighbor</span>(<span class="params">self, index, N</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param index:获胜神经元的下标</span></span><br><span class="line"><span class="string">        :param N: 邻域半径</span></span><br><span class="line"><span class="string">        :return ans: 返回一个集合列表，分别是不同邻域半径内需要更新的神经元坐标</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        a, b = self.output</span><br><span class="line">        length = a*b  <span class="comment"># 获得输出矩阵的长度</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">distence</span>(<span class="params">index1, index2</span>):</span><br><span class="line">            i1_a, i1_b = index1 // a, index1 % b</span><br><span class="line">            i2_a, i2_b = index2 // a, index2 % b</span><br><span class="line">            <span class="keyword">return</span> np.<span class="built_in">abs</span>(i1_a - i2_a), np.<span class="built_in">abs</span>(i1_b - i2_b)</span><br><span class="line">        <span class="comment"># 创建N+1个集合</span></span><br><span class="line">        ans = [<span class="built_in">set</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            <span class="comment"># 求每一个元素与index的距离</span></span><br><span class="line">            dist_a, dist_b = distence(i, index)</span><br><span class="line">            <span class="keyword">if</span> dist_a &lt;= N <span class="keyword">and</span> dist_b &lt;= N:  <span class="comment"># 若小于邻域半径</span></span><br><span class="line">                ans[<span class="built_in">max</span>(dist_a, dist_b)].add(i)  <span class="comment"># ans添加数据</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        train_Y:训练样本与形状为batch_size*(n*m)</span></span><br><span class="line"><span class="string">        winner:一个一维向量，batch_size个获胜神经元的下标</span></span><br><span class="line"><span class="string">        :return:返回值是调整后的W</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        count = <span class="number">0</span>  <span class="comment"># 迭代次数计数器</span></span><br><span class="line">        <span class="keyword">while</span> self.iteration &gt; count:</span><br><span class="line">            <span class="comment"># 开始</span></span><br><span class="line">            <span class="comment"># 从X的总数中随机选择 batch_size 个数做训练</span></span><br><span class="line">            train_X = self.X[np.random.choice(</span><br><span class="line">                self.X.shape[<span class="number">0</span>], self.batch_size)]</span><br><span class="line">            <span class="comment"># 归一化 权值矩阵</span></span><br><span class="line">            normal_W(self.W)</span><br><span class="line">            <span class="comment"># 归一化 训练矩阵</span></span><br><span class="line">            normal_X(train_X)</span><br><span class="line">            <span class="comment"># 训练矩阵[30,2]与权值矩阵[2,25]相乘</span></span><br><span class="line">            train_Y = train_X.dot(self.W)  <span class="comment"># train_Y 为 [30,25]</span></span><br><span class="line">            <span class="comment"># 这里的相似度判别使用的是余弦法，方向越接近，值越接近1</span></span><br><span class="line">            winner = np.argmax(train_Y, axis=<span class="number">1</span>).tolist()  <span class="comment"># 找到每行中最大元素下标</span></span><br><span class="line">            <span class="comment"># 更新权值矩阵</span></span><br><span class="line">            self.updata_W(train_X, count, winner)</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_result</span>(<span class="params">self</span>):</span><br><span class="line">        normal_X(self.X)  <span class="comment"># 归一化数据矩阵</span></span><br><span class="line">        train_Y = self.X.dot(self.W)  <span class="comment"># 输出矩阵为 数据矩阵与权值矩阵叉乘所得</span></span><br><span class="line">        <span class="comment"># train_Y 为 [30,25]</span></span><br><span class="line">        winner = np.argmax(train_Y, axis=<span class="number">1</span>).tolist()  <span class="comment"># 在30行中找到每行最大的元素</span></span><br><span class="line">        <span class="built_in">print</span>(winner)</span><br><span class="line">        <span class="keyword">return</span> winner</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normal_X</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param X:二维矩阵，N*D，N个D维的数据</span></span><br><span class="line"><span class="string">    :return: 将X归一化的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N, D = X.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        temp = np.<span class="built_in">sum</span>(np.multiply(X[i], X[i]))</span><br><span class="line">        X[i] /= np.sqrt(temp)</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normal_W</span>(<span class="params">W</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param W:二维矩阵，D*(n*m)，D个n*m维的数据</span></span><br><span class="line"><span class="string">    :return: 将W归一化的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(W.shape[<span class="number">1</span>]):</span><br><span class="line">        temp = np.<span class="built_in">sum</span>(np.multiply(W[:, i], W[:, i]))</span><br><span class="line">        W[:, i] /= np.sqrt(temp)</span><br><span class="line">    <span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw</span>(<span class="params">C</span>):</span><br><span class="line">    colValue = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;m&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C)):</span><br><span class="line">        coo_X = []  <span class="comment"># x坐标列表</span></span><br><span class="line">        coo_Y = []  <span class="comment"># y坐标列表</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C[i])):</span><br><span class="line">            coo_X.append(C[i][j][<span class="number">0</span>])</span><br><span class="line">            coo_Y.append(C[i][j][<span class="number">1</span>])</span><br><span class="line">        pl.scatter(coo_X, coo_Y, marker=<span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">                   color=colValue[i % <span class="built_in">len</span>(colValue)], label=i)</span><br><span class="line"></span><br><span class="line">    pl.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">    pl.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集：每三个是一组分别是西瓜的编号，密度，含糖量</span></span><br><span class="line">data = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1,0.697,0.46,2,0.774,0.376,3,0.634,0.264,4,0.608,0.318,5,0.556,0.215,</span></span><br><span class="line"><span class="string">6,0.403,0.237,7,0.481,0.149,8,0.437,0.211,9,0.666,0.091,10,0.243,0.267,</span></span><br><span class="line"><span class="string">11,0.245,0.057,12,0.343,0.099,13,0.639,0.161,14,0.657,0.198,15,0.36,0.37,</span></span><br><span class="line"><span class="string">16,0.593,0.042,17,0.719,0.103,18,0.359,0.188,19,0.339,0.241,20,0.282,0.257,</span></span><br><span class="line"><span class="string">21,0.748,0.232,22,0.714,0.346,23,0.483,0.312,24,0.478,0.437,25,0.525,0.369,</span></span><br><span class="line"><span class="string">26,0.751,0.489,27,0.532,0.472,28,0.473,0.376,29,0.725,0.445,30,0.446,0.459&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">a = data.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">dataset = np.mat([[<span class="built_in">float</span>(a[i]), <span class="built_in">float</span>(a[i+<span class="number">1</span>])] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(a)-<span class="number">1</span>, <span class="number">3</span>)])</span><br><span class="line">dataset_old = dataset.copy()</span><br><span class="line"></span><br><span class="line">som = SOM(dataset, (<span class="number">5</span>, <span class="number">5</span>), <span class="number">1</span>, <span class="number">30</span>)</span><br><span class="line">som.train()</span><br><span class="line">res = som.train_result()  <span class="comment"># 返回winner节点的index</span></span><br><span class="line">classify = &#123;&#125;  <span class="comment"># 分类</span></span><br><span class="line"><span class="keyword">for</span> i, win <span class="keyword">in</span> <span class="built_in">enumerate</span>(res):</span><br><span class="line">    <span class="comment"># winner 的index作为类别</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> classify.get(win[<span class="number">0</span>]):</span><br><span class="line">        <span class="comment"># 不存在字典中则添加 win[0]作为key</span></span><br><span class="line">        classify.setdefault(win[<span class="number">0</span>], [i])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 存在则继续append</span></span><br><span class="line">        classify[win[<span class="number">0</span>]].append(i)</span><br><span class="line">C = []  <span class="comment"># 未归一化的数据分类结果</span></span><br><span class="line">D = []  <span class="comment"># 归一化的数据分类结果 </span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> classify.values():</span><br><span class="line">    C.append(dataset_old[i].tolist())</span><br><span class="line">    D.append(dataset[i].tolist())</span><br><span class="line">draw(C) </span><br><span class="line">draw(D) <span class="comment"># 归一化到一个半径为1的圆上</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<!-- 
```dot
digraph G{

    //digraph 是 dot 用于定义有向图的命令，在这里它定义了一幅名为 G 的有向图，
    //花括号中所包含的内容即为该有向图的内容，也就是结点和边。
    //'->' 符号表示有向边，从一个结点指向另一个结点。

    //graph是 dot 用于定义无向图的命令。
    //'--'符号表示无向边。

    //1.定义一个图，并向图中添加需要的顶点和边
    //2.为顶点和边添加样式
    //3.使用布局引擎进行绘制


    //【1】图的属性
    //默认的顶点中的文字为顶点变量的名称，形状为椭圆; 边的默认样式为黑色实线箭头。
    label    = "图的属性设置示例";      //标签
    fontsize = 10;                  //字体大小
    fontname = "Microsoft YaHei";   //字体名称

    //默认结点属性
    node [shape = Mrecord, style = filled, fillcolor = ".7 .3 1.0", color = green, fontsize = 10];

    //默认边属性
    edge [arrowsize = .5];  //箭头为原来的0.5


    //【1】 声明结点ID
    a[shape = component, color = green];  
    b[shape = polygon, sides = 5, peripheries = 3];   //多边形 有五条边 3条边框
    c[shape = polygon, sides = 4, skew = 0.4, label="CC"];  //多边形 有四条边 倾斜角度为0.4 标签文本为CC
    d;
    e;
    f[shape = circle, color = red, style = solid]; //圆形 边框颜色为red
    
    //【2】 构造连接关系，采用 '->'  ，后面的[]中用于定义边的属性
    a->b[color="red"]; //边为red色
    a->c[style = dashed];  //边为虚线
    a->d[style = bold, label="100 times"]; //边加粗，线的标签为100 times

    b->e;  

    e->{f; d}; //同时连接两个

    b->s0[arrowhead = "normal", dir=both]; //边的箭头类型为正常类型，方位为双向
    
    //[3] 结点分组 -- 子图subgraph
    //子图的名称必须以cluster开头，否则graphviz无法设别。
    subgraph cluster_1{ 
        label = "process 1"; //子图的标签
        bgcolor="mintcream"; //子图的背景色
        s0->s1->s2;          //构造连接关系
    };

    //[4] 多条数据的记录 shape = "record"
    //采用'|'分割数据 '\l'换行
    Animal[label = "{Animal | + name : String\l+ age : int\l |+ die() : void\l}", shape = "record" ];
    subgraph clusterAnimalImpl{
        bgcolor = "yellow";
        Dog[label = "{Dog| |+ bark() : void\l}" , shape = "record"];
        Cat[label = "{Cat| |+ meow() : void\l}" , shape = "record"];
    };  

    edge[arrowhead = "empty"];
    Dog->Animal;
    Cat->Animal;
    Dog->Cat[arrowhead="none", label="0..*"];
}
//原文：https://blog.csdn.net/aoshilang2249/article/details/41097603 
//版权声明：本文为博主原创文章，转载请附上博文链接！
``` -->
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" rel="tag">聚类方法</a></li></ul></div><div class="post-nav"><a class="pre" href="/2018/11/11/k210-double-core/">k210_双核测试</a><a class="next" href="/2018/11/04/raspberry-nb/">树莓派NB-IOT使用</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>