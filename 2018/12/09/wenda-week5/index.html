<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>机器学习作业第五周 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习作业第五周</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习作业第五周</h1><div class="post-meta">2018-12-09<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.7k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 16</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ex5.py"><span class="toc-number">1.</span> <span class="toc-text">ex5.py</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E6%95%88%E6%9E%9C"><span class="toc-number">1.1.</span> <span class="toc-text">执行效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fucs5.py"><span class="toc-number">2.</span> <span class="toc-text">fucs5.py</span></a></li></ol></div></div><div class="post-content"><p>这周没有什么好说的,主要就是根据两个误差与方差曲线去判断算法下一步优化的方向,但是这个我觉得还是要做的多了才知道如何去做下一步的判断,毕竟这么多算法,你肯定得都懂点才能去判断优劣.</p>
<span id="more"></span>
<h1 id="ex5.py">ex5.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> c_, r_</span><br><span class="line"><span class="keyword">from</span> fucs5 <span class="keyword">import</span> linearRegCostFunction, trainLinearReg, learningCurve,\</span><br><span class="line">    polyFeatures, featureNormalize, plotFit, validationCurve</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># Machine Learning Online Class</span></span><br><span class="line">    <span class="comment">#  Exercise 5 | Regularized Linear Regression and Bias-Variance</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Instructions</span></span><br><span class="line">    <span class="comment">#  ------------</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  This file contains code that helps you get started on the</span></span><br><span class="line">    <span class="comment">#  exercise. You will need to complete the following functions:</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#     linearRegCostFunction.m</span></span><br><span class="line">    <span class="comment">#     learningCurve.m</span></span><br><span class="line">    <span class="comment">#     validationCurve.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  For this exercise, you will not need to change any code in this file,</span></span><br><span class="line">    <span class="comment">#  or any other files other than those mentioned above.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 1: Loading and Visualizing Data =============</span></span><br><span class="line">    <span class="comment">#  We start the exercise by first loading and visualizing the dataset.</span></span><br><span class="line">    <span class="comment">#  The following code will load the dataset into your environment and plot</span></span><br><span class="line">    <span class="comment">#  the data.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load Training Data</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Loading and Visualizing Data ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load from ex5data1:</span></span><br><span class="line">    <span class="comment"># You will have X, y, Xval, yval, Xtest, ytest in your environment</span></span><br><span class="line">    data = loadmat(<span class="string">&#x27;/media/zqh/程序与工程/Python_study/Machine_learning/\</span></span><br><span class="line"><span class="string">machine_learning_exam/week5/ex5data1.mat&#x27;</span>)</span><br><span class="line">    X = data[<span class="string">&#x27;X&#x27;</span>]</span><br><span class="line">    y = data[<span class="string">&#x27;y&#x27;</span>]</span><br><span class="line">    Xval = data[<span class="string">&#x27;Xval&#x27;</span>]</span><br><span class="line">    yval = data[<span class="string">&#x27;yval&#x27;</span>]</span><br><span class="line">    Xtest = data[<span class="string">&#x27;Xtest&#x27;</span>]</span><br><span class="line">    ytest = data[<span class="string">&#x27;ytest&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># m = Number of examples</span></span><br><span class="line">    m = size(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot training data</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(X, y, color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Change in water level (x)&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Water flowing out of the dam (y)&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 2: Regularized Linear Regression Cost =============</span></span><br><span class="line">    <span class="comment">#  You should now implement the cost function for regularized linear</span></span><br><span class="line">    <span class="comment">#  regression.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    theta = array([<span class="number">1</span>, <span class="number">1</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    J, _ = linearRegCostFunction(c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X], y, theta, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Cost at theta = [1 ; 1]: &#123;&#125;\n \</span></span><br><span class="line"><span class="string">    (this value should be about 303.993192)&#x27;</span>.<span class="built_in">format</span>(J))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 3: Regularized Linear Regression Gradient =============</span></span><br><span class="line">    <span class="comment">#  You should now implement the gradient for regularized linear</span></span><br><span class="line">    <span class="comment">#  regression.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    theta = array([<span class="number">1</span>, <span class="number">1</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    J, grad = linearRegCostFunction(c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X], y, theta, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Gradient at theta = [1 ; 1]:  [&#123;&#125;; &#123;&#125;] \n\</span></span><br><span class="line"><span class="string">        (this value should be about [-15.303016; 598.250744])&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        grad[<span class="number">0</span>], grad[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 4: Train Linear Regression =============</span></span><br><span class="line">    <span class="comment">#  Once you have implemented the cost and gradient correctly, the</span></span><br><span class="line">    <span class="comment">#  trainLinearReg function will use your cost function to train</span></span><br><span class="line">    <span class="comment">#  regularized linear regression.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Write Up Note: The data is non-linear, so this will not give a great</span></span><br><span class="line">    <span class="comment">#                 fit.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Train linear regression with lamda = 0</span></span><br><span class="line">    lamda = <span class="number">0</span></span><br><span class="line">    theta = trainLinearReg(c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X], y, lamda)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Plot fit over the data</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(X, y, color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Change in water level (x)&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Water flowing out of the dam (y)&#x27;</span>)</span><br><span class="line">    plt.plot(X, c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X]@theta, <span class="string">&#x27;b--&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 5: Learning Curve for Linear Regression =============</span></span><br><span class="line">    <span class="comment">#  Next, you should implement the learningCurve function.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Write Up Note: Since the model is underfitting the data, we expect to</span></span><br><span class="line">    <span class="comment">#                 see a graph with &quot;high bias&quot; -- Figure 3 in ex5.pdf</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    lamda = <span class="number">0</span></span><br><span class="line">    error_train, error_val = \</span><br><span class="line">        learningCurve(c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X], y,</span><br><span class="line">                      c_[ones((size(Xval, <span class="number">0</span>), <span class="number">1</span>), <span class="built_in">float</span>), Xval], yval,</span><br><span class="line">                      lamda)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(arange(m), error_train, <span class="string">&#x27;b&#x27;</span>, arange(m), error_val, <span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Learning curve for linear regression&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Cross Validation&#x27;</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Number of training examples&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Error&#x27;</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">13</span>, <span class="number">0</span>, <span class="number">150</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;# Training Examples\tTrain Error\tCross Validation Error&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;  \t&#123;&#125;\t\t&#123;:.6f&#125;\t&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">            i+<span class="number">1</span>, <span class="built_in">float</span>(error_train[i]), <span class="built_in">float</span>(error_val[i])))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 6: Feature Mapping for Polynomial Regression =========</span></span><br><span class="line">    <span class="comment">#  One solution to this is to use polynomial regression. You should now</span></span><br><span class="line">    <span class="comment">#  complete polyFeatures to map each example into its powers</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    p = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Map X onto Polynomial Features and Normalize</span></span><br><span class="line">    X_poly = polyFeatures(X, p)</span><br><span class="line">    X_poly, mu, sigma = featureNormalize(X_poly)  <span class="comment"># Normalize</span></span><br><span class="line">    X_poly = c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X_poly]                   <span class="comment"># Add Ones</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Map X_poly_test and normalize (using mu and sigma)</span></span><br><span class="line">    X_poly_test = polyFeatures(Xtest, p)</span><br><span class="line">    X_poly_test -= mu</span><br><span class="line">    X_poly_test /= sigma</span><br><span class="line">    X_poly_test = c_[ones((size(X_poly_test, <span class="number">0</span>), <span class="number">1</span>), <span class="built_in">float</span>),</span><br><span class="line">                     X_poly_test]         <span class="comment"># Add Ones</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Map X_poly_val and normalize (using mu and sigma)</span></span><br><span class="line">    X_poly_val = polyFeatures(Xval, p)</span><br><span class="line">    X_poly_val -= mu</span><br><span class="line">    X_poly_val /= sigma</span><br><span class="line">    X_poly_val = c_[ones((size(X_poly_val, <span class="number">0</span>), <span class="number">1</span>), <span class="built_in">float</span>),</span><br><span class="line">                    X_poly_val]           <span class="comment"># Add Ones</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Normalized Training Example 1:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;  &#123;&#125;  &#x27;</span>.<span class="built_in">format</span>(X_poly[<span class="number">0</span>, :]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 7: Learning Curve for Polynomial Regression ============</span></span><br><span class="line">    <span class="comment">#  Now, you will get to experiment with polynomial regression with multiple</span></span><br><span class="line">    <span class="comment">#  values of lamda. The code below runs polynomial regression with</span></span><br><span class="line">    <span class="comment">#  lamda = 0. You should try running the code with different values of</span></span><br><span class="line">    <span class="comment">#  lamda to see how the fit and learning curve change.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    lamda = <span class="number">0</span></span><br><span class="line">    theta = trainLinearReg(X_poly, y, lamda)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot training data and fit</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(X, y, c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    plotFit(<span class="built_in">min</span>(X), <span class="built_in">max</span>(X), mu, sigma, theta, p)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Change in water level (x)&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Water flowing out of the dam (y)&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Polynomial Regression Fit (lamda =&#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(lamda))</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    error_train, error_val = learningCurve(</span><br><span class="line">        X_poly, y, X_poly_val, yval, lamda)</span><br><span class="line">    plt.plot(arange(m), error_train, arange(m), error_val)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">&#x27;Polynomial Regression Learning Curve (lamda = &#123;&#125;)&#x27;</span></span><br><span class="line">              .<span class="built_in">format</span>(lamda))</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Number of training examples&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Error&#x27;</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">13</span>, <span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line">    plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Cross Validation&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Polynomial Regression (lamda = &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(lamda))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;# Training Examples\tTrain Error\tCross Validation Error&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;  \t&#123;&#125;\t\t&#123;:.6f&#125;\t&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">            i+<span class="number">1</span>, <span class="built_in">float</span>(error_train[i]), <span class="built_in">float</span>(error_val[i])))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 8: Validation for Selecting Lamda =============</span></span><br><span class="line">    <span class="comment">#  You will now implement validationCurve to test various values of</span></span><br><span class="line">    <span class="comment">#  lamda on a validation set. You will then use this to select the</span></span><br><span class="line">    <span class="comment">#  &quot;best&quot; lamda value.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    lamda_vec, error_train, error_val = validationCurve(</span><br><span class="line">        X_poly, y, X_poly_val, yval)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(lamda_vec, error_train, lamda_vec, error_val)</span><br><span class="line">    plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Cross Validation&#x27;</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;lamda&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Error&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lamda\t\tTrain Error\tValidation Error&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size(lamda_vec)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27; &#123;:.6f&#125;\t&#123;:.6f&#125;\t&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">float</span>(lamda_vec[i]),</span><br><span class="line">                                               <span class="built_in">float</span>(error_train[i]),</span><br><span class="line">                                               <span class="built_in">float</span>(error_val[i])))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="执行效果">执行效果</h2>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week5/ex5.py</span><br><span class="line">Loading and Visualizing Data ...</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Cost at theta = [1 ; 1]: 303.9931922202643</span><br><span class="line">     (this value should be about 303.993192)</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Gradient at theta = [1 ; 1]:  [-15.303015674201186; 598.2507441727035]</span><br><span class="line">        (this value should be about [-15.303016; 598.250744])</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line"><span class="comment"># Training Examples     Train Error     Cross Validation Error</span></span><br><span class="line">        1               0.000000        205.121096</span><br><span class="line">        2               0.000000        110.300366</span><br><span class="line">        3               3.286595        45.010232</span><br><span class="line">        4               2.842678        48.368911</span><br><span class="line">        5               13.154049       35.865141</span><br><span class="line">        6               19.443963       33.829957</span><br><span class="line">        7               20.098522       31.970987</span><br><span class="line">        8               18.172859       30.862446</span><br><span class="line">        9               22.609405       31.135998</span><br><span class="line">        10              23.261462       28.936207</span><br><span class="line">        11              24.317250       29.551432</span><br><span class="line">        12              22.373906       29.433818</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Normalized Training Example 1:</span><br><span class="line">  [ 1.         -0.36214078 -0.75508669  0.18222588 -0.70618991  0.30661792</span><br><span class="line"> -0.59087767  0.3445158  -0.50848117]</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">Polynomial Regression (lamda = 0)</span><br><span class="line"><span class="comment"># Training Examples     Train Error     Cross Validation Error</span></span><br><span class="line">        1               0.000000        160.721900</span><br><span class="line">        2               0.000000        160.121510</span><br><span class="line">        3               0.000000        61.755005</span><br><span class="line">        4               0.000000        61.928895</span><br><span class="line">        5               0.000000        6.597673</span><br><span class="line">        6               0.000000        10.645191</span><br><span class="line">        7               0.000000        27.988538</span><br><span class="line">        8               0.000889        20.280259</span><br><span class="line">        9               0.000236        31.876446</span><br><span class="line">        10              0.040922        19.687148</span><br><span class="line">        11              0.045236        15.815669</span><br><span class="line">        12              0.095506        8.738590</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">lamda           Train Error     Validation Error</span><br><span class="line"> 0.000000       0.095506        8.738590</span><br><span class="line"> 0.001000       0.177934        12.630771</span><br><span class="line"> 0.003000       0.249932        16.348329</span><br><span class="line"> 0.010000       0.385063        17.046746</span><br><span class="line"> 0.030000       0.669275        13.049682</span><br><span class="line"> 0.100000       1.443470        8.149481</span><br><span class="line"> 0.300000       3.101591        5.882464</span><br><span class="line"> 1.000000       7.268148        7.227466</span><br><span class="line"> 3.000000       15.867688       10.089379</span><br><span class="line"> 10.000000      33.372203       19.819785</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/09/wenda-week5/1.png" alt="可视化数据" /> <img
src="/2018/12/09/wenda-week5/2.png" alt="线性回归" /> <img
src="/2018/12/09/wenda-week5/3.png" alt="数据量误差比较" /> <img
src="/2018/12/09/wenda-week5/4.png" alt="多项式拟合" /> <img
src="/2018/12/09/wenda-week5/5.png" alt="特征量误差比较" /> <img
src="/2018/12/09/wenda-week5/6.png" alt="\lambda值误差比较" /></p>
<h1 id="fucs5.py">fucs5.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> c_, r_</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> expit</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> fmin_cg</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linearRegCostFunction</span>(<span class="params">X: ndarray, y: ndarray, theta: ndarray, lamda</span>):</span><br><span class="line">    <span class="comment"># LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear</span></span><br><span class="line">    <span class="comment"># regression with multiple variables</span></span><br><span class="line">    <span class="comment">#   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lamda) computes the</span></span><br><span class="line">    <span class="comment">#   cost of using theta as the parameter for linear regression to fit the</span></span><br><span class="line">    <span class="comment">#   data points in X and y. Returns the cost in J and the gradient in grad</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize some useful values</span></span><br><span class="line">    m = y.shape[<span class="number">0</span>]  <span class="comment"># number of training examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    J = <span class="number">0</span></span><br><span class="line">    theta = theta.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    grad = zeros(shape(theta))  <span class="comment"># type:ndarray</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Compute the cost and gradient of regularized linear</span></span><br><span class="line">    <span class="comment">#               regression for a particular choice of theta.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#               You should set J to the cost and grad to the gradient.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    temp = zeros(shape(theta))</span><br><span class="line">    temp[<span class="number">1</span>:, :] = theta[<span class="number">1</span>:, :]</span><br><span class="line">    J = (<span class="built_in">sum</span>(power(X@theta-y, <span class="number">2</span>))+lamda*<span class="built_in">sum</span>(temp.T @temp))/(<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line">    grad = (X.T@(X@theta-y)+lamda*temp)/m</span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line"></span><br><span class="line">    grad = grad.flatten()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> J, grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">trainLinearReg</span>(<span class="params">X: ndarray, y: ndarray, lamda: <span class="built_in">float</span></span>)-&gt;ndarray:</span><br><span class="line">    <span class="comment"># TRAINLINEARREG Trains linear regression given a dataset(X, y) and a</span></span><br><span class="line">    <span class="comment"># regularization parameter lamda</span></span><br><span class="line">    <span class="comment">#   [theta] = TRAINLINEARREG(X, y, lamda) trains linear regression using</span></span><br><span class="line">    <span class="comment">#   the dataset(X, y) and regularization parameter lamda. Returns the</span></span><br><span class="line">    <span class="comment">#   trained parameters theta.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize Theta</span></span><br><span class="line">    initial_theta = zeros(size(X, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create &quot;short hand&quot; for the cost function to be minimized</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">costFuc</span>(<span class="params">t</span>): <span class="keyword">return</span> linearRegCostFunction(X, y, t, lamda)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gardFuc</span>(<span class="params">t</span>): <span class="keyword">return</span> linearRegCostFunction(X, y, t, lamda)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Now, costFunction is a function that takes in only one argument</span></span><br><span class="line">    <span class="comment"># ops = &#123;&#x27;maxiter&#x27;: 200, &#x27;disp&#x27;: True&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Minimize using fmincg</span></span><br><span class="line">    theta = fmin_cg(costFuc, initial_theta, gardFuc, maxiter=<span class="number">200</span>, disp=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learningCurve</span>(<span class="params">X: ndarray, y: ndarray, Xval: ndarray, yval: ndarray, lamda</span>):</span><br><span class="line">    <span class="comment"># LEARNINGCURVE Generates the train and cross validation set errors needed</span></span><br><span class="line">    <span class="comment"># to plot a learning curve</span></span><br><span class="line">    <span class="comment">#   [error_train, error_val] = ...</span></span><br><span class="line">    <span class="comment">#       LEARNINGCURVE(X, y, Xval, yval, lamda) returns the train and</span></span><br><span class="line">    <span class="comment">#       cross validation set errors for a learning curve. In particular,</span></span><br><span class="line">    <span class="comment">#       it returns two vectors of the same length - error_train and</span></span><br><span class="line">    <span class="comment">#       error_val. Then, error_train(i) contains the training error for</span></span><br><span class="line">    <span class="comment">#       i examples (and similarly for error_val(i)).</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#   In this function, you will compute the train and test errors for</span></span><br><span class="line">    <span class="comment">#   dataset sizes from 1 up to m. In practice, when working with larger</span></span><br><span class="line">    <span class="comment">#   datasets, you might want to do this in larger intervals.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Number of training examples</span></span><br><span class="line">    m = size(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return these values correctly</span></span><br><span class="line">    error_train = zeros((m, <span class="number">1</span>))</span><br><span class="line">    error_val = zeros((m, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Fill in this function to return training errors in</span></span><br><span class="line">    <span class="comment">#               error_train and the cross validation errors in error_val.</span></span><br><span class="line">    <span class="comment">#               i.e., error_train(i) and</span></span><br><span class="line">    <span class="comment">#               error_val(i) should give you the errors</span></span><br><span class="line">    <span class="comment">#               obtained after training on i examples.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: You should evaluate the training error on the first i training</span></span><br><span class="line">    <span class="comment">#       examples (i.e., X(1:i, :) and y(1:i)).</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#       For the cross-validation error, you should instead evaluate on</span></span><br><span class="line">    <span class="comment">#       the _entire_ cross validation set (Xval and yval).</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: If you are using your cost function (linearRegCostFunction)</span></span><br><span class="line">    <span class="comment">#       to compute the training and cross validation error, you should</span></span><br><span class="line">    <span class="comment">#       call the function with the lamda argument set to 0.</span></span><br><span class="line">    <span class="comment">#       Do note that you will still need to use lamda when running</span></span><br><span class="line">    <span class="comment">#       the training to obtain the theta parameters.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Hint: You can loop over the examples with the following:</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#       for i = 1:m</span></span><br><span class="line">    <span class="comment">#           # Compute train/cross validation errors using training examples</span></span><br><span class="line">    <span class="comment">#           # X(1:i, :) and y(1:i), storing the result in</span></span><br><span class="line">    <span class="comment">#           # error_train(i) and error_val(i)</span></span><br><span class="line">    <span class="comment">#           ....</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#       end</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---------------------- Sample Solution ----------------------</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        theta = trainLinearReg(X[:i+<span class="number">1</span>, :], y[:i+<span class="number">1</span>, :], lamda)</span><br><span class="line">        error_train[i, :] = linearRegCostFunction(</span><br><span class="line">            X[:i+<span class="number">1</span>, :], y[:i+<span class="number">1</span>, :], theta, lamda)[<span class="number">0</span>]</span><br><span class="line">        error_val[i, :] = linearRegCostFunction(</span><br><span class="line">            Xval, yval, theta, lamda)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> error_train, error_val</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">polyFeatures</span>(<span class="params">X: ndarray, p</span>)-&gt;ndarray:</span><br><span class="line">    <span class="comment"># POLYFEATURES Maps X (1D vector) into the p-th power</span></span><br><span class="line">    <span class="comment">#   [X_poly] = POLYFEATURES(X, p) takes a data matrix X (size m x 1) and</span></span><br><span class="line">    <span class="comment">#   maps each example into its polynomial features where</span></span><br><span class="line">    <span class="comment">#   X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p];</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly.</span></span><br><span class="line">    X_poly = zeros((size(X), p))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Given a vector X, return a matrix X_poly where the p-th</span></span><br><span class="line">    <span class="comment">#               column of X contains the values of X to the p-th power.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(p):</span><br><span class="line">        X_poly[:, i] = power(X, i+<span class="number">1</span>).flatten()</span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_poly</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">featureNormalize</span>(<span class="params">X: ndarray</span>):</span><br><span class="line">    <span class="comment"># FEATURENORMALIZE Normalizes the features in X</span></span><br><span class="line">    <span class="comment">#   FEATURENORMALIZE(X) returns a normalized version of X where</span></span><br><span class="line">    <span class="comment">#   the mean value of each feature is 0 and the standard deviation</span></span><br><span class="line">    <span class="comment">#   is 1. This is often a good preprocessing step to do when</span></span><br><span class="line">    <span class="comment">#   working with learning algorithms.</span></span><br><span class="line"></span><br><span class="line">    mu = mean(X, axis=<span class="number">0</span>)</span><br><span class="line">    X_norm = X - mu</span><br><span class="line"></span><br><span class="line">    sigma = std(X_norm, axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">    X_norm /= sigma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm, mu, sigma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotFit</span>(<span class="params">min_x, max_x, mu, sigma, theta, p</span>):</span><br><span class="line">    <span class="comment"># PLOTFIT Plots a learned polynomial regression fit over an existing figure</span></span><br><span class="line">    <span class="comment"># Also works with linear regression.</span></span><br><span class="line">    <span class="comment">#   PLOTFIT(min_x, max_x, mu, sigma, theta, p) plots the learned polynomial</span></span><br><span class="line">    <span class="comment">#   fit with power p and feature normalization (mu, sigma).</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Hold on to the current figure</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># We plot a range slightly bigger than the min and max values to get</span></span><br><span class="line">    <span class="comment"># an idea of how the fit will vary outside the range of the data points</span></span><br><span class="line">    x = arange(min_x - <span class="number">15</span>, max_x + <span class="number">25</span>, <span class="number">0.05</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Map the X values</span></span><br><span class="line">    X_poly = polyFeatures(x, p)</span><br><span class="line">    X_poly -= mu</span><br><span class="line">    X_poly /= sigma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add ones</span></span><br><span class="line">    X_poly = c_[ones((size(x, <span class="number">0</span>), <span class="number">1</span>), <span class="built_in">float</span>), X_poly]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    plt.plot(x, X_poly@theta, <span class="string">&#x27;b--&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Hold off to the current figure</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validationCurve</span>(<span class="params">X: ndarray, y: ndarray, Xval: ndarray, yval: ndarray</span>):</span><br><span class="line">    <span class="comment"># VALIDATIONCURVE Generate the train and validation errors needed to</span></span><br><span class="line">    <span class="comment"># plot a validation curve that we can use to select lambda</span></span><br><span class="line">    <span class="comment">#   [lambda_vec, error_train, error_val] = ...</span></span><br><span class="line">    <span class="comment">#       VALIDATIONCURVE(X, y, Xval, yval) returns the train</span></span><br><span class="line">    <span class="comment">#       and validation errors (in error_train, error_val)</span></span><br><span class="line">    <span class="comment">#       for different values of lambda. You are given the training set (X,</span></span><br><span class="line">    <span class="comment">#       y) and validation set (Xval, yval).</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Selected values of lambda (you should not change this)</span></span><br><span class="line">    lambda_vec = array([<span class="number">0</span>, <span class="number">0.001</span>, <span class="number">0.003</span>, <span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>,</span><br><span class="line">                        <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return these variables correctly.</span></span><br><span class="line">    error_train = zeros((size(lambda_vec), <span class="number">1</span>))</span><br><span class="line">    error_val = zeros((size(lambda_vec), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Fill in this function to return training errors in</span></span><br><span class="line">    <span class="comment">#               error_train and the validation errors in error_val. The</span></span><br><span class="line">    <span class="comment">#               vector lambda_vec contains the different lambda parameters</span></span><br><span class="line">    <span class="comment">#               to use for each calculation of the errors, i.e,</span></span><br><span class="line">    <span class="comment">#               error_train(i), and error_val(i) should give</span></span><br><span class="line">    <span class="comment">#               you the errors obtained after training with</span></span><br><span class="line">    <span class="comment">#               lambda = lambda_vec(i)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: You can loop over lambda_vec with the following:</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#       for i = 1:length(lambda_vec)</span></span><br><span class="line">    <span class="comment">#           lambda = lambda_vec(i);</span></span><br><span class="line">    <span class="comment">#           # Compute train / val errors when training linear</span></span><br><span class="line">    <span class="comment">#           # regression with regularization parameter lambda</span></span><br><span class="line">    <span class="comment">#           # You should store the result in error_train(i)</span></span><br><span class="line">    <span class="comment">#           # and error_val(i)</span></span><br><span class="line">    <span class="comment">#           ....</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#       end</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size(lambda_vec)):</span><br><span class="line">        lamda = lambda_vec[i, :]</span><br><span class="line">        theta = trainLinearReg(X, y, lamda)</span><br><span class="line">        error_train[i, :] = linearRegCostFunction(X, y, theta, lamda)[<span class="number">0</span>]</span><br><span class="line">        error_val[i, :] = linearRegCostFunction(Xval, yval, theta, lamda)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> lambda_vec, error_train, error_val</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" rel="tag">吴恩达课程</a></li></ul></div><div class="post-nav"><a class="pre" href="/2018/12/11/bpnnfit/">BP神经网络回归</a><a class="next" href="/2018/12/08/wenda-week4/">机器学习作业第四周</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/03/13/vllm/sglang_attn/">vllm/sglang_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/trt_attn/">vllm/trt_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/vllm_attn/">vllm/vllm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/tvm_attn/">vllm/tvm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/23/torch-trick/">Pytorch中遇到的一些问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>