<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>机器学习作业第七周 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习作业第七周</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习作业第七周</h1><div class="post-meta">2018-12-15<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 25</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>这周是无监督学习算法,比较简单.<code>k means</code>之前就已经写过了,<code>PCA</code>主要是有个矩阵的奇异值分解需要看看矩阵相关知识.</p>
<span id="more"></span>
<h1 id="ex7.py">ex7.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> imageio <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> fucs7 <span class="keyword">import</span> findClosestCentroids, computeCentroids, runkMeans,\</span><br><span class="line">    kMeansInitCentroids</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># Machine Learning Online Class</span></span><br><span class="line">    <span class="comment">#  Exercise 7 | Principle Component Analysis and K-Means Clustering</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Instructions</span></span><br><span class="line">    <span class="comment">#  ------------</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  This file contains code that helps you get started on the</span></span><br><span class="line">    <span class="comment">#  exercise. You will need to complete the following functions:</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#     pca.m</span></span><br><span class="line">    <span class="comment">#     projectData.m</span></span><br><span class="line">    <span class="comment">#     recoverData.m</span></span><br><span class="line">    <span class="comment">#     computeCentroids.m</span></span><br><span class="line">    <span class="comment">#     findClosestCentroids.m</span></span><br><span class="line">    <span class="comment">#     kMeansInitCentroids.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  For this exercise, you will not need to change any code in this file,</span></span><br><span class="line">    <span class="comment">#  or any other files other than those mentioned above.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================= Part 1: Find Closest Centroids ====================</span></span><br><span class="line">    <span class="comment">#  To help you implement K-Means, we have divided the learning algorithm</span></span><br><span class="line">    <span class="comment">#  into two functions -- findClosestCentroids and computeCentroids. In this</span></span><br><span class="line">    <span class="comment">#  part, you shoudl complete the code in the findClosestCentroids function.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finding closest centroids.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load an example dataset that we will be using</span></span><br><span class="line">    data = loadmat(<span class="string">&#x27;ex7data2.mat&#x27;</span>)</span><br><span class="line">    X = data[<span class="string">&#x27;X&#x27;</span>]  <span class="comment"># [300,2]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select an initial set of centroids</span></span><br><span class="line">    K = <span class="number">3</span>  <span class="comment"># 3 Centroids</span></span><br><span class="line">    initial_centroids = array([</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">8.</span>, <span class="number">5.</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find the closest centroids for the examples using the</span></span><br><span class="line">    <span class="comment"># initial_centroids</span></span><br><span class="line">    idx = findClosestCentroids(X, initial_centroids)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Closest centroids for the first 3 examples: &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(idx[<span class="number">0</span>: <span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n(the closest centroids should be 0, 2, 1 respectively)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ===================== Part 2: Compute Means =========================</span></span><br><span class="line">    <span class="comment">#  After implementing the closest centroids function, you should now</span></span><br><span class="line">    <span class="comment">#  complete the computeCentroids function.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nComputing centroids means.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Compute means based on the closest centroids found in the previous part.</span></span><br><span class="line">    centroids = computeCentroids(X, idx, K)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Centroids computed after initial finding of closest centroids: &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(centroids)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n(the centroids should be&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;   [ 2.428301 3.157924 ]&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;   [ 5.813503 2.633656 ]&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;   [ 7.119387 3.616684 ]&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =================== Part 3: K-Means Clustering ======================</span></span><br><span class="line">    <span class="comment">#  After you have completed the two functions computeCentroids and</span></span><br><span class="line">    <span class="comment">#  findClosestCentroids, you have all the necessary pieces to run the</span></span><br><span class="line">    <span class="comment">#  kMeans algorithm. In this part, you will run the K-Means algorithm on</span></span><br><span class="line">    <span class="comment">#  the example dataset we have provided.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nRunning K-Means clustering on example dataset.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load an example dataset</span></span><br><span class="line">    data = loadmat(<span class="string">&#x27;ex7data2.mat&#x27;</span>)</span><br><span class="line">    X = data[<span class="string">&#x27;X&#x27;</span>]</span><br><span class="line">    <span class="comment"># Settings for running K-Means</span></span><br><span class="line">    K = <span class="number">3</span></span><br><span class="line">    max_iters = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># For consistency, here we set centroids to specific values</span></span><br><span class="line">    <span class="comment"># but in practice you want to generate them automatically, such as by</span></span><br><span class="line">    <span class="comment"># settings them to be random examples (as can be seen in</span></span><br><span class="line">    <span class="comment"># kMeansInitCentroids).</span></span><br><span class="line">    initial_centroids = array(</span><br><span class="line">        [[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">6.</span>, <span class="number">2.</span>],</span><br><span class="line">         [<span class="number">8.</span>, <span class="number">5.</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run K-Means algorithm. The &#x27;true&#x27; at the end tells our function to plot</span></span><br><span class="line">    <span class="comment"># the progress of K-Means</span></span><br><span class="line">    centroids, idx = runkMeans(X, initial_centroids, max_iters, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nK-Means Done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============= Part 4: K-Means Clustering on Pixels ===============</span></span><br><span class="line">    <span class="comment">#  In this exercise, you will use K-Means to compress an image. To do this,</span></span><br><span class="line">    <span class="comment">#  you will first run K-Means on the colors of the pixels in the image and</span></span><br><span class="line">    <span class="comment">#  then you will map each pixel on to it&#x27;s closest centroid.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  You should now complete the code in kMeansInitCentroids.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nRunning K-Means clustering on pixels from an image.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Load an image of a bird</span></span><br><span class="line">    A = double(imread(<span class="string">&#x27;bird_small.png&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If imread does not work for you, you can try instead</span></span><br><span class="line">    <span class="comment">#   load (&#x27;bird_small.mat&#x27;);</span></span><br><span class="line"></span><br><span class="line">    A = A / <span class="number">255</span>  <span class="comment"># Divide by 255 so that all values are in the range 0 - 1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Size of the image</span></span><br><span class="line">    img_size = shape(A)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reshape the image into an Nx3 matrix where N = number of pixels.</span></span><br><span class="line">    <span class="comment"># Each row will contain the Red, Green and Blue pixel values</span></span><br><span class="line">    <span class="comment"># This gives us our dataset matrix X that we will use K-Means on.</span></span><br><span class="line">    X = reshape(A, (img_size[<span class="number">0</span>] * img_size[<span class="number">1</span>], <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run your K-Means algorithm on this data</span></span><br><span class="line">    <span class="comment"># You should try different values of K and max_iters here</span></span><br><span class="line">    K = <span class="number">16</span></span><br><span class="line">    max_iters = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># When using K-Means, it is important the initialize the centroids</span></span><br><span class="line">    <span class="comment"># randomly.</span></span><br><span class="line">    <span class="comment"># You should complete the code in kMeansInitCentroids.m before proceeding</span></span><br><span class="line">    initial_centroids = kMeansInitCentroids(X, K)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run K-Means</span></span><br><span class="line">    [centroids, idx] = runkMeans(X, initial_centroids, max_iters)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================= Part 5: Image Compression ======================</span></span><br><span class="line">    <span class="comment">#  In this part of the exercise, you will use the clusters of K-Means to</span></span><br><span class="line">    <span class="comment">#  compress an image. To do this, we first find the closest clusters for</span></span><br><span class="line">    <span class="comment">#  each example. After that, we</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nApplying K-Means to compress an image.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find closest cluster members</span></span><br><span class="line">    idx = findClosestCentroids(X, centroids)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Essentially, now we have represented the image X as in terms of the</span></span><br><span class="line">    <span class="comment"># indices in idx.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># We can now recover the image from the indices (idx) by mapping each pixel</span></span><br><span class="line">    <span class="comment"># (specified by it&#x27;s index in idx) to the centroid value</span></span><br><span class="line">    X_recovered = centroids[idx, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reshape the recovered image into proper dimensions</span></span><br><span class="line">    X_recovered = reshape(X_recovered, (img_size[<span class="number">0</span>], img_size[<span class="number">0</span>], <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display the original image</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(A)</span><br><span class="line">    plt.title(<span class="string">&#x27;Original&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display compressed image side by side</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.imshow(X_recovered)</span><br><span class="line">    plt.title(<span class="string">&#x27;Compressed, with &#123;&#125; colors.&#x27;</span>.<span class="built_in">format</span>(K))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="效果">效果</h2>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Finding closest centroids.</span><br><span class="line">Closest centroids <span class="keyword">for</span> the first 3 examples:</span><br><span class="line">[0 2 1]</span><br><span class="line"></span><br><span class="line">(the closest centroids should be 0, 2, 1 respectively)</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line"></span><br><span class="line">Computing centroids means.</span><br><span class="line">Centroids computed after initial finding of closest centroids:</span><br><span class="line">[[2.42830111 3.15792418]</span><br><span class="line"> [5.81350331 2.63365645]</span><br><span class="line"> [7.11938687 3.6166844 ]]</span><br><span class="line"></span><br><span class="line">(the centroids should be</span><br><span class="line">   [ 2.428301 3.157924 ]</span><br><span class="line">   [ 5.813503 2.633656 ]</span><br><span class="line">   [ 7.119387 3.616684 ]</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line"></span><br><span class="line">Running K-Means clustering on example dataset.</span><br><span class="line">K-Means iteration 0/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 1/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 2/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 3/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 4/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 5/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 6/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 7/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 8/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 9/10...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">K-Means Done.</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line"></span><br><span class="line">Running K-Means clustering on pixels from an image.</span><br><span class="line">K-Means iteration 0/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 1/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 2/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 3/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 4/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 5/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 6/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 7/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 8/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 9/10...</span><br><span class="line"></span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line"></span><br><span class="line">Applying K-Means to compress an image.</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/15/wenda-week7/11.png" /> <img
src="/2018/12/15/wenda-week7/12.png" /></p>
<h1 id="ex7_pca.py">ex7_pca.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> rand, randint</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> imageio <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> fucs7 <span class="keyword">import</span> featureNormalize, pca, drawline, projectData, recoverData,\</span><br><span class="line">    displayData, runkMeans, kMeansInitCentroids, plotDataPoints</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># Machine Learning Online Class</span></span><br><span class="line">    <span class="comment">#  Exercise 7 | Principle Component Analysis and K-Means Clustering</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Instructions</span></span><br><span class="line">    <span class="comment">#  ------------</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  This file contains code that helps you get started on the</span></span><br><span class="line">    <span class="comment">#  exercise. You will need to complete the following functions:</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#     pca.m</span></span><br><span class="line">    <span class="comment">#     projectData.m</span></span><br><span class="line">    <span class="comment">#     recoverData.m</span></span><br><span class="line">    <span class="comment">#     computeCentroids.m</span></span><br><span class="line">    <span class="comment">#     findClosestCentroids.m</span></span><br><span class="line">    <span class="comment">#     kMeansInitCentroids.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  For this exercise, you will not need to change any code in this file,</span></span><br><span class="line">    <span class="comment">#  or any other files other than those mentioned above.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================== Part 1: Load Example Dataset  ===================</span></span><br><span class="line">    <span class="comment">#  We start this exercise by using a small dataset that is easily to</span></span><br><span class="line">    <span class="comment">#  visualize</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Visualizing example dataset for PCA.\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  The following command loads the dataset. You should now have the</span></span><br><span class="line">    <span class="comment">#  variable X in your environment</span></span><br><span class="line">    data = loadmat(<span class="string">&#x27;ex7data1.mat&#x27;</span>)</span><br><span class="line">    X = data[<span class="string">&#x27;X&#x27;</span>]</span><br><span class="line">    <span class="comment">#  Visualize the example dataset</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">    plt.axis([<span class="number">0.5</span>, <span class="number">6.5</span>, <span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line">    <span class="comment"># axis square</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line">    <span class="comment"># pause</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============== Part 2: Principal Component Analysis ===============</span></span><br><span class="line">    <span class="comment">#  You should now implement PCA, a dimension reduction technique. You</span></span><br><span class="line">    <span class="comment">#  should complete the code in pca.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nRunning PCA on example dataset.\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Before running PCA, it is important to first normalize X</span></span><br><span class="line">    X_norm, mu, sigma = featureNormalize(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Run PCA</span></span><br><span class="line">    U, S = pca(X_norm)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Compute mu, the mean of the each feature</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Draw the eigenvectors centered at mean of data. These lines show the</span></span><br><span class="line">    <span class="comment">#  directions of maximum variations in the dataset.</span></span><br><span class="line">    drawline(mu, mu + <span class="number">1.5</span> * S[<span class="number">0</span>, <span class="number">0</span>]*U[:, <span class="number">0</span>], <span class="string">&#x27;k-&#x27;</span>)</span><br><span class="line">    drawline(mu, mu + <span class="number">1.5</span> * S[<span class="number">1</span>, <span class="number">1</span>]*U[:, <span class="number">1</span>], <span class="string">&#x27;k-&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Top eigenvector: \n&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27; U[:,0] = &#123;&#125; &#123;&#125; \n&#x27;</span>.<span class="built_in">format</span>(U[<span class="number">0</span>, <span class="number">0</span>], U[<span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n(you should expect to see -0.707107 -0.707107)\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =================== Part 3: Dimension Reduction ===================</span></span><br><span class="line">    <span class="comment">#  You should now implement the projection step to map the data onto the</span></span><br><span class="line">    <span class="comment">#  first k eigenvectors. The code will then plot the data in this reduced</span></span><br><span class="line">    <span class="comment">#  dimensional space.  This will show you what the data looks like when</span></span><br><span class="line">    <span class="comment">#  using only the corresponding eigenvectors to reconstruct it.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  You should complete the code in projectData.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nDimension reduction on example dataset.\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Plot the normalized dataset (returned from pca)</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(X_norm[:, <span class="number">0</span>], X_norm[:, <span class="number">1</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">    plt.axis([-<span class="number">4</span>, <span class="number">3</span>, - <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Project the data onto K = 1 dimension</span></span><br><span class="line">    K = <span class="number">1</span></span><br><span class="line">    Z = projectData(X_norm, U, K)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Projection of the first example: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(Z[<span class="number">1</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n(this value should be about 1.481274)\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    X_rec = recoverData(Z, U, K)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Approximation of the first example: &#123;&#125; &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">          X_rec[<span class="number">0</span>, <span class="number">0</span>], X_rec[<span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n(this value should be about  -1.047419 -1.047419)\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Draw lines connecting the projected points to the original points</span></span><br><span class="line">    plt.plot(X_rec[:, <span class="number">0</span>], X_rec[:, <span class="number">1</span>], <span class="string">&#x27;ro&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size(X_norm, <span class="number">0</span>)):</span><br><span class="line">        drawline(X_norm[i, :], X_rec[i, :], <span class="string">&#x27;k--&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line">    <span class="comment"># pause</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============== Part 4: Loading and Visualizing Face Data =============</span></span><br><span class="line">    <span class="comment">#  We start the exercise by first loading and visualizing the dataset.</span></span><br><span class="line">    <span class="comment">#  The following code will load the dataset into your environment</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nLoading face dataset.\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Load Face dataset</span></span><br><span class="line">    data = loadmat(<span class="string">&#x27;ex7faces.mat&#x27;</span>)</span><br><span class="line">    X = data[<span class="string">&#x27;X&#x27;</span>]</span><br><span class="line">    <span class="comment">#  Display the first 100 faces in the dataset</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    displayData(X[:<span class="number">99</span>, :])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 5: PCA on Face Data: Eigenfaces  ===================</span></span><br><span class="line">    <span class="comment">#  Run PCA and visualize the eigenvectors which are in this case eigenfaces</span></span><br><span class="line">    <span class="comment">#  We display the first 36 eigenfaces.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nRunning PCA on face dataset.\n\</span></span><br><span class="line"><span class="string">(this mght take a minute or two ...)\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Before running PCA, it is important to first normalize X by subtracting</span></span><br><span class="line">    <span class="comment">#  the mean value from each feature</span></span><br><span class="line">    X_norm, mu, sigma = featureNormalize(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Run PCA</span></span><br><span class="line">    U, S = pca(X_norm)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Visualize the top 36 eigenvectors found</span></span><br><span class="line">    displayData(U[:, :<span class="number">36</span>].T)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line">    <span class="comment"># pause</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============= Part 6: Dimension Reduction for Faces =================</span></span><br><span class="line">    <span class="comment">#  Project images to the eigen space using the top k eigenvectors</span></span><br><span class="line">    <span class="comment">#  If you are applying a machine learning algorithm</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nDimension reduction for face dataset.\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    K = <span class="number">100</span></span><br><span class="line">    Z = projectData(X_norm, U, K)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;The projected data Z has a size of: &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(shape(Z))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n\nProgram paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line">    <span class="comment"># pause</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ==== Part 7: Visualization of Faces after PCA Dimension Reduction ====</span></span><br><span class="line">    <span class="comment">#  Project images to the eigen space using the top K eigen vectors and</span></span><br><span class="line">    <span class="comment">#  visualize only using those K dimensions</span></span><br><span class="line">    <span class="comment">#  Compare to the original input, which is also displayed</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nVisualizing the projected (reduced dimension) faces.\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    K = <span class="number">100</span></span><br><span class="line">    X_rec = recoverData(Z, U, K)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display normalized data</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    displayData(X_norm[:<span class="number">100</span>, :])</span><br><span class="line">    plt.title(<span class="string">&#x27;Original faces&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display reconstructed data from only k eigenfaces</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    displayData(X_rec[:<span class="number">100</span>, :])</span><br><span class="line">    plt.title(<span class="string">&#x27;Recovered faces&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line">    <span class="comment"># pause</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># === Part 8(a): Optional (ungraded) Exercise: PCA for Visualization ===</span></span><br><span class="line">    <span class="comment">#  One useful application of PCA is to use it to visualize high-dimensional</span></span><br><span class="line">    <span class="comment">#  data. In the last K-Means exercise you ran K-Means on 3-dimensional</span></span><br><span class="line">    <span class="comment">#  pixel colors of an image. We first visualize this output in 3D, and then</span></span><br><span class="line">    <span class="comment">#  apply PCA to obtain a visualization in 2D.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Re-load the image from the previous exercise and run K-Means on it</span></span><br><span class="line">    <span class="comment"># For this to work, you need to complete the K-Means assignment first</span></span><br><span class="line">    A = double(imread(<span class="string">&#x27;bird_small.png&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If imread does not work for you, you can try instead</span></span><br><span class="line">    <span class="comment">#   load (&#x27;bird_small.mat&#x27;);</span></span><br><span class="line"></span><br><span class="line">    A = A / <span class="number">255</span></span><br><span class="line">    img_size = shape(A)</span><br><span class="line">    X = reshape(A, (img_size[<span class="number">0</span>] * img_size[<span class="number">1</span>], <span class="number">3</span>))</span><br><span class="line">    K = <span class="number">16</span></span><br><span class="line">    max_iters = <span class="number">10</span></span><br><span class="line">    initial_centroids = kMeansInitCentroids(X, K)</span><br><span class="line">    centroids, idx = runkMeans(X, initial_centroids, max_iters)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Sample 1000 random indexes (since working with all the data is</span></span><br><span class="line">    <span class="comment">#  too expensive. If you have a fast computer, you may increase this.</span></span><br><span class="line">    sel = randint(size(X, <span class="number">0</span>), size=<span class="number">1000</span>, dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Setup Color Palette</span></span><br><span class="line">    <span class="comment"># palette = hsv(K)</span></span><br><span class="line">    <span class="comment"># colors = palette(idx(sel), :)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Visualize the data and centroid memberships in 3D</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">    ax.scatter(X[sel, <span class="number">0</span>], X[sel, <span class="number">1</span>], X[sel, <span class="number">2</span>],  c=idx[sel] % K)</span><br><span class="line">    plt.title(<span class="string">&#x27;Pixel dataset plotted in 3D. Color shows centroid memberships&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># === Part 8(b): Optional (ungraded) Exercise: PCA for Visualization ===</span></span><br><span class="line">    <span class="comment"># Use PCA to project this cloud to 2D for visualization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Subtract the mean to use PCA</span></span><br><span class="line">    X_norm, mu, sigma = featureNormalize(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># PCA and project the data to 2D</span></span><br><span class="line">    U, S = pca(X_norm)</span><br><span class="line">    Z = projectData(X_norm, U, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot in 2D</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plotDataPoints(Z[sel, :], idx[sel], K)</span><br><span class="line">    plt.title(<span class="string">&#x27;Pixel dataset plotted in 2D, using PCA for dimensionality reduction&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.\n&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="效果-1">效果</h2>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Visualizing example dataset <span class="keyword">for</span> PCA.</span><br><span class="line"></span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Running PCA on example dataset.</span><br><span class="line"></span><br><span class="line">Top eigenvector:</span><br><span class="line"> U[:,0] = -0.7071067811865472 -0.7071067811865475</span><br><span class="line">(you should expect to see -0.707107 -0.707107)</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Dimension reduction on example dataset.</span><br><span class="line"></span><br><span class="line">Projection of the first example: [1.48127391]</span><br><span class="line">(this value should be about 1.481274)</span><br><span class="line"></span><br><span class="line">Approximation of the first example: -1.0474188259204957 -1.047418825920496</span><br><span class="line">(this value should be about  -1.047419 -1.047419)</span><br><span class="line"></span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Loading face dataset.</span><br><span class="line"></span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Running PCA on face dataset.</span><br><span class="line">(this mght take a minute or two ...)</span><br><span class="line"></span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Dimension reduction <span class="keyword">for</span> face dataset.</span><br><span class="line"></span><br><span class="line">The projected data Z has a size of:</span><br><span class="line">(5000, 100)</span><br><span class="line"></span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Visualizing the projected (reduced dimension) faces.</span><br><span class="line"></span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">K-Means iteration 0/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 1/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 2/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 3/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 4/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 5/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 6/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 7/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 8/10...</span><br><span class="line"></span><br><span class="line">K-Means iteration 9/10...</span><br><span class="line"></span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/15/wenda-week7/21.png" /> <img
src="/2018/12/15/wenda-week7/22.png" /> <img
src="/2018/12/15/wenda-week7/23.png" /> <img
src="/2018/12/15/wenda-week7/24.png" /> <img
src="/2018/12/15/wenda-week7/25.png" /> <img
src="/2018/12/15/wenda-week7/26.png" /></p>
<h1 id="fucsy.py">fucsy.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> r_, c_, diag</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> permutation</span><br><span class="line"><span class="keyword">from</span> numpy.linalg <span class="keyword">import</span> svd</span><br><span class="line"><span class="keyword">from</span> numpy.matrixlib <span class="keyword">import</span> mat</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cdist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findClosestCentroids</span>(<span class="params">X: ndarray, centroids: ndarray</span>):</span><br><span class="line">    <span class="comment"># FINDCLOSESTCENTROIDS computes the centroid memberships for every example</span></span><br><span class="line">    <span class="comment">#   idx = FINDCLOSESTCENTROIDS (X, centroids) returns the closest centroids</span></span><br><span class="line">    <span class="comment">#   in idx for a dataset X where each row is a single example. idx = m x 1</span></span><br><span class="line">    <span class="comment">#   vector of centroid assignments (i.e. each entry in range [1..K])</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set K</span></span><br><span class="line">    K = size(centroids, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly.</span></span><br><span class="line">    idx = zeros((size(X, <span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Go over every example, find its closest centroid, and store</span></span><br><span class="line">    <span class="comment">#               the index inside idx at the appropriate location.</span></span><br><span class="line">    <span class="comment">#               Concretely, idx(i) should contain the index of the centroid</span></span><br><span class="line">    <span class="comment">#               closest to example i. Hence, it should be a value in the</span></span><br><span class="line">    <span class="comment">#               range 1..K</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: You can use a for-loop over the examples to compute this.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    idx = argmin(cdist(X, centroids), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idx  <span class="comment"># 1d array</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeCentroids</span>(<span class="params">X: ndarray, idx: ndarray, K: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># COMPUTECENTROIDS returs the new centroids by computing the means of the</span></span><br><span class="line">    <span class="comment"># data points assigned to each centroid.</span></span><br><span class="line">    <span class="comment">#   centroids = COMPUTECENTROIDS(X, idx, K) returns the new centroids by</span></span><br><span class="line">    <span class="comment">#   computing the means of the data points assigned to each centroid. It is</span></span><br><span class="line">    <span class="comment">#   given a dataset X where each row is a single data point, a vector</span></span><br><span class="line">    <span class="comment">#   idx of centroid assignments (i.e. each entry in range [1..K]) for each</span></span><br><span class="line">    <span class="comment">#   example, and K, the number of centroids. You should return a matrix</span></span><br><span class="line">    <span class="comment">#   centroids, where each row of centroids is the mean of the data points</span></span><br><span class="line">    <span class="comment">#   assigned to it.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Useful variables</span></span><br><span class="line">    m, n = shape(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly.</span></span><br><span class="line">    centroids = zeros((K, n))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Go over every centroid and compute mean of all points that</span></span><br><span class="line">    <span class="comment">#               belong to it. Concretely, the row vector centroids(i, :)</span></span><br><span class="line">    <span class="comment">#               should contain the mean of the data points assigned to</span></span><br><span class="line">    <span class="comment">#               centroid i.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: You can use a for-loop over the centroids to compute this.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        centroids[i, :] = mean(X[nonzero(idx == i)[<span class="number">0</span>], :], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotDataPoints</span>(<span class="params">X, idx, K</span>):</span><br><span class="line">    <span class="comment"># PLOTDATAPOINTS plots data points in X, coloring them so that those with the same</span></span><br><span class="line">    <span class="comment"># index assignments in idx have the same color</span></span><br><span class="line">    <span class="comment">#   PLOTDATAPOINTS(X, idx, K) plots data points in X, coloring them so that those</span></span><br><span class="line">    <span class="comment">#   with the same index assignments in idx have the same color</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the data</span></span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=idx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotProgresskMeans</span>(<span class="params">X, centroids, previous, idx, K, i</span>):</span><br><span class="line">    <span class="comment"># PLOTPROGRESSKMEANS is a helper function that displays the progress of</span></span><br><span class="line">    <span class="comment"># k-Means as it is running. It is intended for use only with 2D data.</span></span><br><span class="line">    <span class="comment">#   PLOTPROGRESSKMEANS(X, centroids, previous, idx, K, i) plots the data</span></span><br><span class="line">    <span class="comment">#   points with colors assigned to each centroid. With the previous</span></span><br><span class="line">    <span class="comment">#   centroids, it also plots a line between the previous locations and</span></span><br><span class="line">    <span class="comment">#   current locations of the centroids.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the examples</span></span><br><span class="line">    plotDataPoints(X, idx, K)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the centroids as black x&#x27;s</span></span><br><span class="line">    plt.plot(previous[:, <span class="number">0</span>], previous[:, <span class="number">1</span>], <span class="string">&#x27;rx&#x27;</span>)</span><br><span class="line">    plt.plot(centroids[:, <span class="number">0</span>], centroids[:, <span class="number">1</span>], <span class="string">&#x27;bx&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the history of the centroids with lines</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(size(centroids, <span class="number">0</span>)):</span><br><span class="line">        <span class="comment"># matplotlib can&#x27;t draw line like [x1,y1] to [x2,y2]</span></span><br><span class="line">        <span class="comment"># it have to write like [x1,x2] to [y1,y2] f**k!</span></span><br><span class="line">        plt.plot(r_[centroids[j, <span class="number">0</span>], previous[j, <span class="number">0</span>]],</span><br><span class="line">                 r_[centroids[j, <span class="number">1</span>], previous[j, <span class="number">1</span>]], <span class="string">&#x27;k--&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Title</span></span><br><span class="line">    plt.title(<span class="string">&#x27;Iteration number &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">runkMeans</span>(<span class="params">X: ndarray, initial_centroids: ndarray, max_iters: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">              plot_progress=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># RUNKMEANS runs the K-Means algorithm on data matrix X, where each row of X</span></span><br><span class="line">    <span class="comment"># is a single example</span></span><br><span class="line">    <span class="comment">#   [centroids, idx] = RUNKMEANS(X, initial_centroids, max_iters, ...</span></span><br><span class="line">    <span class="comment">#   plot_progress) runs the K-Means algorithm on data matrix X, where each</span></span><br><span class="line">    <span class="comment">#   row of X is a single example. It uses initial_centroids used as the</span></span><br><span class="line">    <span class="comment">#   initial centroids. max_iters specifies the total number of interactions</span></span><br><span class="line">    <span class="comment">#   of K-Means to execute. plot_progress is a true/false flag that</span></span><br><span class="line">    <span class="comment">#   indicates if the function should also plot its progress as the</span></span><br><span class="line">    <span class="comment">#   learning happens. This is set to false by default. runkMeans returns</span></span><br><span class="line">    <span class="comment">#   centroids, a Kxn matrix of the computed centroids and idx, a m x 1</span></span><br><span class="line">    <span class="comment">#   vector of centroid assignments (i.e. each entry in range [1..K])</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the data if we are plotting progress</span></span><br><span class="line">    <span class="keyword">if</span> plot_progress:</span><br><span class="line">        plt.figure()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize values</span></span><br><span class="line">    m, n = shape(X)</span><br><span class="line">    K = size(initial_centroids, <span class="number">0</span>)</span><br><span class="line">    centroids = initial_centroids.copy()</span><br><span class="line">    previous_centroids = centroids.copy()</span><br><span class="line">    idx = zeros((m, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run K-Means</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iters):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output progress</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;K-Means iteration &#123;&#125;/&#123;&#125;...\n&#x27;</span>.<span class="built_in">format</span>(i, max_iters))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For each example in X, assign it to the closest centroid</span></span><br><span class="line">        idx = findClosestCentroids(X, centroids)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Optionally, plot progress here</span></span><br><span class="line">        <span class="keyword">if</span> plot_progress:</span><br><span class="line">            plotProgresskMeans(X, centroids, previous_centroids, idx, K, i)</span><br><span class="line">            previous_centroids = centroids.copy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Given the memberships, compute new centroids</span></span><br><span class="line">        centroids = computeCentroids(X, idx, K)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> plot_progress:</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> centroids, idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kMeansInitCentroids</span>(<span class="params">X: ndarray, K: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># KMEANSINITCENTROIDS This function initializes K centroids that are to be</span></span><br><span class="line">    <span class="comment"># used in K-Means on the dataset X</span></span><br><span class="line">    <span class="comment">#   centroids = KMEANSINITCENTROIDS(X, K) returns K initial centroids to be</span></span><br><span class="line">    <span class="comment">#   used with the K-Means on the dataset X</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You should return this values correctly</span></span><br><span class="line">    centroids = zeros((K, size(X, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: You should set centroids to randomly chosen examples from</span></span><br><span class="line">    <span class="comment">#               the dataset X</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Initialize the centroids to be random examples</span></span><br><span class="line">    <span class="comment"># Randomly reorder the indices of examples</span></span><br><span class="line">    randidx = permutation(size(X, <span class="number">0</span>))</span><br><span class="line">    <span class="comment"># Take the first K examples as centroids</span></span><br><span class="line">    centroids = X[randidx[:K, ], :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">featureNormalize</span>(<span class="params">X: ndarray</span>):</span><br><span class="line">    <span class="comment"># FEATURENORMALIZE Normalizes the features in X</span></span><br><span class="line">    <span class="comment">#   FEATURENORMALIZE(X) returns a normalized version of X where</span></span><br><span class="line">    <span class="comment">#   the mean value of each feature is 0 and the standard deviation</span></span><br><span class="line">    <span class="comment">#   is 1. This is often a good preprocessing step to do when</span></span><br><span class="line">    <span class="comment">#   working with learning algorithms.</span></span><br><span class="line"></span><br><span class="line">    mu = mean(X, axis=<span class="number">0</span>)</span><br><span class="line">    X_norm = X - mu</span><br><span class="line"></span><br><span class="line">    sigma = std(X_norm, axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">    X_norm /= sigma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm, mu, sigma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pca</span>(<span class="params">X: ndarray</span>):</span><br><span class="line">    <span class="comment"># PCA Run principal component analysis on the dataset X</span></span><br><span class="line">    <span class="comment">#   [U, S, X] = pca(X) computes eigenvectors of the covariance matrix of X</span></span><br><span class="line">    <span class="comment">#   Returns the eigenvectors U, the eigenvalues (on diagonal) in S</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Useful values</span></span><br><span class="line">    m, n = shape(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly.</span></span><br><span class="line">    <span class="comment"># U = zeros(n)</span></span><br><span class="line">    <span class="comment"># S = zeros(n)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: You should first compute the covariance matrix. Then, you</span></span><br><span class="line">    <span class="comment">#               should use the &quot;svd&quot; function to compute the eigenvectors</span></span><br><span class="line">    <span class="comment">#               and eigenvalues of the covariance matrix.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: When computing the covariance matrix, remember to divide by m (the</span></span><br><span class="line">    <span class="comment">#       number of examples).</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    Sigma = X.T@ X/m</span><br><span class="line">    U, S, V = svd(Sigma)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> U, diag(S)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drawline</span>(<span class="params">p1, p2, *arg</span>):</span><br><span class="line">    <span class="comment"># DRAWLINE Draws a line from point p1 to point p2</span></span><br><span class="line">    <span class="comment">#   DRAWLINE(p1, p2) Draws a line from point p1 to point p2 and holds the</span></span><br><span class="line">    <span class="comment">#   current figure</span></span><br><span class="line">    plt.plot(r_[p1[<span class="number">0</span>], p2[<span class="number">0</span>]], r_[p1[<span class="number">1</span>], p2[<span class="number">1</span>]], arg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">projectData</span>(<span class="params">X, U, K</span>):</span><br><span class="line">    <span class="comment"># PROJECTDATA Computes the reduced data representation when projecting only</span></span><br><span class="line">    <span class="comment"># on to the top k eigenvectors</span></span><br><span class="line">    <span class="comment">#   Z = projectData(X, U, K) computes the projection of</span></span><br><span class="line">    <span class="comment">#   the normalized inputs X into the reduced dimensional space spanned by</span></span><br><span class="line">    <span class="comment">#   the first K columns of U. It returns the projected examples in Z.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly.</span></span><br><span class="line">    Z = zeros((size(X, <span class="number">0</span>), K))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Compute the projection of the data using only the top K</span></span><br><span class="line">    <span class="comment">#               eigenvectors in U (first K columns).</span></span><br><span class="line">    <span class="comment">#               For the i-th example X(i,:), the projection on to the k-th</span></span><br><span class="line">    <span class="comment">#               eigenvector is given as follows:</span></span><br><span class="line">    <span class="comment">#                    x = X(i, :)&#x27;;</span></span><br><span class="line">    <span class="comment">#                    projection_k = x&#x27; * U(:, k);</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    U_reduce = U[:, : K]</span><br><span class="line">    Z = X @ U_reduce</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">recoverData</span>(<span class="params">Z, U, K</span>):</span><br><span class="line">    <span class="comment"># RECOVERDATA Recovers an approximation of the original data when using the</span></span><br><span class="line">    <span class="comment"># projected data</span></span><br><span class="line">    <span class="comment">#   X_rec = RECOVERDATA(Z, U, K) recovers an approximation the</span></span><br><span class="line">    <span class="comment">#   original data that has been reduced to K dimensions. It returns the</span></span><br><span class="line">    <span class="comment">#   approximate reconstruction in X_rec.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly.</span></span><br><span class="line">    X_rec = zeros((size(Z, <span class="number">0</span>), size(U, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Compute the approximation of the data by projecting back</span></span><br><span class="line">    <span class="comment">#               onto the original space using the top K eigenvectors in U.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#               For the i-th example Z(i,:), the (approximate)</span></span><br><span class="line">    <span class="comment">#               recovered data for dimension j is given as follows:</span></span><br><span class="line">    <span class="comment">#                    v = Z(i, :)&#x27;;</span></span><br><span class="line">    <span class="comment">#                    recovered_j = v&#x27; * U(j, 1:K)&#x27;;</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#               Notice that U(j, 1:K) is a row vector.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    U_reduce = U[:, : K]</span><br><span class="line"></span><br><span class="line">    X_rec = Z @ U_reduce.T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_rec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">displayData</span>(<span class="params">X: ndarray, e_width=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> e_width == <span class="number">0</span>:</span><br><span class="line">        e_width = <span class="built_in">int</span>(<span class="built_in">round</span>(sqrt(X.shape[<span class="number">1</span>])))</span><br><span class="line">    m, n = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单独一个样本的像素大小</span></span><br><span class="line">    e_height = <span class="built_in">int</span>(n/e_width)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割线</span></span><br><span class="line">    pad = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整一副图的像素大小</span></span><br><span class="line">    d_rows = <span class="built_in">int</span>(floor(sqrt(m)))</span><br><span class="line">    d_cols = <span class="built_in">int</span>(ceil(m / d_rows))</span><br><span class="line">    d_array = mat(</span><br><span class="line">        ones((pad+d_rows*(e_height+pad), pad + d_cols * (e_width+pad))))</span><br><span class="line"></span><br><span class="line">    curr_ex = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(d_rows):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(d_cols):</span><br><span class="line">            <span class="keyword">if</span> curr_ex &gt; m:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            max_val = <span class="built_in">max</span>(<span class="built_in">abs</span>(X[curr_ex, :]))</span><br><span class="line">            d_array[pad+j*(e_height+pad) + <span class="number">0</span>:pad+j*(e_height+pad) + e_height,</span><br><span class="line">                    pad+i*(e_width+pad)+<span class="number">0</span>:pad+i*(e_width+pad) + e_width] = \</span><br><span class="line">                X[curr_ex, :].reshape(e_height, e_width)/max_val</span><br><span class="line">            curr_ex += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> curr_ex &gt; m:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 转置一下放正</span></span><br><span class="line">    plt.imshow(d_array.T, cmap=<span class="string">&#x27;Greys&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" rel="tag">吴恩达课程</a></li></ul></div><div class="post-nav"><a class="pre" href="/2018/12/17/wenda-week8/">机器学习作业第八周</a><a class="next" href="/2018/12/13/wenda-week6/">机器学习作业第六周</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/01/11/affine-fusion/">Affine Fusion Pass浅析</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>