<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>机器学习作业第四周 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习作业第四周</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习作业第四周</h1><div class="post-meta">2018-12-08<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.5k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 21</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ex4.py"><span class="toc-number">1.</span> <span class="toc-text">ex4.py</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%88%E6%9E%9C"><span class="toc-number">1.1.</span> <span class="toc-text">效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fucs.py"><span class="toc-number">2.</span> <span class="toc-text">fucs.py</span></a></li></ol></div></div><div class="post-content"><p>第四周的作业是神经网络的训练和预测.这个和我之前写的神经网络有点不一样,吴恩达老师这里所有的都是加上<code>bias</code>节点以及正则化的.主要注意一下梯度函数.</p>
<span id="more"></span>
<p>以下是正则化的梯度函数(代码矩阵形式),假设一共3层: <span
class="math display">\[ \begin{aligned}
    temp^{(2)}&amp;=\Theta^{(2)} \\
    temp^{(1)}&amp;=\Theta^{(1)} \\
    temp^{(2)}[:,1:]&amp;=0 \\
    temp^{(1)}[:,1:]&amp;=0 \\
    \Delta^{(2)} &amp;= (a^{(3)}-y)^T*a^{(2)} \\
    \Delta^{(1)} &amp;= ((a^{(3)}-y)[:,1:]\cdot
g&#39;(z^{(2)}))^T*a^{(1)} \\
    Grad^{(2)} &amp;=\frac{\Delta^{(2)}+\lambda*temp^{(2)} }{m} \\
    Grad^{(1)} &amp;=\frac{\Delta^{(1)}+\lambda*temp^{(1)} }{m}
\end{aligned} \]</span></p>
<h1 id="ex4.py">ex4.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> choice</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> c_, r_</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> fmin_cg</span><br><span class="line"><span class="keyword">from</span> fucs4 <span class="keyword">import</span> displayData, nnCostFunction, sigmoidGradient,\</span><br><span class="line">    randInitializeWeights, checkNNGradients, costFuc, gradFuc, predict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># Machine Learning Online Class - Exercise 4 Neural Network Learning</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Instructions</span></span><br><span class="line">    <span class="comment">#  ------------</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  This file contains code that helps you get started on the</span></span><br><span class="line">    <span class="comment">#  linear exercise. You will need to complete the following functions</span></span><br><span class="line">    <span class="comment">#  in this exericse:</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#     sigmoidGradient.m</span></span><br><span class="line">    <span class="comment">#     randInitializeWeights.m</span></span><br><span class="line">    <span class="comment">#     nnCostFunction.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  For this exercise, you will not need to change any code in this file,</span></span><br><span class="line">    <span class="comment">#  or any other files other than those mentioned above.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Setup the parameters you will use for this exercise</span></span><br><span class="line">    input_layer_size = <span class="number">400</span>  <span class="comment"># 20x20 Input Images of Digits</span></span><br><span class="line">    hidden_layer_size = <span class="number">25</span>   <span class="comment"># 25 hidden units</span></span><br><span class="line">    num_labels = <span class="number">10</span>          <span class="comment"># 10 labels, from 1 to 10</span></span><br><span class="line">    <span class="comment"># (note that we have mapped &quot;0&quot; to label 10)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========== Part 1: Loading and Visualizing Data =============</span></span><br><span class="line">    <span class="comment">#  We start the exercise by first loading and visualizing the dataset.</span></span><br><span class="line">    <span class="comment">#  You will be working with a dataset that contains handwritten digits.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load Training Data</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Loading and Visualizing Data ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># training data stored in arrays X, y</span></span><br><span class="line">    data = loadmat(<span class="string">&#x27;/media/zqh/程序与工程/Python_study/\</span></span><br><span class="line"><span class="string">Machine_learning/machine_learning_exam/week4/ex4data1.mat&#x27;</span>)</span><br><span class="line">    X = data[<span class="string">&#x27;X&#x27;</span>]  <span class="comment"># type:ndarray</span></span><br><span class="line">    y = data[<span class="string">&#x27;y&#x27;</span>]  <span class="comment"># type:ndarray</span></span><br><span class="line"></span><br><span class="line">    m = size(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Randomly select 100 data points to display</span></span><br><span class="line">    rand_indices = choice(m, <span class="number">100</span>)</span><br><span class="line">    sel = X[rand_indices, :]</span><br><span class="line">    displayData(sel)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================ Part 2: Loading Parameters ================</span></span><br><span class="line">    <span class="comment"># In this part of the exercise, we load some pre-initialized</span></span><br><span class="line">    <span class="comment"># neural network parameters.</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Loading Saved Neural Network Parameters ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load the weights into variables Theta1 and Theta2</span></span><br><span class="line">    weightdata = loadmat(<span class="string">&#x27;/media/zqh/程序与工程/Python_study/\</span></span><br><span class="line"><span class="string">Machine_learning/machine_learning_exam/week4/ex4weights.mat&#x27;</span>)</span><br><span class="line">    Theta1 = weightdata[<span class="string">&#x27;Theta1&#x27;</span>]</span><br><span class="line">    Theta2 = weightdata[<span class="string">&#x27;Theta2&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unroll parameters</span></span><br><span class="line">    nn_params = r_[Theta1.reshape(-<span class="number">1</span>, <span class="number">1</span>), Theta2.reshape(-<span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================ Part 3: Compute Cost (Feedforward) ================</span></span><br><span class="line">    <span class="comment">#  To the neural network, you should first start by implementing the</span></span><br><span class="line">    <span class="comment">#  feedforward part of the neural network that returns the cost only. You</span></span><br><span class="line">    <span class="comment">#  should complete the code in nnCostFunction.m to return cost. After</span></span><br><span class="line">    <span class="comment">#  implementing the feedforward to compute the cost, you can verify that</span></span><br><span class="line">    <span class="comment">#  your implementation is correct by verifying that you get the same cost</span></span><br><span class="line">    <span class="comment">#  as us for the fixed debugging parameters.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  We suggest implementing the feedforward cost *without* regularization</span></span><br><span class="line">    <span class="comment">#  first so that it will be easier for you to debug. Later, in part 4, you</span></span><br><span class="line">    <span class="comment">#  will get to implement the regularized cost.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Feedforward Using Neural Network ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weight regularization parameter (we set this to 0 here).</span></span><br><span class="line">    lamda = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    J, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,</span><br><span class="line">                             num_labels, X, y, lamda)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Cost at parameters (loaded from ex4weights): &#123;&#125; \n\</span></span><br><span class="line"><span class="string">    (this value should be about 0.287629)&#x27;</span>.<span class="built_in">format</span>(J))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============== Part 4: Implement Regularization ===============</span></span><br><span class="line">    <span class="comment">#  Once your cost function implementation is correct, you should now</span></span><br><span class="line">    <span class="comment">#  continue to implement the regularization with the cost.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Checking Cost Function (w/ Regularization) ... &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weight regularization parameter (we set this to 1 here).</span></span><br><span class="line">    lamda = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    J, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,</span><br><span class="line">                             num_labels, X, y, lamda)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Cost at parameters (loaded from ex4weights): &#123;&#125;\n\</span></span><br><span class="line"><span class="string">    this value should be about 0.383770)&#x27;</span>.<span class="built_in">format</span>(J))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================ Part 5: Sigmoid Gradient  ================</span></span><br><span class="line">    <span class="comment">#  Before you start implementing the neural network, you will first</span></span><br><span class="line">    <span class="comment">#  implement the gradient for the sigmoid function. You should complete</span></span><br><span class="line">    <span class="comment">#  the code in the sigmoidGradient.m file.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Evaluating sigmoid gradient...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    g = sigmoidGradient(array([-<span class="number">1</span>, -<span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:  &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(g)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================ Part 6: Initializing Pameters ================</span></span><br><span class="line">    <span class="comment">#  In this part of the exercise, you will be starting to implment a two</span></span><br><span class="line">    <span class="comment">#  layer neural network that classifies digits. You will start by</span></span><br><span class="line">    <span class="comment">#  implementing a function to initialize the weights of the neural network</span></span><br><span class="line">    <span class="comment">#  (randInitializeWeights.m)</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Initializing Neural Network Parameters ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)</span><br><span class="line">    initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unroll parameters</span></span><br><span class="line">    initial_nn_params = r_[</span><br><span class="line">        initial_Theta1.reshape(-<span class="number">1</span>, <span class="number">1</span>), initial_Theta2.reshape(-<span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============== Part 7: Implement Backpropagation ===============</span></span><br><span class="line">    <span class="comment">#  Once your cost matches up with ours, you should proceed to implement the</span></span><br><span class="line">    <span class="comment">#  backpropagation algorithm for the neural network. You should add to the</span></span><br><span class="line">    <span class="comment">#  code you&#x27;ve written in nnCostFunction.m to return the partial</span></span><br><span class="line">    <span class="comment">#  derivatives of the parameters.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Checking Backpropagation... &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Check gradients by running checkNNGradients</span></span><br><span class="line">    checkNNGradients()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============== Part 8: Implement Regularization ===============</span></span><br><span class="line">    <span class="comment">#  Once your backpropagation implementation is correct, you should now</span></span><br><span class="line">    <span class="comment">#  continue to implement the regularization with the cost and gradient.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Checking Backpropagation (w/ Regularization) ... &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Check gradients by running checkNNGradients</span></span><br><span class="line">    lamda = <span class="number">3</span></span><br><span class="line">    checkNNGradients(lamda)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Also output the costFunction debugging values</span></span><br><span class="line">    debug_J, _ = nnCostFunction(nn_params, input_layer_size,</span><br><span class="line">                                hidden_layer_size, num_labels, X, y, lamda)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Cost at (fixed) debugging parameters (w/ lamda = &#123;&#125;): &#123;&#125;\n \</span></span><br><span class="line"><span class="string">(for lamda = 3, this value should be about 0.576051)&#x27;</span>.<span class="built_in">format</span>(lamda, debug_J))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =================== Part 8: Training NN ===================</span></span><br><span class="line">    <span class="comment">#  You have now implemented all the code necessary to train a neural</span></span><br><span class="line">    <span class="comment">#  network. To train your neural network, we will now use &quot;fmincg&quot;, which</span></span><br><span class="line">    <span class="comment">#  is a function which works similarly to &quot;fminunc&quot;. Recall that these</span></span><br><span class="line">    <span class="comment">#  advanced optimizers are able to train our cost functions efficiently as</span></span><br><span class="line">    <span class="comment">#  long as we provide them with the gradient computations.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Training Neural Network... &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  After you have completed the assignment, change the MaxIter to a larger</span></span><br><span class="line">    <span class="comment">#  value to see how more training helps.</span></span><br><span class="line">    MaxIter = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  You should also try different values of lamda</span></span><br><span class="line">    lamda = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Now, costFunction is a function that takes in only one argument (the</span></span><br><span class="line">    <span class="comment"># neural network parameters)</span></span><br><span class="line">    Y = zeros((m, num_labels))  <span class="comment"># convrt Y</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        Y[i, y[i, :] - <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    nn_params = fmin_cg(costFuc, initial_nn_params.flatten(), gradFuc,</span><br><span class="line">                        (input_layer_size, hidden_layer_size,</span><br><span class="line">                         num_labels, X, Y, lamda), maxiter=MaxIter)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Obtain Theta1 and Theta2 back from nn_params</span></span><br><span class="line">    Theta1 = nn_params[: hidden_layer_size * (input_layer_size + <span class="number">1</span>)] \</span><br><span class="line">        .reshape(hidden_layer_size, input_layer_size + <span class="number">1</span>)</span><br><span class="line">    Theta2 = nn_params[hidden_layer_size * (input_layer_size + <span class="number">1</span>):] \</span><br><span class="line">        .reshape(num_labels, hidden_layer_size + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================= Part 9: Visualize Weights =================</span></span><br><span class="line">    <span class="comment">#  You can now &quot;visualize&quot; what the neural network is learning by</span></span><br><span class="line">    <span class="comment">#  displaying the hidden units to see what features they are capturing in</span></span><br><span class="line">    <span class="comment">#  the data.</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Visualizing Neural Network... &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    displayData(Theta1[:, <span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Program paused. Press enter to continue.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ================= Part 10: Implement Predict =================</span></span><br><span class="line">    <span class="comment">#  After training the neural network, we would like to use it to predict</span></span><br><span class="line">    <span class="comment">#  the labels. You will now implement the &quot;predict&quot; function to use the</span></span><br><span class="line">    <span class="comment">#  neural network to predict the labels of the training set. This lets</span></span><br><span class="line">    <span class="comment">#  you compute the training set accuracy.</span></span><br><span class="line"></span><br><span class="line">    pred = predict(Theta1, Theta2, X)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Training Set Accuracy: &#123;&#125;%&#x27;</span>.<span class="built_in">format</span>(mean(array(pred == y)) * <span class="number">100</span>))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="效果">效果</h2>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">➜  Machine_learning /usr/bin/python3 /media/zqh/程序与工程/Python_study/Machine_learning/machine_learning_exam/week4/ex4.py</span><br><span class="line">Loading and Visualizing Data ...</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Loading Saved Neural Network Parameters ...</span><br><span class="line">Feedforward Using Neural Network ...</span><br><span class="line">Cost at parameters (loaded from ex4weights): 0.2876291651613189</span><br><span class="line">    (this value should be about 0.287629)</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Checking Cost Function (w/ Regularization) ...</span><br><span class="line">Cost at parameters (loaded from ex4weights): 0.38376985909092365</span><br><span class="line">    this value should be about 0.383770)</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Evaluating sigmoid gradient...</span><br><span class="line">Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:</span><br><span class="line">[[0.19661193]</span><br><span class="line"> [0.23500371]</span><br><span class="line"> [0.25      ]</span><br><span class="line"> [0.23500371]</span><br><span class="line"> [0.19661193]]</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Initializing Neural Network Parameters ...</span><br><span class="line">Checking Backpropagation...</span><br><span class="line">[[ 1.23162247e-02]</span><br><span class="line"> .</span><br><span class="line"> .</span><br><span class="line"> .</span><br><span class="line"> [ 5.02929547e-02]]</span><br><span class="line">The above two columns you get should be very similar.</span><br><span class="line">(Left-Your Numerical Gradient, Right-Analytical Gradient)</span><br><span class="line">If your backpropagation implementation is correct, <span class="keyword">then</span></span><br><span class="line">the relative difference will be small (less than 1e-9).</span><br><span class="line"></span><br><span class="line">Relative Difference: 1.848611973407009e-11</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Checking Backpropagation (w/ Regularization) ...</span><br><span class="line">[[ 0.01231622]</span><br><span class="line"> .</span><br><span class="line"> .</span><br><span class="line"> .</span><br><span class="line"> [ 0.00523372]]</span><br><span class="line">The above two columns you get should be very similar.</span><br><span class="line">(Left-Your Numerical Gradient, Right-Analytical Gradient)</span><br><span class="line">If your backpropagation implementation is correct, <span class="keyword">then</span></span><br><span class="line">the relative difference will be small (less than 1e-9).</span><br><span class="line"></span><br><span class="line">Relative Difference: 1.8083382559674107e-11</span><br><span class="line">Cost at (fixed) debugging parameters (w/ lamda = 3): 0.5760512469501331</span><br><span class="line"> (<span class="keyword">for</span> lamda = 3, this value should be about 0.576051)</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Training Neural Network...</span><br><span class="line">Warning: Maximum number of iterations has been exceeded.</span><br><span class="line">         Current <span class="keyword">function</span> value: 0.483014</span><br><span class="line">         Iterations: 50</span><br><span class="line">         Function evaluations: 116</span><br><span class="line">         Gradient evaluations: 116</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Visualizing Neural Network...</span><br><span class="line">Program paused. Press enter to <span class="built_in">continue</span>.</span><br><span class="line">Training Set Accuracy: 95.88%</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/08/wenda-week4/1.png" alt="标签" /> <img
src="/2018/12/08/wenda-week4/2.png" alt="隐含层可视化" /></p>
<h1 id="fucs.py">fucs.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> c_, r_</span><br><span class="line"><span class="keyword">from</span> numpy.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> numpy.linalg <span class="keyword">import</span> norm</span><br><span class="line"><span class="keyword">from</span> numpy.matlib <span class="keyword">import</span> mat</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> rand</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> fmin_cg, minimize</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> expit, logit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">displayData</span>(<span class="params">X: ndarray, e_width=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> e_width == <span class="number">0</span>:</span><br><span class="line">        e_width = <span class="built_in">int</span>(<span class="built_in">round</span>(math.sqrt(X.shape[<span class="number">1</span>])))</span><br><span class="line">    m, n = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单独一个样本的像素大小</span></span><br><span class="line">    e_height = <span class="built_in">int</span>(n / e_width)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割线</span></span><br><span class="line">    pad = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整一副图的像素大小</span></span><br><span class="line">    d_rows = math.floor(math.sqrt(m))</span><br><span class="line">    d_cols = math.ceil(m / d_rows)</span><br><span class="line">    d_array = mat(</span><br><span class="line">        ones((pad + d_rows * (e_height + pad),</span><br><span class="line">              pad + d_cols * (e_width + pad))))</span><br><span class="line">    curr_ex = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(d_rows):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(d_cols):</span><br><span class="line">            <span class="keyword">if</span> curr_ex &gt; m:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            max_val = <span class="built_in">max</span>(<span class="built_in">abs</span>(X[curr_ex, :]))</span><br><span class="line">            d_array[pad + j * (e_height + pad) + <span class="number">0</span>:pad + j *</span><br><span class="line">                    (e_height + pad) + e_height,</span><br><span class="line">                    pad + i * (e_width + pad) + <span class="number">0</span>:pad + i *</span><br><span class="line">                    (e_width + pad) + e_width] = \</span><br><span class="line">                X[curr_ex, :].reshape(e_height, e_width) / max_val</span><br><span class="line">            curr_ex += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> curr_ex &gt; m:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 转置一下放正</span></span><br><span class="line">    plt.imshow(d_array.T, cmap=<span class="string">&#x27;Greys&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoidGradient</span>(<span class="params">z: ndarray</span>)-&gt;ndarray:</span><br><span class="line">    <span class="comment"># SIGMOIDGRADIENT returns the gradient of the sigmoid function</span></span><br><span class="line">    <span class="comment"># evaluated at z</span></span><br><span class="line">    <span class="comment">#   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function</span></span><br><span class="line">    <span class="comment">#   evaluated at z. This should work regardless if z is a matrix or a</span></span><br><span class="line">    <span class="comment">#   vector. In particular, if z is a vector or matrix, you should return</span></span><br><span class="line">    <span class="comment">#   the gradient for each element.</span></span><br><span class="line"></span><br><span class="line">    g = zeros(shape(z))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Compute the gradient of the sigmoid function evaluated at</span></span><br><span class="line">    <span class="comment">#               each value of z (z can be a matrix, vector or scalar).</span></span><br><span class="line"></span><br><span class="line">    g = expit(z)*(<span class="number">1</span>-expit(z))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">costFuc</span>(<span class="params">nn_params: ndarray,</span></span><br><span class="line"><span class="params">            input_layer_size, hidden_layer_size, num_labels,</span></span><br><span class="line"><span class="params">            X: ndarray, Y: ndarray, lamda: <span class="built_in">float</span></span>):</span><br><span class="line"></span><br><span class="line">    Theta1 = nn_params[: hidden_layer_size * (input_layer_size + <span class="number">1</span>)] \</span><br><span class="line">        .reshape(hidden_layer_size, input_layer_size + <span class="number">1</span>)</span><br><span class="line">    Theta2 = nn_params[hidden_layer_size * (input_layer_size + <span class="number">1</span>):] \</span><br><span class="line">        .reshape(num_labels, hidden_layer_size + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    m = size(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    temp1 = power(Theta1[:, <span class="number">1</span>:], <span class="number">2</span>)  <span class="comment"># power</span></span><br><span class="line">    temp2 = power(Theta2[:, <span class="number">1</span>:], <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    h = expit(c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), expit(</span><br><span class="line">        (c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X] @ Theta1.T))] @ Theta2.T)</span><br><span class="line"></span><br><span class="line">    J = <span class="built_in">sum</span>(-Y * log(h) - (<span class="number">1</span> - Y) * log(<span class="number">1</span> - h)) / m\</span><br><span class="line">        + lamda*(<span class="built_in">sum</span>(temp1)+<span class="built_in">sum</span>(temp2))/(<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradFuc</span>(<span class="params">nn_params: ndarray,</span></span><br><span class="line"><span class="params">            input_layer_size, hidden_layer_size, num_labels,</span></span><br><span class="line"><span class="params">            X: ndarray, Y: ndarray, lamda: <span class="built_in">float</span></span>):</span><br><span class="line"></span><br><span class="line">    Theta1 = nn_params[: hidden_layer_size * (input_layer_size + <span class="number">1</span>)] \</span><br><span class="line">        .reshape(hidden_layer_size, input_layer_size + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    Theta2 = nn_params[hidden_layer_size * (input_layer_size + <span class="number">1</span>):] \</span><br><span class="line">        .reshape(num_labels, hidden_layer_size + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Setup some useful variables</span></span><br><span class="line">    m = size(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    Theta1_grad = zeros(shape(Theta1))</span><br><span class="line">    Theta2_grad = zeros(shape(Theta2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># a_1 = X add cloum</span></span><br><span class="line">    a_1 = c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X]</span><br><span class="line">    z_2 = a_1@ Theta1.T</span><br><span class="line">    a_2 = c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), expit(z_2)]</span><br><span class="line">    a_3 = expit(a_2@Theta2.T)  <span class="comment"># a_3 is h</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># err</span></span><br><span class="line">    err_3 = a_3-Y  <span class="comment"># [5000,10]</span></span><br><span class="line">    <span class="comment"># [5000,10]*[10,26] 取出第一列 =&gt; [5000,25].*[5000,25]</span></span><br><span class="line">    err_2 = (err_3@Theta2)[:, <span class="number">1</span>:]*sigmoidGradient(z_2)</span><br><span class="line"></span><br><span class="line">    temptheta2 = c_[zeros((size(Theta2, <span class="number">0</span>), <span class="number">1</span>)), Theta2[:, <span class="number">1</span>:]]</span><br><span class="line">    temptheta1 = c_[zeros((size(Theta1, <span class="number">0</span>), <span class="number">1</span>)), Theta1[:, <span class="number">1</span>:]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [5000,10].T*[5000,26]</span></span><br><span class="line">    Theta2_grad = (err_3.T@a_2+lamda*temptheta2)/m</span><br><span class="line">    Theta1_grad = (err_2.T@a_1+lamda*temptheta1)/m</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unroll gradients</span></span><br><span class="line">    grad = r_[Theta1_grad.reshape(-<span class="number">1</span>, <span class="number">1</span>), Theta2_grad.reshape(-<span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> grad.flatten()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nnCostFunction</span>(<span class="params">nn_params: ndarray,</span></span><br><span class="line"><span class="params">                   input_layer_size, hidden_layer_size, num_labels,</span></span><br><span class="line"><span class="params">                   X: ndarray, y: ndarray, lamda: <span class="built_in">float</span></span>):</span><br><span class="line">    <span class="comment"># NNCOSTFUNCTION Implements the neural network cost function for</span></span><br><span class="line">    <span class="comment"># a two layer</span></span><br><span class="line">    <span class="comment">#   neural network which performs classification</span></span><br><span class="line">    <span class="comment">#   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...</span></span><br><span class="line">    <span class="comment">#   X, y, lamda) computes the cost and gradient of the neural network. The</span></span><br><span class="line">    <span class="comment">#   parameters for the neural network are &quot;unrolled&quot; into the vector</span></span><br><span class="line">    <span class="comment">#   nn_params and need to be converted back into the weight matrices.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#   The returned parameter grad should be a &quot;unrolled&quot; vector of the</span></span><br><span class="line">    <span class="comment">#   partial derivatives of the neural network.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reshape nn_params back into the parameters Theta1 and Theta2,</span></span><br><span class="line">    <span class="comment"># the weight matrices</span></span><br><span class="line">    <span class="comment"># for our 2 layer neural network</span></span><br><span class="line">    Theta1 = nn_params[: hidden_layer_size * (input_layer_size + <span class="number">1</span>), :] \</span><br><span class="line">        .reshape(hidden_layer_size, input_layer_size + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    Theta2 = nn_params[hidden_layer_size * (input_layer_size + <span class="number">1</span>):] \</span><br><span class="line">        .reshape(num_labels, hidden_layer_size + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Setup some useful variables</span></span><br><span class="line">    m = size(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    J = <span class="number">0</span></span><br><span class="line">    Theta1_grad = zeros(shape(Theta1))</span><br><span class="line">    Theta2_grad = zeros(shape(Theta2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: You should complete the code by working through the</span></span><br><span class="line">    <span class="comment">#               following parts.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Part 1: Feedforward the neural network and return the cost in the</span></span><br><span class="line">    <span class="comment">#         variable J. After implementing Part 1, you can verify that your</span></span><br><span class="line">    <span class="comment">#         cost function computation is correct by verifying the cost</span></span><br><span class="line">    <span class="comment">#         computed in ex4.m</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Part 2: Implement the backpropagation algorithm to compute the gradients</span></span><br><span class="line">    <span class="comment">#         Theta1_grad and Theta2_grad. You should return the partial</span></span><br><span class="line">    <span class="comment">#         derivatives of the cost function with respect to Theta1 and</span></span><br><span class="line">    <span class="comment">#         Theta2 in Theta1_grad and Theta2_grad, respectively.</span></span><br><span class="line">    <span class="comment">#         After implementing Part 2, you can check that your</span></span><br><span class="line">    <span class="comment">#         implementation is correct by running checkNNGradients</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#         Note: The vector y passed into the function is a vector of labels</span></span><br><span class="line">    <span class="comment">#               containing values from 1..K. You need to map this vector</span></span><br><span class="line">    <span class="comment">#               into a binary vector of 1&#x27;s and 0&#x27;s to be used with the</span></span><br><span class="line">    <span class="comment">#               neural network cost function.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#         Hint: We recommend implementing backpropagation using a for-loop</span></span><br><span class="line">    <span class="comment">#               over the training examples if you are implementing it for</span></span><br><span class="line">    <span class="comment">#               the first time.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Part 3: Implement regularization with the cost function and gradients.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#         Hint: You can implement this around the code for</span></span><br><span class="line">    <span class="comment">#               backpropagation. That is, you can compute the gradients for</span></span><br><span class="line">    <span class="comment">#               the regularization separately and then add them to</span></span><br><span class="line">    <span class="comment">#               Theta1_grad and Theta2_grad from Part 2.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># -------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># first covert y=[5000,1] to Y=[5000,10]</span></span><br><span class="line">    Y = zeros((m, num_labels))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        Y[i, y[i, :] - <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># without regularization</span></span><br><span class="line">    <span class="comment"># h = expit(c_[ones((m, 1), float), expit(</span></span><br><span class="line">    <span class="comment">#     (c_[ones((m, 1), float), X] @ Theta1.T))] @ Theta2.T)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># J = sum(sum(-Y * log(h) - (1 - Y) * log(1 - h), axis=1)) / m</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># regularization</span></span><br><span class="line">    temp1 = zeros((size(Theta1, <span class="number">0</span>), size(Theta1, <span class="number">1</span>)-<span class="number">1</span>))  <span class="comment"># [25*400]</span></span><br><span class="line">    temp2 = zeros((size(Theta2, <span class="number">0</span>), size(Theta2, <span class="number">1</span>)-<span class="number">1</span>))  <span class="comment"># [10*25]</span></span><br><span class="line"></span><br><span class="line">    temp1 = power(Theta1[:, <span class="number">1</span>:], <span class="number">2</span>)  <span class="comment"># copy and power</span></span><br><span class="line">    temp2 = power(Theta2[:, <span class="number">1</span>:], <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># a_1 = X add cloum</span></span><br><span class="line">    a_1 = c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X]</span><br><span class="line">    z_2 = a_1@ Theta1.T</span><br><span class="line">    a_2 = c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), expit(z_2)]</span><br><span class="line">    a_3 = expit(a_2@Theta2.T)  <span class="comment"># a_3 is h</span></span><br><span class="line"></span><br><span class="line">    J = <span class="built_in">sum</span>(-Y * log(a_3) - (<span class="number">1</span> - Y) * log(<span class="number">1</span> - a_3)) / m\</span><br><span class="line">        + lamda*(<span class="built_in">sum</span>(temp1)+<span class="built_in">sum</span>(temp2))/(<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># err</span></span><br><span class="line">    err_3 = a_3-Y  <span class="comment"># [5000,10]</span></span><br><span class="line">    <span class="comment"># [5000,10]*[10,26] 取出第一列 =&gt; [5000,25].*[5000,25]</span></span><br><span class="line">    err_2 = (err_3@Theta2)[:, <span class="number">1</span>:]*sigmoidGradient(z_2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># grad</span></span><br><span class="line">    delta_2 = err_3.T@a_2</span><br><span class="line">    delta_1 = err_2.T@a_1</span><br><span class="line"></span><br><span class="line">    temptheta2 = c_[zeros((size(Theta2, <span class="number">0</span>), <span class="number">1</span>)), Theta2[:, <span class="number">1</span>:]]</span><br><span class="line">    temptheta1 = c_[zeros((size(Theta1, <span class="number">0</span>), <span class="number">1</span>)), Theta1[:, <span class="number">1</span>:]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [5000,10].T*[5000,26]</span></span><br><span class="line">    Theta2_grad = (delta_2+lamda*temptheta2)/m</span><br><span class="line">    Theta1_grad = (delta_1+lamda*temptheta1)/m</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unroll gradients</span></span><br><span class="line">    grad = r_[Theta1_grad.reshape(-<span class="number">1</span>, <span class="number">1</span>), Theta2_grad.reshape(-<span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> J, grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">randInitializeWeights</span>(<span class="params">L_in: <span class="built_in">int</span>, L_out: <span class="built_in">int</span></span>)-&gt;ndarray:</span><br><span class="line">    <span class="comment"># RANDINITIALIZEWEIGHTS Randomly initialize the weights of</span></span><br><span class="line">    <span class="comment"># a layer with L_in</span></span><br><span class="line">    <span class="comment"># incoming connections and L_out outgoing connections</span></span><br><span class="line">    <span class="comment">#   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights</span></span><br><span class="line">    <span class="comment">#   of a layer with L_in incoming connections and L_out outgoing</span></span><br><span class="line">    <span class="comment">#   connections.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#   Note that W should be set to a matrix of size(L_out, 1 + L_in) as</span></span><br><span class="line">    <span class="comment">#   the first column of W handles the &quot;bias&quot; terms</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    W = zeros((L_out, <span class="number">1</span> + L_in))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># Instructions: Initialize W randomly so that we break the symmetry while</span></span><br><span class="line">    <span class="comment">#               training the neural network.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: The first column of W corresponds to the parameters for the bias</span></span><br><span class="line">    <span class="comment"># unit</span></span><br><span class="line">    epsilon_init = <span class="number">0.12</span></span><br><span class="line">    W = rand(L_out, L_in+<span class="number">1</span>)*<span class="number">2</span>*epsilon_init-epsilon_init</span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line">    <span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">debugInitializeWeights</span>(<span class="params">fan_out, fan_in</span>):</span><br><span class="line">    <span class="comment"># DEBUGINITIALIZEWEIGHTS Initialize the weights of a layer with fan_in</span></span><br><span class="line">    <span class="comment"># incoming connections and fan_out outgoing connections using a fixed</span></span><br><span class="line">    <span class="comment"># strategy, this will help you later in debugging</span></span><br><span class="line">    <span class="comment">#   W = DEBUGINITIALIZEWEIGHTS(fan_in, fan_out) initializes the weights</span></span><br><span class="line">    <span class="comment">#   of a layer with fan_in incoming connections and fan_out outgoing</span></span><br><span class="line">    <span class="comment">#   connections using a fix set of values</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#   Note that W should be set to a matrix of size(1 + fan_in, fan_out) as</span></span><br><span class="line">    <span class="comment">#   the first row of W handles the &quot;bias&quot; terms</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set W to zeros</span></span><br><span class="line">    W = zeros((fan_out, <span class="number">1</span> + fan_in))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize W using &quot;sin&quot;, this ensures that W is always of the same</span></span><br><span class="line">    <span class="comment"># values and will be useful for debugging</span></span><br><span class="line">    W = sin(arange(<span class="number">1</span>, size(W)+<span class="number">1</span>).reshape(fan_out, <span class="number">1</span> + fan_in)) / <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeNumericalGradient</span>(<span class="params">J, theta: ndarray</span>):</span><br><span class="line">    <span class="comment"># COMPUTENUMERICALGRADIENT Computes the gradient using &quot;finite differences&quot;</span></span><br><span class="line">    <span class="comment"># and gives us a numerical estimate of the gradient.</span></span><br><span class="line">    <span class="comment">#   numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical</span></span><br><span class="line">    <span class="comment">#   gradient of the function J around theta. Calling y = J(theta) should</span></span><br><span class="line">    <span class="comment">#   return the function value at theta.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Notes: The following code implements numerical gradient checking, and</span></span><br><span class="line">    <span class="comment">#        returns the numerical gradient.It sets numgrad(i) to (a numerical</span></span><br><span class="line">    <span class="comment">#        approximation of) the partial derivative of J with respect to the</span></span><br><span class="line">    <span class="comment">#        i-th input argument, evaluated at theta. (i.e., numgrad(i) should</span></span><br><span class="line">    <span class="comment">#        be the (approximately) the partial derivative of J with respect</span></span><br><span class="line">    <span class="comment">#        to theta(i).)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    numgrad = zeros(shape(theta))</span><br><span class="line">    perturb = zeros(shape(theta))</span><br><span class="line">    e = <span class="number">1e-4</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(size(theta)):</span><br><span class="line">        <span class="comment"># Set perturbation vector</span></span><br><span class="line">        perturb[p, :] = e</span><br><span class="line">        loss1 = J(theta - perturb)[<span class="number">0</span>]</span><br><span class="line">        loss2 = J(theta + perturb)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Compute Numerical Gradient</span></span><br><span class="line">        numgrad[p, :] = (loss2 - loss1) / (<span class="number">2</span>*e)</span><br><span class="line">        perturb[p, :] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> numgrad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">checkNNGradients</span>(<span class="params">lamda=<span class="number">0</span></span>):</span><br><span class="line">    <span class="comment"># CHECKNNGRADIENTS Creates a small neural network to check the</span></span><br><span class="line">    <span class="comment"># backpropagation gradients</span></span><br><span class="line">    <span class="comment">#   CHECKNNGRADIENTS(lamda) Creates a small neural network to check the</span></span><br><span class="line">    <span class="comment">#   backpropagation gradients, it will output the analytical gradients</span></span><br><span class="line">    <span class="comment">#   produced by your backprop code and the numerical gradients (computed</span></span><br><span class="line">    <span class="comment">#   using computeNumericalGradient). These two gradient computations should</span></span><br><span class="line">    <span class="comment">#   result in very similar values.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    input_layer_size = <span class="number">3</span></span><br><span class="line">    hidden_layer_size = <span class="number">5</span></span><br><span class="line">    num_labels = <span class="number">3</span></span><br><span class="line">    m = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># We generate some &#x27;random&#x27; test data</span></span><br><span class="line">    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)</span><br><span class="line">    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)</span><br><span class="line">    <span class="comment"># Reusing debugInitializeWeights to generate X</span></span><br><span class="line">    X = debugInitializeWeights(m, input_layer_size - <span class="number">1</span>)</span><br><span class="line">    y = <span class="number">1</span> + mod(arange(<span class="number">1</span>, m+<span class="number">1</span>), num_labels).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unroll parameters</span></span><br><span class="line">    nn_params = r_[Theta1.reshape(-<span class="number">1</span>, <span class="number">1</span>), Theta2.reshape(-<span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Short hand for cost function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">costFunc</span>(<span class="params">p</span>): <span class="keyword">return</span> nnCostFunction(p, input_layer_size,</span><br><span class="line">                                           hidden_layer_size,</span><br><span class="line">                                           num_labels, X, y, lamda)</span><br><span class="line">    <span class="comment"># @(p) nnCostFunction(p, input_layer_size, hidden_layer_size, ...</span></span><br><span class="line">    <span class="comment">#                     num_labels, X, y, lamda)</span></span><br><span class="line"></span><br><span class="line">    [cost, grad] = costFunc(nn_params)</span><br><span class="line">    numgrad = computeNumericalGradient(costFunc, nn_params)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Visually examine the two gradient computations.  The two columns</span></span><br><span class="line">    <span class="comment"># you get should be very similar.</span></span><br><span class="line">    <span class="built_in">print</span>(numgrad, <span class="string">&#x27;\n&#x27;</span>, grad)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;The above two columns you get should be very similar.\n&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;(Left-Your Numerical Gradient, Right-Analytical Gradient)&#x27;</span>, sep=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluate the norm of the difference between two solutions.</span></span><br><span class="line">    <span class="comment"># If you have a correct implementation, and assuming you used</span></span><br><span class="line">    <span class="comment"># EPSILON = 0.0001</span></span><br><span class="line">    <span class="comment"># in computeNumericalGradient.m, then diff below should be less than 1e-9</span></span><br><span class="line">    diff = norm(numgrad-grad)/norm(numgrad+grad)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;If your backpropagation implementation is correct, then \n&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;the relative difference will be small (less than 1e-9). \n&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;\nRelative Difference: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(diff), sep=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">Theta1: ndarray, Theta2: ndarray, X: ndarray</span>)-&gt;ndarray:</span><br><span class="line">    <span class="comment"># PREDICT Predict the label of an input given a trained neural network</span></span><br><span class="line">    <span class="comment">#   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given</span></span><br><span class="line">    <span class="comment">#   the trained weights of a neural network (Theta1, Theta2)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Useful values</span></span><br><span class="line">    m = size(X, <span class="number">0</span>)</span><br><span class="line">    num_labels = size(Theta2, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    h = expit(c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), expit(</span><br><span class="line">        (c_[ones((m, <span class="number">1</span>), <span class="built_in">float</span>), X] @ Theta1.T))] @ Theta2.T)</span><br><span class="line">    p = argmax(h, <span class="number">1</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========================================================================</span></span><br><span class="line">    <span class="keyword">return</span> p</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" rel="tag">吴恩达课程</a></li></ul></div><div class="post-nav"><a class="pre" href="/2018/12/09/wenda-week5/">机器学习作业第五周</a><a class="next" href="/2018/12/04/wenda-week3/">机器学习作业第三周</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/03/13/vllm/sglang_attn/">vllm/sglang_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/trt_attn/">vllm/trt_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/vllm_attn/">vllm/vllm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/tvm_attn/">vllm/tvm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/23/torch-trick/">Pytorch中遇到的一些问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>