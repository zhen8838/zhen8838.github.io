<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>半监督学习：MixMatch | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">半监督学习：MixMatch</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">半监督学习：MixMatch</h1><div class="post-meta">2020-02-03<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.5k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 11</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">算法理论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#mixmatch"><span class="toc-number">1.1.</span> <span class="toc-text">MixMatch</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="toc-number">3.</span> <span class="toc-text">测试结果</span></a></li></ol></div></div><div class="post-content"><p>第七个算法<code>MixMatch: A Holistic Approach toSemi-Supervised Learning</code>。此算法将之前的各个半监督学习算法进行融合，统一了主流方法，得到了最优的效果。此算法好，就是训练的过程慢一些。</p>
<span id="more"></span>
<h1 id="算法理论">算法理论</h1>
<p>半监督学习主要以未标记数据减轻对标记数据的要求，许多半监督学习方法都是添加根据无标签数据所产生的损失，从而使得模型更好的将未知标签数据分类。在最近的算法中，所添加的大部分损失属于以下三类之一，首先设置一个模型<span
class="math inline">\(\text{P}_{model}(y|x\theta)\)</span>，可以从通过参数<span
class="math inline">\(\theta\)</span>从输入<span
class="math inline">\(x\)</span>中得到类别<span
class="math inline">\(y\)</span>。</p>
<ol type="1">
<li><p>熵最小化</p>
<p>代表,Pseudo-label: The simple and efficient semi-supervised learning
method fordeep neural
networks)。做法是鼓励模型输出无标签数据的标签。</p>
<p>在许多半监督学习方法中，一个常见的基本假设是分类器的决策边界不应通过边缘数据分布的高密度区域。强制执行此操作的一种方法是要求分类器对未标记的数据输出低熵预测。如论文(Semi-supervised
learning by entropy minimization)使用这种这种损失函数使得<span
class="math inline">\(\text{P}_{model}(y|x\theta)\)</span>关于无标签数据<span
class="math inline">\(x\)</span>的熵最小，这种方法和<code>VAT</code>结合可以得到更强的效果。<code>pseudo label</code>通过对无标签数据的高置信度预测构造出伪标签进行训练，从而隐式的最小化熵。<code>MixMatch</code>通过在无标签数据的预测分布上使用<code>sharpening</code>函数，也隐式的最小化熵。</p></li>
<li><p>一致性正则</p>
<p>鼓励模型在其输入受到扰动时产生相同的输出分布。最简单的例子如下，对于无标签数据<span
class="math inline">\(x\)</span>： <span class="math display">\[
\begin{align}
\| \text { Pmodel }(y | \text { Augment }(x) ; \theta)-\text { Pmodel
}(y | \text { Augment }(x) ; \theta) \|_{2}^{2}
\end{align}\tag{1}
\]</span> 注意<span class="math inline">\(\text { Augment
}\)</span>是随机变化，所以公式1中的两项是不一样的。<code>mean teacher</code>通过代替公式1中的一项，使用模型参数的滑动平均进行模型输出，这提供了一个更稳定的输出分布，并发现了过去经验可以改善当前结果。这些方法的缺点是它们使用特定于域的数据增强策略。<code>VAT</code>则通过计算扰动来解决这个问题，将这个扰动添加到输入中，从而最大程度的改变输出类别的分布。<code>MixMatch</code>则通过对图像进行标准的数据增强来添加一致性正则。</p></li>
<li><p>一般正则化</p>
<p>传统正则算法鼓励模型得到更好的拟合与泛化效果。在<code>MixMatch</code>中对模型参数用<code>l2</code>正则，同时数据增强使用<code>mixup</code>。</p></li>
</ol>
<p><code>MixMatch</code>就是一种集合上以上三种方法的新方法。总之，<code>MixMatch</code>引入了针对无标签数据的统一的损失项，既可以减少熵又可以保持一致正则性，还与一般正则化方法兼容。</p>
<h2 id="mixmatch">MixMatch</h2>
<p>首先给出一系列符号。给定一个<code>batch</code>的标记数据<span
class="math inline">\(\mathcal{X}\)</span>以及<code>one-hot</code>标签，一个<code>batch</code>的无标签数据<span
class="math inline">\(\mathcal{U}\)</span>，通过数据增强得到<span
class="math inline">\(\mathcal{X}&#39;,\mathcal{U}&#39;\)</span>，然后对他们分别计算损失，最终整合所有损失：</p>
<p><span class="math display">\[
\begin{align}
\mathcal{X}^{\prime},
\mathcal{U}^{\prime}=\operatorname{MixMatch}(\mathcal{X}, \mathcal{U},
T, K, \alpha)
\end{align}\tag{2}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}_{\mathcal{X}}=\frac{1}{\left|\mathcal{X}^{\prime}\right|}
\sum_{x, p \in \mathcal{X}^{\prime}} \mathrm{H}\left(p,
\mathrm{p}_{\text {model }}(y | x ; \theta)\right)
\end{align}\tag{3}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}_{\mathcal{U}}=\frac{1}{L\left|\mathcal{U}^{\prime}\right|}
\sum_{u, q \in \mathcal{U}^{\prime}}\left\|q-\mathrm{p}_{\text {model
}}(y | u ; \theta)\right\|_{2}^{2}
\end{align}\tag{4}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}=\mathcal{L}_{\mathcal{X}}+\lambda_{\mathcal{U}}
\mathcal{L}_{\mathcal{U}}
\end{align}\tag{5}
\]</span></p>
<p>其中<span class="math inline">\(H(p,q)\)</span>是分布<span
class="math inline">\(p\)</span>和<span
class="math inline">\(q\)</span>间的交叉熵，<span
class="math inline">\(T,K,\alpha,\lambda_{\mathcal{U}}\)</span>是超参数。完整的<code>MixMatch</code>如算法1所示。</p>
<p><img src="/2020/02/03/ssl-mixmatch/mixmatch-1.png" /></p>
<p>现在来描述各个部分：</p>
<ol type="1">
<li><p>数据增强</p>
<p>对于一个<code>batch</code><span
class="math inline">\(\mathcal{X}\)</span>中的每一个<span
class="math inline">\(x_b\)</span>，通过变化得到<span
class="math inline">\(\hat{x}_{b}=\text { Augment
}\left(x_{b}\right)\)</span>(算法1第3行)。对于每个无标签数据<span
class="math inline">\(u_b\)</span>，我们生成<span
class="math inline">\(K\)</span>个增强<span
class="math inline">\(\hat{u}_{b, k}=\text { Augment
}\left(u_{b}\right), k \in(1, \ldots,
K)\)</span>(算法1第5行)。再使用每个<span
class="math inline">\(u_b\)</span>送入模型得到对应的<code>猜测标签</code><span
class="math inline">\(q_b\)</span>。</p></li>
<li><p>标签猜测</p>
<p>有了<code>猜测标签</code>，我们将它用在无监督损失中，平均对<span
class="math inline">\(u_b\)</span>做<span
class="math inline">\(K\)</span>个增强的模型预测输出分布：</p>
<p><span class="math display">\[
\begin{align}
\bar{q}_{b}=\frac{1}{K} \sum_{k=1}^{K} \operatorname{Prodel}\left(y |
\hat{u}_{b, k} ; \theta\right)
\end{align}\tag{6}
\]</span></p>
<p><strong>sharpening</strong>：
为了达到对熵最小化的目的，我们需要对给定数据增强预测的平均值<span
class="math inline">\(\bar{q}_{b}\)</span>进行<code>sharpening</code>，通过<code>sharpening</code>函数减小标签的分布熵。在代码中，是调整分类分布的<code>温度</code>系数：</p>
<p><span class="math display">\[
\begin{align}
\text { Sharpen }(p, T)_{i}:=p_{i}^{\frac{1}{T}} / \sum_{j=1}^{L}
p_{j}^{\frac{1}{T}}
\end{align}\tag{7}
\]</span></p>
<p>其中<span
class="math inline">\(p\)</span>是一些输入分类分布(在此算法中为<span
class="math inline">\(\bar{q}_{b}\)</span>)，<span
class="math inline">\(T\)</span>是超参数。当<span
class="math inline">\(T\rightarrow0\)</span>，<span
class="math inline">\(\text{Sharpen}(p,T)\)</span>的输出会趋近于<code>Dirac(one-hot)分布</code>，降低温度系数会鼓励模型产生较低熵的预测。</p></li>
<li><p>mixup</p>
<p>要应用<code>mixup</code>，我们首先需要将所有的带标签的增强数据和所有无标签样本以及对应的猜测标签收集起来(算法1第10-11行):
<span class="math display">\[
\begin{align}
\hat{\mathcal{X}}=\left(\left(\hat{x}_{b}, p_{b}\right) ; b \in(1,
\ldots, B)\right)
\end{align}\tag{12}
\]</span> <span class="math display">\[
\begin{align}
\hat{\mathcal{U}}=\left(\left(\hat{u}_{b, k}, q_{b}\right) ; b \in(1,
\ldots, B), k \in(1, \ldots, K)\right)
\end{align}\tag{13}
\]</span></p>
<p>然后我们联合以上分布并进行混洗得到新的数据集<span
class="math inline">\(\mathcal{W}\)</span>作为<code>mixup</code>的输入，对每第<span
class="math inline">\(i\)</span>个样本对<span
class="math inline">\(\hat{\mathcal{X}}\)</span>，我们计算<span
class="math inline">\(\operatorname{MixUp}\left(\hat{\mathcal{X}}_{i},
\mathcal{W}_{i}\right)\)</span>并将结果添加到<span
class="math inline">\(\mathcal{X}&#39;\)</span>(算法1第13行)，对于<span
class="math inline">\(i\in(1,\ldots,|\bar{\mathcal{U}}|)\)</span>我们计算<span
class="math inline">\(\mathcal{U}_{i}^{\prime}=\operatorname{MixUp}\left(\hat{\mathcal{U}}_{i},
\mathcal{W}_{i+|\hat{\mathcal{X}}|}\right)\)</span> for <span
class="math inline">\(i \in(1,
\ldots,|\hat{\mathcal{U}}|)\)</span>。在这个过程中，带标签数据可能会和无标签数据产生混合。</p></li>
<li><p>损失函数</p>
<p>损失即标签数据的交叉熵结合无标签数据的差异性损失。</p></li>
<li><p>超参数</p>
<p>因为<code>MixMatch</code>结合了很多算法，所以超参数也特别的多，一般固定<span
class="math inline">\(T=0.5，K=2\)</span>，然后<span
class="math inline">\(\alpha=0.75,\lambda_{\mathcal{U}}=100\)</span></p></li>
</ol>
<p>消融测试结果：</p>
<p><img src="/2020/02/03/ssl-mixmatch/mixmatch-2.png" /></p>
<p>可以发现关键提升点在于<code>锐化</code>以及无标签数据间的<code>mixup</code></p>
<h1 id="代码">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">distribution_summary</span>(<span class="params">self, p_data, p_model, p_target=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kl</span>(<span class="params">p, q</span>):</span><br><span class="line">        p /= tf.reduce_sum(p)</span><br><span class="line">        q /= tf.reduce_sum(q)</span><br><span class="line">        <span class="keyword">return</span> -tf.reduce_sum(p * tf.log(q / p))</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;metrics/kld&#x27;</span>, kl(p_data, p_model))</span><br><span class="line">    <span class="keyword">if</span> p_target <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;metrics/kld_target&#x27;</span>, kl(p_data, p_target))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.nclass):</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;matching/class%d_ratio&#x27;</span> % i, p_model[i] / p_data[i])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.nclass):</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;matching/val%d&#x27;</span> % i, p_model[i])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">augment</span>(<span class="params">self, x, l, beta, **kwargs</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span>, <span class="string">&#x27;Do not call.&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">guess_label</span>(<span class="params">self, y, classifier, T, **kwargs</span>):</span><br><span class="line">    <span class="keyword">del</span> kwargs</span><br><span class="line">    logits_y = [classifier(yi, training=<span class="literal">True</span>) <span class="keyword">for</span> yi <span class="keyword">in</span> y]</span><br><span class="line">    logits_y = tf.concat(logits_y, <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Compute predicted probability distribution py.</span></span><br><span class="line">    <span class="comment"># p_model_y shape = [K,batch,calss_num]</span></span><br><span class="line">    p_model_y = tf.reshape(tf.nn.softmax(logits_y), [<span class="built_in">len</span>(y), -<span class="number">1</span>, self.nclass])</span><br><span class="line">    <span class="comment"># 求均值</span></span><br><span class="line">    p_model_y = tf.reduce_mean(p_model_y, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 锐化</span></span><br><span class="line">    p_target = tf.<span class="built_in">pow</span>(p_model_y, <span class="number">1.</span> / T)</span><br><span class="line">    p_target /= tf.reduce_sum(p_target, axis=<span class="number">1</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> EasyDict(p_target=p_target, p_model=p_model_y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">self, batch, lr, wd, ema, beta, w_match, warmup_kimg=<span class="number">1024</span>, nu=<span class="number">2</span>, mixmode=<span class="string">&#x27;xxy.yxy&#x27;</span>, dbuf=<span class="number">128</span>, **kwargs</span>):</span><br><span class="line">    hwc = [self.dataset.height, self.dataset.width, self.dataset.colors]</span><br><span class="line">    xt_in = tf.placeholder(tf.float32, [batch] + hwc, <span class="string">&#x27;xt&#x27;</span>)  <span class="comment"># For training</span></span><br><span class="line">    x_in = tf.placeholder(tf.float32, [<span class="literal">None</span>] + hwc, <span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    y_in = tf.placeholder(tf.float32, [batch, nu] + hwc, <span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    l_in = tf.placeholder(tf.int32, [batch], <span class="string">&#x27;labels&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    w_match *= tf.clip_by_value(tf.cast(self.step, tf.float32) / (warmup_kimg &lt;&lt; <span class="number">10</span>), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    lrate = tf.clip_by_value(tf.to_float(self.step) / (FLAGS.train_kimg &lt;&lt; <span class="number">10</span>), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    lr *= tf.cos(lrate * (<span class="number">7</span> * np.pi) / (<span class="number">2</span> * <span class="number">8</span>))</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;monitors/lr&#x27;</span>, lr)</span><br><span class="line">    <span class="comment"># 设置mixup的模式，默认是标记数据会与(标记数据，无标记数据)混合，无标记数据会与(标记数据，无标记数据)混合</span></span><br><span class="line">    augment = MixMode(mixmode)</span><br><span class="line">    classifier = <span class="keyword">lambda</span> x, **kw: self.classifier(x, **kw, **kwargs).logits</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Moving average of the current estimated label distribution</span></span><br><span class="line">    p_model = layers.PMovingAverage(<span class="string">&#x27;p_model&#x27;</span>, self.nclass, dbuf)</span><br><span class="line">    p_target = layers.PMovingAverage(<span class="string">&#x27;p_target&#x27;</span>, self.nclass, dbuf)  <span class="comment"># Rectified distribution (only for plotting)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Known (or inferred) true unlabeled distribution</span></span><br><span class="line">    p_data = layers.PData(self.dataset)</span><br><span class="line">    <span class="comment"># K个增强就有K个无标签输入</span></span><br><span class="line">    y = tf.reshape(tf.transpose(y_in, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]), [-<span class="number">1</span>] + hwc)</span><br><span class="line">    <span class="comment"># 得到锐化后的猜测标签以及原始猜测标签</span></span><br><span class="line">    guess = self.guess_label(tf.split(y, nu), classifier, T=<span class="number">0.5</span>, **kwargs)</span><br><span class="line">    ly = tf.stop_gradient(guess.p_target) <span class="comment"># 取消梯度</span></span><br><span class="line">    lx = tf.one_hot(l_in, self.nclass)</span><br><span class="line">    <span class="comment"># 对于集合进行mixup</span></span><br><span class="line">    xy, labels_xy = augment([xt_in] + tf.split(y, nu), [lx] + [ly] * nu, [beta, beta])</span><br><span class="line">    x, y = xy[<span class="number">0</span>], xy[<span class="number">1</span>:]</span><br><span class="line">    labels_x, labels_y = labels_xy[<span class="number">0</span>], tf.concat(labels_xy[<span class="number">1</span>:], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">del</span> xy, labels_xy</span><br><span class="line">    <span class="comment"># 只从W中选取一个batch的数据做loss</span></span><br><span class="line">    batches = layers.interleave([x] + y, batch)</span><br><span class="line">    skip_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">    logits = [classifier(batches[<span class="number">0</span>], training=<span class="literal">True</span>)]</span><br><span class="line">    post_ops = [v <span class="keyword">for</span> v <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.UPDATE_OPS) <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> skip_ops]</span><br><span class="line">    <span class="keyword">for</span> batchi <span class="keyword">in</span> batches[<span class="number">1</span>:]:</span><br><span class="line">        logits.append(classifier(batchi, training=<span class="literal">True</span>))</span><br><span class="line">    logits = layers.interleave(logits, batch)</span><br><span class="line">    logits_x = logits[<span class="number">0</span>]</span><br><span class="line">    logits_y = tf.concat(logits[<span class="number">1</span>:], <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 交叉熵</span></span><br><span class="line">    loss_xe = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_x, logits=logits_x)</span><br><span class="line">    loss_xe = tf.reduce_mean(loss_xe)</span><br><span class="line">    <span class="comment"># 一致正则熵</span></span><br><span class="line">    loss_l2u = tf.square(labels_y - tf.nn.softmax(logits_y))</span><br><span class="line">    loss_l2u = tf.reduce_mean(loss_l2u)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/xe&#x27;</span>, loss_xe)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/l2u&#x27;</span>, loss_l2u)</span><br><span class="line">    self.distribution_summary(p_data(), p_model(), p_target())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># L2 regularization</span></span><br><span class="line">    loss_wd = <span class="built_in">sum</span>(tf.nn.l2_loss(v) <span class="keyword">for</span> v <span class="keyword">in</span> utils.model_vars(<span class="string">&#x27;classify&#x27;</span>) <span class="keyword">if</span> <span class="string">&#x27;kernel&#x27;</span> <span class="keyword">in</span> v.name)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/wd&#x27;</span>, loss_wd)</span><br><span class="line"></span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(decay=ema)</span><br><span class="line">    ema_op = ema.apply(utils.model_vars())</span><br><span class="line">    ema_getter = functools.partial(utils.getter_ema, ema)</span><br><span class="line">    post_ops.append(ema_op)</span><br><span class="line"></span><br><span class="line">    train_op = tf.train.MomentumOptimizer(lr, <span class="number">0.9</span>, use_nesterov=<span class="literal">True</span>).minimize(</span><br><span class="line">        loss_xe + w_match * loss_l2u + wd * loss_wd, colocate_gradients_with_ops=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_op]):</span><br><span class="line">        train_op = tf.group(*post_ops)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> EasyDict(</span><br><span class="line">        xt=xt_in, x=x_in, y=y_in, label=l_in, train_op=train_op,</span><br><span class="line">        classify_raw=tf.nn.softmax(classifier(x_in, training=<span class="literal">False</span>)),  <span class="comment"># No EMA, for debugging.</span></span><br><span class="line">        classify_op=tf.nn.softmax(classifier(x_in, getter=ema_getter, training=<span class="literal">False</span>)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="测试结果">测试结果</h1>
<p>使用默认参数以及cifar10中250张标注样本训练128个epoch，得到测试集准确率如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;last01&quot;: 74.08999633789062,</span><br><span class="line">&quot;last10&quot;: 74.16499710083008,</span><br><span class="line">&quot;last20&quot;: 73.82500076293945,</span><br><span class="line">&quot;last50&quot;: 72.84500122070312</span><br></pre></td></tr></table></figure>
<p>的确是超越之前算法太多了，就是训练时期的速度相对慢三倍。</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">半监督学习</a></li></ul></div><div class="post-nav"><a class="pre" href="/2020/02/04/ssl-uda/">半监督学习：Unsupervised Data Augmentation</a><a class="next" href="/2020/02/02/ssl-ict/">半监督学习：Interpolation Consistency Training</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a> <a href="/tags/%E7%AE%97%E5%AD%90/" style="font-size: 15px;">算子</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/10/15/jax-reshard/">探究jax reshard优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/09/26/flashattn/">Flash Attention记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>