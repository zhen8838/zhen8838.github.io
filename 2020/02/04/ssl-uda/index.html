<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>半监督学习：Unsupervised Data Augmentation | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">半监督学习：Unsupervised Data Augmentation</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">半监督学习：Unsupervised Data Augmentation</h1><div class="post-meta">2020-02-04<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.6k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 8</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">算法理论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#unsupervised-data-augmentation-uda"><span class="toc-number">1.1.</span> <span class="toc-text">Unsupervised Data
Augmentation (UDA)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8E%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F%E8%AE%AD%E7%BB%83%E9%80%80%E7%81%AB"><span class="toc-number">1.2.</span> <span class="toc-text">低数据区域训练退火</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="toc-number">3.</span> <span class="toc-text">测试结果</span></a></li></ol></div></div><div class="post-content"><p>第八个算法<code>UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING</code>。此算法与<code>VAT</code>的想法类似，都是通过加强扰动的质量来获得更好的一致性正则化。</p>
<span id="more"></span>
<h1 id="算法理论">算法理论</h1>
<p>我们研究了扰动添加在一致性训练中的作用，并发现先进的数据增强方法不仅在监督学习中效果好，在非监督学习中表现也十分出色，因此提出用一系列高质量的数据增强方法加强一致性训练，此方法命名为<code>UDA</code>。</p>
<p>主要贡献为如下：</p>
<ol type="1">
<li>我们证明了有监督学习中发现的最新数据增强也可以作为半监督学习的噪声来源。</li>
<li><code>UDA</code>可以匹配或超过更多数量级的纯有监督学习</li>
<li><code>UDA</code>还可以和<code>bert</code>结合</li>
</ol>
<h2 id="unsupervised-data-augmentation-uda">Unsupervised Data
Augmentation (UDA)</h2>
<p>半监督一致性训练流程如下：</p>
<p><img src="/2020/02/04/ssl-uda/uda-1.png" /></p>
<p>对于图像数据，<code>UDA</code>使用的是<code>RandAugment</code>方法，来自于<code>Randaugment:  Practical data aug-mentation with no separate search</code>。</p>
<p><img src="/2020/02/04/ssl-uda/uda-2.png" /></p>
<h2 id="低数据区域训练退火">低数据区域训练退火</h2>
<p>在半监督学习中，经常遇到一种情况，即未标记数据量和已标记数据量存在巨大差距。然后模型通常会快速拟合有限的已标记数据，而不足以拟合未标记数据。为了解决这个问题，提出了一种训练信号退火<code>TSA</code>的方法，随着训练会逐渐释放带标签的样本的信号，就是在模型对于样本的置信度低于阈值<span
class="math inline">\(\eta_{t}\)</span>的时候情况下，才使用带标签的样本，如果模型对于正确类别的预测概率高于阈值，那么要从损失函数中删除这个样本，这就是为了防止出现过度训练。</p>
<p>对于<span
class="math inline">\(\eta_{t}\)</span>的变化曲线，设计了三种模式，对于标签训练数据较小时容易过拟合可以使用<code>exp</code>曲线，对于标签训练数据较多时可以使用<code>log</code>曲线。</p>
<p><img src="/2020/02/04/ssl-uda/uda-3.png" /></p>
<h1 id="代码">代码</h1>
<p>这个文章的论文里面讲的理论太少了，但是代码量实际不少。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TSA_MODES = <span class="string">&#x27;no exp linear log&#x27;</span>.split()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tsa_threshold</span>(<span class="params">self, tsa, scale=<span class="number">5</span>, tsa_pos=<span class="number">10</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 训练信号退火阈值计算函数 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">del</span> kwargs</span><br><span class="line">    <span class="comment"># step ratio will be maxed at (2 ** 14) * (2 ** 10) ~ 16.8M updates</span></span><br><span class="line">    step_ratio = tf.to_float(self.step) / tf.to_float(<span class="built_in">min</span>(FLAGS.train_kimg, <span class="number">1</span> &lt;&lt; <span class="number">14</span>) &lt;&lt; tsa_pos)</span><br><span class="line">    <span class="keyword">if</span> tsa == <span class="string">&#x27;linear&#x27;</span>:</span><br><span class="line">        coeff = step_ratio</span><br><span class="line">    <span class="keyword">elif</span> tsa == <span class="string">&#x27;exp&#x27;</span>:  <span class="comment"># [exp(-5), exp(0)] = [1e-2, 1]</span></span><br><span class="line">        coeff = tf.exp((step_ratio - <span class="number">1</span>) * scale)</span><br><span class="line">    <span class="keyword">elif</span> tsa == <span class="string">&#x27;log&#x27;</span>:  <span class="comment"># [1 - exp(0), 1 - exp(-5)] = [0, 0.99]</span></span><br><span class="line">        coeff = <span class="number">1</span> - tf.exp((-step_ratio) * scale)</span><br><span class="line">    <span class="keyword">elif</span> tsa == <span class="string">&#x27;no&#x27;</span>:</span><br><span class="line">        coeff = tf.to_float(<span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> tsa != <span class="string">&#x27;no&#x27;</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(tsa)</span><br><span class="line">    coeff = tf.math.minimum(coeff, <span class="number">1.0</span>)  <span class="comment"># bound the coefficient</span></span><br><span class="line">    p_min = <span class="number">1.</span> / self.nclass</span><br><span class="line">    <span class="keyword">return</span> coeff * (<span class="number">1</span> - p_min) + p_min</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tsa_loss_mask</span>(<span class="params">self, tsa, logits, labels, tsa_pos, **kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 滤置信度高于训练信号退火阈值的对应样本损失 &quot;&quot;&quot;</span></span><br><span class="line">    thresh = self.tsa_threshold(tsa, tsa_pos=tsa_pos, **kwargs)</span><br><span class="line">    p_class = tf.nn.softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    p_correct = tf.reduce_sum(labels * p_class, axis=-<span class="number">1</span>)</span><br><span class="line">    loss_mask = tf.cast(p_correct &lt;= thresh, tf.float32)  <span class="comment"># Ignore confident predictions.</span></span><br><span class="line">    <span class="keyword">return</span> tf.stop_gradient(loss_mask)</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">confidence_based_masking</span>(<span class="params">logits, p_class=<span class="literal">None</span>, thresh=<span class="number">0.9</span></span>):</span><br><span class="line">    <span class="keyword">if</span> logits <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_class = tf.nn.softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    p_class_max = tf.reduce_max(p_class, axis=-<span class="number">1</span>)</span><br><span class="line">    loss_mask = tf.cast(p_class_max &gt;= thresh, tf.float32)  <span class="comment"># Ignore unconfident predictions.</span></span><br><span class="line">    <span class="keyword">return</span> tf.stop_gradient(loss_mask)</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_temperature_controlling</span>(<span class="params">logits, T</span>):</span><br><span class="line">    <span class="comment"># this is essentially the same as sharpening in mixmatch</span></span><br><span class="line">    logits = logits / T</span><br><span class="line">    <span class="keyword">return</span> tf.stop_gradient(logits)</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kl_divergence_from_logits</span>(<span class="params">p_logits, q_logits</span>):</span><br><span class="line">    p = tf.nn.softmax(p_logits)</span><br><span class="line">    log_p = tf.nn.log_softmax(p_logits)</span><br><span class="line">    log_q = tf.nn.log_softmax(q_logits)</span><br><span class="line">    kl = tf.reduce_sum(p * (log_p - log_q), -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> kl</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy_from_logits</span>(<span class="params">logits</span>):</span><br><span class="line">    log_prob = tf.nn.log_softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    prob = tf.exp(log_prob)</span><br><span class="line">    ent = tf.reduce_sum(-prob * log_prob, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, train_nimg, report_nimg</span>):</span><br><span class="line">    <span class="keyword">if</span> FLAGS.eval_ckpt:</span><br><span class="line">        self.eval_checkpoint(FLAGS.eval_ckpt)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    batch = FLAGS.batch</span><br><span class="line">    train_labeled = self.dataset.train_labeled.repeat().shuffle(FLAGS.shuffle).parse().augment()</span><br><span class="line">    train_labeled = train_labeled.batch(batch).prefetch(<span class="number">16</span>).make_one_shot_iterator().get_next()</span><br><span class="line">    train_unlabeled = self.dataset.train_unlabeled.repeat().shuffle(FLAGS.shuffle).parse().augment()</span><br><span class="line">    train_unlabeled = train_unlabeled.batch(batch * self.params[<span class="string">&#x27;uratio&#x27;</span>]).prefetch(<span class="number">16</span>)</span><br><span class="line">    train_unlabeled = train_unlabeled.make_one_shot_iterator().get_next()</span><br><span class="line">    scaffold = tf.train.Scaffold(saver=tf.train.Saver(max_to_keep=FLAGS.keep_ckpt,</span><br><span class="line">                                                        pad_step_number=<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session(config=utils.get_config()) <span class="keyword">as</span> sess:</span><br><span class="line">        self.session = sess</span><br><span class="line">        self.cache_eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.train.MonitoredTrainingSession(</span><br><span class="line">            scaffold=scaffold,</span><br><span class="line">            checkpoint_dir=self.checkpoint_dir,</span><br><span class="line">            config=utils.get_config(),</span><br><span class="line">            save_checkpoint_steps=FLAGS.save_kimg &lt;&lt; <span class="number">10</span>,</span><br><span class="line">            save_summaries_steps=report_nimg - batch) <span class="keyword">as</span> train_session:</span><br><span class="line">        self.session = train_session._tf_sess()</span><br><span class="line">        gen_labeled = self.gen_labeled_fn(train_labeled)</span><br><span class="line">        gen_unlabeled = self.gen_unlabeled_fn(train_unlabeled)</span><br><span class="line">        self.tmp.step = self.session.run(self.step)</span><br><span class="line">        <span class="keyword">while</span> self.tmp.step &lt; train_nimg:</span><br><span class="line">            loop = trange(self.tmp.step % report_nimg, report_nimg, batch,</span><br><span class="line">                            leave=<span class="literal">False</span>, unit=<span class="string">&#x27;img&#x27;</span>, unit_scale=batch,</span><br><span class="line">                            desc=<span class="string">&#x27;Epoch %d/%d&#x27;</span> % (<span class="number">1</span> + (self.tmp.step // report_nimg), train_nimg // report_nimg))</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> loop:</span><br><span class="line">                self.train_step(train_session, gen_labeled, gen_unlabeled)</span><br><span class="line">                <span class="keyword">while</span> self.tmp.print_queue:</span><br><span class="line">                    loop.write(self.tmp.print_queue.pop(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">while</span> self.tmp.print_queue:</span><br><span class="line">            <span class="built_in">print</span>(self.tmp.print_queue.pop(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">self, batch, lr, wd, wu, we, confidence, uratio,</span></span><br><span class="line"><span class="params">            temperature=<span class="number">1.0</span>, tsa=<span class="string">&#x27;no&#x27;</span>, tsa_pos=<span class="number">10</span>, ema=<span class="number">0.999</span>, **kwargs</span>):</span><br><span class="line">    hwc = [self.dataset.height, self.dataset.width, self.dataset.colors]</span><br><span class="line">    xt_in = tf.placeholder(tf.float32, [batch] + hwc, <span class="string">&#x27;xt&#x27;</span>)  <span class="comment"># For training</span></span><br><span class="line">    x_in = tf.placeholder(tf.float32, [<span class="literal">None</span>] + hwc, <span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    y_in = tf.placeholder(tf.float32, [batch * uratio, <span class="number">2</span>] + hwc, <span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    l_in = tf.placeholder(tf.int32, [batch], <span class="string">&#x27;labels&#x27;</span>)</span><br><span class="line">    l = tf.one_hot(l_in, self.nclass)</span><br><span class="line"></span><br><span class="line">    lrate = tf.clip_by_value(tf.to_float(self.step) / (FLAGS.train_kimg &lt;&lt; <span class="number">10</span>), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    lr *= tf.cos(lrate * (<span class="number">7</span> * np.pi) / (<span class="number">2</span> * <span class="number">8</span>))</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;monitors/lr&#x27;</span>, lr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute logits for xt_in and y_in</span></span><br><span class="line">    classifier = <span class="keyword">lambda</span> x, **kw: self.classifier(x, **kw, **kwargs).logits</span><br><span class="line">    skip_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">    <span class="comment"># 这个是多gpu平行前向传播</span></span><br><span class="line">    logits = utils.para_cat(<span class="keyword">lambda</span> x: classifier(x, training=<span class="literal">True</span>), tf.concat([xt_in, y_in[:, <span class="number">0</span>], y_in[:, <span class="number">1</span>]], <span class="number">0</span>))</span><br><span class="line">    post_ops = [v <span class="keyword">for</span> v <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.UPDATE_OPS) <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> skip_ops]</span><br><span class="line">    <span class="comment"># 分离出前向传播的结果</span></span><br><span class="line">    logits_x = logits[:batch]</span><br><span class="line">    logits_weak, logits_strong = tf.split(logits[batch:], <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">del</span> logits, skip_ops</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 锐化 softmax temperature control，和mixmatch中的锐化基本相同</span></span><br><span class="line">    logits_weak_tgt = self.softmax_temperature_controlling(logits_weak, T=temperature)</span><br><span class="line">    <span class="comment"># 对于锐化后的无监督信号也根据置信度mask</span></span><br><span class="line">    pseudo_labels = tf.stop_gradient(tf.nn.softmax(logits_weak))</span><br><span class="line">    pseudo_mask = self.confidence_based_masking(logits=<span class="literal">None</span>, p_class=pseudo_labels, thresh=confidence)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;monitors/mask&#x27;</span>, tf.reduce_mean(pseudo_mask))</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;monitors/conf_weak&#x27;</span>, tf.reduce_mean(tf.reduce_max(tf.nn.softmax(logits_weak), axis=<span class="number">1</span>)))</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;monitors/conf_strong&#x27;</span>, tf.reduce_mean(tf.reduce_max(tf.nn.softmax(logits_strong), axis=<span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 锐化后的分布与未锐化的分布计算一致性损失</span></span><br><span class="line">    kld = self.kl_divergence_from_logits(logits_weak_tgt, logits_strong)</span><br><span class="line">    <span class="comment"># 计算logits_weak的熵</span></span><br><span class="line">    entropy = self.entropy_from_logits(logits_weak)</span><br><span class="line">    <span class="comment"># 对一致性损失进行mask</span></span><br><span class="line">    loss_xeu = tf.reduce_mean(kld * pseudo_mask)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/xeu&#x27;</span>, loss_xeu)</span><br><span class="line">    loss_ent = tf.reduce_mean(entropy)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/entropy&#x27;</span>, loss_ent)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对于监督学习部分使用tsa进行mask</span></span><br><span class="line">    loss_mask = self.tsa_loss_mask(tsa=tsa, logits=logits_x, labels=l, tsa_pos=tsa_pos)</span><br><span class="line">    loss_xe = tf.nn.softmax_cross_entropy_with_logits_v2(labels=l, logits=logits_x)</span><br><span class="line">    loss_xe = tf.reduce_sum(loss_xe * loss_mask) / tf.math.maximum(tf.reduce_sum(loss_mask), <span class="number">1.0</span>)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/xe&#x27;</span>, loss_xe)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/mask_sup&#x27;</span>, tf.reduce_mean(loss_mask))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># L2 regularization</span></span><br><span class="line">    loss_wd = <span class="built_in">sum</span>(tf.nn.l2_loss(v) <span class="keyword">for</span> v <span class="keyword">in</span> utils.model_vars(<span class="string">&#x27;classify&#x27;</span>) <span class="keyword">if</span> <span class="string">&#x27;kernel&#x27;</span> <span class="keyword">in</span> v.name)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;losses/wd&#x27;</span>, loss_wd)</span><br><span class="line"></span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(decay=ema)</span><br><span class="line">    ema_op = ema.apply(utils.model_vars())</span><br><span class="line">    ema_getter = functools.partial(utils.getter_ema, ema)</span><br><span class="line">    post_ops.append(ema_op)</span><br><span class="line"></span><br><span class="line">    train_op = tf.train.MomentumOptimizer(lr, <span class="number">0.9</span>, use_nesterov=<span class="literal">True</span>).minimize(</span><br><span class="line">        loss_xe + loss_xeu * wu + loss_ent * we + loss_wd * wd, colocate_gradients_with_ops=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_op]):</span><br><span class="line">        train_op = tf.group(*post_ops)</span><br></pre></td></tr></table></figure>
<h1 id="测试结果">测试结果</h1>
<p>使用默认参数以及cifar10中250张标注样本训练128个epoch，得到测试集准确率如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;last01&quot;: 83.94999694824219,</span><br><span class="line">&quot;last10&quot;: 83.66000366210938,</span><br><span class="line">&quot;last20&quot;: 83.5,</span><br><span class="line">&quot;last50&quot;: 82.52000045776367</span><br></pre></td></tr></table></figure>
<p>这个方法速度上比相对比较快，并且效果相比与<code>mixmatch</code>还要好，下图是每个<code>step</code>的耗时对比：</p>
<p><img src="/2020/02/04/ssl-uda/uda-4.png" /></p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">半监督学习</a></li></ul></div><div class="post-nav"><a class="pre" href="/2020/02/05/ssl-remixmatch/">半监督学习：ReMixMatch</a><a class="next" href="/2020/02/03/ssl-mixmatch/">半监督学习：MixMatch</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/03/13/vllm/sglang_attn/">vllm/sglang_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/trt_attn/">vllm/trt_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/vllm_attn/">vllm/vllm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/vllm/tvm_attn/">vllm/tvm_attn</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/23/torch-trick/">Pytorch中遇到的一些问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>