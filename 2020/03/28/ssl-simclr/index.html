<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>半监督学习：SimCLR | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">半监督学习：SimCLR</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">半监督学习：SimCLR</h1><div class="post-meta">2020-03-28<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.7k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 7</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p><code>SimCLR</code>实际上是<code>Geoffrey Hinton</code>和谷歌合作的论文<code>A Simple Framework for Contrastive Learning of Visual Representations</code>，严格来说他是一个自监督算法，不过我这里也把他归入半监督中了，他实际上是先无监督预训练然后进行监督微调的。</p>
<span id="more"></span>
<h1 id="算法理论">算法理论</h1>
<p><code>SimCLR</code>实际上是提出了一个简单的表征一致性学习框架。我觉得他的想法能<code>work</code>主要靠下面三点：</p>
<ol type="1">
<li>有效的数据增强策略</li>
<li>隐含层表征一致性损失约束</li>
<li>超大<code>batchsize</code></li>
</ol>
<p>总体框架如下：</p>
<p><img src="/2020/03/28/ssl-simclr/simclr-1.png" /></p>
<p>其中给定一个无标签样本<span
class="math inline">\(x\)</span>，从数据增强策略中采样两个数据增强操作<span
class="math inline">\(t,t&#39; \sim
\mathcal{T}\)</span>，分别应用到<span
class="math inline">\(x\)</span>得到<span
class="math inline">\(\hat{x}_i,\hat{x}_j\)</span>。论文中使用<code>res50</code>作为模型骨干<span
class="math inline">\(f(\cdot)\)</span>，得到中间表征<span
class="math inline">\(\boldsymbol{h}_i,\boldsymbol{h}_j\)</span>，接着论文指出在得到了中间表征不要直接用，再加个非线性投影头(nonlinear
projection head)<span
class="math inline">\(g(\cdot)\)</span>更好,其实这个非线性投影头就是两个全连接层。下一步通过投影头得到了投影表征<span
class="math inline">\(\boldsymbol{z}_i,\boldsymbol{z}_j\)</span>，最后对投影表征计算对比损失(contrastive
loss)，我更愿意称为一致性损失。</p>
<p>对比损失(contrastive loss)定义如下：</p>
<p><span class="math display">\[
\begin{aligned}
  \text{Let}
\text{sim}(\boldsymbol{u},\boldsymbol{v})&amp;=\frac{\boldsymbol{u}^T\boldsymbol{v}}{\parallel\boldsymbol{u}\parallel\parallel\boldsymbol{v}\parallel}\\
  \mathcal{l}_{i,j}&amp;=-\log
\frac{\exp(\text{sim}(\frac{\boldsymbol{z}_i,\boldsymbol{z}_j}{\tau}))}{\sum_{k=1}^{2N}\mathcal{1}_{[k\neq
i]}\exp(\text{sim}(\frac{\boldsymbol{z}_i,\boldsymbol{z}_k}{\tau}))}
\end{aligned}
\]</span></p>
<p>看起来和交叉熵不一样，但就是交叉熵。。。他的定义是是两个对应位置的图像投影表征內积应该越大越好，其他位置应该越小越好。下面代码部分我会讲的更详细些。</p>
<p>总算法流程图：</p>
<p><img src="/2020/03/28/ssl-simclr/simclr-2.png" /></p>
<h1 id="代码">代码</h1>
<p>因为他这个代码跑起来就得要<code>imagenet</code>，我懒得弄了，所以只看了最重要的损失部分的代码，不过还是按流程来讲。</p>
<h2 id="数据输入">数据输入</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">map_fn</span>(<span class="params">image, label</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Produces multiple transformations of the same batch.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> FLAGS.train_mode == <span class="string">&#x27;pretrain&#x27;</span>:</span><br><span class="line">    xs = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># Two transformations</span></span><br><span class="line">      <span class="comment"># 预训练的时候是同一张图像</span></span><br><span class="line">      xs.append(preprocess_fn_pretrain(image))</span><br><span class="line">    image = tf.concat(xs, -<span class="number">1</span>) <span class="comment"># [h,w,2*c]</span></span><br><span class="line">    label = tf.zeros([num_classes])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    image = preprocess_fn_finetune(image)</span><br><span class="line">    label = tf.one_hot(label, num_classes)</span><br><span class="line">  <span class="keyword">return</span> image, label, <span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<p>在无监督预训练的时候对于同一张图像执行两次数据增强，然后<code>concat</code>为<code>[h,w,2*c]</code>的图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Split channels, and optionally apply extra batched augmentation.</span></span><br><span class="line"><span class="comment"># 前面变成了[h,w,2*c]，这里再拆分出来</span></span><br><span class="line">features_list = tf.split(</span><br><span class="line">    features, num_or_size_splits=num_transforms, axis=-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> FLAGS.use_blur <span class="keyword">and</span> is_training <span class="keyword">and</span> FLAGS.train_mode == <span class="string">&#x27;pretrain&#x27;</span>:</span><br><span class="line">  <span class="comment"># 再做一些数据增强</span></span><br><span class="line">  features_list = data_util.batch_random_blur(</span><br><span class="line">      features_list, FLAGS.image_size, FLAGS.image_size)</span><br><span class="line"><span class="comment"># 现在变成了(num_transforms * bsz, h, w, c)</span></span><br><span class="line">features = tf.concat(features_list, <span class="number">0</span>)  <span class="comment"># (num_transforms * bsz, h, w, c)</span></span><br></pre></td></tr></table></figure>
<p>注意他这里的<code>features</code>实际上还是图像，因为之前是<code>[h,w,2*c]</code>的形状，他这里重新分离的同时再分别加了一些数据增强，得到了<code>[n*batch, h, w, c]</code>的数据。</p>
<h2 id="投影获取">投影获取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;base_model&#x27;</span>):</span><br><span class="line">  <span class="keyword">if</span> FLAGS.train_mode == <span class="string">&#x27;finetune&#x27;</span> <span class="keyword">and</span> FLAGS.fine_tune_after_block &gt;= <span class="number">4</span>:</span><br><span class="line">    <span class="comment"># Finetune just supervised (linear) head will not update BN stats.</span></span><br><span class="line">    model_train_mode = <span class="literal">False</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Pretrain or finetuen anything else will update BN stats.</span></span><br><span class="line">    model_train_mode = is_training</span><br><span class="line">  hiddens = model(features, is_training=model_train_mode)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add head and loss.</span></span><br><span class="line"><span class="keyword">if</span> FLAGS.train_mode == <span class="string">&#x27;pretrain&#x27;</span>:</span><br><span class="line">  tpu_context = params[<span class="string">&#x27;context&#x27;</span>] <span class="keyword">if</span> <span class="string">&#x27;context&#x27;</span> <span class="keyword">in</span> params <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">  hiddens_proj = model_util.projection_head(hiddens, is_training)</span><br></pre></td></tr></table></figure>
<p>图像输入到<code>basemodel</code>得到隐含层输出，再通过投影头得到隐含层投影<code>hiddens_proj</code>。</p>
<h2 id="对比损失计算">对比损失计算</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_contrastive_loss</span>(<span class="params">hidden,</span></span><br><span class="line"><span class="params">                         hidden_norm=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                         temperature=<span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">                         tpu_context=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                         weights=<span class="number">1.0</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Compute loss for model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    hidden: hidden vector (`Tensor`) of shape (bsz, dim).</span></span><br><span class="line"><span class="string">    hidden_norm: whether or not to use normalization on the hidden vector.</span></span><br><span class="line"><span class="string">    temperature: a `floating` number for temperature scaling.</span></span><br><span class="line"><span class="string">    tpu_context: context information for tpu.</span></span><br><span class="line"><span class="string">    weights: a weighting number or vector.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A loss scalar.</span></span><br><span class="line"><span class="string">    The logits for contrastive prediction task.</span></span><br><span class="line"><span class="string">    The labels for contrastive prediction task.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># Get (normalized) hidden1 and hidden2.</span></span><br><span class="line">  <span class="keyword">if</span> hidden_norm:</span><br><span class="line">    hidden = tf.math.l2_normalize(hidden, -<span class="number">1</span>)</span><br><span class="line">  hidden1, hidden2 = tf.split(hidden, <span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">  batch_size = tf.shape(hidden1)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Gather hidden1/hidden2 across replicas and create local labels.</span></span><br><span class="line">  <span class="keyword">if</span> tpu_context <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    hidden1_large = tpu_cross_replica_concat(hidden1, tpu_context)</span><br><span class="line">    hidden2_large = tpu_cross_replica_concat(hidden2, tpu_context)</span><br><span class="line">    enlarged_batch_size = tf.shape(hidden1_large)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># TODO(iamtingchen): more elegant way to convert u32 to s32 for replica_id.</span></span><br><span class="line">    replica_id = tf.cast(tf.cast(xla.replica_id(), tf.uint32), tf.int32)</span><br><span class="line">    labels_idx = tf.<span class="built_in">range</span>(batch_size) + replica_id * batch_size</span><br><span class="line">    labels = tf.one_hot(labels_idx, enlarged_batch_size * <span class="number">2</span>)</span><br><span class="line">    masks = tf.one_hot(labels_idx, enlarged_batch_size)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    hidden1_large = hidden1</span><br><span class="line">    hidden2_large = hidden2</span><br><span class="line">    labels = tf.one_hot(tf.<span class="built_in">range</span>(batch_size), batch_size * <span class="number">2</span>)</span><br><span class="line">    masks = tf.one_hot(tf.<span class="built_in">range</span>(batch_size), batch_size)</span><br><span class="line">  </span><br><span class="line">  logits_aa = tf.matmul(hidden1, hidden1_large, transpose_b=<span class="literal">True</span>) / temperature</span><br><span class="line">  logits_aa = logits_aa - masks * LARGE_NUM </span><br><span class="line">  logits_bb = tf.matmul(hidden2, hidden2_large, transpose_b=<span class="literal">True</span>) / temperature</span><br><span class="line">  logits_bb = logits_bb - masks * LARGE_NUM </span><br><span class="line">  logits_ab = tf.matmul(hidden1, hidden2_large, transpose_b=<span class="literal">True</span>) / temperature</span><br><span class="line">  logits_ba = tf.matmul(hidden2, hidden1_large, transpose_b=<span class="literal">True</span>) / temperature</span><br><span class="line"></span><br><span class="line">  loss_a = tf.losses.softmax_cross_entropy(</span><br><span class="line">      labels, tf.concat([logits_ab, logits_aa], <span class="number">1</span>), weights=weights)</span><br><span class="line">  loss_b = tf.losses.softmax_cross_entropy(</span><br><span class="line">      labels, tf.concat([logits_ba, logits_bb], <span class="number">1</span>), weights=weights)</span><br><span class="line">  loss = loss_a + loss_b</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, logits_ab, labels</span><br></pre></td></tr></table></figure>
<p>因为现在的隐含层投影前一半和后一半是对同一张图像的输出，所以拆分为<code>hidden1, hidden2 = tf.split(hidden, 2, 0)</code>，接下来是得到<code>hidden1_large</code>，这里其实我不是很懂<code>tpu</code>上会和<code>gpu</code>有多大区别，不过对于<code>gpu</code>来说<code>hidden1_large = hidden1</code>。</p>
<p>有了投影，接下来制作标签，标签实际是<code>batch</code>单位矩阵并上一个零矩阵，<code>mask</code>是单位矩阵，矩阵的大小都是<code>[batch,batch]</code>，比如当<code>batch=2</code>时：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">label = [[1., 0., 0., 0.],</span><br><span class="line">        [0., 1., 0., 0.]] </span><br><span class="line"></span><br><span class="line">mask =  [[1., 0.],</span><br><span class="line">        [0., 1.]] </span><br></pre></td></tr></table></figure>
<p>下面就是<code>logits_aa,logits_bb</code>，他们就是一个<code>batch</code>内的图像隐含层投影交叉做內积，而其中的对角线元素是相同向量做內积，那么因为他要做对比损失，希望将相似度转换成概率(越相似概率越大，越不相似概率越小)，向量自身的內积肯定是最大的(概率值最大)，所以这里就没有必要把对角线上的结果算到损失里面，他就利用<code>mask</code>将对角线相似度都减去一个极大值，将概率强行降为最小。</p>
<p>对于<code>logits_ab,logits_ba</code>，和上面一样类似，只不过现在的对角线元素是一张源图像两个不同增强后的表征投影向量內积(相似度)。</p>
<p>损失值就很明确了，即<strong>最大化</strong><code>一张源图像两个不同增强后的表征投影向量的相似度</code>，<strong>最小化</strong><code>不同图像间表征投影向量的相似度</code>，<strong>一张源图像一个增强的表征投影相似度被mask矩阵排除了</strong>。</p>
<h1 id="思考">思考</h1>
<p>总的来说他的方法比我想的要简单好多，之前半监督学习里面对数据增强已经玩出花了，这里的数据增强只是简单的线性，都没有上数据增强策略。然后对于一致性学习这块，因为他神似交叉熵，而且他也考虑到了只用向量的夹角表示相似度(上面代码中有个<code>hidden_norm</code>选项)，这样实际上可以考虑和之前那些<a
href="https://zhen8838.github.io/2019/06/03/l-softmax/"><code>am softmax</code>的论文</a>结合起来，构建一个更严格的约束。</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">半监督学习</a></li></ul></div><div class="post-nav"><a class="pre" href="/2020/03/30/ssl-infomax-error/">infomax中一些错误总结</a><a class="next" href="/2020/03/17/h5-to-pb/">H5模型转pb模型</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>