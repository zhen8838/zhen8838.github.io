<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>半监督学习：Virtual Adversarial Training | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">半监督学习：Virtual Adversarial Training</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">半监督学习：Virtual Adversarial Training</h1><div class="post-meta">2020-01-31<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 14</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>第四个算法<code>Virtual Adversarial Training</code>(虚拟对抗训练)，出自论文<code>Virtual Adversarial Training:A Regularization Method for Supervised and Semi-Supervised Learning</code>,下面简称为<code>vat</code>。</p>
<span id="more"></span>
<h1 id="算法理论">算法理论</h1>
<p>经过三篇半监督算法的学习，可以发现半监督学习的精髓大概率就在于正则化(一致性)上面，<code>vat</code>是一种基于熵最小化出发的正则方法。他的出发点在于，我们要学习一致性那就得添加扰动，通常添加的随机扰动无法模拟各种复杂情况的输入，所以需要添加合适的扰动。因此根据一定的理论基础提出了如何找到合适的扰动，以及计算扰动的方法。我们记模型的损失函数为<span
class="math inline">\(J(\theta ;x;y)\)</span>，其中负梯度方向<span
class="math inline">\(− \nabla J_x(\theta
;x;y)\)</span>是模型的损失下降最快的方向，那么也就是说负梯度上模型优化最快，
为了使<span
class="math inline">\(\hat{x}\)</span>对模型的输出分布产生最大的改变，正梯度方向也就是模型梯度下降最慢的方向定为扰动方向，这就是<code>vat</code>需要添加扰动的方向。</p>
<hr />
<p>以下内容来自原论文：</p>
<p><code>vat</code>的引入了<code>虚拟对抗方向</code>的概念，即扰动的方向，在输入数据中加入此方向上的扰动可以最大程度的影响模型分类输出概率分布，根据<code>虚拟对抗方向</code>的定义，可以在不使用监督信号的情况下，量化模型在每个输入点的局部各向异性，将<code>Local Distributional Smoothness (LDS)</code>定义为针对<code>虚拟对抗方向</code>的模型基于散度的分布鲁棒性，提出了一种新颖的使用有效的近似值，以最大程度地提高模型的熵，同时在每个训练输入数据点上提升模型的<code>LDS</code>，此方法成为<code>vat</code>。</p>
<p><code>vat</code>的优点： - 适用于半监督学习任务 -
适用于任何我们可以评估输入和参数梯度的参数模型 - 超参数数量少 -
参数化不变正则化</p>
<h2 id="方法细节">方法细节</h2>
<p>首先定义符号：令<span class="math inline">\(x\in R^I,y\in
Q\)</span>表示输入向量与输出标签，<span
class="math inline">\(I\)</span>表示输入维度，<span
class="math inline">\(Q\)</span>表示标签空间。此外，我们将输出分布通过<span
class="math inline">\(\theta\)</span>参数化为<span
class="math inline">\(p(y|x,\theta)\)</span>，我们使用<span
class="math inline">\(\hat{\theta}\)</span>来表示模型参数在训练过程的特定迭代步骤的向量。使用<span
class="math inline">\(\mathcal{D}_{l}=\left\{x_{l}^{(n)}, y_{l}^{(n)} |
n=1, \ldots, N_{l}\right\}\)</span>表示带标签数据集，<span
class="math inline">\(\mathcal{D}_{ul}=\left\{x_{ul}^{(m)}, y_{l}^{(m)}
| m=1, \ldots, N_{ul}\right\}\)</span>表示无标签数据集，我们使用<span
class="math inline">\(\mathcal{D}_l,\mathcal{D}_{ul}\)</span>训练模型<span
class="math inline">\(p(y|x,\theta)\)</span>。</p>
<h2 id="对抗训练">对抗训练</h2>
<p>因为<code>vat</code>继承自<code>对抗训练</code>，因此，在介绍它之前，需要介绍对抗训练。对抗训练的损失函数可以写成：
<span class="math display">\[
\begin{equation}
L_{\mathrm{adv}}\left(x_{l}, \theta\right):=D\left[q\left(y |
x_{l}\right), p\left(y | x_{l}+r_{\mathrm{adv}}, \theta\right)\right]
\end{equation}\tag{1}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
{\text { where } r_{\text {adv }}:=\underset{r ;\|r\| \leq
\epsilon}{\arg \max } D\left[q\left(y | x_{l}\right), p\left(y |
x_{l}+r, \theta\right)\right]}
\end{equation}\tag{2}
\]</span></p>
<p>其中<span class="math inline">\(D\)</span>为分布<span
class="math inline">\(p\)</span>和<span
class="math inline">\(p&#39;\)</span>间的差异度量函数，通常，我们无法获得精确的对抗性扰动<span
class="math inline">\(r_{adv}\)</span>的封闭形式，不过我们可以通过公式<span
class="math inline">\(2\)</span>中的度量<span
class="math inline">\(D\)</span>来线性近似<span
class="math inline">\(r\)</span>。当使用<code>l2</code>正则时，对抗扰动可以通过此公式近似：
<span class="math display">\[
\begin{align}
r_{\mathrm{adv}} \approx \epsilon \frac{g}{\|g\|_{2}}, \text { where }
g=\nabla_{x_{l}} D\left[h\left(y ; y_{l}\right), p\left(y | x_{l},
\theta\right)\right]
\end{align}\tag{3}
\]</span></p>
<p>当使用<span
class="math inline">\(L_{\infty}\)</span>正则时，对抗扰动可以通过此公式近似：
<span class="math display">\[
\begin{aligned}
    r_{adv}\approx\epsilon sign(g)
\end{aligned}\tag{4}
\]</span></p>
<p>其中<span class="math inline">\(g\)</span>和公式<span
class="math inline">\(3\)</span>相同。传统的对抗训练一般使用公式<span
class="math inline">\(3\)</span>来计算。</p>
<h2 id="虚拟对抗训练">虚拟对抗训练</h2>
<p>对抗训练是一种成功的方法，可以解决任何有监督的问题。但并非始终都有完整的标签信息。
令<span class="math inline">\(x_*\)</span>代表任一<span
class="math inline">\(x_l\)</span>或<span
class="math inline">\(x_{ul}\)</span>，我们的目标函数现在为： <span
class="math display">\[
\begin{align}
\begin{aligned}
&amp;D\left[q\left(y | x_{*}\right), p\left(y | x_{*}+r_{\mathrm{qadv}},
\theta\right)\right]\\
&amp;\text { where } r_{\text {qadv }}:=\underset{r ;\|r\| \leq
\epsilon}{\arg \max } D\left[q\left(y | x_{*}\right), p\left(y |
x_{*}+r, \theta\right)\right]
\end{aligned}
\end{align}
\]</span></p>
<p>实际上没有关于<span
class="math inline">\(q(x|x_{ul})\)</span>直接的标签信息，因此，我们采取了用当前近似值<span
class="math inline">\(p(y|x,\theta)\)</span>代替<span
class="math inline">\(q(y|x)\)</span>的策略，这种近似不一定是<code>naive</code>的，因为当带标签的训练样本数量很大时，<span
class="math inline">\(p(y|x,\theta)\)</span>应该接近<span
class="math inline">\(q(y|x)\)</span>。从字面上看，我们使用从<span
class="math inline">\(p(y|x,\theta)\)</span>概率生成的<code>虚拟</code>标签代替用户不知道的标签，并根据虚拟标签计算对抗方向。因此，使用当前估计值<span
class="math inline">\(p(y|x,\hat{\theta})\)</span>代替<span
class="math inline">\(q(y|x)\)</span>。有了这种折衷，我们得出了新的公式<span
class="math inline">\(2\)</span>的表达式： <span class="math display">\[
\begin{align}
\operatorname{LDS}\left(x_{*}, \theta\right):=D\left[p\left(y | x_{*},
\hat{\theta}\right), p\left(y | x_{*}+r_{\mathrm{vadv}},
\theta\right)\right]
\end{align}\tag{5}
\]</span></p>
<p><span class="math display">\[
\begin{align}
r_{\text {vadv }}:=\underset{r ;\|r\|_{2} \leq \epsilon}{\arg \max }
D\left[p\left(y | x_{*}, \hat{\theta}\right), p\left(y |
x_{*}+r\right)\right]
\end{align}\tag{6}
\]</span></p>
<p><span class="math inline">\(r_{\text {vadv
}}\)</span>定义了我们的虚拟采样扰动，损失函数<span
class="math inline">\(\operatorname{LDS}( x_{*},
\theta)\)</span>可以视为对每个输入样本<span
class="math inline">\(x_{*}\)</span>当前模型的局部平滑度的否定度量，度量的减少将使模型在每个样本点处平滑。同时此损失的正则化项是所有输入样本点上的<span
class="math inline">\(\operatorname{LDS}(x_{*},
\theta)\)</span>的平均值： <span class="math display">\[
\begin{align}
\mathcal{R}_{\mathrm{vadv}}\left(\mathcal{D}_{l}, \mathcal{D}_{u l},
\theta\right):=\frac{1}{N_{l}+N_{u l}} \sum_{x_{*} \in \mathcal{D}_{l},
\mathcal{D}_{u l}} \operatorname{LDS}\left(x_{*}, \theta\right)
\end{align}\tag{7}
\]</span></p>
<p>最终得到完整的目标函数为：</p>
<p><span class="math display">\[
\begin{align}
\ell\left(\mathcal{D}_{l}, \theta\right)+\alpha
\mathcal{R}_{\mathrm{vadv}}\left(\mathcal{D}_{l}, \mathcal{D}_{u l},
\theta\right)
\end{align}\tag{8}
\]</span></p>
<p>其中<span class="math inline">\(\ell(\mathcal{D}_{l},
\theta)\)</span>是标记数据集的负对数似然。<code>vat</code>是使用正则化<span
class="math inline">\(\mathcal{R}_{\mathrm{vadv}}\)</span>的训练方法。</p>
<p><code>vat</code>的一个显着优势是仅有两个标量值超参数：(1)对抗方向的范数约束<span
class="math inline">\(\epsilon&gt;0\)</span>；(2)控制负对数似然与正则化器<span
class="math inline">\(\mathcal{R}_{\mathrm{vadv}}\)</span>之间相对平衡的正则化系数<span
class="math inline">\(\alpha&gt;0\)</span>。实际上，根据实验，<code>vat</code>仅通过调整超参数(固定<span
class="math inline">\(\alpha=1\)</span>)而获得了出色的性能。</p>
<h2 id="r_textvadv的快速逼近方法和目标函数的导数"><span
class="math inline">\(r_{\text{vadv}}\)</span>的快速逼近方法和目标函数的导数</h2>
<p>为了简单起见，我们把<span class="math inline">\(D\left[p\left(y |
x_{*}, \hat{\theta}\right), p\left(y | x_{*}+r_{\mathrm{vadv}},
\theta\right)\right]\)</span>表示成<span
class="math inline">\(D(r,x_*,\theta)\)</span> <span
class="math inline">\(r_{\text{vadv}}\)</span>。我们假设<span
class="math inline">\(p(y|x_*,\theta)\)</span>相对于<span
class="math inline">\(\theta\)</span>和几乎各处的<span
class="math inline">\(x\)</span>的差是两倍。因为<span
class="math inline">\(D(r,x_*,\theta)\)</span>取最小值时<span
class="math inline">\(r=0\)</span>，其一阶导数<span
class="math inline">\(\left.\nabla_{r} D(r, x,
\hat{\theta})\right|_{r=0}\)</span>是<code>0</code>，因此<span
class="math inline">\(D\)</span>的第二项泰勒级数近似为：</p>
<p><span class="math display">\[
\begin{align}
D(r, x, \hat{\theta})=f(0)+f&#39;(0)r+\frac{f&#39;&#39;(0)}{2!}r^2
\approx \frac{1}{2} r^{T} H(x, \hat{\theta}) r
\end{align}\tag{9}
\]</span></p>
<p><span class="math inline">\(H(x,
\hat{\theta})\)</span>的<code>Hessian矩阵</code>为<span
class="math inline">\(H(x,\hat{\theta}) = \nabla \nabla_r
D(r,x,\hat{\theta}) \vert_{r=0}\)</span>。此时<span
class="math inline">\(r_{\text{vadv}}\)</span>成为<span
class="math inline">\(H(x, \hat{\theta})\)</span>的第一个特征向量<span
class="math inline">\(u(x,\theta)\)</span>，并且幅度为<span
class="math inline">\(\epsilon\)</span>(二次型在单位元上的最大值和最小值分别对应其最大特征值和最小特征值，此时<span
class="math inline">\(r\)</span>等于其对应的特征向量，这个具体的证明将Hermite矩阵正交对角化)：
<span class="math display">\[
\begin{align}
\begin{aligned}
r_{\text {vadv }} &amp; \approx \underset{r}{\arg \max }\left\{r^{T}
H(x, \hat{\theta}) r ;\|r\|_{2} \leq \epsilon\right\} \\
&amp;=\overline{\epsilon u(x, \hat{\theta})}
\end{aligned}
\end{align}\tag{10}
\]</span></p>
<p>其中<span
class="math inline">\(\overline{v}\)</span>表示方向与其参数向量<span
class="math inline">\(v\)</span>相同的单位向量，即<span
class="math inline">\(\bar{v} \equiv
\frac{v}{\|v\|_{2}}\)</span>。下面为简单起见，用<span
class="math inline">\(H\)</span>表示<span class="math inline">\(H(x,
\hat{\theta})\)</span>。接下来，我们需要解决计算<code>Hessian矩阵</code>特征向量所需的<span
class="math inline">\(O(I^3)\)</span>运行时间。通过幂迭代法和有限差分法通过逼近来解决此问题。设<span
class="math inline">\(d\)</span>为随机采样的单位向量，<span
class="math inline">\(d\)</span>如果不垂直于主特征向量<span
class="math inline">\(u\)</span>，则迭代计算： <span
class="math display">\[
\begin{equation}
d \gets \overline{Hd}
\end{equation}\tag{11}
\]</span></p>
<p>此时<span class="math inline">\(d\)</span>是收敛到主特征向量<span
class="math inline">\(u\)</span>的，对于<span
class="math inline">\(H\)</span>的计算，不需要直接计算，而是计算近似有限差分：
<span class="math display">\[
\begin{equation}
\begin{aligned}
Hd &amp;\approx \frac{\nabla_r D(r,x,\hat{\theta}) \vert_{r=\xi d}
-\nabla_r D(r,x,\hat{\theta})\vert_{r=0}}{\xi} \\
&amp;= \frac{\nabla_r D(r,x,\hat{\theta})\vert_{r=\xi d}}{\xi}
\end{aligned}
\end{equation}\tag{12}
\]</span></p>
<p>其中<span class="math inline">\(\xi \neq
0\)</span>，在上面的计算中，我们可以再次利用<span
class="math inline">\(\left.\nabla_{r} D(r, x,
\hat{\theta})\right|_{r=0}=0\)</span>。总而言之，我们可以通过以下更新的重复应用来近似<span
class="math inline">\(r_{\text{vadv}}\)</span>： <span
class="math display">\[
\begin{align}
d \leftarrow \overline{\nabla_{r} D(r, x, \hat{\theta})|_{r=\xi d}}
\end{align}\tag{13}
\]</span></p>
<p>在幂迭代下，这种近似可以由迭代次数<span
class="math inline">\(K\)</span>来单调改善，在实验中<span
class="math inline">\(K=1\)</span>就可以实现较好的结果了，此时可以对<span
class="math inline">\(r_{\text{vadv}}\)</span>进一步改写为： <span
class="math display">\[
\begin{aligned}
    r_{\text{vadv}} \approx \epsilon\frac{g}{\|g\|_2}
\end{aligned}\tag{14}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
    \text{where}\ g=\left. \nabla_{r} D[p(y | x, \hat{\theta}), p(y |
x+r, \hat{\theta})]\right|_{r=\xi d}
\end{aligned}\tag{15}
\]</span></p>
<p>计算<span
class="math inline">\(r_{\text{vadv}}\)</span>之后，可以使用神经网络中进行的正向和反向传播轻松计算<span
class="math inline">\(r_{\text{vadv}}\)</span>的导数。但是，加入<span
class="math inline">\(r_{\text{vadv}}\)</span>相对于参数的导数，不仅无用并且计算代价高，而且还为梯度引入了另一种方差来源，并对算法的性能产生负面影响。因此<code>vat</code>忽略了<span
class="math inline">\(r_{\text{vadv}}\)</span>对于<span
class="math inline">\(\theta\)</span>的依赖性。总体而言，包括对数似然项公式<span
class="math inline">\(8\)</span>在内的全目标函数的导数可以用<span
class="math inline">\(K +
2\)</span>组反向传播来计算。具体迭代过程伪代码如下：</p>
<p><img src="/2020/01/31/ssl-vat/vat-1.png" /></p>
<p>对于幂迭代次数<span
class="math inline">\(K\)</span>，可以对<code>vat</code>的正则项做一个表述：
<span class="math display">\[
\begin{equation}
\mathcal R^{(K)}(\theta ,\mathcal D_l,\mathcal D_{ul}) := \frac{1}{N_l +
N_{ul}} \sum_{x \in \mathcal D_l,\mathcal D_{ul}} \mathbb E_{r_K}[D[p(y
\vert x,\hat{\theta}), p(y \vert x+r_K,\theta)]]
\end{equation}\tag{16}
\]</span></p>
<p>对于<code>vat</code>就是幂迭代次数大于等于1次，即<span
class="math inline">\(K\geq1\)</span>。当<span
class="math inline">\(K=0\)</span>时，也就是不采用幂迭代求解<span
class="math inline">\(r_{\text{vadv}}\)</span>，称这种方法为<code>rpt</code>，<code>rpt</code>是<code>vat</code>的降级版本，
不执行幂迭代，<code>rpt</code>仅在每个输入数据点周围各向同性地平滑函数。</p>
<h1 id="代码">代码</h1>
<ol type="1">
<li>总体流程</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hwc = [self.dataset.height, self.dataset.width, self.dataset.colors]</span><br><span class="line">xt_in = tf.placeholder(tf.float32, [batch] + hwc, <span class="string">&#x27;xt&#x27;</span>)  <span class="comment"># For training</span></span><br><span class="line">x_in = tf.placeholder(tf.float32, [<span class="literal">None</span>] + hwc, <span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y_in = tf.placeholder(tf.float32, [batch] + hwc, <span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">l_in = tf.placeholder(tf.int32, [batch], <span class="string">&#x27;labels&#x27;</span>)</span><br><span class="line">wd *= lr</span><br><span class="line">warmup = tf.clip_by_value(tf.to_float(self.step) / (warmup_pos * (FLAGS.train_kimg &lt;&lt; <span class="number">10</span>)), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">classifier = <span class="keyword">lambda</span> x, **kw: self.classifier(x, **kw, **kwargs).logits</span><br><span class="line">l = tf.one_hot(l_in, self.nclass)</span><br><span class="line"><span class="comment"># 带标签数据概率分布</span></span><br><span class="line">logits_x = classifier(xt_in, training=<span class="literal">True</span>)</span><br><span class="line">post_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  <span class="comment"># Take only first call to update batch norm.</span></span><br><span class="line">logits_y = classifier(y_in, training=<span class="literal">True</span>) <span class="comment"># 无标签数据概率分布</span></span><br><span class="line"><span class="comment"># 计算对当前无标签数据需要添加的扰动</span></span><br><span class="line">delta_y = vat_utils.generate_perturbation(y_in, logits_y, <span class="keyword">lambda</span> x: classifier(x, training=<span class="literal">True</span>), vat_eps)</span><br><span class="line"><span class="comment"># （无标签数据+扰动）=&gt; 学生模型输出概率分布</span></span><br><span class="line">logits_student = classifier(y_in + delta_y, training=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># （无标签数据）=&gt; 教师模型输出概率分布</span></span><br><span class="line">logits_teacher = tf.stop_gradient(logits_y)</span><br><span class="line"><span class="comment"># 利用kl散度损失学习一致性</span></span><br><span class="line">loss_vat = layers.kl_divergence_from_logits(logits_student, logits_teacher)</span><br><span class="line">loss_vat = tf.reduce_mean(loss_vat)</span><br><span class="line"><span class="comment"># 最小化无监督概率分布的熵</span></span><br><span class="line">loss_entmin = tf.reduce_mean(tf.distributions.Categorical(logits=logits_y).entropy())</span><br><span class="line"></span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=l, logits=logits_x)</span><br><span class="line">loss = tf.reduce_mean(loss)</span><br><span class="line">tf.summary.scalar(<span class="string">&#x27;losses/xe&#x27;</span>, loss)</span><br><span class="line">tf.summary.scalar(<span class="string">&#x27;losses/vat&#x27;</span>, loss_vat)</span><br><span class="line">tf.summary.scalar(<span class="string">&#x27;losses/entmin&#x27;</span>, loss_entmin)</span><br><span class="line"></span><br><span class="line">ema = tf.train.ExponentialMovingAverage(decay=ema)</span><br><span class="line">ema_op = ema.apply(utils.model_vars())</span><br><span class="line">ema_getter = functools.partial(utils.getter_ema, ema)</span><br><span class="line">post_ops.append(ema_op)</span><br><span class="line">post_ops.extend([tf.assign(v, v * (<span class="number">1</span> - wd)) <span class="keyword">for</span> v <span class="keyword">in</span> utils.model_vars(<span class="string">&#x27;classify&#x27;</span>) <span class="keyword">if</span> <span class="string">&#x27;kernel&#x27;</span> <span class="keyword">in</span> v.name])</span><br><span class="line"></span><br><span class="line">train_op = tf.train.AdamOptimizer(lr).minimize(loss + loss_vat * warmup * vat + entmin_weight * loss_entmin,</span><br><span class="line">                                                colocate_gradients_with_ops=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_op]):</span><br><span class="line">    train_op = tf.group(*post_ops)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>扰动计算</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_perturbation</span>(<span class="params">x, logit, forward, epsilon, xi=<span class="number">1e-6</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generate an adversarial perturbation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: Model inputs.</span></span><br><span class="line"><span class="string">        logit: Original model output without perturbation.</span></span><br><span class="line"><span class="string">        forward: Callable which computs logits given input.</span></span><br><span class="line"><span class="string">        epsilon: Gradient multiplier.</span></span><br><span class="line"><span class="string">        xi: Small constant.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Aversarial perturbation to be applied to x.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    d = tf.random_normal(shape=tf.shape(x))</span><br><span class="line">    <span class="comment"># 迭代次数为1</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 向量d需要单位化</span></span><br><span class="line">        d = xi * get_normalized_vector(d)</span><br><span class="line">        logit_p = logit</span><br><span class="line">        logit_m = forward(x + d) <span class="comment"># 无标签样本+随机噪声的概率分布输出</span></span><br><span class="line">        dist = kl_divergence_with_logit(logit_p, logit_m) <span class="comment"># 计算分布距离度量</span></span><br><span class="line">        <span class="comment"># 求得度量梯度（累积求和）</span></span><br><span class="line">        grad = tf.gradients(tf.reduce_mean(dist), [d], aggregation_method=<span class="number">2</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 删除此操作产生的梯度</span></span><br><span class="line">        d = tf.stop_gradient(grad)</span><br><span class="line">    <span class="comment"># 输出向量r_adv也是单位化向量，同时乘epsilon</span></span><br><span class="line">    <span class="keyword">return</span> epsilon * get_normalized_vector(d)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_normalized_vector</span>(<span class="params">d</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Normalize d by infinity and L2 norms.&quot;&quot;&quot;</span></span><br><span class="line">    d /= <span class="number">1e-12</span> + tf.reduce_max(</span><br><span class="line">        tf.<span class="built_in">abs</span>(d), <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(d.get_shape()))), keepdims=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    d /= tf.sqrt(</span><br><span class="line">        <span class="number">1e-6</span></span><br><span class="line">        + tf.reduce_sum(</span><br><span class="line">            tf.<span class="built_in">pow</span>(d, <span class="number">2.0</span>), <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(d.get_shape()))), keepdims=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<h1 id="测试结果">测试结果</h1>
<p>使用默认参数以及cifar10中250张标注样本训练128个epoch，得到测试集准确率如下，比之前的算法的确有提升：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;last01&quot;: 54.720001220703125,</span><br><span class="line">&quot;last10&quot;: 54.28499984741211,</span><br><span class="line">&quot;last20&quot;: 54.27000045776367,</span><br><span class="line">&quot;last50&quot;: 53.93499946594238</span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">半监督学习</a></li></ul></div><div class="post-nav"><a class="pre" href="/2020/02/01/ssl-mixup/">半监督学习：mixup</a><a class="next" href="/2020/01/30/ssl-mean-teacher/">半监督学习：mean teacher</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>