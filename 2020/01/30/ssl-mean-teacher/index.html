<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>半监督学习：mean teacher | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">半监督学习：mean teacher</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">半监督学习：mean teacher</h1><div class="post-meta">2020-01-30<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 8</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>第三个算法<code>mean teacher</code>，此算法是对<code>Π model</code>的升级。</p>
<span id="more"></span>
<h1 id="算法理论">算法理论</h1>
<p><code>mean teacher</code>的引言写的非常好，所以我翻译并精简：</p>
<p>深度学习模型需要大量参数，从而易于过度拟合(图1a)。此外，将高质量标签手动添加到训练数据通常很昂贵。因此，希望使用可有效利用未标记数据的正则化方法，以减少半监督学习中的过度拟合。
(<strong>半监督实际是减少过拟合</strong>)</p>
<p>当输入略有变化时，人们通常仍将其视为同一对象。相应地，分类模型应该偏向为相似数据点提供一致输出的函数。实现此目的的一种方法是将噪声添加到模型的输入中。为了使模型能够学习更多抽象不变性，可以将噪声添加到中间表示中，这一见解激发了许多正则化技术，例如Dropout。正则化模型不是使输入空间的零维数据点上的分类成本最小化，而是使每个数据点周围的流形上的成本最小化，从而使决策边界远离带标签的数据点(图1b)
(<strong>类间间距大，类内间距小</strong>)</p>
<p>由于对于未标记的示例未定义分类成本，因此噪声正则化本身无助于半监督学习。为了克服这个问题，<code>Γ model</code>评估了有无噪声的每个数据点，然后在两个预测之间应用了<code>consistency cost</code>。在这种情况下，模型承担着<code>teacher</code>和<code>student</code>的双重角色。作为<code>student</code>，它像以前一样学习。作为<code>teacher</code>，它会生成标签让<code>student</code>学习。由于模型本身会生成标签，因此它们很可能是<strong>错误</strong>的。如果对生成的标签给予过多的权重，则<code>consistency cost</code>将超过错误分类的损失，从而阻止学习新信息。实际上，模型存在确认偏差(图1c)，这种风险可以通过提高标签质量来减轻。</p>
<p>至少有两种方法可以提高目标质量。一种方法是仔细选择表示的扰动，而不是仅仅施加加性或乘性噪声。另一种方法是谨慎选择<code>teacher</code>模型，而不是仅仅复制<code>student</code>模型。同时，在我们的研究中，<code>Virtual Adversarial Training</code>采用了第一种方法可以产生令人印象深刻的结果。我们采取第二种方法，并将表明它也提供了显著的好处。同时这两种方法是兼容的，并且它们的组合可能会产生更好的结果。</p>
<p>因此，我们的目标是在没有额外训练的情况下从<code>student</code>模型中形成更好的教师模型。第一步，考虑模型的<code>softmax</code>输出通常不会在训练数据之外提供准确的预测。通过在<code>dropout</code>推理时将噪声添加到模型中，可以部分缓解这种情况，因此，带噪声的<code>teacher</code>可以产生更准确的目标(图1d)。该<code>Π model</code>近来已证明在半监督图像分类中效果很好。我们将使用该名称及其版本作为我们实验的基础。</p>
<p>可以通过<code>Temporal Ensembling</code>进一步改进<code>Π model</code>，该模型为每个训练示例保持了指数移动平均值(EMA)预测。在每个训练步骤中，都会基于新的预测更新该小批中示例的所有EMA预测。因此，每个示例的EMA预测由模型的当前版本和评估同一示例的早期版本组成。这种集成可以提高预测的质量，并将其用作<code>teacher</code>的预测可以改善结果。然而，由于每个目标每个时期仅更新一次，所以学习的信息以缓慢的速度被合并到训练过程中。数据集越大，更新的时间越长，并且在在线学习的情况下，很难完全使用<code>Temporal Ensembling</code>。(一个时期可以周期性地对所有目标进行一次评估，但要使评估范围保持恒定，则每个时期需要O(n2)次评估，而训练示例的数量也是如此。)</p>
<p><img src="/2020/01/30/ssl-mean-teacher/mean-teacher-1.png" /></p>
<p>带有两个标记示例(大蓝点)和一个无标签样本的二元分类任务的草图，展示了无标签样本(黑色圆圈)如何影响拟合函数(灰色曲线)。<strong>(a)</strong>没有正则化的模型可以自由地拟合任何可以很好地预测带标签样本。<strong>(b)</strong>经过训练的模型带有嘈杂的标记数据(小点)，可以学习为带标签样本周围提供一致的预测
<strong>(c)</strong>无标签样本周围的噪声一致性提供了额外的平滑度。为了清楚起见，首先将<code>teacher</code>模型(灰色曲线)拟合到标记的样本点中，然后在训练学生模型时保持不变。同样为了清楚起见，我们将省略图d和e中的小点
<strong>(d)</strong>
<code>teacher</code>模型上的噪音可减少目标的偏差，而无需额外的培训。随机梯度下降的预期方向是朝向各个有噪声目标(小蓝色圆圈)的平均值(大蓝色圆圈)
<strong>(e)</strong>
一组模型给出了更好的预期目标。<code>Temporal Ensembling</code>和<code>mean teacher</code>方法都使用此方法。</p>
<h2 id="mean-teacher">mean teacher</h2>
<p>为了克服<code>Temporal Ensembling</code>的局限性，建议对模型权重取平均而不是预测结果。由于<code>teacher</code>模型是连续的<code>student</code>模型权重平均值，因此将其称为<code>mean teacher</code>方法(图2)。
在训练步骤的上平均模型权重往往会产生比直接使用最终权重更准确的模型。我们可以在培训过程中利用这一优势来构建更好的目标。与使用学生模型共享权重不同，教师模型使用学生模型的EMA权重。现在，它可以在每个步骤之后而不是每个时期都汇总信息。另外，由于权重平均值改善了所有层的输出，而不仅仅是顶部输出，因此目标模型具有更好的中间表示。这些方面在时间合计方面具有两个实践优势：首先，目标标签更准确可导致学生模型与教师模型之间的反馈回路更快，从而提高测试准确性。其次，该方法可扩展到大型数据集和在线学习。</p>
<p><img src="/2020/01/30/ssl-mean-teacher/mean-teacher-2.png" /></p>
<p>好了到这里其实应该清晰了，<code>mean teacher</code>对于<code>Temporal Ensembling</code>的实际改进其实就在与<code>teacher</code>模型的权重更新方式，使用的是<code>student</code>模型权重的滑动平均，而<code>student</code>模型实际上和<code>Π model</code>相同。</p>
<h1 id="代码">代码</h1>
<p>和<code>Π model</code>的区别就在于使用了<code>ema</code>更新<code>teacher</code>模型的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hwc = [self.dataset.height, self.dataset.width, self.dataset.colors]</span><br><span class="line">xt_in = tf.placeholder(tf.float32, [batch] + hwc, <span class="string">&#x27;xt&#x27;</span>)  <span class="comment"># For training</span></span><br><span class="line">x_in = tf.placeholder(tf.float32, [<span class="literal">None</span>] + hwc, <span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y_in = tf.placeholder(tf.float32, [batch, <span class="number">2</span>] + hwc, <span class="string">&#x27;y&#x27;</span>) <span class="comment"># 一次输入两个无标签样本</span></span><br><span class="line">l_in = tf.placeholder(tf.int32, [batch], <span class="string">&#x27;labels&#x27;</span>)</span><br><span class="line">l = tf.one_hot(l_in, self.nclass)</span><br><span class="line"></span><br><span class="line">warmup = tf.clip_by_value(tf.to_float(self.step) / (warmup_pos * (FLAGS.train_kimg &lt;&lt; <span class="number">10</span>)), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">lrate = tf.clip_by_value(tf.to_float(self.step) / (FLAGS.train_kimg &lt;&lt; <span class="number">10</span>), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">lr *= tf.cos(lrate * (<span class="number">7</span> * np.pi) / (<span class="number">2</span> * <span class="number">8</span>))</span><br><span class="line">tf.summary.scalar(<span class="string">&#x27;monitors/lr&#x27;</span>, lr)</span><br><span class="line"><span class="comment"># classifier是网络输入到输出的流程函数，实际上每次调用都构建了一个新的网络，不过他auto reuse了</span></span><br><span class="line">classifier = <span class="keyword">lambda</span> x, **kw: self.classifier(x, **kw, **kwargs).logits</span><br><span class="line">logits_x = classifier(xt_in, training=<span class="literal">True</span>)  <span class="comment"># 定义有标签数据的分类器</span></span><br><span class="line">post_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  <span class="comment"># Take only first call to update batch norm.</span></span><br><span class="line">y = tf.reshape(tf.transpose(y_in, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]), [-<span class="number">1</span>] + hwc)</span><br><span class="line">y_1, y_2 = tf.split(y, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 指定全部的变量getter</span></span><br><span class="line">ema = tf.train.ExponentialMovingAverage(decay=ema)</span><br><span class="line">ema_op = ema.apply(utils.model_vars())</span><br><span class="line">ema_getter = functools.partial(utils.getter_ema, ema)</span><br><span class="line"><span class="comment"># 第一个无标签数据的概率作为teacher</span></span><br><span class="line">logits_y = classifier(y_1, training=<span class="literal">True</span>, getter=ema_getter)  <span class="comment"># 定义teacher模型，并通过ema更新</span></span><br><span class="line">logits_teacher = tf.stop_gradient(logits_y) <span class="comment"># 但是屏蔽梯度，使其无法通过梯度下降更新</span></span><br><span class="line"><span class="comment"># 第二个无标签数据的概率作为student</span></span><br><span class="line">logits_student = classifier(y_2, training=<span class="literal">True</span>)  <span class="comment"># 定义student模型</span></span><br><span class="line"><span class="comment"># 以teacher和student间的mse损失学习其一致性</span></span><br><span class="line">loss_mt = tf.reduce_mean((tf.nn.softmax(logits_teacher) - tf.nn.softmax(logits_student)) ** <span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">loss_mt = tf.reduce_mean(loss_mt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类损失</span></span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=l, logits=logits_x)</span><br><span class="line">loss = tf.reduce_mean(loss)</span><br><span class="line">tf.summary.scalar(<span class="string">&#x27;losses/xe&#x27;</span>, loss)</span><br><span class="line">tf.summary.scalar(<span class="string">&#x27;losses/mt&#x27;</span>, loss_mt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重l2正则化</span></span><br><span class="line">loss_wd = <span class="built_in">sum</span>(tf.nn.l2_loss(v) <span class="keyword">for</span> v <span class="keyword">in</span> utils.model_vars(<span class="string">&#x27;classify&#x27;</span>) <span class="keyword">if</span> <span class="string">&#x27;kernel&#x27;</span> <span class="keyword">in</span> v.name)</span><br><span class="line">tf.summary.scalar(<span class="string">&#x27;losses/wd&#x27;</span>, loss_wd)</span><br><span class="line"></span><br><span class="line">post_ops.append(ema_op)</span><br><span class="line">train_op = tf.train.MomentumOptimizer(lr, <span class="number">0.9</span>, use_nesterov=<span class="literal">True</span>).minimize(</span><br><span class="line">    loss + loss_mt * warmup * consistency_weight + wd * loss_wd, colocate_gradients_with_ops=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_op]):</span><br><span class="line">    <span class="comment"># 梯度下降一次后更新teacher权重</span></span><br><span class="line">    train_op = tf.group(*post_ops)</span><br></pre></td></tr></table></figure>
<h1 id="测试结果">测试结果</h1>
<p>使用默认参数以及cifar10中250张标注样本训练128个epoch，得到测试集准确率如下，效果好于之前两个模型，但训练到后面有退化的迹象：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;last01&quot;: 50.95000076293945,</span><br><span class="line">&quot;last10&quot;: 51.114999771118164,</span><br><span class="line">&quot;last20&quot;: 51.3799991607666,</span><br><span class="line">&quot;last50&quot;: 52.079999923706055</span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">半监督学习</a></li></ul></div><div class="post-nav"><a class="pre" href="/2020/01/31/ssl-vat/">半监督学习：Virtual Adversarial Training</a><a class="next" href="/2020/01/29/ssl-pi-model/">半监督学习：Π model</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>