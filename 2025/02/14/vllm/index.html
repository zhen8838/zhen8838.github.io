<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>推理框架调研 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">推理框架调研</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">推理框架调研</h1><div class="post-meta">2025-02-14<span> | </span><span class="category"><a href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 9.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 50</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#llm%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">LLM模型结构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#vllm"><span class="toc-number">2.</span> <span class="toc-text">vllm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-%E5%BC%80%E5%8F%91"><span class="toc-number">2.1.</span> <span class="toc-text">安装 &amp; 开发</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cuda-graph"><span class="toc-number">2.2.</span> <span class="toc-text">cuda graph</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#attention-layer-size-trace"><span class="toc-number">2.3.</span> <span class="toc-text">attention layer size trace</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vllm-attention-detail"><span class="toc-number">2.4.</span> <span class="toc-text">vllm attention detail</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vllm-fused-moe"><span class="toc-number">2.5.</span> <span class="toc-text">vllm fused moe</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vllm%E5%B9%B6%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.6.</span> <span class="toc-text">vllm并行模式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#trt-llm"><span class="toc-number">3.</span> <span class="toc-text">trt llm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#plugin"><span class="toc-number">3.1.</span> <span class="toc-text">plugin</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#auto-parallel"><span class="toc-number">3.2.</span> <span class="toc-text">auto parallel</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pyexector"><span class="toc-number">3.3.</span> <span class="toc-text">PyExector</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#mlc-llm"><span class="toc-number">4.</span> <span class="toc-text">mlc llm</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sglang"><span class="toc-number">5.</span> <span class="toc-text">sglang</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB"><span class="toc-number">6.</span> <span class="toc-text">问题汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#vllm%E9%97%AE%E9%A2%98"><span class="toc-number">6.1.</span> <span class="toc-text">vllm问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#trt-llm%E9%97%AE%E9%A2%98"><span class="toc-number">6.2.</span> <span class="toc-text">trt llm问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scaled-dot-product-attention-sdpa-%E7%B2%BE%E5%BA%A6%E9%97%AE%E9%A2%98"><span class="toc-number">6.3.</span> <span class="toc-text">scaled dot product
attention (SDPA) 精度问题</span></a></li></ol></li></ol></div></div><div class="post-content"><p>记录一下学习vllm/trt llm等框架的内容。</p>
<span id="more"></span>
<h1 id="llm模型结构">LLM模型结构</h1>
<p>虽然用编译器编译了挺久的LLM，但其实对于宏观上的模型结构还是理解的不够深入。</p>
<p><img src="/2025/02/14/vllm/llama_arch.png" /></p>
<p>首先现在的LLM基本上是重复Attention +
FFN的结构，Attention里面首先有三个权重矩阵来计算得到QKV。
然后把QKV转换为<code>[batch,num_head,seq_len,head_size]</code>的形式,
然后是<code>Q*K^T</code>,这一步是计算每个head的相似度，计算完之后head
size进行了规约，得到<code>[batch,num_head,seq_len,total_seq_len]</code>，然后是scaling以及softmax。后面再和V的<code>total seq len</code>维度进行规约，最后又得到<code>[batch,num_head,seq_len,head_size]</code>的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">q: [batch,num_head,target_len,head_size]</span><br><span class="line">k: [batch,num_head,source_len,head_size]</span><br><span class="line">v: [batch,num_head,source_len,head_size]</span><br><span class="line">s: [batch,num_head,target_len,source_len] = q @ k.T</span><br><span class="line">s = s * scale + mask</span><br><span class="line">s: [batch,num_head,target_len,source_len] = softmax(s,-<span class="number">1</span>)</span><br><span class="line">d: [batch,num_head,target_len,head_size] = s @ v</span><br></pre></td></tr></table></figure>
<p><img src="/2025/02/14/vllm/attention.png" /></p>
<h1 id="vllm">vllm</h1>
<p>vll支持的优化： 1. Continuous Batching 2. Paged Attention 3. Chunked
Prefill
要把decode融合到prefill中一起执行，当然比如要求<code>[prefill_token,decode_token]</code>的排列顺序。</p>
<h2 id="安装-开发">安装 &amp; 开发</h2>
<p>为了构建一个方便调试的开发环境，可以采用官方镜像,
并且需要自己修改entrypoint： <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">docker run -d -it \</span><br><span class="line">  --gpus all \</span><br><span class="line">  --name vllm_dev \</span><br><span class="line">  --cap-add=NET_ADMIN \</span><br><span class="line">  --network=host \</span><br><span class="line">  --privileged=<span class="literal">true</span> \</span><br><span class="line">  --shm-size 50g \</span><br><span class="line">  --entrypoint /bin/bash \</span><br><span class="line">  /2025/02/14/vllm/vllm-openai:latest \</span><br><span class="line">  -c <span class="string">&quot;while true; do sleep 10; done&quot;</span></span><br></pre></td></tr></table></figure></p>
<p>进入官方镜像之后，他自带一个python3的环境，并且安装好了所有依赖，所以直接clone最新的vllm的并只安装python部分即可：
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://gitee.com/mirrors/vllm.git</span><br><span class="line"><span class="built_in">cd</span> vllm</span><br><span class="line"><span class="built_in">export</span> VLLM_TARGET_DEVICE=cuda</span><br><span class="line"><span class="built_in">export</span> VLLM_USE_PRECOMPILED=1</span><br><span class="line">python3 use_existing_torch.py</span><br><span class="line">pip install --no-build-isolation -e .</span><br></pre></td></tr></table></figure></p>
<h2 id="cuda-graph">cuda graph</h2>
<p>基于qwen 2.5 0.5b，
在<code>vllm/worker/model_runner.py</code>中1915行，注意他这里capture并不是一次，而是类似shape
bucket, 迭代这些batch
size<code>[256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1]</code>，他们也只会在batch上进行capture。
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self._graph = torch.cuda.CUDAGraph()</span><br><span class="line">self._graph.enable_debug_mode()</span><br><span class="line"><span class="keyword">with</span> torch.cuda.graph(self._graph, pool=memory_pool, stream=stream):</span><br><span class="line">    output_hidden_or_intermediate_states = self.model(</span><br><span class="line">        input_ids=input_ids, <span class="comment"># [256]</span></span><br><span class="line">        positions=positions, <span class="comment"># [256]</span></span><br><span class="line">        kv_caches=kv_caches, <span class="comment"># [[2, 103168, 16, 2, 64], [2, 103168, 16, 2, 64],...] 24个</span></span><br><span class="line">        attn_metadata=attn_metadata,</span><br><span class="line">        intermediate_tensors=intermediate_inputs,</span><br><span class="line">        **kwargs,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(output_hidden_or_intermediate_states, torch.Tensor):</span><br><span class="line">        hidden_or_intermediate_states = weak_ref_tensor(</span><br><span class="line">            output_hidden_or_intermediate_states)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(output_hidden_or_intermediate_states,</span><br><span class="line">                    IntermediateTensors):</span><br><span class="line">        hidden_or_intermediate_states = IntermediateTensors(</span><br><span class="line">            tensors=&#123;</span><br><span class="line">                key: weak_ref_tensor(value)</span><br><span class="line">                <span class="keyword">for</span> key, value <span class="keyword">in</span></span><br><span class="line">                output_hidden_or_intermediate_states.tensors.items()</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">del</span> output_hidden_or_intermediate_states</span><br><span class="line">    <span class="comment"># make sure `output_hidden_or_intermediate_states` is deleted</span></span><br><span class="line">    <span class="comment"># in the graph&#x27;s memory pool</span></span><br><span class="line">    gc.collect()</span><br><span class="line">torch.cuda.synchronize()</span><br><span class="line">self._graph.debug_dump(<span class="string">&quot;/root/vllm_learn/graph.dot&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<p>不过他这里生成的cuda graph是这样的，并没有shape。 <img
src="/2025/02/14/vllm/cuda_graph.png" /></p>
<h2 id="attention-layer-size-trace">attention layer size trace</h2>
<p>在模型初始化的时候，会用最大的batch size trace一次，
这是每一个att的打印代码： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        positions: torch.Tensor,</span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">        kv_cache: torch.Tensor,</span></span><br><span class="line"><span class="params">        attn_metadata: AttentionMetadata,</span></span><br><span class="line"><span class="params">    </span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;hidden_states&quot;</span>, hidden_states.shape) <span class="comment"># [1, 896]</span></span><br><span class="line">        qkv, _ = self.qkv_proj(hidden_states) </span><br><span class="line">        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;q&quot;</span>, q.shape,<span class="string">&quot;k&quot;</span>, k.shape,<span class="string">&quot;v&quot;</span>, v.shape)</span><br><span class="line">        q, k = self.rotary_emb(positions, q, k)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ro q&quot;</span>, q.shape,<span class="string">&quot;ro k&quot;</span>, k.shape)</span><br><span class="line">        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;attn_output&quot;</span>, attn_output.shape)</span><br><span class="line">        output, _ = self.o_proj(attn_output)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;output&quot;</span>, output.shape)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">hidden_states torch.Size([<span class="number">32768</span>, <span class="number">896</span>])</span><br><span class="line">q torch.Size([<span class="number">32768</span>, <span class="number">896</span>]) k torch.Size([<span class="number">32768</span>, <span class="number">128</span>]) v torch.Size([<span class="number">32768</span>, <span class="number">128</span>])</span><br><span class="line">ro q torch.Size([<span class="number">32768</span>, <span class="number">896</span>]) ro k torch.Size([<span class="number">32768</span>, <span class="number">128</span>])</span><br><span class="line">attn_output torch.Size([<span class="number">32768</span>, <span class="number">896</span>])</span><br><span class="line">output torch.Size([<span class="number">32768</span>, <span class="number">896</span>])</span><br></pre></td></tr></table></figure></p>
<p>然后是prefill： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hidden_states torch.Size([<span class="number">48</span>, <span class="number">896</span>])</span><br><span class="line">q torch.Size([<span class="number">48</span>, <span class="number">896</span>]) k torch.Size([<span class="number">48</span>, <span class="number">128</span>]) v torch.Size([<span class="number">48</span>, <span class="number">128</span>])</span><br><span class="line">ro q torch.Size([<span class="number">48</span>, <span class="number">896</span>]) ro k torch.Size([<span class="number">48</span>, <span class="number">128</span>])</span><br><span class="line">attn_output torch.Size([<span class="number">48</span>, <span class="number">896</span>])</span><br><span class="line">output torch.Size([<span class="number">48</span>, <span class="number">896</span>])</span><br></pre></td></tr></table></figure></p>
<p>接下来是decode: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hidden_states torch.Size([<span class="number">1</span>, <span class="number">896</span>])</span><br><span class="line">q torch.Size([<span class="number">1</span>, <span class="number">896</span>]) k torch.Size([<span class="number">1</span>, <span class="number">128</span>]) v torch.Size([<span class="number">1</span>, <span class="number">128</span>])</span><br><span class="line">ro q torch.Size([<span class="number">1</span>, <span class="number">896</span>]) ro k torch.Size([<span class="number">1</span>, <span class="number">128</span>])</span><br><span class="line">attn_output torch.Size([<span class="number">1</span>, <span class="number">896</span>])</span><br><span class="line">output torch.Size([<span class="number">1</span>, <span class="number">896</span>])</span><br></pre></td></tr></table></figure></p>
<p>所以最重要的就是看decode的时候是如何适配seq len增长的。</p>
<h2 id="vllm-attention-detail">vllm attention detail</h2>
<p>这是vllm中对于attention类的的设计： <img
src="/2025/02/14/vllm/vllm_attn.png" /></p>
<p>这是vllm调度出来的请求与attention的meta data的对应关系图： <img
src="/2025/02/14/vllm/vllm%20scheduled%20requests.png" /></p>
<p>在decode的时候，会调用<code>vllm/attention/layer.py</code>的attention：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        query: torch.Tensor,</span></span><br><span class="line"><span class="params">        key: torch.Tensor,</span></span><br><span class="line"><span class="params">        value: torch.Tensor,</span></span><br><span class="line"><span class="params">        kv_cache: torch.Tensor,</span></span><br><span class="line"><span class="params">        attn_metadata: AttentionMetadata,</span></span><br><span class="line"><span class="params">    </span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> please avoid accessing `kv_cache` and `attn_metadata` arguments</span></span><br><span class="line">        <span class="comment"># directly, use `self.kv_cache` and</span></span><br><span class="line">        <span class="comment"># `get_forward_context().attn_metadata` instead.</span></span><br><span class="line">        <span class="keyword">if</span> self.calculate_kv_scales:</span><br><span class="line">            ctx_attn_metadata = get_forward_context().attn_metadata</span><br><span class="line">            <span class="keyword">if</span> ctx_attn_metadata.enable_kv_scales_calculation:</span><br><span class="line">                self.calc_kv_scales(key, value)</span><br><span class="line">        <span class="keyword">if</span> self.use_output:</span><br><span class="line">            output = torch.empty_like(query)</span><br><span class="line">            hidden_size = query.size(-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># Reshape the query, key, and value tensors.</span></span><br><span class="line">            <span class="comment"># NOTE(woosuk): We do this outside the custom op to minimize the</span></span><br><span class="line">            <span class="comment"># CPU overheads from the non-CUDA-graph regions.</span></span><br><span class="line">            query = query.view(-<span class="number">1</span>, self.num_heads, self.head_size)</span><br><span class="line">            output = output.view(-<span class="number">1</span>, self.num_heads, self.head_size)</span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                key = key.view(-<span class="number">1</span>, self.num_kv_heads, self.head_size)</span><br><span class="line">            <span class="keyword">if</span> value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                value = value.view(-<span class="number">1</span>, self.num_kv_heads, self.head_size)</span><br><span class="line">            <span class="keyword">if</span> self.use_direct_call:</span><br><span class="line">                forward_context: ForwardContext = get_forward_context()</span><br><span class="line">                ctx_attn_metadata = forward_context.attn_metadata</span><br><span class="line">                self_kv_cache = self.kv_cache[forward_context.virtual_engine]</span><br><span class="line">                self.impl.forward(self,</span><br><span class="line">                                  query,</span><br><span class="line">                                  key,</span><br><span class="line">                                  value,</span><br><span class="line">                                  self_kv_cache,</span><br><span class="line">                                  ctx_attn_metadata,</span><br><span class="line">                                  output=output)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># note 上面把q都reshape到 [batch,num_heads,head_size]， kv转换为[num_kv_heads, head_size]</span></span><br><span class="line">                <span class="comment"># q [1,1,64] , k,v [1,2,64]</span></span><br><span class="line">                torch.ops.vllm.unified_attention_with_output(</span><br><span class="line">                    query, key, value, output, self.layer_name)</span><br><span class="line">            <span class="keyword">return</span> output.view(-<span class="number">1</span>, hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.use_direct_call:</span><br><span class="line">                forward_context = get_forward_context()</span><br><span class="line">                ctx_attn_metadata = forward_context.attn_metadata</span><br><span class="line">                self_kv_cache = self.kv_cache[forward_context.virtual_engine]</span><br><span class="line">                <span class="keyword">return</span> self.impl.forward(self, query, key, value,</span><br><span class="line">                                         self_kv_cache, ctx_attn_metadata)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> torch.ops.vllm.unified_attention(</span><br><span class="line">                    query, key, value, self.layer_name)</span><br><span class="line"><span class="comment"># 虽然torch.ops.vllm.unified_attention_with_output看起来没有使用kv cache，但是实际上是使用了的。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unified_attention_with_output</span>(<span class="params"></span></span><br><span class="line"><span class="params">    query: torch.Tensor,</span></span><br><span class="line"><span class="params">    key: torch.Tensor,</span></span><br><span class="line"><span class="params">    value: torch.Tensor,</span></span><br><span class="line"><span class="params">    output: torch.Tensor,</span></span><br><span class="line"><span class="params">    layer_name: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    forward_context: ForwardContext = get_forward_context()</span><br><span class="line">    attn_metadata = forward_context.attn_metadata</span><br><span class="line">    self = forward_context.attn_layers[layer_name]</span><br><span class="line">    kv_cache = self.kv_cache[forward_context.virtual_engine]</span><br><span class="line">    self.impl.forward(self,</span><br><span class="line">                      query,</span><br><span class="line">                      key,</span><br><span class="line">                      value,</span><br><span class="line">                      kv_cache,</span><br><span class="line">                      attn_metadata,</span><br><span class="line">                      output=output)</span><br></pre></td></tr></table></figure></p>
<p>检查当前的attn metadata： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">FlashAttentionMetadata(num_prefills=<span class="number">0</span>, num_prefill_tokens=<span class="number">0</span>, num_decode_tokens=<span class="number">1</span>, slot_mapping=tensor([<span class="number">54</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>), multi_modal_placeholder_index_maps=&#123;&#125;, enable_kv_scales_calculation=<span class="literal">True</span>, seq_lens=[<span class="number">55</span>], seq_lens_tensor=tensor([<span class="number">55</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), max_prefill_seq_len=<span class="number">0</span>, max_decode_seq_len=<span class="number">55</span>, context_lens_tensor=tensor([<span class="number">54</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), block_tables=tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), use_cuda_graph=<span class="literal">False</span>, max_query_len=<span class="number">1</span>, max_decode_query_len=<span class="number">1</span>, query_start_loc=tensor([<span class="number">0</span>, <span class="number">1</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), seq_start_loc=tensor([ <span class="number">0</span>, <span class="number">55</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), _cached_prefill_metadata=<span class="literal">None</span>, _cached_decode_metadata=FlashAttentionMetadata(num_prefills=<span class="number">0</span>, num_prefill_tokens=<span class="number">0</span>, num_decode_tokens=<span class="number">1</span>, slot_mapping=tensor([<span class="number">54</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>), multi_modal_placeholder_index_maps=<span class="literal">None</span>, enable_kv_scales_calculation=<span class="literal">True</span>, seq_lens=<span class="literal">None</span>, seq_lens_tensor=tensor([<span class="number">55</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), max_prefill_seq_len=<span class="number">0</span>, max_decode_seq_len=<span class="number">55</span>, context_lens_tensor=<span class="literal">None</span>, block_tables=tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), use_cuda_graph=<span class="literal">False</span>, max_query_len=<span class="number">1</span>, max_decode_query_len=<span class="number">1</span>, query_start_loc=tensor([<span class="number">0</span>, <span class="number">1</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), seq_start_loc=tensor([ <span class="number">0</span>, <span class="number">55</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), _cached_prefill_metadata=<span class="literal">None</span>, _cached_decode_metadata=FlashAttentionMetadata(num_prefills=<span class="number">0</span>, num_prefill_tokens=<span class="number">0</span>, num_decode_tokens=<span class="number">1</span>, slot_mapping=tensor([<span class="number">54</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>), multi_modal_placeholder_index_maps=<span class="literal">None</span>, enable_kv_scales_calculation=<span class="literal">True</span>, seq_lens=<span class="literal">None</span>, seq_lens_tensor=tensor([<span class="number">55</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), max_prefill_seq_len=<span class="number">0</span>, max_decode_seq_len=<span class="number">55</span>, context_lens_tensor=<span class="literal">None</span>, block_tables=tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), use_cuda_graph=<span class="literal">False</span>, max_query_len=<span class="number">1</span>, max_decode_query_len=<span class="number">1</span>, query_start_loc=tensor([<span class="number">0</span>, <span class="number">1</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), seq_start_loc=tensor([ <span class="number">0</span>, <span class="number">55</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32), _cached_prefill_metadata=<span class="literal">None</span>, _cached_decode_metadata=<span class="literal">None</span>, encoder_seq_lens=<span class="literal">None</span>, encoder_seq_lens_tensor=<span class="literal">None</span>, encoder_seq_start_loc=<span class="literal">None</span>, max_encoder_seq_len=<span class="literal">None</span>, num_encoder_tokens=<span class="literal">None</span>, cross_slot_mapping=<span class="literal">None</span>, cross_block_tables=<span class="literal">None</span>), encoder_seq_lens=<span class="literal">None</span>, encoder_seq_lens_tensor=<span class="literal">None</span>, encoder_seq_start_loc=<span class="literal">None</span>, max_encoder_seq_len=<span class="literal">None</span>, num_encoder_tokens=<span class="literal">None</span>, cross_slot_mapping=<span class="literal">None</span>, cross_block_tables=<span class="literal">None</span>), encoder_seq_lens=<span class="literal">None</span>, encoder_seq_lens_tensor=<span class="literal">None</span>, encoder_seq_start_loc=<span class="literal">None</span>, max_encoder_seq_len=<span class="literal">None</span>, num_encoder_tokens=<span class="literal">None</span>, cross_slot_mapping=<span class="literal">None</span>, cross_block_tables=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">kv_cache: torch.Size([<span class="number">2</span>, <span class="number">101291</span>, <span class="number">16</span>, <span class="number">2</span>, <span class="number">64</span>])</span><br></pre></td></tr></table></figure></p>
<p>然后走到了<code>FlashAttentionImpl</code>.</p>
<p>注意这里在启用chunked
prefill之后，一批token里面会同时存在prefill和decode，因此需要拆分为两个部分分别执行prefill的decode的attention。
同时这个只是在动态的情况下会被执行到，如果开启了cuda
graph，那么decode阶段会直接走cuda graph replay，同时</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">If the input tensors contain prompt tokens, the layout is as follows:</span></span><br><span class="line"><span class="string">|&lt;--------------- num_prefill_tokens -----------------&gt;|	</span></span><br><span class="line"><span class="string">|&lt;--prefill_0--&gt;|&lt;--prefill_1--&gt;|...|&lt;--prefill_N-1---&gt;|</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Otherwise, the layout is as follows:	</span></span><br><span class="line"><span class="string">|&lt;----------------- num_decode_tokens ------------------&gt;|	</span></span><br><span class="line"><span class="string">|&lt;--decode_0--&gt;|..........|&lt;--decode_M-1--&gt;|&lt;--padding--&gt;|</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generation tokens can contain padding when cuda-graph is used.</span></span><br><span class="line"><span class="string">Currently, prompt tokens don&#x27;t contain any padding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The prompts might have different lengths, while the generation tokens</span></span><br><span class="line"><span class="string">always have length 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">If chunked prefill is enabled, prefill tokens and decode tokens can be</span></span><br><span class="line"><span class="string">batched together in a flattened 1D query.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">|&lt;----- num_prefill_tokens ----&gt;|&lt;------- num_decode_tokens ---------&gt;|</span></span><br><span class="line"><span class="string">|&lt;-prefill_0-&gt;|...|&lt;-prefill_N-1-&gt;|&lt;--decode_0--&gt;|...|&lt;--decode_M-1--&gt;|</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Currently, cuda graph is disabled for chunked prefill, meaning there&#x27;s no</span></span><br><span class="line"><span class="string">padding between prefill and decode tokens.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    layer: AttentionLayer,</span></span><br><span class="line"><span class="params">    query: torch.Tensor,</span></span><br><span class="line"><span class="params">    key: torch.Tensor,</span></span><br><span class="line"><span class="params">    value: torch.Tensor,</span></span><br><span class="line"><span class="params">    kv_cache: torch.Tensor,</span></span><br><span class="line"><span class="params">    attn_metadata: FlashAttentionMetadata,</span></span><br><span class="line"><span class="params">    output: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Forward pass with FlashAttention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query: shape = [num_tokens, num_heads, head_size]</span></span><br><span class="line"><span class="string">        key: shape = [num_tokens, num_kv_heads, head_size]</span></span><br><span class="line"><span class="string">        value: shape = [num_tokens, num_kv_heads, head_size]</span></span><br><span class="line"><span class="string">        output: shape = [num_tokens, num_heads, head_size]</span></span><br><span class="line"><span class="string">        kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]</span></span><br><span class="line"><span class="string">            NOTE: kv_cache will be an empty tensor with shape [0]</span></span><br><span class="line"><span class="string">            for profiling run.</span></span><br><span class="line"><span class="string">        attn_metadata: Metadata for attention.</span></span><br><span class="line"><span class="string">    NOTE: It in-place updates the output tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># NOTE(woosuk): FlashAttention does not support FP8 KV cache.</span></span><br><span class="line"></span><br><span class="line">    (num_prefill_query_tokens, num_prefill_kv_tokens,</span><br><span class="line">    num_decode_query_tokens) = \</span><br><span class="line">        get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)</span><br><span class="line">    decode_query = query[num_prefill_query_tokens:]</span><br><span class="line">    decode_output = output[num_prefill_query_tokens:]</span><br><span class="line">    <span class="comment"># QKV for prefill.</span></span><br><span class="line">    query = query[:num_prefill_query_tokens]</span><br><span class="line">    prefill_output = output[:num_prefill_query_tokens]</span><br><span class="line">    <span class="keyword">assert</span> query.shape[<span class="number">0</span>] == num_prefill_query_tokens</span><br><span class="line">    <span class="keyword">assert</span> decode_query.shape[<span class="number">0</span>] == num_decode_query_tokens</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> prefill_meta := attn_metadata.prefill_metadata:</span><br><span class="line">        <span class="comment"># Prompt run.</span></span><br><span class="line">        <span class="keyword">if</span> (kv_cache.numel() == <span class="number">0</span> <span class="keyword">or</span> prefill_meta.block_tables <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                <span class="keyword">or</span> prefill_meta.block_tables.numel() == <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># normal attention</span></span><br><span class="line">            <span class="comment"># When block_tables are not filled, it means q and k are the</span></span><br><span class="line">            <span class="comment"># prompt, and they have the same length.</span></span><br><span class="line">            q_seq_start_loc, q_seq_len, k_seq_start_loc, k_seq_len = \</span><br><span class="line">                _get_query_key_seq_metadata(prefill_meta, <span class="literal">True</span>, attn_type)</span><br><span class="line"></span><br><span class="line">            key = key[:num_prefill_kv_tokens]</span><br><span class="line">            value = value[:num_prefill_kv_tokens]</span><br><span class="line"></span><br><span class="line">            flash_attn_varlen_func(</span><br><span class="line">                q=query,</span><br><span class="line">                k=key,</span><br><span class="line">                v=value,</span><br><span class="line">                cu_seqlens_q=q_seq_start_loc,</span><br><span class="line">                cu_seqlens_k=k_seq_start_loc,</span><br><span class="line">                max_seqlen_q=q_seq_len,</span><br><span class="line">                max_seqlen_k=k_seq_len,</span><br><span class="line">                softmax_scale=softmax_scale,</span><br><span class="line">                causal=_get_causal_option(attn_type),</span><br><span class="line">                window_size=window_size,</span><br><span class="line">                alibi_slopes=alibi_slopes,</span><br><span class="line">                softcap=logits_soft_cap,</span><br><span class="line">                out=prefill_output,</span><br><span class="line">                fa_version=self.fa_version,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># prefix-enabled attention</span></span><br><span class="line">            <span class="keyword">assert</span> attn_type == AttentionType.DECODER, (</span><br><span class="line">                <span class="string">&quot;Only decoder-only models support prefix caching&quot;</span>)</span><br><span class="line">            <span class="keyword">assert</span> prefill_meta.seq_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            max_seq_len = <span class="built_in">max</span>(prefill_meta.seq_lens)</span><br><span class="line">            flash_attn_varlen_func(  <span class="comment"># noqa</span></span><br><span class="line">                q=query,</span><br><span class="line">                k=key_cache,</span><br><span class="line">                v=value_cache,</span><br><span class="line">                cu_seqlens_q=prefill_meta.query_start_loc,</span><br><span class="line">                max_seqlen_q=prefill_meta.max_query_len,</span><br><span class="line">                seqused_k=prefill_meta.seq_lens_tensor,</span><br><span class="line">                max_seqlen_k=max_seq_len,</span><br><span class="line">                softmax_scale=softmax_scale,</span><br><span class="line">                causal=<span class="literal">True</span>,</span><br><span class="line">                window_size=window_size,</span><br><span class="line">                alibi_slopes=alibi_slopes,</span><br><span class="line">                block_table=prefill_meta.block_tables,</span><br><span class="line">                softcap=logits_soft_cap,</span><br><span class="line">                out=prefill_output,</span><br><span class="line">                fa_version=self.fa_version,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> decode_meta := attn_metadata.decode_metadata:</span><br><span class="line">        <span class="comment"># Decoding run.</span></span><br><span class="line">        <span class="comment"># Use flash_attn_varlen_func kernel for speculative decoding</span></span><br><span class="line">        <span class="comment"># because different queries might have different lengths.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> decode_meta.max_decode_query_len <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># use only for actual varlen decoding</span></span><br><span class="line">        <span class="keyword">if</span> decode_meta.max_decode_query_len &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">assert</span> attn_type == AttentionType.DECODER, (</span><br><span class="line">                <span class="string">&quot;Only decoder-only models support max_decode_query_len &gt; 1&quot;</span></span><br><span class="line">            )</span><br><span class="line">            flash_attn_varlen_func(</span><br><span class="line">                q=decode_query,</span><br><span class="line">                k=key_cache,</span><br><span class="line">                v=value_cache,</span><br><span class="line">                cu_seqlens_q=decode_meta.query_start_loc,</span><br><span class="line">                max_seqlen_q=decode_meta.max_decode_query_len,</span><br><span class="line">                seqused_k=decode_meta.seq_lens_tensor,</span><br><span class="line">                max_seqlen_k=decode_meta.max_decode_seq_len,</span><br><span class="line">                softmax_scale=softmax_scale,</span><br><span class="line">                causal=<span class="literal">True</span>,</span><br><span class="line">                window_size=window_size,</span><br><span class="line">                alibi_slopes=alibi_slopes,</span><br><span class="line">                softcap=logits_soft_cap,</span><br><span class="line">                block_table=decode_meta.block_tables,</span><br><span class="line">                out=decode_output,</span><br><span class="line">                fa_version=self.fa_version,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Use flash_attn_with_kvcache for normal decoding.</span></span><br><span class="line">            (</span><br><span class="line">                seq_lens_arg,</span><br><span class="line">                _,</span><br><span class="line">                block_tables_arg,</span><br><span class="line">            ) = get_seq_len_block_table_args(decode_meta, <span class="literal">False</span>, attn_type)</span><br><span class="line">            flash_attn_with_kvcache(</span><br><span class="line">                q=decode_query.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                k_cache=key_cache,</span><br><span class="line">                v_cache=value_cache,</span><br><span class="line">                block_table=block_tables_arg,</span><br><span class="line">                cache_seqlens=seq_lens_arg,</span><br><span class="line">                softmax_scale=softmax_scale,</span><br><span class="line">                causal=<span class="literal">True</span>,</span><br><span class="line">                window_size=window_size,</span><br><span class="line">                alibi_slopes=alibi_slopes,</span><br><span class="line">                softcap=logits_soft_cap,</span><br><span class="line">                out=decode_output.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                fa_version=self.fa_version,</span><br><span class="line">            )</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>他上面实际上就是分三部分，首先是kv的reshape: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> kv_cache.numel() &gt; <span class="number">0</span>:</span><br><span class="line">    key_cache = kv_cache[<span class="number">0</span>]</span><br><span class="line">    value_cache = kv_cache[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># We skip updating the KV cache under two conditions:</span></span><br><span class="line">    <span class="comment">#  a. When the Attention Type is ENCODER. In this phase, we compute</span></span><br><span class="line">    <span class="comment">#     only the encoder attention without updating the cache.</span></span><br><span class="line">    <span class="comment">#  b. When both Key and Value are None. This occurs during</span></span><br><span class="line">    <span class="comment">#     cross-attention computation in the decoding phase, where the</span></span><br><span class="line">    <span class="comment">#     KV cache is already populated with the cross-attention</span></span><br><span class="line">    <span class="comment">#     tensor. Thus, we skip cache updates during this time.</span></span><br><span class="line">    <span class="keyword">if</span> (attn_type != AttentionType.ENCODER) <span class="keyword">and</span> (key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (</span><br><span class="line">            value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">        <span class="keyword">if</span> attn_type == AttentionType.ENCODER_DECODER:</span><br><span class="line">            <span class="comment"># Update cross-attention KV cache (prefill-only)</span></span><br><span class="line">            updated_slot_mapping = attn_metadata.cross_slot_mapping</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Update self-attention KV cache (prefill/decode)</span></span><br><span class="line">            updated_slot_mapping = attn_metadata.slot_mapping</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Reshape the input keys and values and store them in the cache.</span></span><br><span class="line">        <span class="comment"># If kv_cache is not provided, the new key and value tensors are</span></span><br><span class="line">        <span class="comment"># not cached. This happens during the initial memory</span></span><br><span class="line">        <span class="comment"># profiling run.</span></span><br><span class="line">        torch.ops._C_cache_ops.reshape_and_cache_flash(</span><br><span class="line">            key,</span><br><span class="line">            value,</span><br><span class="line">            kv_cache[<span class="number">0</span>],</span><br><span class="line">            kv_cache[<span class="number">1</span>],</span><br><span class="line">            updated_slot_mapping.flatten(),  <span class="comment"># type: ignore[union-attr]</span></span><br><span class="line">            kv_cache_dtype,</span><br><span class="line">            layer._k_scale,</span><br><span class="line">            layer._v_scale,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">(num_prefill_query_tokens, num_prefill_kv_tokens,</span><br><span class="line">num_decode_query_tokens) = \</span><br><span class="line">    get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)</span><br><span class="line">decode_query = query[num_prefill_query_tokens:]</span><br><span class="line">decode_output = output[num_prefill_query_tokens:]</span><br><span class="line"><span class="comment"># QKV for prefill.</span></span><br><span class="line">query = query[:num_prefill_query_tokens]</span><br><span class="line">prefill_output = output[:num_prefill_query_tokens]</span><br><span class="line"><span class="keyword">assert</span> query.shape[<span class="number">0</span>] == num_prefill_query_tokens</span><br><span class="line"><span class="keyword">assert</span> decode_query.shape[<span class="number">0</span>] == num_decode_query_tokens</span><br></pre></td></tr></table></figure></p>
<p>然后根据场景走prefill和decode的flash attn,这里我主要关注decode部分。
在reshape
kv的过程中，他的updated_slot_mapping是<code>[54]</code>，而实际上当前的seq_lens_arg是<code>[55]</code>,
block_tables_arg为<code>[1,2,3,4]</code>。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> decode_meta := attn_metadata.decode_metadata:</span><br><span class="line">    <span class="comment"># Decoding run.</span></span><br><span class="line">    <span class="comment"># Use flash_attn_varlen_func kernel for speculative decoding</span></span><br><span class="line">    <span class="comment"># because different queries might have different lengths.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> decode_meta.max_decode_query_len <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    <span class="comment"># use only for actual varlen decoding</span></span><br><span class="line">    <span class="keyword">if</span> decode_meta.max_decode_query_len &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">assert</span> attn_type == AttentionType.DECODER, (</span><br><span class="line">            <span class="string">&quot;Only decoder-only models support max_decode_query_len &gt; 1&quot;</span></span><br><span class="line">        )</span><br><span class="line">        flash_attn_varlen_func(</span><br><span class="line">            q=decode_query,</span><br><span class="line">            k=key_cache,</span><br><span class="line">            v=value_cache,</span><br><span class="line">            cu_seqlens_q=decode_meta.query_start_loc,</span><br><span class="line">            max_seqlen_q=decode_meta.max_decode_query_len,</span><br><span class="line">            seqused_k=decode_meta.seq_lens_tensor,</span><br><span class="line">            max_seqlen_k=decode_meta.max_decode_seq_len,</span><br><span class="line">            softmax_scale=softmax_scale,</span><br><span class="line">            causal=<span class="literal">True</span>,</span><br><span class="line">            window_size=window_size,</span><br><span class="line">            alibi_slopes=alibi_slopes,</span><br><span class="line">            softcap=logits_soft_cap,</span><br><span class="line">            block_table=decode_meta.block_tables,</span><br><span class="line">            out=decode_output,</span><br><span class="line">            fa_version=self.fa_version,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Use flash_attn_with_kvcache for normal decoding.</span></span><br><span class="line">        (</span><br><span class="line">            seq_lens_arg,</span><br><span class="line">            _,</span><br><span class="line">            block_tables_arg,</span><br><span class="line">        ) = get_seq_len_block_table_args(decode_meta, <span class="literal">False</span>, attn_type)</span><br><span class="line">        flash_attn_with_kvcache(</span><br><span class="line">            q=decode_query.unsqueeze(<span class="number">1</span>),</span><br><span class="line">            k_cache=key_cache,</span><br><span class="line">            v_cache=value_cache,</span><br><span class="line">            block_table=block_tables_arg,</span><br><span class="line">            cache_seqlens=seq_lens_arg,</span><br><span class="line">            softmax_scale=softmax_scale,</span><br><span class="line">            causal=<span class="literal">True</span>,</span><br><span class="line">            window_size=window_size,</span><br><span class="line">            alibi_slopes=alibi_slopes,</span><br><span class="line">            softcap=logits_soft_cap,</span><br><span class="line">            out=decode_output.unsqueeze(<span class="number">1</span>),</span><br><span class="line">            fa_version=self.fa_version,</span><br><span class="line">        )</span><br><span class="line"><span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>目前调用的是flash attn 2: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">flash_attn_with_kvcache</span>(<span class="params"></span></span><br><span class="line"><span class="params">    q,</span></span><br><span class="line"><span class="params">    k_cache,</span></span><br><span class="line"><span class="params">    v_cache,</span></span><br><span class="line"><span class="params">    k=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    v=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    rotary_cos=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    rotary_sin=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    cache_seqlens: <span class="type">Optional</span>[<span class="type">Union</span>[(<span class="params"><span class="built_in">int</span>, torch.Tensor</span>)]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    cache_batch_idx: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    cache_leftpad: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    block_table: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    softmax_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    causal=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    window_size=(<span class="params">-<span class="number">1</span>, -<span class="number">1</span></span>),  <span class="comment"># -1 means infinite context window</span></span></span><br><span class="line"><span class="params">    softcap=<span class="number">0.0</span>, <span class="comment"># 0.0 means deactivated</span></span></span><br><span class="line"><span class="params">    rotary_interleaved=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    alibi_slopes=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    num_splits=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">    return_softmax_lse=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    *,</span></span><br><span class="line"><span class="params">    out=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    fa_version: <span class="built_in">int</span> = DEFAULT_FA_VERSION,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    If k and v are not None, k_cache and v_cache will be updated *inplace* with the new values from</span></span><br><span class="line"><span class="string">    k and v. This is useful for incremental decoding: you can pass in the cached keys/values from</span></span><br><span class="line"><span class="string">    the previous step, and update them with the new keys/values from the current step, and do</span></span><br><span class="line"><span class="string">    attention with the updated cache, all in 1 kernel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If you pass in k / v, you must make sure that the cache is large enough to hold the new values.</span></span><br><span class="line"><span class="string">    For example, the KV cache could be pre-allocated with the max sequence length, and you can use</span></span><br><span class="line"><span class="string">    cache_seqlens to keep track of the current sequence lengths of each sequence in the batch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Also apply rotary embedding if rotary_cos and rotary_sin are passed in. The key @k will be</span></span><br><span class="line"><span class="string">    rotated by rotary_cos and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.</span></span><br><span class="line"><span class="string">    If causal or local (i.e., window_size != (-1, -1)), the query @q will be rotated by rotary_cos</span></span><br><span class="line"><span class="string">    and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.</span></span><br><span class="line"><span class="string">    If not causal and not local, the query @q will be rotated by rotary_cos and rotary_sin at</span></span><br><span class="line"><span class="string">    indices cache_seqlens only (i.e. we consider all tokens in @q to be at position cache_seqlens).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    See tests/test_flash_attn.py::test_flash_attn_kvcache for examples of how to use this function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads</span></span><br><span class="line"><span class="string">    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.</span></span><br><span class="line"><span class="string">    For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head</span></span><br><span class="line"><span class="string">    0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.</span></span><br><span class="line"><span class="string">    For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:</span></span><br><span class="line"><span class="string">        1 1 1 1 0</span></span><br><span class="line"><span class="string">        1 1 1 1 1</span></span><br><span class="line"><span class="string">    If seqlen_q = 5 and seqlen_k = 2, the causal mask is:</span></span><br><span class="line"><span class="string">        0 0</span></span><br><span class="line"><span class="string">        0 0</span></span><br><span class="line"><span class="string">        0 0</span></span><br><span class="line"><span class="string">        1 0</span></span><br><span class="line"><span class="string">        1 1</span></span><br><span class="line"><span class="string">    If the row of the mask is all zero, the output will be zero.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If window_size != (-1, -1), implements sliding window local attention. Query at position i</span></span><br><span class="line"><span class="string">    will only attend to keys between</span></span><br><span class="line"><span class="string">    [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: Does not support backward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        q: (batch_size, seqlen, nheads, headdim)</span></span><br><span class="line"><span class="string">        k_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there&#x27;s no block_table,</span></span><br><span class="line"><span class="string">            or (num_blocks, page_block_size, nheads_k, headdim) if there&#x27;s a block_table (i.e. paged KV cache)</span></span><br><span class="line"><span class="string">            page_block_size must be a multiple of 256.</span></span><br><span class="line"><span class="string">        v_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there&#x27;s no block_table,</span></span><br><span class="line"><span class="string">            or (num_blocks, page_block_size, nheads_k, headdim) if there&#x27;s a block_table (i.e. paged KV cache)</span></span><br><span class="line"><span class="string">        k [optional]: (batch_size, seqlen_new, nheads_k, headdim). If not None, we concatenate</span></span><br><span class="line"><span class="string">            k with k_cache, starting at the indices specified by cache_seqlens.</span></span><br><span class="line"><span class="string">        v [optional]: (batch_size, seqlen_new, nheads_k, headdim). Similar to k.</span></span><br><span class="line"><span class="string">        rotary_cos [optional]: (seqlen_ro, rotary_dim / 2). If not None, we apply rotary embedding</span></span><br><span class="line"><span class="string">            to k and q. Only applicable if k and v are passed in. rotary_dim must be divisible by 16.</span></span><br><span class="line"><span class="string">        rotary_sin [optional]: (seqlen_ro, rotary_dim / 2). Similar to rotary_cos.</span></span><br><span class="line"><span class="string">        cache_seqlens: int, or (batch_size,), dtype torch.int32. The sequence lengths of the</span></span><br><span class="line"><span class="string">            KV cache.</span></span><br><span class="line"><span class="string">        block_table [optional]: (batch_size, max_num_blocks_per_seq), dtype torch.int32.</span></span><br><span class="line"><span class="string">        cache_batch_idx: (batch_size,), dtype torch.int32. The indices used to index into the KV cache.</span></span><br><span class="line"><span class="string">            If None, we assume that the batch indices are [0, 1, 2, ..., batch_size - 1].</span></span><br><span class="line"><span class="string">            If the indices are not distinct, and k and v are provided, the values updated in the cache</span></span><br><span class="line"><span class="string">                 might come from any of the duplicate indices.</span></span><br><span class="line"><span class="string">        softmax_scale: float. The scaling of QK^T before applying softmax.</span></span><br><span class="line"><span class="string">            Default to 1 / sqrt(headdim).</span></span><br><span class="line"><span class="string">        causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).</span></span><br><span class="line"><span class="string">        window_size: (left, right). If not (-1, -1), implements sliding window local attention.</span></span><br><span class="line"><span class="string">        softcap: float. Anything &gt; 0 activates softcapping attention.</span></span><br><span class="line"><span class="string">        rotary_interleaved: bool. Only applicable if rotary_cos and rotary_sin are passed in.</span></span><br><span class="line"><span class="string">            If True, rotary embedding will combine dimensions 0 &amp; 1, 2 &amp; 3, etc. If False,</span></span><br><span class="line"><span class="string">            rotary embedding will combine dimensions 0 &amp; rotary_dim / 2, 1 &amp; rotary_dim / 2 + 1</span></span><br><span class="line"><span class="string">            (i.e. GPT-NeoX style).</span></span><br><span class="line"><span class="string">        alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of</span></span><br><span class="line"><span class="string">            (-alibi_slope * |i + seqlen_k - seqlen_q - j|)</span></span><br><span class="line"><span class="string">            is added to the attention score of query i and key j.</span></span><br><span class="line"><span class="string">        num_splits: int. If &gt; 1, split the key/value into this many chunks along the sequence.</span></span><br><span class="line"><span class="string">           If num_splits == 1, we don&#x27;t split the key/value. If num_splits == 0, we use a heuristic</span></span><br><span class="line"><span class="string">           to automatically determine the number of splits.</span></span><br><span class="line"><span class="string">           Don&#x27;t change this unless you know what you are doing.</span></span><br><span class="line"><span class="string">        return_softmax_lse: bool. Whether to return the logsumexp of the attention scores.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        out: (batch_size, seqlen, nheads, headdim).</span></span><br><span class="line"><span class="string">        softmax_lse [optional, if return_softmax_lse=True]: (batch_size, nheads, seqlen). The</span></span><br><span class="line"><span class="string">            logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax</span></span><br><span class="line"><span class="string">            normalization factor).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> k_cache.stride(-<span class="number">1</span>) == <span class="number">1</span>, <span class="string">&quot;k_cache must have contiguous last dimension&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> v_cache.stride(-<span class="number">1</span>) == <span class="number">1</span>, <span class="string">&quot;v_cache must have contiguous last dimension&quot;</span></span><br><span class="line">    q, k, v = [maybe_contiguous(x) <span class="keyword">for</span> x <span class="keyword">in</span> (q, k, v)]</span><br><span class="line">    <span class="keyword">if</span> softmax_scale <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        softmax_scale = q.shape[-<span class="number">1</span>] ** (-<span class="number">0.5</span>)</span><br><span class="line">    <span class="keyword">if</span> cache_seqlens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(cache_seqlens, <span class="built_in">int</span>):</span><br><span class="line">        cache_seqlens = torch.full(</span><br><span class="line">            (k_cache.shape[<span class="number">0</span>],), cache_seqlens, dtype=torch.int32, device=k_cache.device</span><br><span class="line">        )</span><br><span class="line">        cache_seqlens = maybe_contiguous(cache_seqlens)</span><br><span class="line">    cache_batch_idx = maybe_contiguous(cache_batch_idx)</span><br><span class="line">    block_table = maybe_contiguous(block_table)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> fa_version == <span class="number">2</span>:</span><br><span class="line">        out, softmax_lse = torch.ops._vllm_fa2_C.fwd_kvcache(</span><br><span class="line">            q, k_cache, v_cache,</span><br><span class="line">            k, v,             <span class="comment"># k_new, v_new</span></span><br><span class="line">            cache_seqlens,</span><br><span class="line">            rotary_cos,</span><br><span class="line">            rotary_sin,</span><br><span class="line">            cache_batch_idx,</span><br><span class="line">            cache_leftpad,</span><br><span class="line">            block_table,</span><br><span class="line">            alibi_slopes,</span><br><span class="line">            out,</span><br><span class="line">            softmax_scale,</span><br><span class="line">            causal,</span><br><span class="line">            window_size[<span class="number">0</span>],</span><br><span class="line">            window_size[<span class="number">1</span>],</span><br><span class="line">            softcap,</span><br><span class="line">            rotary_interleaved,</span><br><span class="line">            num_splits,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> (out, softmax_lse) <span class="keyword">if</span> return_softmax_lse <span class="keyword">else</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>其中这个<code>torch.ops._vllm_fa2_C.fwd_kvcache</code>实际上位于vllm的flash
attention的<code>csrc/flash_attn/flash_api_torch_lib.cpp</code>。
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;at::Tensor&gt;</span></span><br><span class="line"><span class="function"><span class="title">mha_fwd_kvcache</span><span class="params">(at::Tensor &amp;q,                 <span class="comment">// batch_size x seqlen_q x num_heads x head_size</span></span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">const</span> at::Tensor &amp;kcache,            <span class="comment">// batch_size_c x seqlen_k x num_heads_k x head_size or num_blocks x page_block_size x num_heads_k x head_size if there&#x27;s a block_table.</span></span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">const</span> at::Tensor &amp;vcache,            <span class="comment">// batch_size_c x seqlen_k x num_heads_k x head_size or num_blocks x page_block_size x num_heads_k x head_size if there&#x27;s a block_table.</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;<span class="type">const</span> at::Tensor&gt; &amp;k_, <span class="comment">// batch_size x seqlen_knew x num_heads_k x head_size</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;<span class="type">const</span> at::Tensor&gt; &amp;v_, <span class="comment">// batch_size x seqlen_knew x num_heads_k x head_size</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;<span class="type">const</span> at::Tensor&gt; &amp;seqlens_k_, <span class="comment">// batch_size</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;<span class="type">const</span> at::Tensor&gt; &amp;rotary_cos_, <span class="comment">// seqlen_ro x (rotary_dim / 2)</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;<span class="type">const</span> at::Tensor&gt; &amp;rotary_sin_, <span class="comment">// seqlen_ro x (rotary_dim / 2)</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;<span class="type">const</span> at::Tensor&gt; &amp;cache_batch_idx_, <span class="comment">// indices to index into the KV cache</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;<span class="type">const</span> at::Tensor&gt; &amp;leftpad_k_, <span class="comment">// batch_size</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;at::Tensor&gt; &amp;block_table_, <span class="comment">// batch_size x max_num_blocks_per_seq</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;at::Tensor&gt; &amp;alibi_slopes_, <span class="comment">// num_heads or batch_size x num_heads</span></span></span></span><br><span class="line"><span class="params"><span class="function">                std::optional&lt;at::Tensor&gt; &amp;out_,             <span class="comment">// batch_size x seqlen_q x num_heads x head_size</span></span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">const</span> <span class="type">float</span> softmax_scale,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">bool</span> is_causal,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> window_size_left,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> window_size_right,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">const</span> <span class="type">float</span> softcap,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">bool</span> is_rotary_interleaved,   <span class="comment">// if true, rotary combines indices 0 &amp; 1, else indices 0 &amp; rotary_dim / 2</span></span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> num_splits</span></span></span><br><span class="line"><span class="params"><span class="function">                )</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Otherwise the kernel will be launched from cuda:0 device</span></span><br><span class="line">    at::cuda::CUDAGuard device_guard&#123;q.<span class="built_in">device</span>()&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> [cc_major, cc_minor] = <span class="built_in">get_compute_capability</span>(<span class="built_in">get_current_device</span>());</span><br><span class="line">    <span class="type">bool</span> is_sm8x_min = cc_major &gt;= <span class="number">8</span>;</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(is_sm8x_min, <span class="string">&quot;FlashAttention only supports Ampere GPUs or newer.&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> q_dtype = q.<span class="built_in">dtype</span>();</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(q_dtype == torch::kFloat16 || q_dtype == torch::kBFloat16,</span><br><span class="line">                <span class="string">&quot;FlashAttention only support fp16 and bf16 data type&quot;</span>);</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(kcache.<span class="built_in">dtype</span>() == q_dtype, <span class="string">&quot;query and key must have the same dtype&quot;</span>);</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(vcache.<span class="built_in">dtype</span>() == q_dtype, <span class="string">&quot;query and value must have the same dtype&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_DEVICE</span>(q); <span class="built_in">CHECK_DEVICE</span>(kcache); <span class="built_in">CHECK_DEVICE</span>(vcache);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(q.<span class="built_in">stride</span>(<span class="number">-1</span>) == <span class="number">1</span>, <span class="string">&quot;Input tensor must have contiguous last dimension&quot;</span>);</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(kcache.<span class="built_in">stride</span>(<span class="number">-1</span>) == <span class="number">1</span>, <span class="string">&quot;Input tensor must have contiguous last dimension&quot;</span>);</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(vcache.<span class="built_in">stride</span>(<span class="number">-1</span>) == <span class="number">1</span>, <span class="string">&quot;Input tensor must have contiguous last dimension&quot;</span>);</span><br><span class="line"></span><br><span class="line">    at::Tensor block_table;</span><br><span class="line">    <span class="type">const</span> <span class="type">bool</span> paged_KV = block_table_.<span class="built_in">has_value</span>();</span><br><span class="line">    <span class="keyword">if</span> (paged_KV) &#123;</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(!cache_batch_idx_.<span class="built_in">has_value</span>(), <span class="string">&quot;Paged KVcache does not support cache_batch_idx&quot;</span>);</span><br><span class="line">        block_table = block_table_.<span class="built_in">value</span>();</span><br><span class="line">        <span class="built_in">CHECK_DEVICE</span>(block_table);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(block_table.<span class="built_in">dtype</span>() == torch::kInt32, <span class="string">&quot;block_table must have dtype torch.int32&quot;</span>);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(block_table.<span class="built_in">stride</span>(<span class="number">-1</span>) == <span class="number">1</span>, <span class="string">&quot;block_table must have contiguous last dimension&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> sizes = q.<span class="built_in">sizes</span>();</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> batch_size = sizes[<span class="number">0</span>];</span><br><span class="line">    <span class="type">int</span> seqlen_q = sizes[<span class="number">1</span>];</span><br><span class="line">    <span class="type">int</span> num_heads = sizes[<span class="number">2</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> head_size_og = sizes[<span class="number">3</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> seqlen_q_og = seqlen_q;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_heads_og = num_heads;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> max_num_blocks_per_seq = !paged_KV ? <span class="number">0</span> : block_table.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_blocks = !paged_KV ? <span class="number">0</span> : kcache.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> page_block_size = !paged_KV ? <span class="number">1</span> : kcache.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(!paged_KV || page_block_size % <span class="number">16</span> == <span class="number">0</span>, <span class="string">&quot;Paged KV cache block size must be divisible by 16&quot;</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> seqlen_k = !paged_KV ? kcache.<span class="built_in">size</span>(<span class="number">1</span>) : max_num_blocks_per_seq * page_block_size;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_heads_k = kcache.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> batch_size_c = !paged_KV ? kcache.<span class="built_in">size</span>(<span class="number">0</span>) : batch_size;</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(batch_size &gt; <span class="number">0</span>, <span class="string">&quot;batch size must be positive&quot;</span>);</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(head_size_og &lt;= <span class="number">256</span>, <span class="string">&quot;FlashAttention forward only supports head dimension at most 256&quot;</span>);</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(num_heads % num_heads_k == <span class="number">0</span>, <span class="string">&quot;Number of heads in key/value must divide number of heads in query&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// causal=true is the same as causal=false in this case</span></span><br><span class="line">    <span class="keyword">if</span> (seqlen_q == <span class="number">1</span> &amp;&amp; !alibi_slopes_.<span class="built_in">has_value</span>()) &#123; is_causal = <span class="literal">false</span>; &#125;</span><br><span class="line">    <span class="keyword">if</span> (is_causal) &#123; window_size_right = <span class="number">0</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Faster to transpose q from (b, 1, (nheads_kv ngroups), d) to (b, ngroups, nheads_kv, d) in this case</span></span><br><span class="line">    <span class="comment">// H/t Daniel Haziza</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> seqlenq_ngroups_swapped = seqlen_q == <span class="number">1</span> &amp;&amp; num_heads &gt; num_heads_k &amp;&amp; window_size_left &lt; <span class="number">0</span> &amp;&amp; window_size_right &lt; <span class="number">0</span> &amp;&amp; head_size_og % <span class="number">8</span> == <span class="number">0</span> &amp;&amp; !alibi_slopes_.<span class="built_in">has_value</span>();</span><br><span class="line">    <span class="keyword">if</span> (seqlenq_ngroups_swapped) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> ngroups = num_heads / num_heads_k;</span><br><span class="line">        q = q.<span class="built_in">reshape</span>(&#123;batch_size, num_heads_k, ngroups, head_size_og&#125;).<span class="built_in">transpose</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">        seqlen_q = ngroups;</span><br><span class="line">        num_heads = num_heads_k;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (window_size_left &gt;= seqlen_k) &#123; window_size_left = <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="keyword">if</span> (window_size_right &gt;= seqlen_k) &#123; window_size_right = <span class="number">-1</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_SHAPE</span>(q, batch_size, seqlen_q, num_heads, head_size_og);</span><br><span class="line">    <span class="keyword">if</span> (!paged_KV) &#123;</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(kcache, batch_size_c, seqlen_k, num_heads_k, head_size_og);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(vcache, batch_size_c, seqlen_k, num_heads_k, head_size_og);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(kcache, num_blocks, page_block_size, num_heads_k, head_size_og);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(vcache, num_blocks, page_block_size, num_heads_k, head_size_og);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(block_table, batch_size, max_num_blocks_per_seq);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    at::Tensor q_padded, kcache_padded, vcache_padded;</span><br><span class="line">    <span class="keyword">if</span> (head_size_og % <span class="number">8</span> != <span class="number">0</span>) &#123;</span><br><span class="line">        q_padded = torch::nn::functional::<span class="built_in">pad</span>(q, torch::nn::functional::<span class="built_in">PadFuncOptions</span>(&#123;<span class="number">0</span>, <span class="number">8</span> - head_size_og % <span class="number">8</span>&#125;));</span><br><span class="line">        kcache_padded = torch::nn::functional::<span class="built_in">pad</span>(kcache, torch::nn::functional::<span class="built_in">PadFuncOptions</span>(&#123;<span class="number">0</span>, <span class="number">8</span> - head_size_og % <span class="number">8</span>&#125;));</span><br><span class="line">        vcache_padded = torch::nn::functional::<span class="built_in">pad</span>(vcache, torch::nn::functional::<span class="built_in">PadFuncOptions</span>(&#123;<span class="number">0</span>, <span class="number">8</span> - head_size_og % <span class="number">8</span>&#125;));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        q_padded = q;</span><br><span class="line">        kcache_padded = kcache;</span><br><span class="line">        vcache_padded = vcache;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    at::Tensor out;</span><br><span class="line">    <span class="keyword">if</span> (out_.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">        out = out_.<span class="built_in">value</span>();</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(out.<span class="built_in">dtype</span>() == q_dtype, <span class="string">&quot;Output must have the same dtype as inputs&quot;</span>);</span><br><span class="line">        <span class="built_in">CHECK_DEVICE</span>(out);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(out.<span class="built_in">stride</span>(<span class="number">-1</span>) == <span class="number">1</span>, <span class="string">&quot;Output tensor must have contiguous last dimension&quot;</span>);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(out, batch_size, seqlen_q_og, num_heads_og, head_size_og);</span><br><span class="line">        <span class="keyword">if</span> (head_size_og % <span class="number">8</span> != <span class="number">0</span>) &#123;</span><br><span class="line">            out = torch::<span class="built_in">empty_like</span>(q_padded);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (seqlenq_ngroups_swapped) &#123;</span><br><span class="line">            out = out.<span class="built_in">reshape</span>(&#123;batch_size, num_heads, seqlen_q, head_size_og&#125;).<span class="built_in">transpose</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        out = torch::<span class="built_in">empty_like</span>(q_padded);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> round_multiple = [](<span class="type">int</span> x, <span class="type">int</span> m) &#123; <span class="built_in">return</span> (x + m - <span class="number">1</span>) / m * m; &#125;;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> head_size = <span class="built_in">round_multiple</span>(head_size_og, <span class="number">8</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> head_size_rounded = head_size &lt;= <span class="number">192</span> ? <span class="built_in">round_multiple</span>(head_size, <span class="number">32</span>) : <span class="number">256</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> seqlen_q_rounded = <span class="built_in">round_multiple</span>(seqlen_q, <span class="number">128</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> seqlen_k_rounded = <span class="built_in">round_multiple</span>(seqlen_k, <span class="number">128</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> opts = q.<span class="built_in">options</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> softmax_lse = torch::<span class="built_in">empty</span>(&#123;batch_size, num_heads, seqlen_q&#125;, opts.<span class="built_in">dtype</span>(at::kFloat));</span><br><span class="line"></span><br><span class="line">    Flash_fwd_params params;</span><br><span class="line">    <span class="built_in">set_params_fprop</span>(params,</span><br><span class="line">                     batch_size,</span><br><span class="line">                     seqlen_q, seqlen_k,</span><br><span class="line">                     seqlen_q_rounded, seqlen_k_rounded,</span><br><span class="line">                     num_heads, num_heads_k,</span><br><span class="line">                     head_size, head_size_rounded,</span><br><span class="line">                     q_padded, kcache_padded, vcache_padded, out,</span><br><span class="line">                     <span class="comment">/*cu_seqlens_q_d=*/</span><span class="literal">nullptr</span>,</span><br><span class="line">                     <span class="comment">/*cu_seqlens_k_d=*/</span><span class="literal">nullptr</span>,</span><br><span class="line">                     <span class="comment">/*seqused_k=*/</span><span class="literal">nullptr</span>,</span><br><span class="line">                     <span class="comment">/*p_ptr=*/</span><span class="literal">nullptr</span>,</span><br><span class="line">                     softmax_lse.<span class="built_in">data_ptr</span>(),</span><br><span class="line">                     <span class="comment">/*p_dropout=*/</span><span class="number">0.f</span>,</span><br><span class="line">                     softmax_scale,</span><br><span class="line">                     window_size_left,</span><br><span class="line">                     window_size_right,</span><br><span class="line">                     softcap</span><br><span class="line">                     );</span><br><span class="line"></span><br><span class="line">    at::Tensor k, v, k_padded, v_padded;</span><br><span class="line">    <span class="keyword">if</span> (k_.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(v_.<span class="built_in">has_value</span>(), <span class="string">&quot;If key is supplied, value must also be passed in&quot;</span>);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(seqlens_k_.<span class="built_in">has_value</span>(), <span class="string">&quot;If key is supplied, seqlens_k must also be passed in&quot;</span>);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(seqlen_q &lt;= seqlen_k, <span class="string">&quot;If key is supplied, it must have seqlen &lt;= the seqlen of the KV cache&quot;</span>);</span><br><span class="line">        k = k_.<span class="built_in">value</span>();</span><br><span class="line">        v = v_.<span class="built_in">value</span>();</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(k.<span class="built_in">dtype</span>() == q_dtype, <span class="string">&quot;Key must have the same dtype as query&quot;</span>);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(v.<span class="built_in">dtype</span>() == q_dtype, <span class="string">&quot;Value must have the same dtype as query&quot;</span>);</span><br><span class="line">        <span class="built_in">CHECK_DEVICE</span>(k); <span class="built_in">CHECK_DEVICE</span>(v);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(k.<span class="built_in">stride</span>(<span class="number">-1</span>) == <span class="number">1</span>, <span class="string">&quot;Key tensor must have contiguous last dimension&quot;</span>);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(v.<span class="built_in">stride</span>(<span class="number">-1</span>) == <span class="number">1</span>, <span class="string">&quot;Value tensor must have contiguous last dimension&quot;</span>);</span><br><span class="line">        <span class="type">int</span> seqlen_knew = k.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(k, batch_size, seqlen_knew, num_heads_k, head_size_og);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(v, batch_size, seqlen_knew, num_heads_k, head_size_og);</span><br><span class="line">        <span class="keyword">if</span> (head_size_og % <span class="number">8</span> != <span class="number">0</span>) &#123;</span><br><span class="line">            k_padded = torch::nn::functional::<span class="built_in">pad</span>(k, torch::nn::functional::<span class="built_in">PadFuncOptions</span>(&#123;<span class="number">0</span>, <span class="number">8</span> - head_size_og % <span class="number">8</span>&#125;));</span><br><span class="line">            v_padded = torch::nn::functional::<span class="built_in">pad</span>(v, torch::nn::functional::<span class="built_in">PadFuncOptions</span>(&#123;<span class="number">0</span>, <span class="number">8</span> - head_size_og % <span class="number">8</span>&#125;));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            k_padded = k;</span><br><span class="line">            v_padded = v;</span><br><span class="line">        &#125;</span><br><span class="line">        params.seqlen_knew = seqlen_knew;</span><br><span class="line">        params.knew_ptr = k_padded.<span class="built_in">data_ptr</span>();</span><br><span class="line">        params.vnew_ptr = v_padded.<span class="built_in">data_ptr</span>();</span><br><span class="line">        <span class="comment">// All stride are in elements, not bytes.</span></span><br><span class="line">        params.knew_batch_stride = k_padded.<span class="built_in">stride</span>(<span class="number">0</span>);</span><br><span class="line">        params.vnew_batch_stride = v_padded.<span class="built_in">stride</span>(<span class="number">0</span>);</span><br><span class="line">        params.knew_row_stride = k_padded.<span class="built_in">stride</span>(<span class="number">-3</span>);</span><br><span class="line">        params.vnew_row_stride = v_padded.<span class="built_in">stride</span>(<span class="number">-3</span>);</span><br><span class="line">        params.knew_head_stride = k_padded.<span class="built_in">stride</span>(<span class="number">-2</span>);</span><br><span class="line">        params.vnew_head_stride = v_padded.<span class="built_in">stride</span>(<span class="number">-2</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (seqlens_k_.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">        <span class="keyword">auto</span> seqlens_k = seqlens_k_.<span class="built_in">value</span>();</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(seqlens_k.<span class="built_in">dtype</span>() == torch::kInt32, <span class="string">&quot;seqlens_k must have dtype int32&quot;</span>);</span><br><span class="line">        <span class="built_in">CHECK_DEVICE</span>(seqlens_k);</span><br><span class="line">        <span class="built_in">CHECK_CONTIGUOUS</span>(seqlens_k);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(seqlens_k, batch_size);</span><br><span class="line">        params.cu_seqlens_k = <span class="built_in">static_cast</span>&lt;<span class="type">int</span> *&gt;(seqlens_k.<span class="built_in">data_ptr</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    params.is_seqlens_k_cumulative = !(seqlens_k_.<span class="built_in">has_value</span>());</span><br><span class="line">    <span class="keyword">if</span> (leftpad_k_.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(!paged_KV, <span class="string">&quot;We don&#x27;t support Paged KV and leftpad_k running at the same time yet&quot;</span>);</span><br><span class="line">        <span class="keyword">auto</span> leftpad_k = leftpad_k_.<span class="built_in">value</span>();</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(leftpad_k.<span class="built_in">dtype</span>() == torch::kInt32, <span class="string">&quot;leftpad_k must have dtype int32&quot;</span>);</span><br><span class="line">        <span class="built_in">CHECK_DEVICE</span>(leftpad_k);</span><br><span class="line">        <span class="built_in">CHECK_CONTIGUOUS</span>(leftpad_k);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(leftpad_k, batch_size);</span><br><span class="line">        params.leftpad_k = <span class="built_in">static_cast</span>&lt;<span class="type">int</span> *&gt;(leftpad_k.<span class="built_in">data_ptr</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (rotary_cos_.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(k_.<span class="built_in">has_value</span>(), <span class="string">&quot;If rotary cos/sin are provided, new key / value to be appended to KV cache must also be provided&quot;</span>);</span><br><span class="line">        <span class="keyword">auto</span> rotary_cos = rotary_cos_.<span class="built_in">value</span>();</span><br><span class="line">        <span class="built_in">CHECK_DEVICE</span>(rotary_cos);</span><br><span class="line">        params.rotary_dim = rotary_cos.<span class="built_in">size</span>(<span class="number">1</span>) * <span class="number">2</span>;</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(params.rotary_dim &lt;= head_size, <span class="string">&quot;rotary_dim must be &lt;= headdim&quot;</span>);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(params.rotary_dim % <span class="number">16</span> == <span class="number">0</span>, <span class="string">&quot;Only rotary dimensions divisible by 16 are currently supported&quot;</span>);</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> seqlen_ro = rotary_cos.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(seqlen_ro &gt;= seqlen_k, <span class="string">&quot;cos/sin seqlen must be at least the seqlen of KV cache&quot;</span>);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(rotary_cos, seqlen_ro, params.rotary_dim / <span class="number">2</span>);</span><br><span class="line">        <span class="built_in">CHECK_CONTIGUOUS</span>(rotary_cos);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(rotary_cos.<span class="built_in">scalar_type</span>() == q_dtype, <span class="string">&quot;rotary_cos must have the same dtype as query&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(rotary_sin_.<span class="built_in">has_value</span>(), <span class="string">&quot;If rotary cos is provided, rotary sin must also be provided&quot;</span>);</span><br><span class="line">        <span class="keyword">auto</span> rotary_sin = rotary_sin_.<span class="built_in">value</span>();</span><br><span class="line">        <span class="built_in">CHECK_DEVICE</span>(rotary_sin);</span><br><span class="line">        <span class="built_in">CHECK_SHAPE</span>(rotary_sin, seqlen_ro, params.rotary_dim / <span class="number">2</span>);</span><br><span class="line">        <span class="built_in">CHECK_CONTIGUOUS</span>(rotary_sin);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(rotary_sin.<span class="built_in">scalar_type</span>() == q_dtype, <span class="string">&quot;rotary_cos must have the same dtype as query&quot;</span>);</span><br><span class="line">        params.rotary_cos_ptr = rotary_cos.<span class="built_in">data_ptr</span>();</span><br><span class="line">        params.rotary_sin_ptr = rotary_sin.<span class="built_in">data_ptr</span>();</span><br><span class="line">        params.is_rotary_interleaved = is_rotary_interleaved;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        params.rotary_dim = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cache_batch_idx_.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">        <span class="keyword">auto</span> cache_batch_idx = cache_batch_idx_.<span class="built_in">value</span>();</span><br><span class="line">        <span class="built_in">CHECK_DEVICE</span>(cache_batch_idx);</span><br><span class="line">        <span class="built_in">CHECK_CONTIGUOUS</span>(cache_batch_idx);</span><br><span class="line">        <span class="built_in">TORCH_CHECK</span>(cache_batch_idx.<span class="built_in">scalar_type</span>() == torch::kInt32, <span class="string">&quot;cache_batch_idx must have dtype int32&quot;</span>);</span><br><span class="line">        params.cache_batch_idx = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">int</span> *&gt;(cache_batch_idx.<span class="built_in">data_ptr</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Keep references to these tensors to extend their lifetime</span></span><br><span class="line">    at::Tensor softmax_lse_accum, out_accum;</span><br><span class="line">    std::<span class="built_in">tie</span>(softmax_lse_accum, out_accum) = <span class="built_in">set_params_splitkv</span>(</span><br><span class="line">        params, batch_size, num_heads, head_size, seqlen_k, seqlen_q,</span><br><span class="line">        head_size_rounded, <span class="comment">/*dropout*/</span> <span class="number">0.f</span>, num_splits, <span class="built_in">get_num_sm</span>(<span class="built_in">get_current_device</span>()), opts);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (paged_KV) &#123;</span><br><span class="line">        params.block_table = block_table.<span class="built_in">data_ptr</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line">        params.block_table_batch_stride = block_table.<span class="built_in">stride</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    params.page_block_size = page_block_size;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">set_params_alibi</span>(params, alibi_slopes_, batch_size, num_heads);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> stream = at::cuda::<span class="built_in">getCurrentCUDAStream</span>().<span class="built_in">stream</span>();</span><br><span class="line">    <span class="comment">// Only split kernel supports appending to KV cache, or indexing to the cache with cache_batch_idx,</span></span><br><span class="line">    <span class="comment">// or paged KV cache</span></span><br><span class="line">    <span class="built_in">run_mha_fwd</span>(params, stream, <span class="comment">/*force_split_kernel=*/</span>k_.<span class="built_in">has_value</span>() || cache_batch_idx_.<span class="built_in">has_value</span>() || paged_KV);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (head_size_og % <span class="number">8</span> != <span class="number">0</span>) &#123;</span><br><span class="line">        out = out.<span class="built_in">index</span>(&#123;<span class="string">&quot;...&quot;</span>, torch::indexing::<span class="built_in">Slice</span>(torch::indexing::None, head_size_og)&#125;);</span><br><span class="line">        <span class="keyword">if</span> (out_.<span class="built_in">has_value</span>()) &#123; out_.<span class="built_in">value</span>().<span class="built_in">copy_</span>(out); &#125;</span><br><span class="line">        <span class="keyword">if</span> (k_.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">            <span class="comment">// It&#x27;s expensive to copy the KV cache here for the case where head size not divisible by 8,</span></span><br><span class="line">            <span class="comment">// but we don&#x27;t expect to get this case in practice. This is just so that the code works for that case.</span></span><br><span class="line">            kcache.<span class="built_in">copy_</span>(kcache_padded.<span class="built_in">index</span>(&#123;<span class="string">&quot;...&quot;</span>, torch::indexing::<span class="built_in">Slice</span>(torch::indexing::None, head_size_og)&#125;));</span><br><span class="line">            vcache.<span class="built_in">copy_</span>(vcache_padded.<span class="built_in">index</span>(&#123;<span class="string">&quot;...&quot;</span>, torch::indexing::<span class="built_in">Slice</span>(torch::indexing::None, head_size_og)&#125;));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (seqlenq_ngroups_swapped) &#123;</span><br><span class="line">        out = out.<span class="built_in">transpose</span>(<span class="number">1</span>, <span class="number">2</span>).<span class="built_in">reshape</span>(&#123;batch_size, <span class="number">1</span>, num_heads_k * seqlen_q, head_size_og&#125;);</span><br><span class="line">        softmax_lse = softmax_lse.<span class="built_in">reshape</span>(&#123;batch_size, num_heads_k * seqlen_q, <span class="number">1</span>&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &#123;out, softmax_lse&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>实际上他这里也是有padding的。当k存在时，强行走split k:
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">run_mha_fwd</span><span class="params">(Flash_fwd_params &amp;params, cudaStream_t stream, <span class="type">bool</span> force_split_kernel=<span class="literal">false</span>)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">FP16_SWITCH</span>(!params.is_bf16, [&amp;] &#123;</span><br><span class="line">        <span class="built_in">HEADDIM_SWITCH</span>(params.d, [&amp;] &#123;</span><br><span class="line">            <span class="built_in">BOOL_SWITCH</span>(params.is_causal, Is_causal, [&amp;] &#123;</span><br><span class="line">                <span class="keyword">if</span> (params.num_splits &lt;= <span class="number">1</span> &amp;&amp; !force_split_kernel) &#123;  <span class="comment">// If we don&#x27;t set it num_splits == 0</span></span><br><span class="line">                    <span class="built_in">run_mha_fwd_</span>&lt;elem_type, kHeadDim, Is_causal&gt;(params, stream);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="built_in">run_mha_fwd_splitkv_dispatch</span>&lt;elem_type, kHeadDim, Is_causal&gt;(params, stream);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后在这个dispatch中还有许多变体，不过重要的就是tile的kv都是从远端读取过来的。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (block_table != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    <span class="keyword">auto</span> final_block_size = binfo.actual_seqlen_k - (n_block_max - <span class="number">1</span>) * kBlockN;</span><br><span class="line">    tKgK.<span class="built_in">data</span>() = gK.<span class="built_in">data</span>() + flash::<span class="built_in">resolve_thread_kv_page_slice_offset</span>&lt;Kernel_traits&gt;(tidx, n_block_max - <span class="number">1</span>, params.page_block_size,</span><br><span class="line">        block_table, params.k_batch_stride, params.k_row_stride, final_block_size);</span><br><span class="line">    tVgV.<span class="built_in">data</span>() = gV.<span class="built_in">data</span>() + flash::<span class="built_in">resolve_thread_kv_page_slice_offset</span>&lt;Kernel_traits&gt;(tidx, n_block_max - <span class="number">1</span>, params.page_block_size,</span><br><span class="line">        block_table, params.v_batch_stride, params.v_row_stride, final_block_size);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="vllm-fused-moe">vllm fused moe</h2>
<p>要理解vllm fused
moe，首先从最naive的huggingface版来理解moe的执行流程，首先整体流程如下：</p>
<p><img src="/2025/02/14/vllm/moe_ep.png" /></p>
<p>实际上就是一共64个expert，假设4个设备，ep=4，那么每个设备放8个expert。到gating时的输入都需要是broadcast的，然后gating计算hidden
states对应每个expert的概率，将概率排序后进行all to
all后使用当前设备中expert的计算outputs。注意他这里的all to
all之后实际上每个设备还是算64个expert，只不过是8个expert重复了4次。
等计算完outpus之后再all to all就可以恢复到每个节点64个expert的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">moe_infer</span>(<span class="params">self, x, topk_ids, topk_weight</span>):</span><br><span class="line">    cnts = topk_ids.new_zeros((topk_ids.shape[<span class="number">0</span>], <span class="built_in">len</span>(self.experts))) <span class="comment"># [global batch, n_experts]</span></span><br><span class="line">    cnts.scatter_(<span class="number">1</span>, topk_ids, <span class="number">1</span>) <span class="comment"># assgin activated experts to 1</span></span><br><span class="line">    tokens_per_expert = cnts.<span class="built_in">sum</span>(dim=<span class="number">0</span>) <span class="comment"># 每个token会选择不同的，那么一个专家会处理多个token，统计每个专家要处理的token数量</span></span><br><span class="line">    idxs = topk_ids.view(-<span class="number">1</span>).argsort() <span class="comment"># 先sort topk id, 这样idx 为 [global batch * n_experts], 其中expert的索引从小到大排序</span></span><br><span class="line">    sorted_tokens = x[idxs // topk_ids.shape[<span class="number">1</span>]] <span class="comment"># 因为他前面是先view再argsort，除shape[1]是为了保证只选当前token对应的expert。</span></span><br><span class="line">    sorted_tokens_shape = sorted_tokens.shape</span><br><span class="line">    <span class="keyword">if</span> self.ep_size &gt; <span class="number">1</span>:</span><br><span class="line">        tokens_per_ep_rank = tokens_per_expert.view(self.ep_size, -<span class="number">1</span>).<span class="built_in">sum</span>(dim=<span class="number">1</span>) <span class="comment"># [ep_size, n_experts/ep_size] -&gt; [ep_size]</span></span><br><span class="line">        tokens_per_expert_group = tokens_per_expert.new_empty(</span><br><span class="line">            tokens_per_expert.shape[<span class="number">0</span>]</span><br><span class="line">        )</span><br><span class="line">        dist.all_to_all_single(tokens_per_expert_group, tokens_per_expert) <span class="comment"># 先all to all拿到当前的节点上每个expert要处理的token数量</span></span><br><span class="line">        output_splits = (</span><br><span class="line">            tokens_per_expert_group.view(self.ep_size, -<span class="number">1</span>)</span><br><span class="line">            .<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">            .cpu()</span><br><span class="line">            .numpy()</span><br><span class="line">            .tolist()</span><br><span class="line">        )</span><br><span class="line">        gathered_tokens = sorted_tokens.new_empty(</span><br><span class="line">            tokens_per_expert_group.<span class="built_in">sum</span>(dim=<span class="number">0</span>).cpu().item(), sorted_tokens.shape[<span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line">        input_split_sizes = tokens_per_ep_rank.cpu().numpy().tolist()</span><br><span class="line">        dist.all_to_all(</span><br><span class="line">            <span class="built_in">list</span>(gathered_tokens.split(output_splits)),</span><br><span class="line">            <span class="built_in">list</span>(sorted_tokens.split(input_split_sizes)),</span><br><span class="line">        ) <span class="comment"># all to all拿到当前的节点上每个expert要处理的token</span></span><br><span class="line">        tokens_per_expert_post_gather = tokens_per_expert_group.view(</span><br><span class="line">            self.ep_size, self.experts_per_rank</span><br><span class="line">        ).<span class="built_in">sum</span>(dim=<span class="number">0</span>)</span><br><span class="line">        gatherd_idxs = np.zeros(shape=(gathered_tokens.shape[<span class="number">0</span>],), dtype=np.int32)</span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens_per_expert_group.cpu().numpy()):</span><br><span class="line">            gatherd_idxs[s : s + k] = i % self.experts_per_rank</span><br><span class="line">            s += k</span><br><span class="line">        gatherd_idxs = gatherd_idxs.argsort()</span><br><span class="line">        sorted_tokens = gathered_tokens[gatherd_idxs]</span><br><span class="line">        tokens_per_expert = tokens_per_expert_post_gather</span><br><span class="line">    tokens_per_expert = tokens_per_expert.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    outputs = []</span><br><span class="line">    start_idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, num_tokens <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens_per_expert):</span><br><span class="line">        end_idx = start_idx + num_tokens</span><br><span class="line">        <span class="keyword">if</span> num_tokens == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        expert = self.experts[i + self.ep_rank * self.experts_per_rank]</span><br><span class="line">        tokens_for_this_expert = sorted_tokens[start_idx:end_idx] <span class="comment"># 由于sort之后，分到同一个expert上token已经都排好了，所以直接取</span></span><br><span class="line">        expert_out = expert(tokens_for_this_expert)</span><br><span class="line">        outputs.append(expert_out)</span><br><span class="line">        start_idx = end_idx</span><br><span class="line"></span><br><span class="line">    outs = torch.cat(outputs, dim=<span class="number">0</span>) <span class="keyword">if</span> <span class="built_in">len</span>(outputs) <span class="keyword">else</span> sorted_tokens.new_empty(<span class="number">0</span>) <span class="comment"># [global batch*n_act_experts, hidden_size]</span></span><br><span class="line">    <span class="keyword">if</span> self.ep_size &gt; <span class="number">1</span>:</span><br><span class="line">        new_x = torch.empty_like(outs)</span><br><span class="line">        new_x[gatherd_idxs] = outs</span><br><span class="line">        gathered_tokens = new_x.new_empty(*sorted_tokens_shape)</span><br><span class="line">        dist.all_to_all(</span><br><span class="line">            <span class="built_in">list</span>(gathered_tokens.split(input_split_sizes)),</span><br><span class="line">            <span class="built_in">list</span>(new_x.split(output_splits)),</span><br><span class="line">        )</span><br><span class="line">        outs = gathered_tokens <span class="comment"># 再来一次all to all拿到当前节点处理好的gathered tokens</span></span><br><span class="line"></span><br><span class="line">    new_x = torch.empty_like(outs)</span><br><span class="line">    new_x[idxs] = outs</span><br><span class="line">    final_out = (</span><br><span class="line">        new_x.view(*topk_ids.shape, -<span class="number">1</span>)</span><br><span class="line">        .<span class="built_in">type</span>(topk_weight.dtype)</span><br><span class="line">        .mul_(topk_weight.unsqueeze(dim=-<span class="number">1</span>))</span><br><span class="line">        .<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">        .<span class="built_in">type</span>(new_x.dtype)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> final_out</span><br></pre></td></tr></table></figure>
<h2 id="vllm并行模式">vllm并行模式</h2>
<table>
<colgroup>
<col style="width: 5%" />
<col style="width: 31%" />
<col style="width: 19%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Attention</strong></th>
<th><strong>MoE - Shared Experts</strong></th>
<th><strong>MoE - Routed Experts</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>DP</strong></td>
<td>权重复制 <br> <em>可在推理框架外部处理</em></td>
<td>权重复制</td>
<td><strong>EP关闭:</strong> 权重复制<br><strong>EP开启:</strong>
按EP(=DP)数量切分</td>
</tr>
<tr class="even">
<td><strong>TP</strong></td>
<td>按head切分<br><em>需同步LM head</em><br><em>需支持广播meta
data</em></td>
<td>切分n和k维度<br><em>需做all-reduce</em></td>
<td><strong>EP关闭:</strong> 权重复制<br><strong>EP开启:</strong>
按EP(=TP)数量切分</td>
</tr>
<tr class="odd">
<td><strong>TP+DP</strong></td>
<td>TP组内切分<br>DP组内复制</td>
<td>TP组内切分<br>DP组内复制</td>
<td><strong>EP关闭:</strong> 权重复制<br><strong>EP开启:</strong>
按EP(=TP*DP)总数切分</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>EP是依赖于DP/TP的MoE专用切分模式，EP = TP *
DP</strong>，主要影响Routed Experts的分布</li>
</ul>
<p><img src="/2025/02/14/vllm/dp_and_tp.png" /></p>
<p>同时开启DP + TP时，对于权重分布如上图所示，在group 0/group
1间为权重复制，在每个group内为权重切分。此时需要主要attn
metadata是group间不同，group内相同。</p>
<h1 id="trt-llm">trt llm</h1>
<p>trt
llm的整体流程其实还蛮像一个端到端的AI编译器的。他可以直接吃整个模型，然后直接生成c代码，并且替换已有的plugin算子。</p>
<h2 id="plugin">plugin</h2>
<p>先看看他如何定义plugin的，首先他在python端提供了一些辅助函数用于定义plugin算子的输入输出信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fmha_kernel_meta_data</span>():</span><br><span class="line">    <span class="keyword">return</span> KernelMetaData(</span><br><span class="line">        kernel_name=<span class="string">&#x27;fused_attention_kernel&#x27;</span>,</span><br><span class="line">        ios=[</span><br><span class="line">            <span class="comment"># outputs</span></span><br><span class="line">            OutputArg(<span class="string">&#x27;Out&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;tensor[fp16]&#x27;</span>), hints=[<span class="string">&#x27;16&#x27;</span>, <span class="string">&#x27;16&#x27;</span>]),</span><br><span class="line">            OutputArg(<span class="string">&#x27;L&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;tensor[fp32]&#x27;</span>), hints=[<span class="string">&#x27;16&#x27;</span>, <span class="string">&#x27;16&#x27;</span>]),</span><br><span class="line">            OutputArg(<span class="string">&#x27;M&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;tensor[fp16]&#x27;</span>), hints=[<span class="string">&#x27;16&#x27;</span>, <span class="string">&#x27;16&#x27;</span>]),</span><br><span class="line">            <span class="comment"># inputs</span></span><br><span class="line">            InputArg(<span class="string">&#x27;Q&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;tensor[fp16]&#x27;</span>), hints=[<span class="string">&#x27;16&#x27;</span>, <span class="string">&#x27;16&#x27;</span>]),</span><br><span class="line">            InputArg(<span class="string">&#x27;K&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;tensor[fp16]&#x27;</span>), hints=[<span class="string">&#x27;16&#x27;</span>, <span class="string">&#x27;16&#x27;</span>]),</span><br><span class="line">            InputArg(<span class="string">&#x27;V&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;tensor[fp16]&#x27;</span>), hints=[<span class="string">&#x27;16&#x27;</span>, <span class="string">&#x27;16&#x27;</span>]),</span><br><span class="line">            ParamArg(<span class="string">&#x27;sm_scale&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;fp32&#x27;</span>)),</span><br><span class="line">            DimSizeArg(<span class="string">&#x27;batch_size&#x27;</span>),</span><br><span class="line">            ParamArg(<span class="string">&#x27;num_heads&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;i32&#x27;</span>)),</span><br><span class="line">            DimSizeArg(<span class="string">&#x27;seq_len&#x27;</span>, hints=[<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;16&#x27;</span>]),</span><br><span class="line">            <span class="comment"># constexprs</span></span><br><span class="line">            Constexpr(<span class="number">128</span>),</span><br><span class="line">            Constexpr(<span class="number">64</span>),</span><br><span class="line">            Constexpr(<span class="number">128</span>),</span><br><span class="line">        ],</span><br><span class="line">        shape_infer_rules=[</span><br><span class="line">            <span class="comment"># The following rules helps to deduce the shapes of the output tensors</span></span><br><span class="line">            <span class="string">&quot;Q[*] -&gt; Out[*]&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Q[m,n,k,*] -&gt; L[m,n,k]&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Q[m,n,k,*] -&gt; M[m,n,k]&quot;</span>,</span><br><span class="line"></span><br><span class="line">            <span class="comment"># The following rules helps to deduce both DimSizeArgs: batch_size and seq_len</span></span><br><span class="line">            <span class="string">&quot;Q[m,n,k,*] : m -&gt; batch_size&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Q[m,n,k,*] : k -&gt; seq_len&quot;</span>,</span><br><span class="line">        ],</span><br><span class="line">        version=<span class="number">0</span>,</span><br><span class="line">        kernel_file=<span class="string">f&#x27;<span class="subst">&#123;openai_triton_example_root&#125;</span>/fmha_triton.py&#x27;</span>,</span><br><span class="line">        num_warps=<span class="number">1</span>,</span><br><span class="line">        grid_dims=(<span class="string">&quot;(seq_len + 127) / 128&quot;</span>, <span class="string">&quot;batch_size * num_heads&quot;</span>, <span class="string">&quot;1&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">KERNELS = [</span><br><span class="line">    get_fmha_kernel_meta_data(),</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>甚至还支持了一套读取复杂计算表达式参数的转换器，用于把算子的计算逻辑转换成c接口代码。
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;expr, target&#x27;</span>, [</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;a[m,n,k]:m*2+k+(n+1) -&gt; b&quot;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">     <span class="string">&quot;((inputDesc[0].dims.d[0] * 2) + (inputDesc[0].dims.d[2] + (inputDesc[0].dims.d[1] + 1)))&quot;</span></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">     </span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;a[m,n,k]:m*(2+k)+n+1 -&gt; b&quot;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">     <span class="string">&quot;((inputDesc[0].dims.d[0] * (2 + inputDesc[0].dims.d[2])) + (inputDesc[0].dims.d[1] + 1))&quot;</span></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">     </span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;a[m,n,k] -&gt; b[m*((((n+1))))]&quot;</span>, <span class="string">&quot;&quot;&quot;</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">if (outputIndex == 0) &#123;</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">  outputDims.nbDims = 1;</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">  outputDims.d[0] = (inputDims[0].d[0] * (inputDims[0].d[1] + 1));</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">&#125;</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">     &quot;&quot;&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;a[m,n,k] -&gt; b[m*(n+k), 2*n, k+3]&quot;</span>, <span class="string">&quot;&quot;&quot;</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">nvinfer1::DimsExprs outputDims;</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">if (outputIndex == 0) &#123;</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">  outputDims.nbDims = 3;</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">  outputDims.d[0] = (inputDims[0].d[0] * (inputDims[0].d[1] + inputDims[0].d[2]));</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">  outputDims.d[1] = (2 * inputDims[0].d[1]);</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">  outputDims.d[2] = (inputDims[0].d[2] + 3);</span></span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="params"><span class="meta">&#125;&quot;&quot;&quot;</span></span>)</span></span></span><br><span class="line"><span class="params"><span class="meta">]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_CppCodeTranspiler</span>(<span class="params">expr: <span class="built_in">str</span>, target: <span class="built_in">str</span></span>):</span><br><span class="line">    args = <span class="built_in">dict</span>(</span><br><span class="line">        a=InputArg(<span class="string">&#x27;a&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;fp16&#x27;</span>)),</span><br><span class="line">        b=InputArg(<span class="string">&#x27;b&#x27;</span>, <span class="type">Type</span>(<span class="string">&#x27;fp16&#x27;</span>)),</span><br><span class="line">    )</span><br><span class="line">    target = target.strip()</span><br><span class="line"></span><br><span class="line">    transpiler = CppCodeTranspiler(args)</span><br><span class="line"></span><br><span class="line">    shape_infer_code, dim_infer_code = transpiler([expr])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># we don&#x27;t check the correctness of the code since the lark produces unstable ast tree</span></span><br><span class="line">    <span class="comment"># refer to https://github.com/lark-parser/lark/issues/324</span></span><br><span class="line">    <span class="keyword">assert</span> shape_infer_code <span class="keyword">or</span> dim_infer_code</span><br></pre></td></tr></table></figure></p>
<h2 id="auto-parallel">auto parallel</h2>
<p>首先是切分描述，trt
llm和shardy类似，是对一个维度来描述切分，如果为空，那么当前维度就是复制，否则在对应的mesh上进行切分：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DimSpec</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, shard_list</span>):</span><br><span class="line">        self.is_replica = <span class="built_in">len</span>(shard_list) == <span class="number">0</span></span><br><span class="line">        self.shard_list = shard_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.is_replica:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;R&#x27;</span></span><br><span class="line">        target = <span class="string">f&quot;S(<span class="subst">&#123;<span class="string">&#x27;,&#x27;</span>.join(<span class="built_in">str</span>(dim) <span class="keyword">for</span> dim <span class="keyword">in</span> self.shard_list)&#125;</span>)&quot;</span></span><br><span class="line">        <span class="keyword">return</span> target</span><br></pre></td></tr></table></figure></p>
<p>然后一个tensor的切分信息由多个dim spec组成:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ShardingSpec</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, entire_shape, sharding_sequence, device_mesh</span>):</span><br><span class="line">      ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        res = <span class="string">&quot;DistSpec(&quot;</span></span><br><span class="line">        res += <span class="string">f&quot;shape=<span class="subst">&#123;self.entire_shape&#125;</span>,&quot;</span></span><br><span class="line">        res += <span class="string">f&quot;shard=<span class="subst">&#123;self.sharding_sequence&#125;</span>,&quot;</span></span><br><span class="line">        res += <span class="string">f&quot;mesh=<span class="subst">&#123;self.device_mesh.mesh_shape&#125;</span>&quot;</span></span><br><span class="line">        res += <span class="string">&quot;)&quot;</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>然后构造一个分布式切分搜索图： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_cost_graph</span>(<span class="params">self, lmesh</span>):</span><br><span class="line">    leaf_strategies = []</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes:</span><br><span class="line">        <span class="keyword">if</span> node.is_replicated:</span><br><span class="line">            node.set_strategy(<span class="literal">None</span>, lmesh)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node.collect_strategies(lmesh)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes:</span><br><span class="line">        strategies_vector = node.update_resharding_cost()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(strategies_vector) != <span class="number">0</span>:</span><br><span class="line">            leaf_strategies.append(strategies_vector)</span><br><span class="line">    cost_graph = CostGraph(leaf_strategies)</span><br><span class="line">    <span class="keyword">return</span> cost_graph</span><br></pre></td></tr></table></figure></p>
<p>首先是遍历所有节点，如果这个是<code>is_replicated</code>，也就是提前标记了不可分布式，那么直接设置为None，否则遍历所有可能的切分策略，然后计算这个节点的cost（包含了通信和计算）。其中<code>collect_strategies</code>核心代码如下,
他似乎是只考虑在2维拓扑以下进行分布式，但是这里加的策略还是全的，不过好像只添加考虑输出节点的切分，并不是SBP的基于推导的方法。
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_collect_strategies</span>(<span class="params">self, device_mesh</span>):</span><br><span class="line">    dim_partition_list = []</span><br><span class="line">    dim_size = <span class="built_in">len</span>(self.op_data[<span class="string">&#x27;output0&#x27;</span>].shape)</span><br><span class="line">    dim_partition_list.append(&#123;&#125;)</span><br><span class="line">    dim_partition_list.extend(</span><br><span class="line">        self._enumerate_all_possible_1d_sharding([<span class="number">0</span>, <span class="number">1</span>], dim_size))</span><br><span class="line">    dim_partition_list.extend(</span><br><span class="line">        self._enumerate_all_possible_2d_sharding([<span class="number">0</span>], [<span class="number">1</span>], dim_size))</span><br><span class="line">    dim_partition_list.extend(</span><br><span class="line">        self._enumerate_all_possible_1d_sharding([<span class="number">0</span>], dim_size))</span><br><span class="line">    dim_partition_list.extend(</span><br><span class="line">        self._enumerate_all_possible_1d_sharding([<span class="number">1</span>], dim_size))</span><br><span class="line"></span><br><span class="line">    strategies_vector = StrategiesVector(self)</span><br><span class="line">    <span class="keyword">for</span> dim_partition_dict <span class="keyword">in</span> dim_partition_list:</span><br><span class="line">        dim_partition_dict_mapping = &#123;<span class="string">&#x27;output0&#x27;</span>: dim_partition_dict&#125;</span><br><span class="line">        sharding_spec_mapping = self._to_sharding_spec_mapping(</span><br><span class="line">            dim_partition_dict_mapping, device_mesh)</span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> == <span class="built_in">len</span>(sharding_spec_mapping):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        sharding_seq = sharding_spec_mapping[<span class="string">&#x27;output0&#x27;</span>].sharding_sequence</span><br><span class="line">        sharding_strategy = self._get_sharding_strategy(</span><br><span class="line">            name=<span class="string">f&#x27;constant-op <span class="subst">&#123;sharding_seq&#125;</span>&#x27;</span>,</span><br><span class="line">            sharding_spec_mapping=sharding_spec_mapping,</span><br><span class="line">            communication_action_mapping=&#123;&#125;)</span><br><span class="line">        strategies_vector.append(sharding_strategy)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> strategies_vector</span><br></pre></td></tr></table></figure></p>
<p>然后发现实际上这个方法是可以被override的，对于matmul来说有自己的collect方法：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_collect_strategies</span>(<span class="params">self, device_mesh</span>):</span><br><span class="line">    strategies_vector = StrategiesVector(self)</span><br><span class="line">    dp_strategies = self._dp_strategies(device_mesh)</span><br><span class="line">    tp_strategies = self._tp_strategies(device_mesh)</span><br><span class="line">    mix_strategies = self._mix_strategies(device_mesh)</span><br><span class="line">    bmm_strategies = self._bmm_strategies(device_mesh)</span><br><span class="line">    strategies_vector.extend(dp_strategies)</span><br><span class="line">    strategies_vector.extend(tp_strategies)</span><br><span class="line">    strategies_vector.extend(mix_strategies)</span><br><span class="line">    strategies_vector.extend(bmm_strategies)</span><br><span class="line">    <span class="keyword">return</span> strategies_vector</span><br></pre></td></tr></table></figure> 这是matmul所collect出来的切分方式： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RR = RS(<span class="number">0</span>) x S(<span class="number">0</span>)R_allreduceS(<span class="number">0</span>)</span><br><span class="line">RR = RS(<span class="number">1</span>) x S(<span class="number">1</span>)R_allreduceS(<span class="number">1</span>)</span><br><span class="line">RR = RS(<span class="number">0</span>,<span class="number">1</span>) x S(<span class="number">0</span>,<span class="number">1</span>)R_allreduceS(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">[R, S(<span class="number">0</span>)] = [R, S(<span class="number">0</span>)] x [S(<span class="number">0</span>), R]_reducescatter(<span class="number">1</span>, S(<span class="number">0</span>))</span><br><span class="line">[R, S(<span class="number">1</span>)] = [R, S(<span class="number">1</span>)] x [S(<span class="number">1</span>), R]_reducescatter(<span class="number">1</span>, S(<span class="number">1</span>))</span><br><span class="line">[R, S(<span class="number">0</span>,<span class="number">1</span>)] = [R, S(<span class="number">0</span>,<span class="number">1</span>)] x [S(<span class="number">0</span>,<span class="number">1</span>), R]_reducescatter(<span class="number">1</span>, S(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">RS(<span class="number">0</span>) = RR x RS(<span class="number">0</span>)</span><br><span class="line">RS(<span class="number">1</span>) = RR x RS(<span class="number">1</span>)</span><br><span class="line">RS(<span class="number">0</span>,<span class="number">1</span>) = RR x RS(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">RS(<span class="number">1</span>) = RS(<span class="number">0</span>) x S(<span class="number">0</span>)S(<span class="number">1</span>)_allreduceS(<span class="number">0</span>)</span><br><span class="line">RS(<span class="number">0</span>) = RS(<span class="number">1</span>) x S(<span class="number">1</span>)S(<span class="number">0</span>)_allreduceS(<span class="number">1</span>)</span><br><span class="line">RR = RR x RR</span><br></pre></td></tr></table></figure></p>
<p>同样attention或其他节点也有自己的分布式切分收集函数，这里和oneflow不同的是，他并不会传播partial的切分，我的理解是拆分的越细那么灵活性更高，可以后续做自动的通算融合，大粒度后面还是走匹配替换来优化。</p>
<p>每个算子的分布式策略收集好之后，还需要构建reshard
cost，因为可能上一个节点的切分策略并不是下一个节点所需要的切分策略，所以需要计算reshard
cost。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_update_resharding_cost</span>(<span class="params">self, strategies</span>):</span><br><span class="line">    <span class="keyword">for</span> strategy <span class="keyword">in</span> strategies:</span><br><span class="line">        resharding_costs = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> pre_node, out_index <span class="keyword">in</span> self.predecessor_nodes_out_index.items():</span><br><span class="line">            <span class="keyword">if</span> pre_node <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            pre_node_out_data_name = pre_node.get_output(out_index).name</span><br><span class="line">            pre_node_out_data_lname = pre_node.global_to_local_op_name[</span><br><span class="line">                pre_node_out_data_name]</span><br><span class="line">            <span class="keyword">if</span> pre_node_out_data_name <span class="keyword">not</span> <span class="keyword">in</span> self.global_to_local_op_name:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;pre_node_out_data_name = <span class="subst">&#123;pre_node_out_data_name&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            cur_node_inp_data_lname = self.global_to_local_op_name[</span><br><span class="line">                pre_node_out_data_name]</span><br><span class="line">            cur_sharding_spec = strategy.sharding_specs[</span><br><span class="line">                cur_node_inp_data_lname]</span><br><span class="line"></span><br><span class="line">            pre_node_out_sharding_specs = []</span><br><span class="line">            <span class="keyword">for</span> pre_strategy <span class="keyword">in</span> pre_node.strategies_vector:</span><br><span class="line">                pre_node_out_sharding_specs.append(</span><br><span class="line">                    pre_strategy.sharding_specs[pre_node_out_data_lname])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> pre_node <span class="keyword">not</span> <span class="keyword">in</span> resharding_costs:</span><br><span class="line">                resharding_costs[pre_node.node_name] = []</span><br><span class="line">            <span class="keyword">for</span> prev_sharding_spec <span class="keyword">in</span> pre_node_out_sharding_specs:</span><br><span class="line">                resharding_cost = self._compute_resharding_cost(</span><br><span class="line">                    prev_sharding_spec, cur_sharding_spec,</span><br><span class="line">                    self.op_data[cur_node_inp_data_lname]) <span class="comment"># 话说为什么他要限制这个op的切分类型，本来这个gather就可以切输入啊</span></span><br><span class="line">                resharding_costs[pre_node.node_name].append(resharding_cost)</span><br><span class="line">        strategy.resharding_costs = resharding_costs</span><br></pre></td></tr></table></figure></p>
<h2 id="pyexector">PyExector</h2>
<p>trt
llm之前通过编译的方式发现使用起来不方便，因此他也模仿vllm，提供了一个python的executor，直接动态执行大模型。并且他新的接口就做了相当的精简：</p>
<p><img src="/2025/02/14/vllm/trt_attn.png" /></p>
<h1 id="mlc-llm">mlc llm</h1>
<p>mlc这里就不看调度的过程了，只看kv
cache，attention是如何和编译器集成的。</p>
<p><img src="/2025/02/14/vllm/tvm_attn.png" /></p>
<p>tvm中首先是在vm中定义了基础的kv cache state接口，然后实现了具体的kv
cache管理逻辑。然后AttentionKVCacheObj中包含了一个最重要的函数AttentionWithFusedQKV。tvm为了在python端可以集成不同的
attention实现，是在python中调用vm的kv
cache的create，在create中将不同的attention计算函数作为packed
func传入。最终执行时是vm调用AttentionWithFusedQKV，AttentionWithFusedQKV中调用自身的tir_attention_decode_cpu/tir_attention_decode_gpu等等函数。</p>
<p>他这套逻辑还是比较复杂的，需要在python和cpp中转换多次。</p>
<h1 id="sglang">sglang</h1>
<p>这是sglang在attention部分的类图： <img
src="/2025/02/14/vllm/sglang_attn.png" /></p>
<h1 id="问题汇总">问题汇总</h1>
<h2 id="vllm问题">vllm问题</h2>
<ol type="1">
<li>TypeError: must be called with a dataclass type or instance</li>
</ol>
<p>发现是torch 2.5.1 cu118不能用triton 3.2，需要降级到3.1</p>
<ol start="2" type="1">
<li>RuntimeError: NCCL error: unhandled cuda error (run with
NCCL_DEBUG=INFO for details)</li>
</ol>
<p>检查detail发现是runtime的nccl是cu12的</p>
<ol start="3" type="1">
<li>json.decoder.JSONDecodeError: Extra data: line 5298 column 2 (char
479924)</li>
</ol>
<p>发现是之前下载的模型的json有问题，重新下载就行了</p>
<ol start="4" type="1">
<li>RuntimeError: Triton Error [CUDA]: device kernel image is
invalid</li>
</ol>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[rank0]: torch._dynamo.exc.BackendCompilerFailed: backend=<span class="string">&#x27;inductor&#x27;</span> raised:</span><br><span class="line">[rank0]: RuntimeError: Triton Error [CUDA]: device kernel image is invalid</span><br><span class="line"></span><br><span class="line">[rank0]: Set TORCH_LOGS=<span class="string">&quot;+dynamo&quot;</span> and TORCHDYNAMO_VERBOSE=1 <span class="keyword">for</span> more information</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[rank0]: You can suppress this exception and fall back to eager by setting:</span><br><span class="line">[rank0]:     import torch._dynamo</span><br><span class="line">[rank0]:     torch._dynamo.config.suppress_errors = True</span><br></pre></td></tr></table></figure>
<p>这个应该是triton默认只能编译出最新的cuda版本的代码,把本地的ptxas替换掉triton中的：
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> /usr/local/cuda/bin/ptxas /root/miniconda3/envs/vllm/lib/python3.10/site-packages/triton/backends/nvidia/bin/ptxas</span><br></pre></td></tr></table></figure></p>
<ol start="5" type="1">
<li><code>NCCL WARN NCCL cannot be captured in a graph</code></li>
</ol>
<p>好像是安装的nccl版本和cuda 11.8还是兼容？ <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">misc/strongstream.cc:53 NCCL WARN NCCL cannot be captured <span class="keyword">in</span> a graph <span class="keyword">if</span> either it wasn<span class="string">&#x27;t built with CUDA runtime &gt;= 11.3 or if the installed CUDA driver &lt; R465.</span></span><br></pre></td></tr></table></figure></p>
<p>我现在的版本是<code>nvidia-nccl-cu11-2.21.5</code>,卸载之后重装老的版本
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">❯ pip install nvidia-nccl-cu11==2.19.3 -i https://download.pytorch.org/whl/cu118</span><br></pre></td></tr></table></figure></p>
<p>好像也不行，可能是因为老的版本的nccl就没有支持capture
graph的功能，需要重新编译nccl，但是实际上可以<code>--enforce-eager</code>跳过这个问题。</p>
<h2 id="trt-llm问题">trt llm问题</h2>
<ol type="1">
<li>cuda版本问题</li>
</ol>
<p>trt llm和cuda版本是强绑定的，所以如果cuda
版本不到，就直接没法运行，所以只能安装老版本的trt llm。</p>
<ol start="2" type="1">
<li>tensorrt bindings的问题</li>
</ol>
<p>我发现使用pip安装的tensorrt llm中的tensorrt 调用的是tensorrt
bindings，然后我安装他得到的是8.6.1版本，这个不是我想要的。解决方法是pip卸载tensorrt，然后用trt
llm中的shell脚本安装tensorrt。</p>
<ol start="3" type="1">
<li>mpi版本问题</li>
</ol>
<p>pip安装trt
llm的时候总是报错编译mpi4py失败，然后我查看了一下，发现是mpi版本的问题，默认apt安装openmpi得到40+版本了，但是trt
llm中需要的是3.1.5，所以解决方案是不要apt装openmpi，通过conda-forge安装3.1.5版本</p>
<h2 id="scaled-dot-product-attention-sdpa-精度问题">scaled dot product
attention (SDPA) 精度问题</h2>
<p>我发现在mac上使用
<code>torch.nn.functional.scaled_dot_product_attention</code>
与自己实现的 <code>scaled_dot_product_attention</code>
得到的结果精度有差异。但是如果设定了SDPA的实现为MATH就不会有精度差异，并且是否设定MATH的backend也会导致精度问题，我就很难理解默认的backend是什么。
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.attention <span class="keyword">import</span> SDPBackend, sdpa_kernel</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>, dropout_p: <span class="built_in">float</span> = <span class="number">0.0</span>, is_causal: <span class="built_in">bool</span> = <span class="literal">False</span>, scale: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span>) -&gt; torch.Tensor:</span><br><span class="line">    L, S = query.size(-<span class="number">2</span>), key.size(-<span class="number">2</span>)</span><br><span class="line">    scale_factor = <span class="number">1</span> / math.sqrt(query.size(-<span class="number">1</span>)) <span class="keyword">if</span> scale <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> scale</span><br><span class="line">    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)</span><br><span class="line">    <span class="keyword">if</span> is_causal:</span><br><span class="line">        <span class="keyword">assert</span> attn_mask <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">        temp_mask = torch.ones(L, S, dtype=torch.<span class="built_in">bool</span>).tril(diagonal=<span class="number">0</span>)</span><br><span class="line">        attn_bias.masked_fill_(temp_mask.logical_not(), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">        attn_bias.to(query.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> attn_mask.dtype == torch.<span class="built_in">bool</span>:</span><br><span class="line">            attn_bias.masked_fill_(attn_mask.logical_not(), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_bias = attn_mask + attn_bias</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if enable_gqa:</span></span><br><span class="line">    <span class="comment">#     key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)</span></span><br><span class="line">    <span class="comment">#     value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)</span></span><br><span class="line"></span><br><span class="line">    attn_weight = query @ key.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * scale_factor</span><br><span class="line">    attn_weight += attn_bias</span><br><span class="line">    attn_weight = torch.softmax(attn_weight, dim=-<span class="number">1</span>)</span><br><span class="line">    attn_weight = torch.dropout(attn_weight, dropout_p, train=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> attn_weight @ value</span><br><span class="line"></span><br><span class="line">q = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">101</span>, <span class="number">64</span>)</span><br><span class="line">k = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">101</span>, <span class="number">64</span>)</span><br><span class="line">v = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">101</span>, <span class="number">64</span>)</span><br><span class="line">mask = torch.zeros(<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">101</span>, <span class="number">101</span>)</span><br><span class="line"></span><br><span class="line">y1 = scaled_dot_product_attention(</span><br><span class="line">    q, k, v, attn_mask=mask, dropout_p=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, is_causal=mask <span class="keyword">is</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">q2 = q.reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">101</span>, <span class="number">64</span>)</span><br><span class="line">k2 = torch.repeat_interleave(k, <span class="number">7</span>, <span class="number">2</span>).reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">101</span>, <span class="number">64</span>)</span><br><span class="line">v2 = torch.repeat_interleave(v, <span class="number">7</span>, <span class="number">2</span>).reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">101</span>, <span class="number">64</span>)</span><br><span class="line">mask2 = mask.reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">101</span>, <span class="number">101</span>)</span><br><span class="line">y2 = scaled_dot_product_attention(</span><br><span class="line">    q2, k2, v2, attn_mask=mask2, dropout_p=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, is_causal=mask2 <span class="keyword">is</span> <span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">assert</span> torch.allclose(y1, y2.reshape(<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">101</span>, <span class="number">64</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sdpa_kernel([SDPBackend.MATH]):</span><br><span class="line">  y3 = torch.nn.functional.scaled_dot_product_attention(</span><br><span class="line">      q2, k2, v2, attn_mask=mask2, dropout_p=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, is_causal=mask2 <span class="keyword">is</span> <span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">assert</span> torch.allclose(y2, y3)</span><br><span class="line"></span><br><span class="line">y3_1 = torch.nn.functional.scaled_dot_product_attention(</span><br><span class="line">    q2, k2, v2, attn_mask=mask2, dropout_p=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, is_causal=mask2 <span class="keyword">is</span> <span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">assert</span> torch.allclose(y3, y3_1) <span class="comment"># will fail</span></span><br></pre></td></tr></table></figure></p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vllm/" rel="tag">vllm</a></li></ul></div><div class="post-nav"><a class="pre" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a><a class="next" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a> <a href="/tags/%E7%AE%97%E5%AD%90/" style="font-size: 15px;">算子</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/09/26/flashattn/">Flash Attention记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>