<pre class="plantuml"><code>@startuml

package interface {

abstract AttentionMetadata {
    - _seq_lens: Optional[torch.Tensor]
    + seq_lens: Optional[torch.Tensor]
    + seq_lens_cuda
    + seq_lens_kv: Optional[torch.Tensor]
    + seq_lens_kv_cuda
    + context_lens: torch.Tensor
    + num_generations: int
    + num_seqs: int
    + num_ctx_tokens: int
    + num_tokens: int
    + is_cross: bool
    + has_cross_sub_metadata: bool
    + __post_init__()
    + prepare()
    + create_cuda_graph_metadata(): Self
}

abstract AttentionBackend&lt;TMetadata&gt; {
    - layer_idx: int
    - num_heads: int
    - head_dim: int
    - num_kv_heads: int
    - quant_config: Optional[QuantConfig]
    + forward(q, k, v, metadata): torch.Tensor
}

AttentionMetadata -d-&gt; AttentionBackend
}

package flashinfer {

class FlashInferAttentionMetadata {
  
}

class FlashInferAttention&lt;FlashInferAttentionMetadata&gt; {

}

FlashInferAttentionMetadata -d-&gt; FlashInferAttention

AttentionMetadata .|&gt; FlashInferAttentionMetadata
AttentionBackend .|&gt; FlashInferAttention
}

package star {
class StarAttentionMetadata&lt;FlashInferAttentionMetadata&gt; {
}

class StarAttention&lt;StarAttentionMetadata&gt; {
}

FlashInferAttentionMetadata -&gt; StarAttentionMetadata
StarAttentionMetadata -d-&gt; StarAttention
AttentionBackend .|&gt; StarAttention
}

@enduml</code></pre>
