<pre class="plantuml"><code>@startuml

abstract AttentionBackend {
    accept_output_buffer: bool
    + get_name(): str
    + get_impl_cls(): Type[AttentionImpl]
    + get_metadata_cls(): Type[AttentionMetadata]
    + get_state_cls(): Type[AttentionState]
    + make_metadata(*args, **kwargs): AttentionMetadata
    + get_builder_cls(): Type[AttentionMetadataBuilder]
    + get_kv_cache_shape(num_blocks: int, block_size: int, num_kv_heads: int, head_size: int): Tuple[int, ...]
    + swap_blocks(src_kv_cache: torch.Tensor, dst_kv_cache: torch.Tensor, src_to_dst: torch.Tensor): None
    + copy_blocks(kv_caches: List[torch.Tensor], src_to_dists: torch.Tensor): None
    + advance_step(model_input: ModelRunnerInputBase, sampled_token_ids: Optional[torch.Tensor], block_size: int, num_seqs: int, num_queries: int): None
}

together {

abstract AttentionMetadata {
    num_prefills: int
    num_prefill_tokens: int
    num_decode_tokens: int
    slot_mapping: torch.Tensor
    multi_modal_placeholder_index_maps: Optional[Dict[str, MultiModalPlaceholderMap.IndexMap]]
    enable_kv_scales_calculation: bool
    + prefill_metadata: Optional[AttentionMetadata]
    + decode_metadata: Optional[AttentionMetadata]
    + asdict_zerocopy(skip_fields: Optional[Set[str]] = None): Dict[str, Any]
}

abstract AttentionState&lt;T&gt; {
    + __init__(runner: ModelRunnerBase)
    + graph_capture(max_batch_size: int): ContextManager
    + graph_clone(batch_size: int): AttentionState[T]
    + graph_capture_get_metadata_for_batch(batch_size: int, is_encoder_decoder_model: bool = False): T
    + get_graph_input_buffers(attn_metadata: T, is_encoder_decoder_model: bool = False): Dict[str, Any]
    + prepare_graph_input_buffers(input_buffers: Dict[str, Any], attn_metadata: T, is_encoder_decoder_model: bool = False): None
    + begin_forward(model_input: ModelRunnerInputBase): None
}

abstract AttentionMetadataBuilder&lt;T&gt; {
    + __init__(input_builder: ModelRunnerInputBuilderBase)
    + prepare(): None
    + build(seq_lens: List[int], query_lens: List[int], cuda_graph_pad_size: int, batch_size: int): T
}

abstract AttentionImpl&lt;T&gt; {
    + __init__(...)
    + forward(...): torch.Tensor
}

}

package impl {

class MLAAttentionImpl&lt;T&gt; {
    + forward(...): torch.Tensor
}

class FlashAttentionImpl&lt;T&gt; {
    + forward(...): torch.Tensor
}

}

AttentionBackend -right&gt; AttentionImpl : get_impl_cls()
AttentionBackend -right&gt; AttentionMetadata : get_metadata_cls()
AttentionBackend -right&gt; AttentionMetadataBuilder : get_builder_cls()
AttentionBackend -right&gt; AttentionState : get_state_cls()

AttentionImpl -down[hidden]-&gt; AttentionMetadata
AttentionMetadata  -down[hidden]-&gt; AttentionMetadataBuilder
AttentionMetadataBuilder  -down[hidden]-&gt; AttentionState

MLAAttentionImpl -left|&gt; AttentionImpl
FlashAttentionImpl -left|&gt; AttentionImpl

MLAAttentionImpl -down[hidden]-&gt; FlashAttentionImpl

@enduml</code></pre>
