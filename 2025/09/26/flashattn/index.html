<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Flash Attention记录 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Flash Attention记录</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Flash Attention记录</h1><div class="post-meta">2025-09-26<span> | </span><span class="category"><a href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 11</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>简单记录一下flash attention的推导和实现。</p>
<span id="more"></span>
<h1 id="naive-attention">Naive Attention</h1>
<p><span class="math display">\[
\begin{aligned}
  S &amp;= QK^T \in \mathbb{R}^{N\times N} \\
  P &amp;= softmax(S) \\
  O &amp;= PV \in \mathbb{R}^{N \times d}
\end{aligned}
\]</span></p>
<p>本质上就是两个matmul中间有一个伪elementwise操作。
主要由于softmax每个输出点依赖了他的所有input，导致无法进行tiling
fusion。原因如下</p>
<h1 id="softmax">SoftMax</h1>
<h2 id="naive-softmax实现">naive softmax实现：</h2>
<p><span class="math display">\[
\begin{aligned}
softmax(x_i, \ldots, x_N) = \frac{e ^ {x_i}}{\sum_{j=1}^{N} e^{x_j}}, i
\in [1, N]
\end{aligned}
\]</span></p>
<p>由于<span
class="math inline">\(e^x\)</span>会很大，容易出现数值溢出，因此出现safe
softmax</p>
<h2 id="safe-softmax实现">safe softmax实现</h2>
<p><span class="math display">\[
\begin{aligned}
  max_{N} &amp;= max(x_i), \ i \in [1, N] \\
  softmax(x_i, \ldots, x_N) &amp;= \frac{e ^ {x_i -
max_{N}}}{\sum_{j=1}^{N} e^{x_j - max_{N}}}, i \in [1, N]
\end{aligned}
\]</span></p>
<p>但是在实现上需要循环三次，因为<span
class="math inline">\(max_N\)</span>和<span
class="math inline">\(x_{sum}\)</span>都需要单独的循环</p>
<p><span class="math display">\[
\begin{aligned}
  max_i &amp;= max(m_{i-1}, x_i)\\
  sum_i &amp;= sum_{i-1} + e^{x_i - max_N} \\
  a_i &amp;= \frac{e^{x_i - max_N}}{sum_N}, \ i \in [1, N]
\end{aligned}
\]</span></p>
<h2 id="pass-softmax">2-pass softmax</h2>
<p>说实话，让我是没办法能想出来融合max以及sum的公式，可能作者也是受Welford方法启发的。
首先我们考虑把<span
class="math inline">\(sum_N\)</span>的公式展开，通过<span
class="math inline">\(exp\)</span>的计算性质把<span
class="math inline">\(- max_N\)</span>这个拆分为两个部分 <span
class="math display">\[
\begin{aligned}
  sum_{N} &amp;= \sum_{j = 1} ^ N e^{x_j - max_N} \\ \\
    &amp;= \sum_{j = 1} ^ {N-1} e^{x_j - max_N} + e^{x_N - max_N}  \\
    &amp;= \sum_{j = 1} ^ {N-1} e^{x_j - max_{N-1} + max_{N-1} - max_N}
+ e^{x_N - max_N} \\
    &amp;= (\sum_{j = 1} ^ {N-1} e^{x_j - max_{N-1}}) e^{max_{N-1} -
max_N} +e^{x_N - max_N} \\
\end{aligned}
\]</span></p>
<p>观察上面的公式，发现如果从另一个视角去定义变量就可以让他们递归起来
<span class="math display">\[
\begin{aligned}
\text{let}\ sum_{N}^\prime &amp;=\sum_{j=1}^{N} e^{x_j - max_N} =
sum_{N} \\
  &amp;= (\sum_{j = 1} ^ {N-1} e^{x_j - max_{N-1}}) e^{max_{N-1} -
max_N} +e^{x_N - max_N}  \\
  &amp;= sum_{N-1}^\prime e^{max_{N-1} - max_N} +e^{x_N - max_N} \\
  sum_i ^\prime &amp;= sum_{i-1} ^\prime e^{max_{i-1} - max_i} +e^{x_i -
max_i}
\end{aligned}
\]</span></p>
<p>通过视角的转换将<span class="math inline">\(max_N\)</span>与<span
class="math inline">\(sum_{i}\)</span>进行了解耦，并且当迭代到最后时<span
class="math inline">\(sum_{N}^\prime = sum_{N}\)</span>。
虽然2-pass的方式需要在每次迭代添加额外的乘<span
class="math inline">\(e^{max_{i-1} -
max_i}\)</span>的运算，但显然比访存开销低很多。</p>
<h1 id="flash-attention">Flash Attention</h1>
<h2 id="pass-attention">2-pass Attention</h2>
<p>首先使用2-pass的softmax来实现一个attention,这里为了不混淆<code>query len</code>和<code>seq len</code>，
分别用<code>k</code>和<code>i</code>来表示。</p>
<p><span class="math display">\[
\begin{aligned}
\text{for i in [1, N]}:&amp;\\
  x_i &amp;= Q[k, :]K^T[:, i]\\
  max_i &amp;= \max(max_{i-1}, x_i) \\
  sum_i^\prime &amp;= sum_{i-1} ^\prime e^{max_{i-1} - max_i} +e^{x_i -
max_i}\\
\text{end} \qquad \qquad \\
\text{for i in [1, N]}:&amp;\\
  a_i &amp;= \frac{e^{x_i - max_N}}{sum_N^\prime} \\
  o_i &amp;= o_{i-1} + a_i V[i,:] \\
\text{end} \qquad \qquad \\
  O[k,:] &amp; = o_N
\end{aligned}
\]</span>
<!-- 这里的o_{i-1} + a_i 是因为第二个矩阵乘k维度对应的N，也就是要进行mul add的操作 --></p>
<h2 id="pass-attention-1">1-pass Attention</h2>
<p>在和V做矩阵乘时，每一个<span
class="math inline">\(o_i\)</span>还是依赖了<span
class="math inline">\(max_N\)</span>。 接下来就是找到办法把<span
class="math inline">\(max_N\)</span>的依赖消除。参考<code>2-pass softmax</code>的套路先定义：
<span class="math display">\[
\begin{aligned}
o_N^\prime &amp;= \sum_{i = 1} ^ {N} a_i V[i,:] \\
           &amp;= \sum_{i = 1} ^ {N} \frac{e^{x_i -
max_N}}{sum_N^\prime} V[i,:] \\
           &amp;= (\sum_{i = 1} ^ {N-1} \frac{e^{x_i -
max_N}}{sum_N^\prime} V[i,:]) + \frac{e^{x_N - max_N}}{sum_N^\prime}
V[N,:] \\
           &amp;= (\sum_{i = 1} ^ {N-1} \frac{e^{x_i -
max_N}}{sum_N^\prime} \frac{sum_{N-1}^\prime}{sum_{N-1}^\prime}
\frac{e^{x_i - max_{N-1}}}{e^{x_i - max_{N-1}}}  V[i,:]) + \frac{e^{x_N
- max_N}}{sum_N^\prime} V[N,:] \\
           &amp;= (\sum_{i = 1} ^ {N-1} \frac{e^{x_i -
max_{N-1}}}{sum_{N-1}^\prime} V[i,:])
\frac{sum_{N-1}^\prime}{sum_{N}^\prime}\frac{e^{x_i - max_N}}{e^{x_i -
max_{N-1}}} + \frac{e^{x_N - max_N}}{sum_N^\prime} V[N,:] \\
           &amp;= (\sum_{i = 1} ^ {N-1} \frac{e^{x_i -
max_{N-1}}}{sum_{N-1}^\prime} V[i,:])
\frac{sum_{N-1}^\prime}{sum_{N}^\prime}e^{max_{N-1} - max_N} +
\frac{e^{x_N - max_N}}{sum_N^\prime} V[N,:] \\
           &amp;= o_{N-1}^\prime
\frac{sum_{N-1}^\prime}{sum_{N}^\prime}e^{max_{N-1} - max_N} +
\frac{e^{x_N - max_N}}{sum_N^\prime} V[N,:] \\
\end{aligned}
\]</span> 然后归纳得到不包含<span
class="math inline">\(max_N\)</span>的<span
class="math inline">\(o_i^\prime\)</span>公式为： <span
class="math display">\[
\begin{aligned}
  o_i^\prime &amp;= o_{i-1}^\prime
\frac{sum_{i-1}^\prime}{sum_{i}^\prime}e^{max_{i-1} - max_i} +
\frac{e^{x_i - max_i}}{sum_i^\prime} V[i,:]
\end{aligned}
\]</span></p>
<p>最终列出标量化的1-pass Attention形式： <span class="math display">\[
\begin{aligned}
\text{for i in [1, N]}:&amp;\\
  x_i &amp;= Q[k, :]K^T[:, i]\\
  max_i &amp;= \max(max_{i-1}, x_i) \\
  sum_i^\prime &amp;= sum_{i-1} ^\prime e^{max_{i-1} - max_i} +e^{x_i -
max_i}\\
    o_i^\prime &amp;= o_{i-1}^\prime
\frac{sum_{i-1}^\prime}{sum_{i}^\prime}e^{max_{i-1} - max_i} +
\frac{e^{x_i - max_i}}{sum_i^\prime} V[i,:] \\
\text{end} \qquad \qquad\\
  O[k,:] &amp; = o_N
\end{aligned}
\]</span></p>
<h2 id="flash-attention-v1">Flash Attention v1</h2>
<p>上面推导出来的1-pass attention是基于标量循环的，对于flash
attention是需要按tile进行计算的，所以具体的公式还需要稍作修改。</p>
<p>首先列出普通的softmax计算公式：</p>
<p><span class="math display">\[
\begin{aligned}
X &amp; =  [x_1, \ldots , x_N] \\
max_N &amp; = \max(X) \\
  &amp;= \max([x_1, \ldots , x_N]) \\
f(X) &amp; = [f(x_1), \ldots , f(x_N)] \\
  &amp;= [e^{x_1 - max_N}, \ldots, e^{x_N - max_N}] \\
sum_N &amp; = \sum_{i = 1}^N f(x_i) \\
  &amp; = \sum_{i = 1}^N e^{x_i - max_N} \\
softmax(X) &amp; = \frac{ f(X)}{sum_N}
\end{aligned}
\]</span></p>
<p>现在来推导tiled softmax的计算公式， 那么假设现在的<span
class="math inline">\(X\)</span>是由两个长度为<span
class="math inline">\(N\)</span>的子向量组成的,
那么首先把它看成单个向量计算，然后拆分转换为可分治的公式： <span
class="math display">\[
\begin{aligned}
X &amp; = [x^1, x^2] \\
max_{2N} &amp; = \max([\max(x^1),\max(x^2)]) \\
  &amp; = \max([max_N^1, max_N^2]) \\
f(X) &amp;= \left[ [e^{x_1^1 - max_{2N}},\ldots, e^{x_N^1 - max_{2N}}] ,
[e^{x_1^2 - max_{2N}},\ldots, e^{x_N^2 - max_{2N}}] \right] \\
    &amp;= \left[ e^{max_N^1 - max_{2N}} [e^{x_1^1 - max_N^1},\ldots,
e^{x_N^1 - max_N^1}] , e^{max_N^2 - max_{2N}} [e^{x_1^2 -
max_N^2},\ldots, e^{x_N^2 - max_N^2}] \right] \\
    &amp;= \left[ e^{max_N^1 - max_{2N}} f(x^1) , e^{max_N^2 - max_{2N}}
f(x^2) \right] \\
sum_{2N} &amp; = \sum_{i = 1}^N e^{x_i^1 - max_{2N}} + \sum_{i = 1}^N
e^{x_i^2 - max_{2N}} \\
         &amp; = e^{max_N^1 - max_{2N}} \sum_{i = 1}^N e^{x_i^1 -
max_{N}^1} + e^{max_N^2 - max_{2N}} \sum_{i = 1}^N e^{x_i^2 - max_{N}^2}
\\
         &amp; = e^{max_N^1 - max_{2N}} sum_N^1 + e^{max_N^2 - max_{2N}}
sum_N^2 \\
softmax(X) &amp;= \frac{f(X)}{sum_{2N}}
\end{aligned}
\]</span></p>
<p>此时可以发现，除了每个子向量的 <span
class="math inline">\(max^j\)</span> 用于计算 <span
class="math inline">\(f(x^j), sum^j\)</span>，还需要维护整体的<span
class="math inline">\(max, sum\)</span>用于计算最终的结果。</p>
<p>flash attention的tiling就是将<span
class="math inline">\(x_i\)</span>向量化,基于1-pass
attention的公式，结合tiled softmax公式，只需要略微修改<span
class="math inline">\(max_i,sum_i\)</span>的计算即可得到flash
attention的公式： <span class="math display">\[
\begin{aligned}
\text{for i in [1, N/b]}:&amp;\\
  x_i &amp;= Q[k, :]K^T[:, i:i+b]\\
  max_i^{local} &amp;= max(x_i) \\
  max_i &amp;= \max(max_{i-1}, max_i^{local}) \\
  sum_i^\prime &amp;= sum_{i-1} ^\prime e^{max_{i-1} - max_i} +
\sum_{j=1}^{b} e^{x_i[j] - max_i}\\
  o_i^\prime &amp;= o_{i-1}^\prime
\frac{sum_{i-1}^\prime}{sum_{i}^\prime}e^{max_{i-1} - max_i} +
\sum_{j=1}^b \frac{e^{x_i[j] - max_i}}{sum_i^\prime} V[(i-1)b+j,:] \\
\text{end}\qquad \qquad\\
  O[k,:] &amp; = o_N
\end{aligned}
\]</span></p>
<p>附上一个简易的flash attention实现供参考：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pytest</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.set_printoptions(suppress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">flash_attn</span>(<span class="params">query: np.ndarray, key: np.ndarray, value: np.ndarray, attn_mask=<span class="literal">None</span>, dropout_p=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">               is_causal=<span class="literal">False</span>, scale=<span class="literal">None</span>, enable_gqa=<span class="literal">False</span></span>):</span><br><span class="line">  L, S = query.shape[-<span class="number">2</span>], key.shape[-<span class="number">2</span>]</span><br><span class="line">  scale_factor = <span class="number">1</span> / math.sqrt(query.size(-<span class="number">1</span>)) <span class="keyword">if</span> scale <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> scale</span><br><span class="line">  attn_bias = np.zeros((L, S), dtype=query.dtype)</span><br><span class="line">  <span class="keyword">if</span> is_causal:</span><br><span class="line">    <span class="keyword">assert</span> attn_mask <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    temp_mask = np.tril(np.ones((L, S), dtype=np.bool_), k=<span class="number">0</span>)</span><br><span class="line">    attn_bias[<span class="literal">False</span> == temp_mask] = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> attn_mask.dtype == np.bool_:</span><br><span class="line">      attn_bias[<span class="literal">False</span> == attn_mask] = -np.inf</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      attn_bias = attn_mask + attn_bias</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> enable_gqa <span class="keyword">is</span> <span class="literal">False</span>, <span class="string">&quot;GQA not implemented&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> head <span class="keyword">in</span> <span class="built_in">range</span>(query.shape[<span class="number">0</span>]):</span><br><span class="line">    Q = query[head]  <span class="comment"># [query_len, dim]</span></span><br><span class="line">    K = key[head]  <span class="comment"># [seq_len, dim]</span></span><br><span class="line">    V = value[head]  <span class="comment"># [seq_len, dim]</span></span><br><span class="line">    O = np.zeros_like(Q, dtype=np.float32)  <span class="comment"># [query_len, dim]</span></span><br><span class="line">    Tc = <span class="number">4</span></span><br><span class="line">    Tr = <span class="number">16</span></span><br><span class="line">    <span class="keyword">assert</span> L % Tr == <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> S % Tc == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    global_maxs = np.zeros([Tr, L // Tr], dtype=np.float32)</span><br><span class="line">    global_sums = np.zeros([Tr, L // Tr], dtype=np.float32)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, S, Tc):</span><br><span class="line">      <span class="comment"># outer loop is seq_len, because seq_len is `K` dimension, we can reuse Kj,Vj `query_len/Tc` times</span></span><br><span class="line">      Kj = K[j:j + Tc, :]  <span class="comment"># [Tc, dim]</span></span><br><span class="line">      Vj = V[j:j + Tc, :]  <span class="comment"># [Tc, dim]</span></span><br><span class="line">      <span class="keyword">for</span> (ii, i) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">range</span>(<span class="number">0</span>, L, Tr)):</span><br><span class="line">        <span class="comment"># load</span></span><br><span class="line">        Qi = Q[i:i + Tr, :]  <span class="comment"># [Tr, dim]</span></span><br><span class="line">        O_last = O[i:i + Tr, :]  <span class="comment"># [Tr, dim]</span></span><br><span class="line">        max_last = (np.zeros([Tr, <span class="number">1</span>], dtype=np.float32) - np.inf) <span class="keyword">if</span> j == <span class="number">0</span> <span class="keyword">else</span> global_maxs[:, ii:ii + <span class="number">1</span>]</span><br><span class="line">        sum_last = np.zeros([Tr, <span class="number">1</span>], dtype=np.float32) <span class="keyword">if</span> j == <span class="number">0</span> <span class="keyword">else</span> global_sums[:, ii:ii + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        a_ij = (Qi @ Kj.T) * scale_factor  <span class="comment"># [Tr, Tc]</span></span><br><span class="line">        a_ij += attn_bias[i:i + Tr, j:j + Tc]</span><br><span class="line">        max_local = np.<span class="built_in">max</span>(a_ij, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)  <span class="comment"># [Tr, 1]</span></span><br><span class="line">        max_i = np.maximum(max_last, max_local)  <span class="comment"># [Tr, 1]</span></span><br><span class="line">        p_ij = np.exp(a_ij - max_i)  <span class="comment"># [Tr, Tc]</span></span><br><span class="line">        sum_i = sum_last * np.exp(max_last - max_i) + np.<span class="built_in">sum</span>(p_ij, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)  <span class="comment"># [Tr, 1]</span></span><br><span class="line">        O_i = O_last * (sum_last * np.exp(max_last - max_i)) / sum_i + (p_ij / sum_i) @ Vj  <span class="comment"># [Tr, dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># store</span></span><br><span class="line">        O[i:i + Tr, :] = O_i</span><br><span class="line">        global_maxs[:, ii:ii + <span class="number">1</span>] = max_i</span><br><span class="line">        global_sums[:, ii:ii + <span class="number">1</span>] = sum_i</span><br><span class="line">    <span class="keyword">return</span> O</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;head_q, head_kv&quot;</span>, [(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>)]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;query_len, seq_len&quot;</span>, [(<span class="params"><span class="number">64</span>, <span class="number">64</span></span>)]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;dim&quot;</span>, [<span class="number">128</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;is_causal&quot;</span>, [<span class="literal">False</span>, <span class="literal">True</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;scale&quot;</span>, [<span class="number">1.0</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_flash_attention</span>(<span class="params">head_q, head_kv, query_len, seq_len, dim, is_causal, scale</span>):</span><br><span class="line">  query = np.random.rand(head_q, query_len, dim).astype(np.float32)  <span class="comment"># [head_q, query_len, dim]</span></span><br><span class="line">  key = np.random.rand(head_kv, seq_len, dim).astype(np.float32)  <span class="comment"># [head_kv, seq_len, dim]</span></span><br><span class="line">  value = np.random.rand(head_kv, seq_len, dim).astype(np.float32)  <span class="comment"># [head_kv, seq_len, dim]</span></span><br><span class="line"></span><br><span class="line">  o = F.scaled_dot_product_attention(</span><br><span class="line">      torch.tensor(query), torch.tensor(key), torch.tensor(value), is_causal=is_causal, scale=scale)</span><br><span class="line">  o_np = o.numpy()  <span class="comment"># [q_head,query,dim]</span></span><br><span class="line"></span><br><span class="line">  o_actual = flash_attn(query, key, value, is_causal=is_causal, scale=scale)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> np.allclose(o_np, o_actual, atol=<span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  pytest.main([__file__, <span class="string">&quot;-vvs&quot;</span>])</span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E5%AD%90/" rel="tag">算子</a></li></ul></div><div class="post-nav"><a class="next" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a> <a href="/tags/%E7%AE%97%E5%AD%90/" style="font-size: 15px;">算子</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/09/26/flashattn/">Flash Attention记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>