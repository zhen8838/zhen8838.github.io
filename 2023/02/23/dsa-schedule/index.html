<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>带宽受限下的DSA后端Compute Schedule | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">带宽受限下的DSA后端Compute Schedule</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">带宽受限下的DSA后端Compute Schedule</h1><div class="post-meta">2023-02-23<span> | </span><span class="category"><a href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4.6k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 20</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">0. 准备工作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">1. 最简单的卷积实现</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E8%BF%9B%E8%A1%8C%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F"><span class="toc-number">3.</span> <span class="toc-text">2. 尝试进行硬件加速</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E5%87%8F%E5%B0%91%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">4.</span> <span class="toc-text">3. 尝试减少重复的数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E5%B0%86weights-stage%E5%88%B0oc%E5%BE%AA%E7%8E%AF%E5%A4%96"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 尝试将Weights
Stage到OC循环外</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E5%B0%86weights-stage%E5%88%B0oc%E5%BE%AA%E7%8E%AF%E5%86%85"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 尝试将Weights
Stage到OC循环内</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E6%96%B0%E7%9A%84%E5%88%87%E5%88%86%E7%BB%B4%E5%BA%A6"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 尝试新的切分维度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95stream-input"><span class="toc-number">5.</span> <span class="toc-text">4. 尝试stream input</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E8%BF%9B%E8%A1%8Celemwise%E7%AE%97%E5%AD%90%E7%9A%84layer-fusion."><span class="toc-number">6.</span> <span class="toc-text">5.
尝试进行Elemwise算子的Layer Fusion.</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E7%8B%AC%E6%89%A7%E8%A1%8C%E6%AF%8F%E4%B8%AA%E7%AE%97%E5%AD%90"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 单独执行每个算子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8Cfusion%E5%90%8E%E7%9A%84%E7%AE%97%E5%AD%90"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 执行Fusion后的算子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E8%BF%9B%E8%A1%8C%E9%9D%9Eelemwise%E7%AE%97%E5%AD%90%E7%9A%84layer-fusion."><span class="toc-number">7.</span> <span class="toc-text">6.
尝试进行非Elemwise算子的Layer Fusion.</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E7%8B%AC%E6%89%A7%E8%A1%8C%E6%AF%8F%E4%B8%AA%E7%AE%97%E5%AD%90-1"><span class="toc-number">7.1.</span> <span class="toc-text">6.1 单独执行每个算子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8Cfusion%E5%90%8E%E7%9A%84%E7%AE%97%E5%AD%90-1"><span class="toc-number">7.2.</span> <span class="toc-text">6.2 执行Fusion后的算子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">8.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><p>之前写过一篇带宽受限下的DSA后端优化, 不过主要是针对已经构建好Compute
Schedule之后的优化, 今天准备展开讲讲. 从单层卷积到优化计算,再到Layer
Fusion,以及后续各种优化,下面将通过一系列的例子来介绍:</p>
<span id="more"></span>
<h1 id="准备工作">0. 准备工作</h1>
<p>首先需要实现高层IR的Index Mapping进行Infer Bounds,
这里我导入一个已经实现好的卷积的BoundsInfer.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> TracedArray <span class="keyword">import</span> TarcedArray, GlobalHierarchy</span><br><span class="line"><span class="keyword">from</span> Conv2dBoundsInfer <span class="keyword">import</span> Conv2dBoundsInfer, Segments</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">Infer = Conv2dBoundsInfer(in_channels=<span class="number">2048</span>, out_channels=<span class="number">512</span>,</span><br><span class="line">    kernel_size=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding=(<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    stride=(<span class="number">1</span>, <span class="number">1</span>), dilation=(<span class="number">1</span>, <span class="number">1</span>), intput_shape=(<span class="number">1</span>, <span class="number">2048</span>, <span class="number">56</span>, <span class="number">56</span>),</span><br><span class="line">    test=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h1 id="最简单的卷积实现">1. 最简单的卷积实现</h1>
<p>假设我们的DSA有一个比较大的SRAM,
并且可以在这个SRAM上执行Tensor级别的操作,
约定好SRAM大小为<code>L2SIZE</code>.
这里引入GlobalHierarchy作为多级内存存储抽象,用于计算数据加载次数,
检查存储是否溢出. 那么考虑在上面编写一个最Navie的卷积.
为了匹配Tensor级别的计算操作,
我们将原本按1进行for循环执行的逻辑看作为按tile大小为1取tensor进行计算.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L2SIZE = <span class="number">1536</span> * <span class="number">1024</span> <span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo1</span>(<span class="params">imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  weight = TarcedArray(weightArr)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  (tileB, tileOC, tileOH, tileOW) = (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, tileB):</span><br><span class="line">    <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, tileOC):</span><br><span class="line">      <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, tileOH):</span><br><span class="line">        <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, tileOW):</span><br><span class="line">          <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">            <span class="comment"># 进入SRAM之后 从DDR中加在数据并计算.</span></span><br><span class="line">            outputTile = output[b, oc, oh, ow]</span><br><span class="line">            imageTile = image[Infer.get_input_segment(b, oc, oh, ow)]</span><br><span class="line">            weightTile = weight[Infer.get_w_segment(oc)]</span><br><span class="line">            outputTile += np.<span class="built_in">sum</span>((imageTile * weightTile), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;demo1 total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">demo1 total loaded : <span class="number">6578274304</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="尝试进行硬件加速">2. 尝试进行硬件加速</h1>
<p>可以发现我们重复加载了许多数据,也并没有加速计算,
但是实际上芯片中存在加速计算的硬件, 所以用以下方法来加速计算.</p>
<ol type="1">
<li><p>SRAM有足够空间的情况下,
可以尝试一次计算更大的tensor,也就是选择更大的tile size. 比如把W上的tile
size设置为最大, 把H上tile size加大.</p></li>
<li><p>假设我们有一个并行计算卷积部分输出的TensorCore,
一次最大并行输入16个input channel, 并行输出24个output channel.</p></li>
</ol>
<p>接下来就可以来改造compute schedule:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CORE_OC = <span class="number">24</span> <span class="comment"># TensorCore并行限制</span></span><br><span class="line">CORE_IC = <span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">TensorCore</span>(<span class="params">image: np.ndarray, weight: np.ndarray</span>) -&gt; np.ndarray:</span><br><span class="line">  <span class="comment"># 这里假设硬件可以自动循环</span></span><br><span class="line">  <span class="keyword">return</span> torch.conv2d(torch.tensor(image), torch.tensor(weight)).numpy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo2</span>(<span class="params">imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  weight = TarcedArray(weightArr)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, CORE_OC):</span><br><span class="line">      <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, <span class="number">2</span>):</span><br><span class="line">        <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">          <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">            outputTile = output[b, oc, oh, ow]</span><br><span class="line">            imageTile = image[Infer.get_input_segment(b, oc, oh, ow)]</span><br><span class="line">            weightTile = weight[Infer.get_w_segment(oc)]</span><br><span class="line">            outputTile += TensorCore(imageTile, weightTile)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;demo2 total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line">demo2 total loaded : <span class="number">179651584</span></span><br></pre></td></tr></table></figure>
<h1 id="尝试减少重复的数据加载">3. 尝试减少重复的数据加载</h1>
<p>可以发现demo2减少了许多数据加载,
但从事高性能计算的朋友们应该可以发现对weight来说, 如果在OH/OW有切分,
那么在OH/OW循环内都是每次加载相同的weights[ic,kh,kw].
那么我们就有两个选择来解决这个问题:</p>
<ol type="1">
<li><p>把weights加载的时机移动到OC循环内部或OC循环外部去加载,
这样在OH/OW的循环中可以不用load重复的weights了.</p></li>
<li><p>我们还可以增加OH/OW的tile size,然后再添加一个IC的切分维度,
这样每个循环也不会重复加载weights了, 但是值得注意的是此时output
tile需要移动到oc循环内.</p></li>
</ol>
<h2 id="尝试将weights-stage到oc循环外">3.1 尝试将Weights
Stage到OC循环外</h2>
<p>这里就是在SRAM中保存所有的weights, 实际上在stage到OC循环外之后,
我们还可以选择在OC循环内逐步的加载weights以进行流水.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">demo3_1</span>(<span class="params">imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  weight = TarcedArray(weightArr)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">      reuse = <span class="literal">False</span> <span class="comment"># 为了简单起见, 添加reuse的参数来避免重复统计load的数据, 其实应该把allocate buffer和load/store的逻辑分离出来.</span></span><br><span class="line">      weightTile = weight[Infer.get_w_segment(<span class="built_in">slice</span>(<span class="number">0</span>, OC))]  <span class="comment"># 将weights加载移动到OC循环外 也就是一次加载所有的权重</span></span><br><span class="line">      <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, CORE_OC):</span><br><span class="line">        <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, <span class="number">2</span>):</span><br><span class="line">          <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">            outputTile = output[b, oc, oh, ow]</span><br><span class="line">            imageTile = image[Infer.get_input_segment(b, oc, oh, ow), reuse]  <span class="comment"># 重用同一份SRAM</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> reuse:</span><br><span class="line">              reuse = <span class="literal">True</span></span><br><span class="line">            weightSubTile = weightTile[oc]</span><br><span class="line">            outputTile += TensorCore(imageTile, weightSubTile)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;demo3-1 total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line">demo3-<span class="number">1</span> We can<span class="string">&#x27;t move load weights statement out of OC!</span></span><br></pre></td></tr></table></figure>
<p>不过实际情况会发现因SRAM空间不够而出错. 这就是SRAM大小影响compute
schedule.</p>
<h2 id="尝试将weights-stage到oc循环内">3.2 尝试将Weights
Stage到OC循环内</h2>
<p>把weights stage在OC循环内部, 这样可以在OH/OW循环中复用.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">demo3_2</span>(<span class="params">imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  weight = TarcedArray(weightArr)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, CORE_OC):</span><br><span class="line">      <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">        reuse = <span class="literal">False</span></span><br><span class="line">        weightTile = weight[Infer.get_w_segment(oc)]</span><br><span class="line">        <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, <span class="number">2</span>):</span><br><span class="line">          <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">            outputTile = output[b, oc, oh, ow]</span><br><span class="line">            imageTile = image[Infer.get_input_segment(b, oc, oh, ow), reuse]  <span class="comment"># 重用同一份SRAM</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> reuse:</span><br><span class="line">              reuse = <span class="literal">True</span></span><br><span class="line">            outputTile += TensorCore(imageTile, weightTile)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;demo3-2 total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line">demo3-<span class="number">2</span> total loaded : <span class="number">9586432</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>虽然这个例子和<code>demo3-1</code>实际上差别不大,
但是主要是用于说明SRAM对于不同的Compute Schedule的限制.</p>
<h2 id="尝试新的切分维度">3.3 尝试新的切分维度</h2>
<p>注意这里我们添加一个IC的切分维度,
这样每次内部循环加载的就是不同的weights[oc,ic,:,:]的tile了.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">demo3_3</span>(<span class="params">imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray, prefix=<span class="string">&quot;demo3-3&quot;</span></span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  weight = TarcedArray(weightArr)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  IC = imageArr.shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, <span class="number">8</span>):</span><br><span class="line">      <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, OH):</span><br><span class="line">        <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">          <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">            reuse = <span class="literal">False</span></span><br><span class="line">            outputTile = output[b, oc, oh, ow]</span><br><span class="line">            <span class="keyword">for</span> ic <span class="keyword">in</span> Segments(<span class="number">0</span>, IC, CORE_IC):</span><br><span class="line">              wSeg = Infer.get_w_segment(oc)</span><br><span class="line">              wSeg[<span class="number">1</span>] = ic  <span class="comment"># add slice in ic</span></span><br><span class="line">              weightTile = weight[wSeg, reuse]</span><br><span class="line">              imageSeg = Infer.get_input_segment(b, oc, oh, ow)</span><br><span class="line">              imageSeg[<span class="number">1</span>] = ic  <span class="comment"># add slice in ic</span></span><br><span class="line">              imageTile = image[imageSeg, reuse]</span><br><span class="line">              outputTile += TensorCore(imageTile, weightTile)</span><br><span class="line">              <span class="keyword">if</span> reuse <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">                reuse = <span class="literal">True</span>  <span class="comment"># reuse same buffer.</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line">  <span class="built_in">print</span>(prefix, <span class="string">&quot;total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line">demo3-<span class="number">3</span> total loaded : <span class="number">4825088</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>此时对于weights的加载比之前减少了一倍, 但是如果此时OH/OW有切分,
那么同一份ic的weights也会被多次加载. 而需要注意的是SRAM的大小有限,
所以必须缩小OC上的tile size, 来保证当前策略较优. 这就是tile
size与SRAM大小共同影响compute schedule.</p>
<h1 id="尝试stream-input">4. 尝试stream input</h1>
<p>我们还可以尝试移动image stage到外层的循环,
因为每个oc内都加载了全部的input image,
那么将image移动到外层循环就可以减少许多重复的数据加载.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo4</span>(<span class="params">imageArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  weight = TarcedArray(weightArr)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  IC = imageArr.shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">      reuse = <span class="literal">False</span></span><br><span class="line">      imageTile = image[Infer.get_input_segment(b, <span class="built_in">slice</span>(<span class="number">0</span>, OC), <span class="built_in">slice</span>(<span class="number">0</span>, OH), <span class="built_in">slice</span>(<span class="number">0</span>, OW))]</span><br><span class="line">      <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, <span class="number">8</span>):</span><br><span class="line">        <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, OH):</span><br><span class="line">          <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">            outputTile = output[(b, oc, oh, ow), reuse]</span><br><span class="line">            <span class="keyword">for</span> ic <span class="keyword">in</span> Segments(<span class="number">0</span>, IC, CORE_IC):</span><br><span class="line">              wSeg = Infer.get_w_segment(oc)</span><br><span class="line">              wSeg[<span class="number">1</span>] = ic  <span class="comment"># add slice in ic</span></span><br><span class="line">              weightTile = weight[wSeg, reuse]</span><br><span class="line">              outputTile += TensorCore(imageTile[:, ic, :, :], weightTile)</span><br><span class="line">              <span class="keyword">if</span> reuse <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">                reuse = <span class="literal">True</span>  <span class="comment"># reuse same buffer.</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;demo4 total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line">demo4 We can<span class="string">&#x27;t move load image statement out of OC!</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>
<p>但是很可惜这个卷积的image比较大,
如果所以移动到外循环会因为SRAM存不下而报错. 这就是compute
schedule与不同的tile size可以相互影响的情况.</p>
<h1 id="尝试进行elemwise算子的layer-fusion.">5.
尝试进行Elemwise算子的Layer Fusion.</h1>
<h2 id="单独执行每个算子">5.1 单独执行每个算子</h2>
<p>以上实验了单层卷积的情况, 我们尝试了调整tile size/调整buffer
stage的位置来减少总的数据加载次数.
接下来我们需要考虑多个算子fusion的情况,
假设卷积前面不是一个带有reduction的算子, 比如binary add,
首先单独执行两个算子.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo5_1</span>(<span class="params">imageArr: np.ndarray, biasArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  bias = TarcedArray(biasArr)</span><br><span class="line">  mid = TarcedArray(np.zeros_like(imageArr))</span><br><span class="line">  <span class="comment"># binary add</span></span><br><span class="line">  (B, C, H, W) = imageArr.shape</span><br><span class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> Segments(<span class="number">0</span>, C, <span class="number">8</span>):</span><br><span class="line">      <span class="keyword">for</span> h <span class="keyword">in</span> Segments(<span class="number">0</span>, H, H):</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> Segments(<span class="number">0</span>, W, W):</span><br><span class="line">          <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">            imageTile = image[b, c, h, w]</span><br><span class="line">            biasTile = bias[b, c, h, w]</span><br><span class="line">            midTile = mid[b, c, h, w]</span><br><span class="line">            midTile += imageTile + biasTile</span><br><span class="line"></span><br><span class="line">  demo3_3(mid._array, weightArr, outputArr, targetOutput, <span class="string">&quot;demo5-1&quot;</span>)</span><br><span class="line"></span><br><span class="line">demo5-<span class="number">1</span> total loaded : <span class="number">30515200</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>那么每次算子执行结束后, 数据需要出DDR再回到SRAM,
这样就消耗了许多带宽.</p>
<h2 id="执行fusion后的算子">5.2 执行Fusion后的算子</h2>
<p>我们可以发现,后面卷积的循环[B,OH,OW,IC]分别可以对应前面binary的[B,H,W,C],
其实即前面binary的H与W是可以依据卷积的tile size来确定,
他的C维度依据卷积的IC维度确定, 并且因为这个binary计算时没有元素依赖关系,
所以简单调整他的循环顺序我们就可以进行算子Fusion了,
也就是在IC的循环中计算elemwise的计算操作.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo5_2</span>(<span class="params">imageArr: np.ndarray, biasArr: np.ndarray, weightArr: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  bias = TarcedArray(biasArr)</span><br><span class="line">  weight = TarcedArray(weightArr)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  IC = imageArr.shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, <span class="number">8</span>):</span><br><span class="line">      <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, OH):</span><br><span class="line">        <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">          <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">            reuse = <span class="literal">False</span></span><br><span class="line">            outputTile = output[b, oc, oh, ow]</span><br><span class="line">            <span class="keyword">for</span> ic <span class="keyword">in</span> Segments(<span class="number">0</span>, IC, CORE_IC):</span><br><span class="line">              wSeg = Infer.get_w_segment(oc)</span><br><span class="line">              wSeg[<span class="number">1</span>] = ic  <span class="comment"># add slice in ic</span></span><br><span class="line">              weightTile = weight[wSeg, reuse]</span><br><span class="line">              imageSeg = Infer.get_input_segment(b, oc, oh, ow)</span><br><span class="line">              imageSeg[<span class="number">1</span>] = ic  <span class="comment"># add slice in ic</span></span><br><span class="line">              imageTile = image[imageSeg, reuse]</span><br><span class="line">              biasTile = bias[imageSeg, reuse]</span><br><span class="line">              outputTile += TensorCore(imageTile + biasTile, weightTile)</span><br><span class="line">              <span class="keyword">if</span> reuse <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">                reuse = <span class="literal">True</span>  <span class="comment"># reuse same buffer.</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;demo5-2 total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line">demo5-<span class="number">2</span> total loaded : <span class="number">8036352</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以发现减少了两倍的数据搬运.</p>
<h1 id="尝试进行非elemwise算子的layer-fusion.">6.
尝试进行非Elemwise算子的Layer Fusion.</h1>
<h2 id="单独执行每个算子-1">6.1 单独执行每个算子</h2>
<p>首先测试两层卷积单独执行的数据加载.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo6</span>(<span class="params">Infer1: Conv2dBoundsInfer, Infer2: Conv2dBoundsInfer, imageArr: np.ndarray, weightArr1: np.ndarray, weightArr2: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  weight1 = TarcedArray(weightArr1)</span><br><span class="line">  tempOutput = TarcedArray(np.zeros(Infer2.in_shape).astype(np.float32))</span><br><span class="line">  (B, OC, OH, OW) = Infer2.in_shape</span><br><span class="line">  <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">    reuse = <span class="literal">False</span></span><br><span class="line">    weight1Tile = weight1[:, :, :, :]</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">      <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, <span class="number">16</span>):</span><br><span class="line">        <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, <span class="number">48</span>):</span><br><span class="line">          <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">            outputTile = tempOutput[(b, oc, oh, ow), reuse]</span><br><span class="line">            imageSeg = Infer1.get_input_segment(b, oc, oh, ow)</span><br><span class="line">            imageTile = image[imageSeg, reuse]</span><br><span class="line">            outputTile += TensorCore(imageTile, weight1Tile[oc])</span><br><span class="line">            <span class="keyword">if</span> reuse <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">              reuse = <span class="literal">True</span>  <span class="comment"># reuse same buffer.</span></span><br><span class="line"></span><br><span class="line">  weight2 = TarcedArray(weightArr2)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">    reuse = <span class="literal">False</span></span><br><span class="line">    weight2Tile = weight2[:, :, :, :]</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">      <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, <span class="number">16</span>):</span><br><span class="line">        <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, <span class="number">48</span>):</span><br><span class="line">          <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">            outputTile = output[(b, oc, oh, ow), reuse]</span><br><span class="line">            imageSeg = Infer2.get_input_segment(b, oc, oh, ow)</span><br><span class="line">            imageTile = tempOutput[imageSeg, reuse]</span><br><span class="line">            outputTile += TensorCore(imageTile, weight2Tile[oc])</span><br><span class="line">            <span class="keyword">if</span> reuse <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">              reuse = <span class="literal">True</span>  <span class="comment"># reuse same buffer.</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;demo6 total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line">demo6 total loaded : <span class="number">722528</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="执行fusion后的算子-1">6.2 执行Fusion后的算子</h2>
<p>假设我们遇到前面一个算子是带有reduction的情况, 比如卷积+卷积.
那么只需要考虑将两层卷积的循环直接合并即可, 同时现在的compute
schedule就不能和单层卷积时相同了, 在两层卷积的循环直接合并时,
我们无法在最后一层卷积的input channel上切分, 因为后一个卷积的每一份input
channel都依赖前面一个卷积的所有input channel,
这样切分会导致前面的卷积的weights反复加载,
目前我实现的多层卷积的合并必须要在SRAM中可以存下所有的weights才可以.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">demo6_1</span>(<span class="params">Infer1: Conv2dBoundsInfer, Infer2: Conv2dBoundsInfer, imageArr: np.ndarray, weightArr1: np.ndarray, weightArr2: np.ndarray, outputArr: np.ndarray, targetOutput: np.ndarray</span>):</span><br><span class="line">  image = TarcedArray(imageArr)</span><br><span class="line">  weight1 = TarcedArray(weightArr1)</span><br><span class="line">  weight2 = TarcedArray(weightArr2)</span><br><span class="line">  output = TarcedArray(outputArr)</span><br><span class="line">  (B, OC, OH, OW) = outputArr.shape</span><br><span class="line">  IC = imageArr.shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">with</span> GlobalHierarchy(L2SIZE):</span><br><span class="line">    reuse = <span class="literal">False</span></span><br><span class="line">    weight1Tile = weight1[:, :, :, :]</span><br><span class="line">    weight2Tile = weight2[:, :, :, :]</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> Segments(<span class="number">0</span>, B, <span class="number">1</span>):</span><br><span class="line">      <span class="keyword">for</span> oc <span class="keyword">in</span> Segments(<span class="number">0</span>, OC, <span class="number">16</span>):</span><br><span class="line">        <span class="keyword">for</span> oh <span class="keyword">in</span> Segments(<span class="number">0</span>, OH, <span class="number">48</span>):</span><br><span class="line">          <span class="keyword">for</span> ow <span class="keyword">in</span> Segments(<span class="number">0</span>, OW, OW):</span><br><span class="line">            outputTile = output[(b, oc, oh, ow), reuse]</span><br><span class="line">            imageSeg2 = Infer2.get_input_segment(b, oc, oh, ow)</span><br><span class="line">            imageSeg1 = Infer1.get_input_segment(</span><br><span class="line">                imageSeg2[<span class="number">0</span>], imageSeg2[<span class="number">1</span>], imageSeg2[<span class="number">2</span>], imageSeg2[<span class="number">3</span>])</span><br><span class="line">            imageTile1 = image[imageSeg1, reuse]</span><br><span class="line">            imageTile2 = TensorCore(imageTile1, weight1Tile)</span><br><span class="line">            outputTile += TensorCore(imageTile2, weight2Tile[oc])</span><br><span class="line">            <span class="keyword">if</span> reuse <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">              reuse = <span class="literal">True</span>  <span class="comment"># reuse same buffer.</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> (np.allclose(output._array, targetOutput, atol=<span class="number">1e-5</span>))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;demo6-1 total loaded :&quot;</span>, GlobalHierarchy.TotalLoaded)</span><br><span class="line">  GlobalHierarchy.Reset()</span><br><span class="line"></span><br><span class="line">demo6-<span class="number">1</span> total loaded : <span class="number">206432</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="总结">总结</h1>
<p>宏观上, 我把一个Fused Layer内部计算(循环切分与buffer
stage等)优化称为<code>Compute Schedule</code>, 而我之前的一篇<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/585176512">文章</a>在Fused
Layer外部流水(buffer size search/ping pong
buffer)优化称为<code>Buffer Schedule</code>.
我的理解是硬件架构决定了目前的<code>Compute Schedule</code>可能性,
接下来由软件来实现各种<code>Compute Schedule Pattern</code>,
后续再根据这些计算模式进行<code>Buffer Schedule</code>,
此时根据<code>Buffer Schedule</code>的结果来选择最优的<code>Compute Schedule</code>,
如此迭代才能尽量发挥硬件性能. 当然如果硬件架构给出的执行方式少,
那么对应的软件也简单, 否则硬件的灵活性大, 软件优化的难度也高.
整个系统的能力也就是软硬件协调程度的体现.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    ┌───────────────────────────────────────────────────────────────────────┐</span><br><span class="line">    │                                                                       │</span><br><span class="line">    │                                                                       │</span><br><span class="line">    │                    ┌──────────────────────────────┐                   │</span><br><span class="line">    │                    │                              │                   │</span><br><span class="line">    │                    │                              │                   │</span><br><span class="line">┌───┴────┐       ┌───────┴────────┐             ┌───────▼───────┐      ┌────▼───┐</span><br><span class="line">│Hardware├───────►Compute Schedule│             │Buffer Schedule├──────►Software│</span><br><span class="line">└───▲────┘       └───────▲────────┘             └───────┬───────┘      └────┬───┘</span><br><span class="line">    │                    │                              │                   │</span><br><span class="line">    │                    │                              │                   │</span><br><span class="line">    │                    └╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┘                   │</span><br><span class="line">    │                                                                       │</span><br><span class="line">    └╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┘</span><br></pre></td></tr></table></figure>
<p>微观上, 对于一个算子来说,
根据循环顺序/变量分配时机/切分方式的不同会导致显著的性能差距. 比如卷积,
为了尽量减少重复load weights可以在OC/IC进行切分.
同时如果考虑存储足够大还可以在不同循环维度去stage local buffer,
在内层循环里进行复用. 然后还有tile size search的优化,
上面的例子可能描述的不多, 但其实每种不同的计算模式还需要与之配套的tile
search逻辑.
比如多层卷积fusion时需要尽量同时增大H和W来减少overlap还是先增大W保持连续的load;
单层卷积时先增大IC维度的tile还是OC维度的tile来满足内部TensorCore的利用率;
单层卷积是选择多占用一些SRAM来stream
input还是增大一些OC/IC的tile来减少循环次数, 总之这套search tile
size逻辑都需要和compute schedule以及硬件特性相匹配的. 接下来还有buffer
schedule优化, 也就是多个tiled block之间,
开多少块buffer进行并行流水比较合适,
不同buffer数量时需要自动安排好每块buffer的生命周期, 内外循环都有ping
pong时对于buffer正确访问, 硬件对于buffer的stride/shape限制,
代码展开的时候需要考虑软件流水,
最后还需要分析buffer读写依赖自动插入同步指令等.
最后假设如果search出来的tile size不合适,
比如切分的OC/IC太小硬件利用率不高, 那么可能还需要调整到别的compute
schedule来重新走一遍上述流程.</p>
<p>关于手写算子与自动Fusion. 如果是对于我上文描述的那样,
有很多种不同的计算方式可供选择,
那么对于手写算子来说就需要消耗许多的精力维护很多看似相同但是无法复用的逻辑,
所以需要有一种简单的DSL描述这些过程从而加速开发的迭代过程. 比如<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/451854416">TVM TensorIR</a>或<a
target="_blank" rel="noopener" href="https://exo-lang.dev">exo-lang</a>.
但感觉目前已有的技术还没法做到更自动化的Fusion, 因为多层Fusion的时候,
需要处理各种Index的变化, 比如DDR上的Tensor加载到SRAM之后不均匀切分,
每个循环所占据的SRAM Buffer大小并不一样,
并且对于卷积来说还有Padding的问题需要在SRAM中的Tile上处理好.
DSL本身最好是可以将中间依赖关系以及index变换隐藏在其背后,
降低编写算子时需要记忆的内容,
并且对于多个手写的代码块可以做到自动的分析循环的依赖来进行Fusion.</p>
<p>我在实现自动Fusion的过程中也发现了不少问题:</p>
<ol type="1">
<li>缺乏分析循环间的依赖性的技术</li>
</ol>
<blockquote>
<p>我首先是构建了Tensor维度与循环依赖的表达式,
发现无法去分析循环间的依赖性, 比如DW卷积的OC维度就等于IC维度,
Weights的IC维度此时依赖了OC维度,
而普通卷积的OC维度并不影响weights的IC维度,
因此就需要手动额外引入在IC维度上的TileVar.</p>
</blockquote>
<ol start="2" type="1">
<li>缺乏分析空间局部性的技术</li>
</ol>
<blockquote>
<p>如果可以直接从当前的Compute Schedule中发现如何移动循环或stage
buffer可以减少数据重复加载, 那么可以指导Compute Schedule,
目前以上的优化方案还都是靠观察得到的.</p>
</blockquote>
<ol start="3" type="1">
<li>如何将硬件限制更好的描述到Fusion中</li>
</ol>
<blockquote>
<p>因为不同的case下总是需要为了兼容硬件bug/执行效率做出奇奇怪怪的修改.
比如我设计的规则是所有的L2上的Buffer按使用大小来申请,
但是由于硬件对于数据加载的速度问题, 有时候还需要申请更大的空间.
或者是硬件存在某种bug, 特定算子Fuse在一起时不能开启某些功能.
但是这些约束很难用一种通用的接口描述到自动生成的规则中.
只能在自动化的过程中hard code.</p>
</blockquote>
<ol start="4" type="1">
<li>如何更好将自动Fusion与手写算子结合</li>
</ol>
<blockquote>
<p>比如上面<code>demo4</code>的情况, 在SRAM有空余的时候,
我想在合适的地方stage image buffer,
对于手写算子来说可能就是移动几行代码的事情,
坐标变换就按当前的情况手写一个公式即可.
而自动优化为了通用还需要写许多的判断/转换.</p>
</blockquote>
<p>最终就是各种硬件规则限制/性能优化trick的问题把原本规整的自动Fusion代码切分的支离破碎,
因为整套逻辑都通用,
每次都需要这一处改动还需要考虑是不是会对其他应用的地方产生额外的影响,
导致我花费了更多的精力, 算是让我体会到了<code>worse is better</code>.
不过也有可能是我实现的自动Fusion的功能太弱,
后续再学习一些多面体的知识看一下能否有帮助.</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" rel="tag">后端优化</a></li></ul></div><div class="post-nav"><a class="pre" href="/2023/04/09/halide-metal/">halide metal 初体验</a><a class="next" href="/2023/02/10/egg-bad-case/">Equality Saturation优化在AI编译器中遇到的挑战</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>