<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Tensor DSL总结 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Tensor DSL总结</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Tensor DSL总结</h1><div class="post-meta">2023-12-20<span> | </span><span class="category"><a href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 5.5k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 30</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>本文旨在总结一些张量优化的DSL是如何设计的, 尝试从其中发现一些共同点.
接下来我将统一使用<code>Matmul(Transpose(Conv(lhs)),rhs)</code>的例子在不同的框架中进行测试.</p>
<span id="more"></span>
<h1 id="jittor">1. <a
target="_blank" rel="noopener" href="http://scis.scichina.com/en/2020/222103.pdf">Jittor</a></h1>
<h2 id="dsl语法">1.1 DSL语法</h2>
<p>首先结合论文中的例子讲一下<code>reindex</code>的原理:
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv</span>(<span class="params">x, p</span>):</span><br><span class="line">  N,C,H,W = x.shape </span><br><span class="line">  o,i,h,w = p.shape </span><br><span class="line">  xx = x.reindex(shape=(N,o,H,W,i,h,w),</span><br><span class="line">                 indexes=(<span class="string">&quot;i0&quot;</span>, <span class="string">&quot;i4&quot;</span>, <span class="string">&quot;i2-i5&quot;</span>, <span class="string">&quot;i3-i6&quot;</span>) )</span><br><span class="line">  pp = p.broadcast(xx.shape, dims=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">  yy = xx*pp</span><br><span class="line">  y = yy.<span class="built_in">sum</span>(dims=(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">  <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure></p>
<p>这里其实是把<code>shape</code>看作为循环层级,
这里的<code>reindex</code>相当于在7层循环的最内层中做类似<code>xx[N,o,H,W,i,h,w] = x[N,i,H-h,W-w]</code>的索引.
然后再把<code>weights</code>也通过<code>boradcast</code>扩展到同样的循环层级<code>pp[N,o,H,W,i,h,w] = p[o,i,h,w]</code>,
在7层循环内部执行<code>xx[N,o,H,W,i,h,w]*pp[N,o,H,W,i,h,w]</code>的操作,
等价于执行<code>x[N,i,H-h,W-w] * p[o,i,h,w]</code>,
然后对<code>i,h,w</code>三层循环做求和.</p>
<p>可以说通过<code>reindex</code>+<code>broadcast</code>操作,
完成了类似于<code>polyhedral</code>中2d+1表示中的<code>loop dimension align</code>和修改<code>access relation</code>(由<code>indexes</code>指定).
<code>Jittor</code>这里并没有考虑让开发者自行调度算子,
后续的优化都交给编译器自动化.</p>
<h2 id="测试例子">1.2 测试例子</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jittor <span class="keyword">as</span> jt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv</span>(<span class="params">x, p</span>):</span><br><span class="line">  N,C,H,W = x.shape </span><br><span class="line">  o,i,h,w = p.shape </span><br><span class="line">  xx = x.reindex(shape=(N,o,H,W,i,h,w),</span><br><span class="line">                 indexes=(<span class="string">&quot;i0&quot;</span>, <span class="string">&quot;i4&quot;</span>, <span class="string">&quot;i2-i5&quot;</span>, <span class="string">&quot;i3-i6&quot;</span>) )</span><br><span class="line">  pp = p.broadcast(xx.shape, dims=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">  yy = xx*pp</span><br><span class="line">  y = yy.<span class="built_in">sum</span>(dims=(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul</span>(<span class="params">a,b</span>):</span><br><span class="line">  bc, c, m,k = a.shape</span><br><span class="line">  _, _, _,n = b.shape</span><br><span class="line">  shape = [bc, c, m, k, n]</span><br><span class="line">  a = a.broadcast(shape, [-<span class="number">1</span>]) <span class="comment"># [m,k, ] -&gt; [m,k,n]</span></span><br><span class="line">  b = b.broadcast(shape, [-<span class="number">3</span>]) <span class="comment"># [ ,k,n] -&gt; [m,k,n]</span></span><br><span class="line">  <span class="keyword">return</span> (a*b).<span class="built_in">sum</span>(-<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">lhs = jt.randn(<span class="number">8</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">kernel = jt.randn(<span class="number">16</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">rhs = jt.randn(<span class="number">8</span>,<span class="number">32</span>,<span class="number">16</span>,<span class="number">64</span>)</span><br><span class="line">jt.flags.compile_options=&#123;<span class="string">&quot;compile_shapes&quot;</span>:<span class="number">1</span>&#125;</span><br><span class="line"><span class="keyword">with</span> jt.profile_scope() <span class="keyword">as</span> report:</span><br><span class="line">    output = matmul(jt.transpose(conv(lhs, kernel), [<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]), rhs).fetch_sync()</span><br><span class="line">jt.flags.compile_options=&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>编译后得到: <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Profile result, sorted by TotalTime</span><br><span class="line">(<span class="string">&#x27;it/s&#x27;</span> represent number of iterations per sec)</span><br><span class="line">      Name  FileName     Count TotalTime    %,cum%   AvgTime   MinTime   MaxTime     Input    Output     InOut   Compute</span><br><span class="line">Total time:    12.8ms</span><br><span class="line">Total Memory Access:    6.19MB</span><br><span class="line">[opkey0:broadcast_to[Tx:float32][DIM=7][BCAST=d][JIT:1][JIT_cpu:1][index_t:int32]][opkey1:reindex[Tx:float32][XDIM=4][YDIM=7][OVERFLOW:itof(0x0)][INDEX0:i0][INDEX1:i4][INDEX2:i2-i5][INDEX3:i3-i6][OSIZE=0][ESIZE=0][JIT:1][JIT_cpu:1][index_t:int32]][opkey2:binary[Tx:float32][Ty:float32][Tz:float32][OP:multiply][JIT:1][JIT_cpu:1][index_t:int32]][opkey3:reduce[Tx:float32][Ty:float32][Tz:float32][OP:add][DIM=7][REDUCE=70][JIT:1][JIT_cpu:1][index_t:int32]][JIT:1][JIT_cpu:1][graph:040000,062010,010020,000021,020030,][var_info::041704171724][shapes:[10,3,3,3,],[8,10,20,20,3,3,3,],[8,3,20,20,],[8,10,20,20,3,3,3,],[8,10,20,20,3,3,3,],[8,10,20,20,],][choices:compile_shapes:1,]</span><br><span class="line">          /root/.cache/jittor/jt1.3.1/g++10.5.0/py3.8.18/Linux-5.4.0-42xae/AMDEPYC7T8364-x8f_debug/default/jit/_opkey0_broadcast_to_Tx_float32__DIM_7__BCAST_d__JIT_1__JIT_cpu_1__index_t_int32___opkey1____hash_a2d65b1fd1c3f3d0_op.cc</span><br><span class="line">                             1    8.12ms(63.3%,63.3%)    8.12ms    8.12ms    8.12ms  11.7MB/s  61.6MB/s  73.3MB/s  436Mit/s</span><br><span class="line">random[T:float32][R:normal][JIT:1][JIT_cpu:1][index_t:int32]</span><br><span class="line">          /root/.cache/jittor/jt1.3.1/g++10.5.0/py3.8.18/Linux-5.4.0-42xae/AMDEPYC7T8364-x8f_debug/default/jit/random_T_float32__R_normal__JIT_1__JIT_cpu_1__index_t_int32__hash_c27874d0aacc5d25_op.cc</span><br><span class="line">                             3    3.68ms(28.7%,91.9%)    1.23ms    5.58us    3.37ms     0 B/s   298MB/s   298MB/s   78Mit/s</span><br><span class="line">[opkey0:broadcast_to[Tx:float32][DIM=5][BCAST=4][JIT:1][JIT_cpu:1][index_t:int32]][opkey1:broadcast_to[Tx:float32][DIM=5][BCAST=10][JIT:1][JIT_cpu:1][index_t:int32]][opkey2:binary[Tx:float32][Ty:float32][Tz:float32][OP:multiply][JIT:1][JIT_cpu:1][index_t:int32]][opkey3:reduce[Tx:float32][Ty:float32][Tz:float32][OP:add][DIM=5][REDUCE=8][JIT:1][JIT_cpu:1][index_t:int32]][JIT:1][JIT_cpu:1][graph:040000,062010,010020,000021,020030,][var_info::041504151524][shapes:[8,20,10,40,],[8,20,20,10,40,],[8,20,20,10,],[8,20,20,10,40,],[8,20,20,10,40,],[8,20,20,40,],][choices:compile_shapes:1,]</span><br><span class="line">          /root/.cache/jittor/jt1.3.1/g++10.5.0/py3.8.18/Linux-5.4.0-42xae/AMDEPYC7T8364-x8f_debug/default/jit/_opkey0_broadcast_to_Tx_float32__DIM_5__BCAST_4__JIT_1__JIT_cpu_1__index_t_int32___opkey1____hash_ef35f9063cf2acdf_op.cc</span><br><span class="line">                             1     828us(6.45%,98.4%)     828us     828us     828us  1.77GB/s  2.36GB/s  4.13GB/s 10.1Git/s</span><br><span class="line">transpose[Tx:float32][DIM=4][AXES0=0][AXES2=1][AXES3=2][AXES1=3][JIT:1][JIT_cpu:1][index_t:int32]</span><br><span class="line">          /root/.cache/jittor/jt1.3.1/g++10.5.0/py3.8.18/Linux-5.4.0-42xae/AMDEPYC7T8364-x8f_debug/default/jit/transpose_Tx_float32__DIM_4__AXES0_0__AXES2_1__AXES3_2__AXES1_3__JIT_1__JIT_cpu_1__index_t_int32__hash_998b34c8052fe15_op.cc</span><br><span class="line">                             1     208us(1.62%,100%)     208us     208us     208us  2.35GB/s  2.35GB/s  4.71GB/s  632Mit/s</span><br></pre></td></tr></table></figure></p>
<p>最终我检查他的输出, 发现是分成了三个部分,
<code>_opkey0_broadcast_to_Tx_float32__DIM_7__BCAST_d__JIT_1__JIT_cpu_1__index_t_int32___opkey1____hash_a2d65b1fd1c3f3d0_op</code>为卷积实现,
<code>_opkey0_broadcast_to_Tx_float32__DIM_5__BCAST_4__JIT_1__JIT_cpu_1__index_t_int32___opkey1____hash_ef35f9063cf2acdf_op</code>为矩阵乘,
<code>transpose_Tx_float32__DIM_4__AXES0_0__AXES2_1__AXES3_2__AXES1_3__JIT_1__JIT_cpu_1__index_t_int32__hash_998b34c8052fe15_op</code>为转置.</p>
<h1 id="halide">2. <a target="_blank" rel="noopener" href="https://halide-lang.org">Halide</a></h1>
<h2 id="dsl语法-1">2.1 DSL语法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> halide <span class="keyword">as</span> hl</span><br><span class="line">inputLhs = hl.ImageParam(hl.Float(<span class="number">32</span>), <span class="number">2</span>, <span class="string">&quot;inputLhs&quot;</span>)</span><br><span class="line">inputRhs = hl.ImageParam(hl.Float(<span class="number">32</span>), <span class="number">2</span>, <span class="string">&quot;inputRhs&quot;</span>)</span><br><span class="line">output = hl.Func(<span class="string">&quot;output&quot;</span>)</span><br><span class="line">(m, n) = hl.Var(<span class="string">&quot;m&quot;</span>), hl.Var(<span class="string">&quot;n&quot;</span>)</span><br><span class="line">k = hl.RDom([hl.Range(<span class="number">0</span>, self.K)], <span class="string">&quot;k&quot;</span>)</span><br><span class="line">output[n, m] = <span class="number">0.0</span></span><br><span class="line">output[n, m] += inputLhs[k.x, m] * inputRhs[n, k.x]</span><br></pre></td></tr></table></figure>
<p><code>Halide</code>使用<code>Var</code>来表示循环,对于规约的循环需要用<code>RDom</code>来标识(并且如果定义了规约循环,那么还需要为数据设定初值).
循环层级也是由<code>Var</code>来确定,
他这里默认应该都会把规约的循环放到最内层.
使用<code>Var</code>对张量<code>inputLhs[k.x, m]</code>进行索引操作用于建立<code>access relation</code>.</p>
<p>提前声明的循环变量的缺点在于需要开发者手动管理好所有的循环变量,
书写起来较为复杂; 优点在于可以确定上下游操作循环之间的关系,
可以轻易的做到自动<code>fusion</code>上下两层算子.</p>
<h2 id="测试例子-1">2.2 测试例子</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> halide <span class="keyword">as</span> hl</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = hl.ImageParam(hl.Float(<span class="number">32</span>), <span class="number">4</span>, <span class="string">&quot;input&quot;</span>)</span><br><span class="line">weight = hl.ImageParam(hl.Float(<span class="number">32</span>), <span class="number">4</span>, <span class="string">&quot;weight&quot;</span>)</span><br><span class="line">act = hl.ImageParam(hl.Float(<span class="number">32</span>), <span class="number">2</span>, <span class="string">&quot;act&quot;</span>)</span><br><span class="line">pad_w_before = <span class="number">0</span>  <span class="comment"># hl.Param(hl.Int(32), &quot;pad_w_before&quot;)</span></span><br><span class="line">pad_h_before = <span class="number">0</span>  <span class="comment"># hl.Param(hl.Int(32), &quot;pad_h_before&quot;)</span></span><br><span class="line">stride_w = <span class="number">1</span>  <span class="comment"># hl.Param(hl.Int(32), &quot;stride_w&quot;)</span></span><br><span class="line">stride_h = <span class="number">1</span>  <span class="comment"># hl.Param(hl.Int(32), &quot;stride_h&quot;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">WO, HO, CI, B, CO = hl.Var(<span class="string">&quot;WO&quot;</span>), hl.Var(<span class="string">&quot;HO&quot;</span>), hl.Var(<span class="string">&quot;CI&quot;</span>), hl.Var(<span class="string">&quot;B&quot;</span>), hl.Var(<span class="string">&quot;CO&quot;</span>)</span><br><span class="line">Padding, Paded, Conv, Acted, Clamped, Psumed = hl.Func(<span class="string">&quot;Padding&quot;</span>), hl.Func(</span><br><span class="line">    <span class="string">&quot;Paded&quot;</span>), hl.Func(<span class="string">&quot;Conv&quot;</span>), hl.Func(<span class="string">&quot;Acted&quot;</span>), hl.Func(<span class="string">&quot;Clamped&quot;</span>), hl.Func(<span class="string">&quot;Psumed&quot;</span>)</span><br><span class="line"></span><br><span class="line">r = hl.RDom([hl.Range(<span class="number">0</span>, weight.width()), hl.Range(<span class="number">0</span>, weight.height()),</span><br><span class="line">            hl.Range(<span class="number">0</span>, weight.dim(<span class="number">2</span>).extent())])  <span class="comment"># w,h,ic</span></span><br><span class="line"></span><br><span class="line">Padding = hl.BoundaryConditions.constant_exterior(</span><br><span class="line">    <span class="built_in">input</span>, <span class="number">0</span>, [hl.Range(<span class="number">0</span>, <span class="built_in">input</span>.width()), hl.Range(<span class="number">0</span>, <span class="built_in">input</span>.height())])</span><br><span class="line"></span><br><span class="line">in_channels = <span class="built_in">input</span>.dim(<span class="number">2</span>).extent()</span><br><span class="line">out_channels = weight.dim(<span class="number">3</span>).extent()</span><br><span class="line"></span><br><span class="line">Paded[WO, HO, CI, B] = Padding[WO - pad_w_before, HO - pad_h_before, CI, B]</span><br><span class="line"></span><br><span class="line">Conv[WO, HO, CO, B] = <span class="number">0.0</span></span><br><span class="line">Conv[WO, HO, CO, B] += weight[r[<span class="number">0</span>], r[<span class="number">1</span>], r[<span class="number">2</span>], CO] * Paded[WO * stride_w + r[<span class="number">0</span>], HO * stride_h + r[<span class="number">1</span>], r[<span class="number">2</span>], B]  <span class="comment"># use float to sum</span></span><br><span class="line"></span><br><span class="line">Acted[WO, HO, CO, B] = hl.select(</span><br><span class="line">    Conv[WO, HO, CO, B] &lt; act[<span class="number">0</span>, CO],</span><br><span class="line">    Conv[WO, HO, CO, B] * act[<span class="number">1</span>, CO] + act[<span class="number">2</span>, CO],</span><br><span class="line">    Conv[WO, HO, CO, B] * act[<span class="number">3</span>, CO] + act[<span class="number">4</span>, CO])  <span class="comment"># float</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Transpose = hl.Func(<span class="string">&quot;Transpose&quot;</span>)</span><br><span class="line">Transpose[CO, WO, HO, B] = Acted[WO, HO, CO, B]</span><br><span class="line"></span><br><span class="line">rhs = hl.ImageParam(hl.Float(<span class="number">32</span>), <span class="number">4</span>, <span class="string">&quot;rhs&quot;</span>)  <span class="comment"># [x,x,K,N]</span></span><br><span class="line"></span><br><span class="line">N = hl.Var(<span class="string">&quot;N&quot;</span>)</span><br><span class="line"></span><br><span class="line">kdom = hl.RDom([hl.Range(<span class="number">0</span>, rhs.dim(<span class="number">2</span>).extent())], <span class="string">&quot;k&quot;</span>)</span><br><span class="line"></span><br><span class="line">Matmul = hl.Func(<span class="string">&quot;Matmul&quot;</span>)</span><br><span class="line">Matmul[N, WO, HO, B] = <span class="number">0.0</span></span><br><span class="line">Matmul[N, WO, HO, B] += Transpose[kdom.x, WO, HO, B] * rhs[N, kdom.x, HO, B]</span><br><span class="line"></span><br><span class="line">Matmul.print_loop_nest()</span><br></pre></td></tr></table></figure>
<p>得到的循环嵌套如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">produce Matmul:</span><br><span class="line">  <span class="keyword">for</span> B:</span><br><span class="line">    <span class="keyword">for</span> HO:</span><br><span class="line">      <span class="keyword">for</span> WO:</span><br><span class="line">        <span class="keyword">for</span> N:</span><br><span class="line">          Matmul(...) = ...</span><br><span class="line">  <span class="keyword">for</span> B:</span><br><span class="line">    <span class="keyword">for</span> HO:</span><br><span class="line">      <span class="keyword">for</span> WO:</span><br><span class="line">        <span class="keyword">for</span> N:</span><br><span class="line">          <span class="keyword">for</span> k:</span><br><span class="line">            produce Conv:</span><br><span class="line">              Conv(...) = ...</span><br><span class="line">              <span class="keyword">for</span> r14:</span><br><span class="line">                <span class="keyword">for</span> r14:</span><br><span class="line">                  <span class="keyword">for</span> r14:</span><br><span class="line">                    Conv(...) = ...</span><br><span class="line">            consume Conv:</span><br><span class="line">              Matmul(...) = ...</span><br></pre></td></tr></table></figure>
<p>矩阵层的初始化他默认放到<code>root</code>层级,
下面是自动把<code>Transpose</code>的操作<code>inline</code>了,
也自动把矩阵乘和卷积进行了<code>fusion</code>.</p>
<h1 id="tvm">3. <a target="_blank" rel="noopener" href="https://tvm.apache.org">TVM</a></h1>
<p><code>TVM</code>中脱胎于<code>Halide</code>,
他提供了一套<code>Tensor Expression</code>的<code>DSL</code>来协助我们定义算子计算逻辑.</p>
<h2 id="dsl语法-2">3.1 DSL语法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = te.var(<span class="string">&quot;n&quot;</span>)</span><br><span class="line">A = te.placeholder((n,), name=<span class="string">&quot;A&quot;</span>)</span><br><span class="line">B = te.placeholder((n,), name=<span class="string">&quot;B&quot;</span>)</span><br><span class="line">C = te.compute(A.shape, <span class="keyword">lambda</span> i: A[i] + B[i], name=<span class="string">&quot;C&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>也是使用<code>shape</code>来表示完美循环,
<code>fcompute</code>的回调函数的参数映射迭代变量,
同时也会在最内层循环执行它. 同<code>Jittor</code>类似,
不过使用回调函数的方式更增加了灵活性. 可以使用<code>reduce_axis</code>,
类似于<code>RDom</code>, 会自动最内层循环加上规约的循环,
他这里默认的初始化会放到规约循环外面.</p>
<h2 id="测试例子-2">3.2 测试例子</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> te</span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> tir</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">in_channel = <span class="number">3</span></span><br><span class="line">out_channel = <span class="number">16</span></span><br><span class="line">in_height = <span class="number">32</span></span><br><span class="line">in_width = <span class="number">32</span></span><br><span class="line">kernel_height = <span class="number">3</span></span><br><span class="line">kernel_width = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">N = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">Input = te.placeholder(</span><br><span class="line">    (batch_size, in_channel, in_height, in_width), name=<span class="string">&#x27;Input&#x27;</span>)</span><br><span class="line">Kernel = te.placeholder(</span><br><span class="line">    (out_channel, in_channel, kernel_height, kernel_width), name=<span class="string">&#x27;Kernel&#x27;</span>)</span><br><span class="line"></span><br><span class="line">rc = te.reduce_axis((<span class="number">0</span>, in_channel), name=<span class="string">&#x27;rc&#x27;</span>)</span><br><span class="line">ry = te.reduce_axis((<span class="number">0</span>, kernel_height), name=<span class="string">&#x27;ry&#x27;</span>)</span><br><span class="line">rx = te.reduce_axis((<span class="number">0</span>, kernel_width), name=<span class="string">&#x27;rx&#x27;</span>)</span><br><span class="line"></span><br><span class="line">Conv = te.compute(</span><br><span class="line">    (batch_size, out_channel, in_height -</span><br><span class="line">     kernel_height + <span class="number">1</span>, in_width - kernel_width + <span class="number">1</span>),</span><br><span class="line">    <span class="keyword">lambda</span> n, f, y, x: te.<span class="built_in">sum</span>(</span><br><span class="line">        Input[n, rc, y + ry, x + rx] * Kernel[f, rc, ry, rx],</span><br><span class="line">        axis=[rc, ry, rx]</span><br><span class="line">    ),</span><br><span class="line">    name=<span class="string">&#x27;Conv&#x27;</span></span><br><span class="line">)  <span class="comment"># (b,oc,oh,ow)  -&gt; (b,oh,ow,oc)</span></span><br><span class="line"></span><br><span class="line">oh, ow = <span class="number">30</span>, <span class="number">30</span></span><br><span class="line">rhs = te.placeholder((batch_size, oh, out_channel, N), name=<span class="string">&#x27;rhs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">Trans = te.compute(</span><br><span class="line">    (batch_size, oh, ow, out_channel),</span><br><span class="line">    <span class="keyword">lambda</span> i0, i1, i2, i3: Conv[i0, i3, i1, i2])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rk = te.reduce_axis((<span class="number">0</span>, out_channel), name=<span class="string">&#x27;rk&#x27;</span>)</span><br><span class="line">MatMul = te.compute(</span><br><span class="line">    (batch_size, oh, ow, N),</span><br><span class="line">    <span class="keyword">lambda</span> i0, i1, i2, i3: te.<span class="built_in">sum</span>(</span><br><span class="line">        Trans[i0, i1, i2, rk] * rhs[i0, i1, rk, i3], axis=[rk]),</span><br><span class="line">    name=<span class="string">&#x27;MatMul&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">s: te.Schedule = te.create_schedule([Conv.op, MatMul.op])</span><br><span class="line">ir = tvm.lower(s, [Input, Kernel, rhs])</span><br><span class="line">ir.show()</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@I.ir_module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>:</span><br><span class="line"><span class="meta">    @T.prim_func</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">Input: T.Buffer(<span class="params">(<span class="params"><span class="number">8</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span></span>), <span class="string">&quot;float32&quot;</span></span>), Kernel: T.Buffer(<span class="params">(<span class="params"><span class="number">16</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span></span>), <span class="string">&quot;float32&quot;</span></span>), rhs: T.Buffer(<span class="params">(<span class="params"><span class="number">8</span>, <span class="number">30</span>, <span class="number">16</span>, <span class="number">64</span></span>), <span class="string">&quot;float32&quot;</span></span>)</span>):</span><br><span class="line">        T.func_attr(&#123;<span class="string">&quot;from_legacy_te_schedule&quot;</span>: T.<span class="built_in">bool</span>(<span class="literal">True</span>), <span class="string">&quot;tir.noalias&quot;</span>: T.<span class="built_in">bool</span>(<span class="literal">True</span>)&#125;)</span><br><span class="line">        Conv = T.allocate([<span class="number">460800</span>], <span class="string">&quot;float32&quot;</span>, <span class="string">&quot;global&quot;</span>)</span><br><span class="line">        compute = T.allocate([<span class="number">115200</span>], <span class="string">&quot;float32&quot;</span>, <span class="string">&quot;global&quot;</span>)</span><br><span class="line">        Conv_1 = T.Buffer((<span class="number">115200</span>,), data=Conv)</span><br><span class="line">        <span class="keyword">for</span> n, f, y, x <span class="keyword">in</span> T.grid(<span class="number">8</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">30</span>):</span><br><span class="line">            Conv_1[n * <span class="number">14400</span> + f * <span class="number">900</span> + y * <span class="number">30</span> + x] = T.float32(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">for</span> rc, ry, rx <span class="keyword">in</span> T.grid(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>):</span><br><span class="line">                cse_var_1: T.int32 = n * <span class="number">14400</span> + f * <span class="number">900</span> + y * <span class="number">30</span> + x</span><br><span class="line">                Input_1 = T.Buffer((<span class="number">24576</span>,), data=Input.data)</span><br><span class="line">                Kernel_1 = T.Buffer((<span class="number">432</span>,), data=Kernel.data)</span><br><span class="line">                Conv_1[cse_var_1] = Conv_1[cse_var_1] + Input_1[n * <span class="number">3072</span> + rc * <span class="number">1024</span> + y * <span class="number">32</span> + ry * <span class="number">32</span> + x + rx] * Kernel_1[f * <span class="number">27</span> + rc * <span class="number">9</span> + ry * <span class="number">3</span> + rx]</span><br><span class="line">        compute_1 = T.Buffer((<span class="number">115200</span>,), data=compute)</span><br><span class="line">        <span class="keyword">for</span> i0, i1, i2, i3 <span class="keyword">in</span> T.grid(<span class="number">8</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">16</span>):</span><br><span class="line">            cse_var_2: T.int32 = i0 * <span class="number">14400</span></span><br><span class="line">            compute_1[cse_var_2 + i1 * <span class="number">480</span> + i2 * <span class="number">16</span> + i3] = Conv_1[cse_var_2 + i3 * <span class="number">900</span> + i1 * <span class="number">30</span> + i2]</span><br><span class="line">        <span class="keyword">for</span> i0, i1, i2, i3 <span class="keyword">in</span> T.grid(<span class="number">8</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">64</span>):</span><br><span class="line">            Conv_2 = T.Buffer((<span class="number">460800</span>,), data=Conv)</span><br><span class="line">            Conv_2[i0 * <span class="number">57600</span> + i1 * <span class="number">1920</span> + i2 * <span class="number">64</span> + i3] = T.float32(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">for</span> rk <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">                cse_var_3: T.int32 = i0 * <span class="number">57600</span> + i1 * <span class="number">1920</span> + i2 * <span class="number">64</span> + i3</span><br><span class="line">                rhs_1 = T.Buffer((<span class="number">245760</span>,), data=rhs.data)</span><br><span class="line">                Conv_2[cse_var_3] = Conv_2[cse_var_3] + compute_1[i0 * <span class="number">14400</span> + i1 * <span class="number">480</span> + i2 * <span class="number">16</span> + rk] * rhs_1[i0 * <span class="number">30720</span> + i1 * <span class="number">1024</span> + rk * <span class="number">64</span> + i3]</span><br></pre></td></tr></table></figure>
<p>他这里不像<code>Halide</code>一样需要提前定义好循环变量,
但可以从输出中获取<code>axis</code>然后使用类似<code>Halide</code>的调度,
也可以在<code>lower</code>到<code>tir</code>之后使用基于<code>tensor ir</code>的调度.
这里进行<code>lower</code>依据默认的优化流程后,
并无法自动<code>fusion</code>.</p>
<h1 id="mlir">4. <a target="_blank" rel="noopener" href="https://mlir.llvm.org">Mlir</a></h1>
<p><code>Mlir</code>基于<code>linalg</code>dialect中的<code>linalg.generic</code>op提供了一套<code>OpDSL</code>.</p>
<h2 id="dsl语法-3">4.1 DSL语法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@linalg_structured_op</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_2d_nhwc_hwcf</span>(<span class="params"></span></span><br><span class="line"><span class="params">    I=TensorDef(<span class="params">T1, S.N, S.OH * S.SH + S.KH * S.DH, S.OW * S.SW + S.KW * S.DW, S.C</span>),</span></span><br><span class="line"><span class="params">    K=TensorDef(<span class="params">T2, S.KH, S.KW, S.C, S.F</span>),</span></span><br><span class="line"><span class="params">    O=TensorDef(<span class="params">U, S.N, S.OH, S.OW, S.F, output=<span class="literal">True</span></span>),</span></span><br><span class="line"><span class="params">    strides=IndexAttrDef(<span class="params">S.SH, S.SW, default=[<span class="number">1</span>, <span class="number">1</span>]</span>),</span></span><br><span class="line"><span class="params">    dilations=IndexAttrDef(<span class="params">S.DH, S.DW, default=[<span class="number">1</span>, <span class="number">1</span>]</span>),</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Performs 2-D convolution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Layout:</span></span><br><span class="line"><span class="string">      * Input: NHWC.</span></span><br><span class="line"><span class="string">      * Kernel: HWCF.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Numeric casting is performed on the operands to the inner multiply, promoting</span></span><br><span class="line"><span class="string">    them to the same data type as the accumulator/output.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    implements(ConvolutionOpInterface)</span><br><span class="line">    domain(D.n, D.oh, D.ow, D.f, D.kh, D.kw, D.c)</span><br><span class="line">    O[D.n, D.oh, D.ow, D.f] += TypeFn.cast_signed(</span><br><span class="line">        U, I[D.n, D.oh * S.SH + D.kh * S.DH, D.ow * S.SW + D.kw * S.DW, D.c]</span><br><span class="line">    ) * TypeFn.cast_signed(U, K[D.kh, D.kw, D.c, D.f])</span><br></pre></td></tr></table></figure>
<p><code>Mlir</code>这里不使用<code>shape</code>,
使用和<code>polyhedral</code>更加贴近的称呼<code>domain</code>来表示嵌套循环.
我觉得这里更加激进的一点就是完全抛弃循环中的初始化,
也就是忠实的翻译这个<code>OpDSL</code>所描述的内容.</p>
<h2 id="测试例子-3">4.2 测试例子</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mlir.dialects <span class="keyword">import</span> arith, builtin, func, linalg, tensor, memref</span><br><span class="line"><span class="keyword">from</span> mlir.dialects.linalg.opdsl.lang <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> mlir.ir <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@linalg_structured_op</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_nchw_nhwc</span>(<span class="params"></span></span><br><span class="line"><span class="params">    I=TensorDef(<span class="params">TV.T1, S.d0, S.d1, S.d2, S.d3</span>),</span></span><br><span class="line"><span class="params">    O=TensorDef(<span class="params">TV.T1, S.d0, S.d2, S.d3, S.d1, output=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    domain(D.d0, D.d1, D.d2, D.d3)</span><br><span class="line">    implements(ContractionOpInterface)</span><br><span class="line">    O[D.d0, D.d2, D.d3, D.d1] = I[D.d0, D.d1, D.d2, D.d3]</span><br><span class="line"></span><br><span class="line"><span class="meta">@linalg_structured_op</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_4d</span>(<span class="params"></span></span><br><span class="line"><span class="params">    A=TensorDef(<span class="params">TV.T1, S.d0, S.d1, S.M, S.K</span>),</span></span><br><span class="line"><span class="params">    B=TensorDef(<span class="params">TV.T1, S.d0, S.d1, S.K, S.N</span>),</span></span><br><span class="line"><span class="params">    C=TensorDef(<span class="params">TV.T1, S.d0, S.d2, S.M, S.N, output=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    domain(D.d0, D.d1, D.m, D.n, D.k)</span><br><span class="line">    implements(ContractionOpInterface)</span><br><span class="line">    C[D.d0, D.d1, D.m, D.n] += A[D.d0, D.d1, D.m, D.k] * B[D.d0, D.d1, D.k, D.n]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">testOpResultFromOtherOp</span>():</span><br><span class="line">    <span class="keyword">with</span> Context(), Location.unknown():</span><br><span class="line">        module = Module.create()</span><br><span class="line">        f32 = F32Type.get()</span><br><span class="line">        index_type = IndexType.get()</span><br><span class="line">        <span class="keyword">with</span> InsertionPoint(module.body):</span><br><span class="line">            batch_size = <span class="number">8</span></span><br><span class="line">            in_channel = <span class="number">3</span></span><br><span class="line">            out_channel = <span class="number">16</span></span><br><span class="line">            in_height = <span class="number">32</span></span><br><span class="line">            out_height = <span class="number">30</span></span><br><span class="line">            in_width = <span class="number">32</span></span><br><span class="line">            out_width = <span class="number">30</span></span><br><span class="line">            kernel_height = <span class="number">3</span></span><br><span class="line">            kernel_width = <span class="number">3</span></span><br><span class="line">            N = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="meta">            @func.FuncOp.from_py_func(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">                MemRefType.get(<span class="params"></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">                    (<span class="params">batch_size, in_channel, in_height, in_width</span>), f32</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">                MemRefType.get(<span class="params"></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">                    (<span class="params">out_channel, in_channel, kernel_height, kernel_width</span>), f32</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">                MemRefType.get(<span class="params">(<span class="params">batch_size, out_height, out_channel, N</span>), f32</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">            </span>)</span></span><br><span class="line">            <span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">lhs, weight, rhs</span>):</span><br><span class="line">                <span class="comment"># conv = tensor.EmptyOp([batch_size, out_channel, out_height, out_width], f32)</span></span><br><span class="line">                zero = arith.ConstantOp(F32Type.get(), <span class="number">0.0</span>)</span><br><span class="line">                <span class="comment"># CHECK: %[[LHS:.*]] = linalg.fill</span></span><br><span class="line">                conv = memref.AllocOp(MemRefType.get(</span><br><span class="line">                    [batch_size, out_channel, out_height, out_width], f32), [], [])</span><br><span class="line">                linalg.fill(zero, outs=[conv])</span><br><span class="line">                linalg.conv_2d_nchw_fchw(lhs, weight, outs=[conv])</span><br><span class="line">                trans = memref.AllocOp(MemRefType.get(</span><br><span class="line">                    [batch_size, out_height, out_width, out_channel], f32), [], [])</span><br><span class="line">                transpose_nchw_nhwc(conv, outs=[trans])</span><br><span class="line">                matmul = memref.AllocOp(MemRefType.get(</span><br><span class="line">                    [batch_size, out_height, out_width, N], f32), [], [])</span><br><span class="line">                matmul_4d(trans, rhs, outs=[matmul])</span><br><span class="line">                <span class="keyword">return</span> matmul</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(module)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">testOpResultFromOtherOp()</span><br></pre></td></tr></table></figure>
<p>得到<code>convmatmul.mlir</code>: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#map = affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;</span><br><span class="line">#map1 = affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d2, d3, d1)&gt;</span><br><span class="line">#map2 = affine_map&lt;(d0, d1, d2, d3, d4) -&gt; (d0, d1, d2, d4)&gt;</span><br><span class="line">#map3 = affine_map&lt;(d0, d1, d2, d3, d4) -&gt; (d0, d1, d4, d3)&gt;</span><br><span class="line">#map4 = affine_map&lt;(d0, d1, d2, d3, d4) -&gt; (d0, d1, d2, d3)&gt;</span><br><span class="line">module &#123;</span><br><span class="line">  func.func @main(%arg0: memref&lt;8x3x32x32xf32&gt;, %arg1: memref&lt;16x3x3x3xf32&gt;, %arg2: memref&lt;8x30x16x64xf32&gt;) -&gt; memref&lt;8x30x30x64xf32&gt; &#123;</span><br><span class="line">    %cst = arith.constant 0.000000e+00 : f32</span><br><span class="line">    %alloc = memref.alloc() : memref&lt;8x16x30x30xf32&gt;</span><br><span class="line">    linalg.fill ins(%cst : f32) outs(%alloc : memref&lt;8x16x30x30xf32&gt;)</span><br><span class="line">    linalg.conv_2d_nchw_fchw ins(%arg0, %arg1 : memref&lt;8x3x32x32xf32&gt;, memref&lt;16x3x3x3xf32&gt;) outs(%alloc : memref&lt;8x16x30x30xf32&gt;)</span><br><span class="line">    %alloc_0 = memref.alloc() : memref&lt;8x30x30x16xf32&gt;</span><br><span class="line">    linalg.generic &#123;indexing_maps = [#map, #map1], iterator_types = [&quot;parallel&quot;, &quot;parallel&quot;, &quot;parallel&quot;, &quot;parallel&quot;]&#125; ins(%alloc : memref&lt;8x16x30x30xf32&gt;) outs(%alloc_0 : memref&lt;8x30x30x16xf32&gt;) &#123;</span><br><span class="line">    ^bb0(%in: f32, %out: f32):</span><br><span class="line">      linalg.yield %in : f32</span><br><span class="line">    &#125;</span><br><span class="line">    %alloc_1 = memref.alloc() : memref&lt;8x30x30x64xf32&gt;</span><br><span class="line">    linalg.generic &#123;indexing_maps = [#map2, #map3, #map4], iterator_types = [&quot;parallel&quot;, &quot;parallel&quot;, &quot;parallel&quot;, &quot;parallel&quot;, &quot;reduction&quot;]&#125; ins(%alloc_0, %arg2 : memref&lt;8x30x30x16xf32&gt;, memref&lt;8x30x16x64xf32&gt;) outs(%alloc_1 : memref&lt;8x30x30x64xf32&gt;) &#123;</span><br><span class="line">    ^bb0(%in: f32, %in_2: f32, %out: f32):</span><br><span class="line">      %0 = arith.mulf %in, %in_2 : f32</span><br><span class="line">      %1 = arith.addf %out, %0 : f32</span><br><span class="line">      linalg.yield %1 : f32</span><br><span class="line">    &#125;</span><br><span class="line">    return %alloc_1 : memref&lt;8x30x30x64xf32&gt;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>使用<code>mlir-opt</code>进行<code>fusion</code>: <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">mlir-opt -allow-unregistered-dialect convmatmul.mlir --convert-linalg-to-affine-loops -o convmatmul1.mlir</span><br><span class="line">mlir-opt -allow-unregistered-dialect convmatmul1.mlir -pass-pipeline=<span class="string">&#x27;builtin.module(func.func(affine-loop-fusion))&#x27;</span> -o convmatmul2.mlir</span><br></pre></td></tr></table></figure></p>
<p>得到: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#map = affine_map&lt;(d0, d1) -&gt; (d0 + d1)&gt;</span><br><span class="line">module &#123;</span><br><span class="line">  func.func @main(%arg0: memref&lt;8x3x32x32xf32&gt;, %arg1: memref&lt;16x3x3x3xf32&gt;, %arg2: memref&lt;8x30x16x64xf32&gt;) -&gt; memref&lt;8x30x30x64xf32&gt; &#123;</span><br><span class="line">    %alloc = memref.alloc() : memref&lt;1x1x1x16xf32&gt;</span><br><span class="line">    %alloc_0 = memref.alloc() : memref&lt;1x1x1x1xf32&gt;</span><br><span class="line">    %cst = arith.constant 0.000000e+00 : f32</span><br><span class="line">    %alloc_1 = memref.alloc() : memref&lt;8x30x30x64xf32&gt;</span><br><span class="line">    affine.for %arg3 = 0 to 8 &#123;</span><br><span class="line">      affine.for %arg4 = 0 to 30 &#123;</span><br><span class="line">        affine.for %arg5 = 0 to 30 &#123;</span><br><span class="line">          affine.for %arg6 = 0 to 16 &#123;</span><br><span class="line">            affine.store %cst, %alloc_0[0, 0, 0, 0] : memref&lt;1x1x1x1xf32&gt;</span><br><span class="line">            affine.for %arg7 = 0 to 3 &#123;</span><br><span class="line">              affine.for %arg8 = 0 to 3 &#123;</span><br><span class="line">                affine.for %arg9 = 0 to 3 &#123;</span><br><span class="line">                  %1 = affine.apply #map(%arg4, %arg8)</span><br><span class="line">                  %2 = affine.apply #map(%arg5, %arg9)</span><br><span class="line">                  %3 = affine.load %arg0[%arg3, %arg7, %1, %2] : memref&lt;8x3x32x32xf32&gt;</span><br><span class="line">                  %4 = affine.load %arg1[%arg6, %arg7, %arg8, %arg9] : memref&lt;16x3x3x3xf32&gt;</span><br><span class="line">                  %5 = affine.load %alloc_0[0, 0, 0, 0] : memref&lt;1x1x1x1xf32&gt;</span><br><span class="line">                  %6 = arith.mulf %3, %4 : f32</span><br><span class="line">                  %7 = arith.addf %5, %6 : f32</span><br><span class="line">                  affine.store %7, %alloc_0[0, 0, 0, 0] : memref&lt;1x1x1x1xf32&gt;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            %0 = affine.load %alloc_0[0, 0, 0, 0] : memref&lt;1x1x1x1xf32&gt;</span><br><span class="line">            affine.store %0, %alloc[0, 0, 0, %arg6] : memref&lt;1x1x1x16xf32&gt;</span><br><span class="line">          &#125;</span><br><span class="line">          affine.for %arg6 = 0 to 64 &#123;</span><br><span class="line">            affine.for %arg7 = 0 to 16 &#123;</span><br><span class="line">              %0 = affine.load %alloc[0, 0, 0, %arg7] : memref&lt;1x1x1x16xf32&gt;</span><br><span class="line">              %1 = affine.load %arg2[%arg3, %arg4, %arg7, %arg6] : memref&lt;8x30x16x64xf32&gt;</span><br><span class="line">              %2 = affine.load %alloc_1[%arg3, %arg4, %arg5, %arg6] : memref&lt;8x30x30x64xf32&gt;</span><br><span class="line">              %3 = arith.mulf %0, %1 : f32</span><br><span class="line">              %4 = arith.addf %2, %3 : f32</span><br><span class="line">              affine.store %4, %alloc_1[%arg3, %arg4, %arg5, %arg6] : memref&lt;8x30x30x64xf32&gt;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return %alloc_1 : memref&lt;8x30x30x64xf32&gt;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>经过<code>affine-loop-fusion</code>之后的ir基本符合我的预期.</p>
<h1 id="tiramisu">5. <a
target="_blank" rel="noopener" href="http://tiramisu-compiler.org">Tiramisu</a></h1>
<h2 id="dsl语法-4">5.1 DSL语法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tiramisu <span class="keyword">as</span> tm</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">tm.init(<span class="string">&quot;matmul&quot;</span>)</span><br><span class="line"></span><br><span class="line">M = <span class="number">64</span></span><br><span class="line">K = <span class="number">256</span></span><br><span class="line">N = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Level I: specifies &quot;what&quot; should be computed</span></span><br><span class="line"></span><br><span class="line">A = tm.<span class="built_in">input</span>(<span class="string">&quot;A&quot;</span>, [<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;k&#x27;</span>], [M, K], tm.primitive_t.p_float32)</span><br><span class="line">B = tm.<span class="built_in">input</span>(<span class="string">&quot;B&quot;</span>, [<span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;n&#x27;</span>], [K, N], tm.primitive_t.p_float32)</span><br><span class="line"></span><br><span class="line">m, k, n = tm.var(<span class="string">&quot;m&quot;</span>, <span class="number">0</span>, M), tm.var(<span class="string">&quot;k&quot;</span>, <span class="number">0</span>, K), tm.var(<span class="string">&quot;n&quot;</span>, <span class="number">0</span>, N)</span><br><span class="line">C_init = tm.computation(<span class="string">&quot;C_init&quot;</span>, [m, n], tm.expr(<span class="number">0.0</span>))</span><br><span class="line">C = tm.computation(<span class="string">&quot;C&quot;</span>, [m, n, k], tm.primitive_t.p_float32)</span><br><span class="line">C.set_expression(C[m, n, k - <span class="number">1</span>] + A[m, k] * B[k, n])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Level II: level specifies &quot;when&quot; and &quot;where&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># schedule the computation oerder</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Level III: level specifies &quot;stored&quot;</span></span><br><span class="line"></span><br><span class="line">bufA = tm.buffer(<span class="string">&quot;bufA&quot;</span>, [M, K], tm.primitive_t.p_float32, tm.argument_t.a_input)</span><br><span class="line">bufB = tm.buffer(<span class="string">&quot;bufB&quot;</span>, [K, N], tm.primitive_t.p_float32, tm.argument_t.a_input)</span><br><span class="line">bufC = tm.buffer(<span class="string">&quot;bufC&quot;</span>, [M, N], tm.primitive_t.p_float32, tm.argument_t.a_output)</span><br><span class="line">A.store_in(bufA)</span><br><span class="line">B.store_in(bufB)</span><br><span class="line">C_init.store_in(bufC, [m, n])</span><br><span class="line">C.store_in(bufC, [m, n])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f = tm.get_implicit_function()</span><br><span class="line">f.codegen([bufA, bufB, bufC], <span class="string">&quot;matmul.o&quot;</span>, <span class="number">0</span>, <span class="literal">False</span>)</span><br><span class="line">f.dump_halide_stmt()</span><br></pre></td></tr></table></figure>
<p><code>tiramisu</code>其实是使用更加贴近于<code>polyhedral</code>的思想,
<code>computation</code>可以等价于<code>statement</code>.
类似<code>halide</code>一样使用<code>var</code>表示循环,
用于指定当前<code>computation</code>的循环位置.
不过他这里内部都是基于<code>polyhedral</code>,
定义<code>computation</code>后直接得到了<code>iteration domain</code>,
经过<code>loop dimension align</code>可以得到<code>schedule</code>.</p>
<h2 id="测试例子-4">5.2 测试例子</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tiramisu <span class="keyword">as</span> tm</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># f = tm.function(&quot;matmul&quot;)</span></span><br><span class="line">tm.init(<span class="string">&quot;convmatmul&quot;</span>)</span><br><span class="line"></span><br><span class="line">B = <span class="number">8</span></span><br><span class="line">IC = <span class="number">3</span></span><br><span class="line">OC = <span class="number">16</span></span><br><span class="line">IH, OH = <span class="number">32</span>, <span class="number">30</span></span><br><span class="line">IW, OW = <span class="number">32</span>, <span class="number">30</span></span><br><span class="line">KH = <span class="number">3</span></span><br><span class="line">KW = <span class="number">3</span></span><br><span class="line">N = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Level I: specifies &quot;what&quot; should be computed</span></span><br><span class="line"></span><br><span class="line">lhs = tm.<span class="built_in">input</span>(<span class="string">&quot;lhs&quot;</span>, [<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;IC&#x27;</span>, <span class="string">&#x27;IH&#x27;</span>, <span class="string">&#x27;IW&#x27;</span>], [</span><br><span class="line">               B, IC, IH, IW], tm.primitive_t.p_float64)</span><br><span class="line">rhs = tm.<span class="built_in">input</span>(<span class="string">&quot;rhs&quot;</span>, [<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;OH&#x27;</span>, <span class="string">&#x27;OC&#x27;</span>, <span class="string">&#x27;N&#x27;</span>], [</span><br><span class="line">               B, OH, OC, N], tm.primitive_t.p_float64)</span><br><span class="line">kernel = tm.<span class="built_in">input</span>(<span class="string">&quot;kernel&quot;</span>, [<span class="string">&#x27;OC&#x27;</span>, <span class="string">&#x27;IC&#x27;</span>, <span class="string">&#x27;KH&#x27;</span>, <span class="string">&#x27;KW&#x27;</span>], [</span><br><span class="line">    OC, IC, KH, KW], tm.primitive_t.p_float64)</span><br><span class="line"></span><br><span class="line">b, oc, oh, ow, ic, kh, kw = tm.var(<span class="string">&#x27;b&#x27;</span>, <span class="number">0</span>, B), tm.var(<span class="string">&#x27;oc&#x27;</span>, <span class="number">0</span>, OC), tm.var(<span class="string">&#x27;oh&#x27;</span>, <span class="number">0</span>, OH), tm.var(</span><br><span class="line">    <span class="string">&#x27;ow&#x27;</span>, <span class="number">0</span>,  OW), tm.var(<span class="string">&#x27;ic&#x27;</span>, <span class="number">0</span>, IC), tm.var(<span class="string">&#x27;kh&#x27;</span>, <span class="number">0</span>, KH), tm.var(<span class="string">&#x27;kw&#x27;</span>, <span class="number">0</span>, KW)</span><br><span class="line">ConvInit = tm.computation(<span class="string">&quot;conv_init&quot;</span>, [b, oc, oh, ow], tm.expr(<span class="number">0.0</span>))</span><br><span class="line">Conv = tm.computation(</span><br><span class="line">    <span class="string">&quot;Conv&quot;</span>, [b, oc, oh, ow, ic, kh, kw], tm.primitive_t.p_float64)</span><br><span class="line">Conv.set_expression(Conv[b, oc, oh, ow, ic, kh, kw] +</span><br><span class="line">                    lhs[b, oc, oh + kh, ow + kw] * kernel[oc, ic, kh, kw])</span><br><span class="line">n = tm.var(<span class="string">&#x27;n&#x27;</span>, <span class="number">0</span>, N)</span><br><span class="line">Transpose = tm.computation(<span class="string">&quot;transpose&quot;</span>, [b, oh, ow, oc], ConvInit[b, oc, oh, ow])</span><br><span class="line"></span><br><span class="line">MatmulInit = tm.computation(<span class="string">&quot;matmul_init&quot;</span>, [b, oh, ow, n], tm.expr(<span class="number">0.0</span>))</span><br><span class="line">Matmul = tm.computation(<span class="string">&quot;Matmul&quot;</span>, [b, oh, ow, n], tm.primitive_t.p_float64)</span><br><span class="line">Matmul.set_expression(</span><br><span class="line">    MatmulInit[b, oh, ow, n] + Transpose[b, oh, ow, oc] * rhs[b, oh, oc, n])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Level II: level specifies &quot;when&quot; and &quot;where&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Level III: level specifies &quot;stored&quot;</span></span><br><span class="line"></span><br><span class="line">buflhs = tm.buffer(<span class="string">&quot;buflhs&quot;</span>,  [B, IC, IH, IW], tm.primitive_t.p_float64, tm.argument_t.a_input)</span><br><span class="line">bufrhs = tm.buffer(<span class="string">&quot;bufrhs&quot;</span>,  [B, OH, OC, N], tm.primitive_t.p_float64, tm.argument_t.a_input)</span><br><span class="line">bufkernel = tm.buffer(<span class="string">&quot;bufkernel&quot;</span>,  [OC, IC, KH, KW], tm.primitive_t.p_float64, tm.argument_t.a_input)</span><br><span class="line">bufconv = tm.buffer(<span class="string">&quot;bufconv&quot;</span>,  [B, OC, OH, OW], tm.primitive_t.p_float64, tm.argument_t.a_temporary)</span><br><span class="line">bufmatmul = tm.buffer(<span class="string">&quot;bufmatmul&quot;</span>,  [B, OH, OW, N], tm.primitive_t.p_float64, tm.argument_t.a_output)</span><br><span class="line"></span><br><span class="line">lhs.store_in(buflhs)</span><br><span class="line">rhs.store_in(bufrhs)</span><br><span class="line">kernel.store_in(bufkernel)</span><br><span class="line"></span><br><span class="line">ConvInit.store_in(bufconv, [b, oc, oh, ow])</span><br><span class="line">Conv.store_in(bufconv, [b, oc, oh, ow])</span><br><span class="line">Transpose.store_in(bufconv, [b, oh, ow, oc])</span><br><span class="line">MatmulInit.store_in(bufmatmul, [b, oh, ow, n])</span><br><span class="line">Matmul.store_in(bufmatmul, [b, oh, ow, n])</span><br><span class="line"></span><br><span class="line">f = tm.get_implicit_function()</span><br><span class="line">f.codegen([buflhs, bufrhs, bufkernel, bufconv, bufmatmul], <span class="string">&quot;matmul.o&quot;</span>, <span class="number">0</span>, <span class="literal">False</span>)</span><br><span class="line">f.dump_halide_stmt()</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">produce  &#123;</span><br><span class="line"> allocate _transpose_b5[float64 * (16 - 0) * (30 - 0) * (30 - 0) * (8 - 0)] <span class="keyword">in</span> Heap</span><br><span class="line"> allocate _rhs_b1[float64 * (64 - 0) * (16 - 0) * (30 - 0) * (8 - 0)] <span class="keyword">in</span> Heap</span><br><span class="line"> allocate _matmul_init_b6[float64 * (64 - 0) * (30 - 0) * (30 - 0) * (8 - 0)] <span class="keyword">in</span> Heap</span><br><span class="line"> allocate _lhs_b0[float64 * (32 - 0) * (32 - 0) * (3 - 0) * (8 - 0)] <span class="keyword">in</span> Heap</span><br><span class="line"> allocate _kernel_b2[float64 * (3 - 0) * (3 - 0) * (3 - 0) * (16 - 0)] <span class="keyword">in</span> Heap</span><br><span class="line"> allocate _conv_init_b3[float64 * (30 - 0) * (30 - 0) * (16 - 0) * (8 - 0)] <span class="keyword">in</span> Heap</span><br><span class="line"> allocate _Matmul_b7[float64 * (64 - 0) * (30 - 0) * (30 - 0) * (8 - 0)] <span class="keyword">in</span> Heap</span><br><span class="line"> allocate _Conv_b4[float64 * (3 - 0) * (3 - 0) * (3 - 0) * (30 - 0) * (30 - 0) * (16 - 0) * (8 - 0)] <span class="keyword">in</span> Heap</span><br><span class="line"> <span class="keyword">for</span> (c1, 0, 8 - 0) &#123;</span><br><span class="line">  <span class="keyword">for</span> (c3, 0, 30 - 0) &#123;</span><br><span class="line">   <span class="keyword">for</span> (c5, 0, 30 - 0) &#123;</span><br><span class="line">    <span class="keyword">for</span> (c7, 0, 64 - 0) &#123;</span><br><span class="line">     <span class="keyword">if</span> (c3 &gt;= 16) &#123;</span><br><span class="line">      bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] = 0.000000</span><br><span class="line">      bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] = (float64)bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] + ((float64)bufconv[(((<span class="number">0</span> + (oc*<span class="number">1</span>)) + (c5*30)) + (c3*900)) + (c1*14400)]*(float64)bufrhs[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (oc*64)) + (c3*1024)) + (c1*30720)])</span><br><span class="line">      <span class="keyword">if</span> (c7 &lt;= 15) &#123;</span><br><span class="line">       bufconv[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*30)) + (c3*900)) + (c1*14400)] = (float64)bufconv[(((<span class="number">0</span> + (c5*<span class="number">1</span>)) + (c3*30)) + (c7*900)) + (c1*14400)]</span><br><span class="line">      &#125;</span><br><span class="line">     &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c7 &gt;= 30) &#123;</span><br><span class="line">      bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] = 0.000000</span><br><span class="line">      bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] = (float64)bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] + ((float64)bufconv[(((<span class="number">0</span> + (oc*<span class="number">1</span>)) + (c5*30)) + (c3*900)) + (c1*14400)]*(float64)bufrhs[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (oc*64)) + (c3*1024)) + (c1*30720)])</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> (c9, 0, 3 - 0) &#123;</span><br><span class="line">       <span class="keyword">for</span> (c11, 0, 3 - 0) &#123;</span><br><span class="line">        <span class="keyword">for</span> (c13, 0, 3 - 0) &#123;</span><br><span class="line">         <span class="keyword">if</span> (((c9 == <span class="number">0</span>) &amp;&amp; (c11 == <span class="number">0</span>)) &amp;&amp; (c13 == 0)) &#123;</span><br><span class="line">          bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] = 0.000000</span><br><span class="line">         &#125;</span><br><span class="line">         bufconv[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*30)) + (c3*900)) + (c1*14400)] = (float64)bufconv[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*30)) + (c3*900)) + (c1*14400)] + ((float64)buflhs[(((<span class="number">0</span> + ((c7 + c13)*<span class="number">1</span>)) + ((c5 + c11)*<span class="number">32</span>)) + (c3*1024)) + (c1*3072)]*(float64)bufkernel[(((<span class="number">0</span> + (c13*<span class="number">1</span>)) + (c11*3)) + (c9*9)) + (c3*27)])</span><br><span class="line">         <span class="keyword">if</span> (((c9 == <span class="number">0</span>) &amp;&amp; (c11 == <span class="number">0</span>)) &amp;&amp; (c13 == 0)) &#123;</span><br><span class="line">          bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] = (float64)bufmatmul[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*64)) + (c3*1920)) + (c1*57600)] + ((float64)bufconv[(((<span class="number">0</span> + (oc*<span class="number">1</span>)) + (c5*30)) + (c3*900)) + (c1*14400)]*(float64)bufrhs[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (oc*64)) + (c3*1024)) + (c1*30720)])</span><br><span class="line">          <span class="keyword">if</span> (c7 &lt;= 15) &#123;</span><br><span class="line">           bufconv[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*30)) + (c3*900)) + (c1*14400)] = (float64)bufconv[(((<span class="number">0</span> + (c5*<span class="number">1</span>)) + (c3*30)) + (c7*900)) + (c1*14400)]</span><br><span class="line">          &#125;</span><br><span class="line">          bufconv[(((<span class="number">0</span> + (c7*<span class="number">1</span>)) + (c5*30)) + (c3*900)) + (c1*14400)] = 0.000000</span><br><span class="line">         &#125;</span><br><span class="line">        &#125;</span><br><span class="line">       &#125;</span><br><span class="line">      &#125;</span><br><span class="line">     &#125;</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为<code>tiramisu</code>是完全依赖手动调度,
所以这里的<code>fusion</code>使用的<code>buffer</code>需要提前手动指定,
对于手工优化算子应该会省事情, 但是集成到编译器并不是很合适.</p>
<h1 id="总结">总结</h1>
<p>我认为一个算子的实现取决于<code>compute order</code>,
<code>tiling</code>, <code>buffer binding</code>三部分,
作为编译器或者开发者需要利用尽可能多的信息在这三个<code>design space</code>进行选择从而优化计算性能.
我们期待<code>Tensor DSL</code>能带来足够多的信息支持做好这件事.</p>
<p>目前这些<code>DSL</code>都提供了基本的<code>iteration domain</code>和<code>access relation</code>的信息.
但是只有<code>halide</code>直接记录了循环变量被上下算子共享的信息(目前暂未查到这种信息的书面语),
<code>mlir</code>应该是在<code>fusion</code>的优化中通过<code>affine map</code>来推导出这个信息的.
在<a
target="_blank" rel="noopener" href="https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/"><code>linalg</code>的基本原理</a>文档中,
<code>mlir</code>总结了不同编译器实现的经验, 需要提供有灵活调度的能力,
又不能像<code>Polyhedral</code>那样复杂,
并且还要保持<code>SSA</code>的形式. 从<code>fusion</code>
pass的优化结果上来看, <code>mlir</code>所提炼的方案还是比较实用的.</p>
<p>在我的设想中, 应该是基于一套<code>symbolic</code>的维度变量,
可以类似<code>einsum</code>一样定义出深度学习中的绝大部分算子(或许<code>einops</code>就是个不错的选择),
定义算子就相当于声明了<code>iteration domain</code>和<code>access relation</code>,
然后也可以在这个图级别<code>ir</code>上能做到<code>tiling/schedule</code>,
最后<code>lower</code>到循环级别或是多面体调度树(可能不支持参数化)时利用好映射关系安排好各种细节的索引,
最终生成正确的代码.</p>
<p>当然还有许多问题需要考虑:</p>
<ol type="1">
<li>如何自然的处理循环顺序?</li>
</ol>
<p>上面提到的DSL都是直接写出迭代域的, 但是如果不想手动写出,
直接通过tensor操作来决定循环顺序可能就会和所需要的有差异,
比如<code>loop tool</code>的做法: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> loop_tool <span class="keyword">as</span> lt</span><br><span class="line"></span><br><span class="line">[m, n, k] = lt.symbols(<span class="string">&quot;m n k&quot;</span>)</span><br><span class="line">a = lt.Tensor(<span class="number">32</span>, <span class="number">32</span>).to(m, k)</span><br><span class="line">b = lt.Tensor(<span class="number">32</span>, <span class="number">32</span>).to(k, n)</span><br><span class="line">c = (a * b).<span class="built_in">sum</span>(k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ir:</span></span><br><span class="line"><span class="keyword">for</span> m_0 <span class="keyword">in</span> <span class="number">32</span>                     </span><br><span class="line"> <span class="keyword">for</span> k_2 <span class="keyword">in</span> <span class="number">32</span></span><br><span class="line">  <span class="keyword">for</span> n_1 <span class="keyword">in</span> <span class="number">32</span></span><br><span class="line">   %<span class="number">2</span>[m_0, k_2, n_1] &lt;- multiply(%<span class="number">0</span>, %<span class="number">1</span>)</span><br><span class="line">   %<span class="number">3</span>[m_0, n_1] &lt;- add(%<span class="number">2</span>)</span><br><span class="line"> <span class="keyword">for</span> n_1 <span class="keyword">in</span> <span class="number">32</span></span><br><span class="line">  %<span class="number">4</span>[m_0, n_1] &lt;- write(%<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
他这里<code>[m, k] * [k, n]</code>的时候其实就决定了循环是<code>m k n</code>的顺序,
后面<code>sum(k)</code>并不影响循环顺序.
同时他这里<code>%2,%3</code>的buffer大小都是自动按访问数据区域来分配的.</p>
<!-- 4.  比如如何支持规约操作的写法? 非完美循环组成的算子如何支持, 直接可以写`native code`或者别的方式? -->
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DSL/" rel="tag">DSL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Halide/" rel="tag">Halide</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Jittor/" rel="tag">Jittor</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tiramisu/" rel="tag">Tiramisu</a></li></ul></div><div class="post-nav"><a class="pre" href="/2023/12/26/hf-llama/">hugging face llama使用</a><a class="next" href="/2023/11/25/mlircsharp/">MLIRSharp</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/30/model-driven-optimization/">Model Driven Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx/">探索AMX: 解锁Apple Silicon隐藏性能</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/23/mac-amx_en/">Explore AMX instructions: Unlock the performance of Apple Silicon</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/13/macos-bundle/">macos中bundle的使用</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>