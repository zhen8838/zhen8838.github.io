<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>mlc-llm 浅析 | Zheng's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">mlc-llm 浅析</h1><a id="logo" href="/.">Zheng's Notes</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">mlc-llm 浅析</h1><div class="post-meta">2023-11-01<span> | </span><span class="category"><a href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 11</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#model-arch-generator"><span class="toc-number">1.</span> <span class="toc-text">1. Model Arch Generator</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#module-transform"><span class="toc-number">2.</span> <span class="toc-text">2. Module Transform</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#module-build"><span class="toc-number">3.</span> <span class="toc-text">3. Module Build</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chat"><span class="toc-number">4.</span> <span class="toc-text">4. Chat</span></a></li></ol></div></div><div class="post-content"><p>学习tvm是如何解决LLM推理问题.</p>
<span id="more"></span>
<h1 id="model-arch-generator">1. Model Arch Generator</h1>
<p>LLM有一个特点就是其动态与自回归的特性,
传统CNN的模型的计算通路都保存在模型中, 对于DL
Compiler来说只需要将固定shape下的模型进行编译优化即可,
而LLM的计算通路并没有体现在模型中,
万幸的是没有多少厂商会大改LLM的模型结构, 所以DL
Compiler的前端去手动去处理也问题不大.</p>
<p>使用<code>mlc.build</code>对模型进行编译,
进入<code>build_model_from_args</code>函数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_from_args</span>(<span class="params">args: argparse.Namespace</span>):</span><br><span class="line">    <span class="comment"># 各种配置处理</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择模型进行解析</span></span><br><span class="line">    model_generators = &#123;</span><br><span class="line">        <span class="string">&quot;llama&quot;</span>: llama,</span><br><span class="line">        <span class="string">&quot;mistral&quot;</span>: llama,</span><br><span class="line">        <span class="string">&quot;stablelm_epoch&quot;</span>: stablelm_3b,</span><br><span class="line">        <span class="string">&quot;gpt_neox&quot;</span>: gpt_neox,</span><br><span class="line">        <span class="string">&quot;gpt_bigcode&quot;</span>: gpt_bigcode,</span><br><span class="line">        <span class="string">&quot;minigpt&quot;</span>: minigpt,</span><br><span class="line">        <span class="string">&quot;gptj&quot;</span>: gptj,</span><br><span class="line">        <span class="string">&quot;rwkv&quot;</span>: rwkv,</span><br><span class="line">        <span class="string">&quot;rwkv_world&quot;</span>: rwkv,</span><br><span class="line">        <span class="string">&quot;chatglm&quot;</span>: chatglm,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># </span></span><br></pre></td></tr></table></figure>
<p>目前tvm是基于relax的分支支持LLM的,
构建模型的过程主要就是使用relax的主要特性按原始模型结构重新构造了一遍tvm的ir
module:</p>
<p>首先是构造<code>BlockBuilder</code>的scope,
然后在其中构造整个模型运行的每个阶段.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_model</span>(<span class="params">args, hf_config</span>):</span><br><span class="line">    <span class="comment"># 处理配置...</span></span><br><span class="line">    param_manager = ParamManager()</span><br><span class="line">    bb = relax.BlockBuilder()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> sep_embed:</span><br><span class="line">        create_embed_func(bb, param_manager, config, args.quantization)</span><br><span class="line">    <span class="comment"># 省略batching的构造...</span></span><br><span class="line">    create_prefill_func_for_single_seq(bb, param_manager, config, args.quantization, sep_embed)</span><br><span class="line">    create_decoding_func_for_single_seq(bb, param_manager, config, args.quantization)</span><br><span class="line">    create_kv_cache_func(bb, config)</span><br><span class="line">    create_softmax_func_for_single_seq(bb, config)</span><br><span class="line"></span><br><span class="line">    create_metadata_func(</span><br><span class="line">        bb,</span><br><span class="line">        model_name=model_name,</span><br><span class="line">        max_window_size=config.max_sequence_length,</span><br><span class="line">        stop_tokens=[<span class="number">2</span>],</span><br><span class="line">        add_prefix_space=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 设定动态dim的上下界</span></span><br><span class="line">    mod = bb.get()</span><br><span class="line">    <span class="keyword">for</span> gv <span class="keyword">in</span> mod.functions:</span><br><span class="line">        func = mod[gv]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(func, relax.Function):</span><br><span class="line">            mod[gv] = func.with_attr( <span class="string">&quot;tir_var_upper_bound&quot;</span>, &#123; <span class="string">&quot;n&quot;</span>: config.max_sequence_length, <span class="string">&quot;m&quot;</span>: config.max_sequence_length, &#125;, )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.build_model_only:</span><br><span class="line">        <span class="keyword">return</span> mod, param_manager, <span class="literal">None</span>, config</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> setup_params(mod, param_manager, dtype, config, args)</span><br></pre></td></tr></table></figure>
<p>在relax中支持同时包含构造relay的数据流以及tir,
所以下面会使用<code>nn.emit</code>以及<code>nn.emit_te</code>,
同时还可以使用一些手动优化的vm函数<code>relax.extern("vm.builtin.paged_attention_kv_cache_append")</code>以及直接编写的<code>prim_func</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>: relax.Expr</span>) -&gt; relax.Var:</span><br><span class="line">        <span class="keyword">return</span> nn.emit(relax.op.linear(<span class="built_in">input</span>, self.weight, self.bias))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_pos_emb</span>(<span class="params">q, k, position_embedding_base, offset: <span class="built_in">int</span> = <span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">f_rotary_embedding</span>(<span class="params">tensor, offset</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">rotary_compute</span>(<span class="params">*idx</span>):</span><br><span class="line">            pos = (offset + idx[-<span class="number">3</span>]).astype(<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> rotary_modulate_by_freq(</span><br><span class="line">                tensor,</span><br><span class="line">                idx,</span><br><span class="line">                pos,</span><br><span class="line">                position_embedding_base,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tvm.te.compute(tensor.shape, rotary_compute, name=<span class="string">&quot;rotary&quot;</span>)</span><br><span class="line"></span><br><span class="line">    q_embed = nn.emit_te(f_rotary_embedding, q, offset, primfunc_name_hint=<span class="string">&quot;rotary_embedding&quot;</span>)</span><br><span class="line">    k_embed = nn.emit_te(f_rotary_embedding, k, offset, primfunc_name_hint=<span class="string">&quot;rotary_embedding&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> q_embed, k_embed</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaPagedAttention</span>(<span class="title class_ inherited__">LlamaAttentionBase</span>):</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_fwd</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        query_states: relax.Expr,</span></span><br><span class="line"><span class="params">        key_states: relax.Expr,</span></span><br><span class="line"><span class="params">        value_states: relax.Expr,</span></span><br><span class="line"><span class="params">        past_key_values: relax.Expr,</span></span><br><span class="line"><span class="params">        batch_size: tir.PrimExpr,</span></span><br><span class="line"><span class="params">        q_len: tir.PrimExpr,</span></span><br><span class="line"><span class="params">        **kwargs,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[relax.Expr, relax.Expr]:</span><br><span class="line">        <span class="keyword">assert</span> <span class="string">&quot;layer_id&quot;</span> <span class="keyword">in</span> kwargs <span class="keyword">and</span> <span class="built_in">isinstance</span>(kwargs[<span class="string">&quot;layer_id&quot;</span>], <span class="built_in">int</span>)</span><br><span class="line">        layer_id = kwargs[<span class="string">&quot;layer_id&quot;</span>]</span><br><span class="line"></span><br><span class="line">        f_kv_cache_append = relax.extern(<span class="string">&quot;vm.builtin.paged_attention_kv_cache_append&quot;</span>)</span><br><span class="line">        past_key_values = nn.emit(</span><br><span class="line">            relax.call_pure_packed(</span><br><span class="line">                f_kv_cache_append,</span><br><span class="line">                past_key_values,</span><br><span class="line">                self.kv_cache_transpose_append,</span><br><span class="line">                key_states,</span><br><span class="line">                value_states,</span><br><span class="line">                relax.PrimValue(layer_id),</span><br><span class="line">                sinfo_args=relax.ObjectStructInfo(),</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="keyword">return</span> attn_output, past_key_values</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">emit_paged_kv_cache_op</span>(<span class="params">bb: relax.BlockBuilder, dtype: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">from</span> tvm.script <span class="keyword">import</span> tir <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># fmt: off</span></span><br><span class="line"><span class="meta">    @T.prim_func</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kv_cache_transpose_append</span>(<span class="params"></span></span><br><span class="line"><span class="params">        var_pages: T.handle,</span></span><br><span class="line"><span class="params">        var_k_data: T.handle,</span></span><br><span class="line"><span class="params">        var_v_data: T.handle,</span></span><br><span class="line"><span class="params">        var_page_table_indptr: T.handle,</span></span><br><span class="line"><span class="params">        var_page_table_values: T.handle,</span></span><br><span class="line"><span class="params">        var_last_page_offset: T.handle,</span></span><br><span class="line"><span class="params">        var_append_length_indptr: T.handle,</span></span><br><span class="line"><span class="params">        var_pos2seqidx: T.handle,</span></span><br><span class="line"><span class="params">        layer_id: T.int32,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="comment"># 省略buffer构造...</span></span><br><span class="line">        <span class="keyword">for</span> global_pos, h, f <span class="keyword">in</span> T.grid(ntoken, nhead, nfeat):</span><br><span class="line">            <span class="keyword">with</span> T.block(<span class="string">&quot;k_transpose_append&quot;</span>):</span><br><span class="line">                vgpos, vh, vf = T.axis.remap(<span class="string">&quot;SSS&quot;</span>, [global_pos, h, f])</span><br><span class="line">                seq_idx = pos2seqidx[vgpos]</span><br><span class="line">                seqlen: T.int32 = (page_table_indptr[seq_idx + <span class="number">1</span>] - page_table_indptr[seq_idx] - <span class="number">1</span>) * page_size + last_page_offset[seq_idx]</span><br><span class="line">                pages[</span><br><span class="line">                    page_table_values[page_table_indptr[seq_idx] + T.floordiv(seqlen - (append_length_indptr[seq_idx + <span class="number">1</span>] - vgpos), page_size)],</span><br><span class="line">                    layer_id,</span><br><span class="line">                    <span class="number">0</span>,</span><br><span class="line">                    vh,</span><br><span class="line">                    T.floormod(seqlen - (append_length_indptr[seq_idx + <span class="number">1</span>] - vgpos), page_size),</span><br><span class="line">                    vf,</span><br><span class="line">                ] = k_data[vgpos, vh, vf]</span><br><span class="line">            <span class="keyword">with</span> T.block(<span class="string">&quot;v_transpose_append&quot;</span>):</span><br><span class="line">                vgpos, vh, vf = T.axis.remap(<span class="string">&quot;SSS&quot;</span>, [global_pos, h, f])</span><br><span class="line">                seq_idx = pos2seqidx[vgpos]</span><br><span class="line">                seqlen: T.int32 = (page_table_indptr[seq_idx + <span class="number">1</span>] - page_table_indptr[seq_idx] - <span class="number">1</span>) * page_size + last_page_offset[seq_idx]</span><br><span class="line">                pages[</span><br><span class="line">                    page_table_values[page_table_indptr[seq_idx] + T.floordiv(seqlen - (append_length_indptr[seq_idx + <span class="number">1</span>] - vgpos), page_size)],</span><br><span class="line">                    layer_id,</span><br><span class="line">                    <span class="number">1</span>,</span><br><span class="line">                    vh,</span><br><span class="line">                    T.floormod(seqlen - (append_length_indptr[seq_idx + <span class="number">1</span>] - vgpos), page_size),</span><br><span class="line">                    vf,</span><br><span class="line">                ] = v_data[vgpos, vh, vf]</span><br><span class="line">    <span class="comment"># fmt: on</span></span><br><span class="line"></span><br><span class="line">    bb.add_func(kv_cache_transpose_append, <span class="string">&quot;kv_cache_transpose_append&quot;</span>)</span><br><span class="line">    <span class="comment"># Todo: integrating attention TIR func/kernel.</span></span><br><span class="line">    bb.add_func(relax.extern(<span class="string">&quot;attention_func&quot;</span>), <span class="string">&quot;attention&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在源代码中检索了一下, 发现是在vm中是直接实现了kv cache, 同时将kv
cache的接口进行了封装, 让relax可以进行调用.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionKVCacheObj</span> : <span class="keyword">public</span> Object &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief Underlying support data.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  NDArray data;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief number of slots already filled.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="type">int64_t</span> fill_count&#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief View all current cached values as one array.</span></span><br><span class="line"><span class="comment">   * \param shape The cached values.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">NDArray <span class="title">View</span><span class="params">(<span class="type">const</span> ShapeTuple&amp; shape)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ..</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Clear the cache */</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Clear</span><span class="params">()</span> </span>&#123; <span class="comment">/* ... */</span> &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** pop n entries */</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">PopN</span><span class="params">(<span class="type">size_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Update</span><span class="params">(NDArray value)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief Append value to the cache.</span></span><br><span class="line"><span class="comment">   * \param value The value to be appended.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Append</span><span class="params">(NDArray value)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">static</span> <span class="keyword">constexpr</span> <span class="type">const</span> <span class="type">uint32_t</span> _type_index = TypeIndex::kDynamic;</span><br><span class="line">  <span class="type">static</span> <span class="keyword">constexpr</span> <span class="type">const</span> <span class="type">char</span>* _type_key = <span class="string">&quot;relax.vm.AttentionKVCache&quot;</span>;</span><br><span class="line">  <span class="built_in">TVM_DECLARE_FINAL_OBJECT_INFO</span>(AttentionKVCacheObj, Object);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// register</span></span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;vm.builtin.attention_kv_cache_create&quot;</span>)</span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;vm.builtin.attention_kv_cache_create_multiple&quot;</span>)</span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;vm.builtin.attention_kv_cache_update&quot;</span>)</span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;vm.builtin.attention_kv_cache_append&quot;</span>)</span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;vm.builtin.attention_kv_cache_view&quot;</span>)</span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;vm.builtin.attention_kv_cache_array_popn&quot;</span>)</span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;vm.builtin.attention_kv_cache_array_clear&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>其实tvm这种直接在module中构造操作的方式也是很方便的,
如果是传统的编译流程对于每个层还需要写pattern去切子图, 并且一些kv
cache相关的优化可能还需要通过一些选项去在某些位置强行添加.</p>
<p><a
target="_blank" rel="noopener" href="https://gist.github.com/zhen8838/ad8bbe9286e6fa13798fbdb70f1de4ac">mod_after_get_model.py</a></p>
<h1 id="module-transform">2. Module Transform</h1>
<p>如果开启了量化还需要更新全部的参数,
然后对构造好的<code>IR.Module</code>进行优化,
这里也是一些比较有针对性的优化pass: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mod_transform_before_build</span>(<span class="params"></span></span><br><span class="line"><span class="params">    mod: tvm.IRModule,</span></span><br><span class="line"><span class="params">    param_manager: param_manager.ParamManager,</span></span><br><span class="line"><span class="params">    args: argparse.Namespace,</span></span><br><span class="line"><span class="params">    config: <span class="type">Dict</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; tvm.IRModule:</span><br><span class="line">  <span class="comment"># </span></span><br><span class="line">  mod = param_manager.transform_dequantize()(mod)</span><br><span class="line">  mod = relax.transform.BundleModelParams()(mod)</span><br><span class="line">  use_ft_quant = args.quantization.name <span class="keyword">in</span> [<span class="string">&quot;q4f16_ft&quot;</span>, <span class="string">&quot;q8f16_ft&quot;</span>]</span><br><span class="line">  mod = mlc_llm.transform.FuseDecodeTranspose(skip_gemm=<span class="keyword">not</span> use_ft_quant)(mod)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> max_seq_len:</span><br><span class="line">    num_key_value_heads = config.get_num_key_value_heads()</span><br><span class="line">    mod = fuse_split_rotary_embedding(</span><br><span class="line">            config.num_attention_heads // args.num_shards,</span><br><span class="line">            num_key_value_heads // args.num_shards,</span><br><span class="line">            config.hidden_size // args.num_shards,</span><br><span class="line">            config.position_embedding_base,</span><br><span class="line">        )(mod)</span><br><span class="line">  <span class="keyword">if</span> args.target_kind == <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">  mod = mlc_llm.transform.FuseTransposeMatmul()(mod)</span><br><span class="line">  mod = relax.pipeline.get_pipeline()(mod)  <span class="comment"># pylint: disable=no-value-for-parameter</span></span><br><span class="line">  mod = mlc_llm.transform.FuseDecodeMatmulEwise()(mod)</span><br><span class="line">  mod = mlc_llm.transform.FuseDecodeTake()(mod)</span><br><span class="line">  mod = relax.transform.DeadCodeElimination(model_names)(mod)</span><br><span class="line">  mod = mlc_llm.transform.CleanUpTIRAttrs()(mod)</span><br><span class="line">  mod_deploy = mod</span><br><span class="line">  <span class="keyword">return</span> mod_deploy</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>修改后的Module如下, 相比原本的Module多了许多Fused的算子.</p>
<p><a
target="_blank" rel="noopener" href="https://gist.github.com/zhen8838/1cecf3800d37fbaf5bb9d1c39e0700f2">mod_depoly.py</a></p>
<h1 id="module-build">3. Module Build</h1>
<p>build的过程就是调用原本tvm中的编译下降进行处理,
这里我的target为<code>m1-metal</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">mod_deploy: tvm.IRModule, args: argparse.Namespace</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># dump ...</span></span><br><span class="line">    <span class="keyword">if</span> target_kind != <span class="string">&quot;cpu&quot;</span>:</span><br><span class="line">        dispatch_target = (</span><br><span class="line">            args.target</span><br><span class="line">            <span class="keyword">if</span> args.target_kind != <span class="string">&quot;webgpu&quot;</span></span><br><span class="line">            <span class="keyword">else</span> tvm.target.Target(<span class="string">&quot;apple/m1-gpu-restricted&quot;</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">with</span> dispatch_target:</span><br><span class="line">            mod_deploy = dl.ApplyDefaultSchedule(  <span class="comment"># pylint: disable=not-callable</span></span><br><span class="line">                dl.gpu.Matmul(),</span><br><span class="line">                dl.gpu.GEMV(),</span><br><span class="line">                dl.gpu.Reduction(),</span><br><span class="line">                dl.gpu.GeneralReduction(),</span><br><span class="line">                dl.gpu.Fallback(),</span><br><span class="line">            )(mod_deploy)</span><br><span class="line">            mod_deploy = (</span><br><span class="line">                mlc_llm.transform.LiftTIRGlobalBufferAlloc()(  <span class="comment"># pylint: disable=not-callable</span></span><br><span class="line">                    mod_deploy</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">            mod_deploy = tvm.tir.transform.ForceNarrowIndexToInt32()(mod_deploy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 省略使用cuda...</span></span><br><span class="line">    args.lib_path = os.path.join(args.artifact_path, output_filename)</span><br><span class="line">    ex.export_library(args.lib_path, **args.export_kwargs)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Finish exporting to <span class="subst">&#123;args.lib_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>relax中的原生支持动态shape,
所以在<code>decode</code>过程中是通过<code>dataflow</code>的形式来执行:
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@R.function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">input_ids1: R.Tensor(<span class="params">(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>), dtype=<span class="string">&quot;int32&quot;</span></span>), all_seq_len: R.Shape(<span class="params">[<span class="string">&quot;n&quot;</span>]</span>), kv_cache:...</span>):</span><br><span class="line">        cls = Module</span><br><span class="line">        <span class="keyword">with</span> R.dataflow():</span><br><span class="line">          <span class="comment"># ...</span></span><br><span class="line">          lv1897 = R.call_tir(cls.transpose5, (lv1894,), out_sinfo=R.Tensor((<span class="number">1</span>, <span class="number">32</span>, n, <span class="number">80</span>), dtype=<span class="string">&quot;float16&quot;</span>))</span><br><span class="line">          lv1898 = R.call_tir(cls.transpose5, (lv1895,), out_sinfo=R.Tensor((<span class="number">1</span>, <span class="number">32</span>, n, <span class="number">80</span>), dtype=<span class="string">&quot;float16&quot;</span>))</span><br><span class="line">          lv722 = R.call_tir(cls.fused_NT_matmul7_divide2_maximum1_minimum1_cast9, (lv1896, lv1897, lv1871), out_sinfo=R.Tensor((<span class="number">1</span>, <span class="number">32</span>, <span class="number">1</span>, n), dtype=<span class="string">&quot;float32&quot;</span>))</span><br><span class="line">          <span class="comment"># ...</span></span><br></pre></td></tr></table></figure></p>
<p>例如<code>decode</code>中的<code>transpose5</code>函数在<code>before build</code>阶段,
tir中是以动态的方式进行构造的:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@T.prim_func(<span class="params">private=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose5</span>(<span class="params">var_A: T.handle, var_T_transpose: T.handle</span>):</span><br><span class="line">    T.func_attr(&#123;<span class="string">&quot;tir.noalias&quot;</span>: T.<span class="built_in">bool</span>(<span class="literal">True</span>)&#125;)</span><br><span class="line">    n = T.int64()</span><br><span class="line">    A = T.match_buffer(var_A, (T.int64(<span class="number">1</span>), n, T.int64(<span class="number">32</span>), T.int64(<span class="number">80</span>)), <span class="string">&quot;float16&quot;</span>)</span><br><span class="line">    T_transpose = T.match_buffer(var_T_transpose, (T.int64(<span class="number">1</span>), T.int64(<span class="number">32</span>), n, T.int64(<span class="number">80</span>)), <span class="string">&quot;float16&quot;</span>)</span><br><span class="line">    <span class="comment"># with T.block(&quot;root&quot;):</span></span><br><span class="line">    <span class="keyword">for</span> ax0, ax1, ax2, ax3 <span class="keyword">in</span> T.grid(T.int64(<span class="number">1</span>), T.int64(<span class="number">32</span>), n, T.int64(<span class="number">80</span>)):</span><br><span class="line">        <span class="keyword">with</span> T.block(<span class="string">&quot;T_transpose&quot;</span>):</span><br><span class="line">            v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(<span class="string">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])</span><br><span class="line">            T.reads(A[v_ax0, v_ax2, v_ax1, v_ax3])</span><br><span class="line">            T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])</span><br><span class="line">            T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax2, v_ax1, v_ax3]</span><br></pre></td></tr></table></figure>
<p>在<code>after build</code>阶段,
经过编译下降之后的block中的<code>iterVar</code>被映射到了<code>thread</code>和<code>block</code>两个层级.
我估计在tvm中对于动态申请的内存默认都是连续的,
所以这里<code>match buffer</code>也没有特别的<code>stride</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@T.prim_func(<span class="params">private=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose5</span>(<span class="params">var_A: T.handle, var_T_transpose: T.handle</span>):</span><br><span class="line">    T.func_attr(&#123;<span class="string">&quot;tir.is_scheduled&quot;</span>: <span class="number">1</span>, <span class="string">&quot;tir.noalias&quot;</span>: T.<span class="built_in">bool</span>(<span class="literal">True</span>)&#125;)</span><br><span class="line">    n = T.int32()</span><br><span class="line">    A = T.match_buffer(var_A, (<span class="number">1</span>, n, <span class="number">32</span>, <span class="number">80</span>), <span class="string">&quot;float16&quot;</span>)</span><br><span class="line">    T_transpose = T.match_buffer(var_T_transpose, (<span class="number">1</span>, <span class="number">32</span>, n, <span class="number">80</span>), <span class="string">&quot;float16&quot;</span>)</span><br><span class="line">    <span class="comment"># with T.block(&quot;root&quot;):</span></span><br><span class="line">    <span class="keyword">for</span> ax0_ax1_ax2_fused_0 <span class="keyword">in</span> T.thread_binding((n * <span class="number">2560</span> + <span class="number">1023</span>) // <span class="number">1024</span>, thread=<span class="string">&quot;blockIdx.x&quot;</span>):</span><br><span class="line">        <span class="keyword">for</span> ax0_ax1_ax2_fused_1 <span class="keyword">in</span> T.thread_binding(<span class="number">1024</span>, thread=<span class="string">&quot;threadIdx.x&quot;</span>):</span><br><span class="line">            <span class="keyword">with</span> T.block(<span class="string">&quot;T_transpose&quot;</span>):</span><br><span class="line">                v0 = T.axis.spatial(<span class="number">32</span>, (ax0_ax1_ax2_fused_0 * <span class="number">1024</span> + ax0_ax1_ax2_fused_1) // (<span class="number">80</span> * n))</span><br><span class="line">                v1 = T.axis.spatial(n, (ax0_ax1_ax2_fused_0 * <span class="number">1024</span> + ax0_ax1_ax2_fused_1) % (<span class="number">80</span> * n) // <span class="number">80</span>)</span><br><span class="line">                v2 = T.axis.spatial(<span class="number">80</span>, (ax0_ax1_ax2_fused_0 * <span class="number">1024</span> + ax0_ax1_ax2_fused_1) % <span class="number">80</span>)</span><br><span class="line">                T.where(ax0_ax1_ax2_fused_0 * <span class="number">1024</span> + ax0_ax1_ax2_fused_1 &lt; n * <span class="number">2560</span>)</span><br><span class="line">                T.reads(A[<span class="number">0</span>, v1, v0, v2])</span><br><span class="line">                T.writes(T_transpose[<span class="number">0</span>, v0, v1, v2])</span><br><span class="line">                T_transpose[<span class="number">0</span>, v0, v1, v2] = A[<span class="number">0</span>, v1, v0, v2]</span><br></pre></td></tr></table></figure>
<p>编译后的模型如下:</p>
<p><a
target="_blank" rel="noopener" href="https://gist.github.com/zhen8838/9a17fa51763ee24baea00f57c2e4e73f">mod_build_stage.py</a></p>
<h1 id="chat">4. Chat</h1>
<p>chat 其实经过之前的编译过程后会非常的精简,
只需要获取对应编译后模型的<code>packed func</code>然后反复调用即可.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ChatModule</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief Constructor</span></span><br><span class="line"><span class="comment">   * \param device the device to run the chat on.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">ChatModule</span><span class="params">(<span class="type">const</span> DLDevice&amp; device)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;chat_mod_ = mlc::llm::<span class="built_in">CreateChatModule</span>(device);</span><br><span class="line">    <span class="keyword">this</span>-&gt;prefill_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;prefill&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;decode_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;decode&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;stopped_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;stopped&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;get_message_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;get_message&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;reload_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;reload&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;get_role0_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;get_role0&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;get_role1_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;get_role1&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;runtime_stats_text_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;runtime_stats_text&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;verbose_runtime_stats_text_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;verbose_runtime_stats_text&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;reset_chat_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;reset_chat&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;process_system_prompts_ = <span class="keyword">this</span>-&gt;chat_mod_-&gt;<span class="built_in">GetFunction</span>(<span class="string">&quot;process_system_prompts&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;lib_path_ = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="keyword">this</span>-&gt;executable_ = tvm::runtime::<span class="built_in">Module</span>(<span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(prefill_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(decode_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(stopped_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(get_message_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(reload_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(get_role0_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(get_role1_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(runtime_stats_text_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(verbose_runtime_stats_text_ != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">ICHECK</span>(reset_chat_ != <span class="literal">nullptr</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tvm/" rel="tag">tvm</a></li></ul></div><div class="post-nav"><a class="pre" href="/2023/11/15/dynamic-shape/">tvm dynamic shape 学习</a><a class="next" href="/2023/09/19/easydist/">Alibaba EasyDist 浅析</a></div><script src="https://utteranc.es/client.js" repo="zhen8838/zhen8838.github.io" issue-term="url" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://zhen8838.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>A Believing Heart Is Your Magic</p><a class="info-icon" href="mailto:597323109@qq.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/zhen8838" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">工具使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">推理框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/">编译器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%AD%B9%E5%AD%A6/">运筹学</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 15px;">树莓派</a> <a href="/tags/%E8%93%9D%E7%89%99/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 15px;">遗传算法</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">半监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E9%A6%99%E6%A9%99%E6%B4%BE/" style="font-size: 15px;">香橙派</a> <a href="/tags/%E8%B8%A9%E5%9D%91%E7%BB%8F%E9%AA%8C/" style="font-size: 15px;">踩坑经验</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/Qt/" style="font-size: 15px;">Qt</a> <a href="/tags/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">多面体模型</a> <a href="/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">后端优化</a> <a href="/tags/Ampl/" style="font-size: 15px;">Ampl</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15px;">图像处理</a> <a href="/tags/K210/" style="font-size: 15px;">K210</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" style="font-size: 15px;">二分法</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 15px;">科学上网</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/cmake/" style="font-size: 15px;">cmake</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/Conan/" style="font-size: 15px;">Conan</a> <a href="/tags/OrTools/" style="font-size: 15px;">OrTools</a> <a href="/tags/CSharp/" style="font-size: 15px;">CSharp</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 15px;">数据增强</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" style="font-size: 15px;">聚类方法</a> <a href="/tags/CostModel/" style="font-size: 15px;">CostModel</a> <a href="/tags/Vscode/" style="font-size: 15px;">Vscode</a> <a href="/tags/%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15px;">声音信号处理</a> <a href="/tags/TVM/" style="font-size: 15px;">TVM</a> <a href="/tags/%E5%8A%A8%E6%80%81shape/" style="font-size: 15px;">动态shape</a> <a href="/tags/%E4%B8%AD%E7%AB%AF%E4%BC%98%E5%8C%96/" style="font-size: 15px;">中端优化</a> <a href="/tags/Equality-Saturation/" style="font-size: 15px;">Equality Saturation</a> <a href="/tags/stm32/" style="font-size: 15px;">stm32</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Halide/" style="font-size: 15px;">Halide</a> <a href="/tags/DSL/" style="font-size: 15px;">DSL</a> <a href="/tags/%E5%A0%86%E6%A0%88/" style="font-size: 15px;">堆栈</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">大语言模型</a> <a href="/tags/llama/" style="font-size: 15px;">llama</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 15px;">归一化</a> <a href="/tags/Makefile/" style="font-size: 15px;">Makefile</a> <a href="/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">元学习</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E5%85%83%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">模板元编程</a> <a href="/tags/mindspore/" style="font-size: 15px;">mindspore</a> <a href="/tags/LLM/" style="font-size: 15px;">LLM</a> <a href="/tags/tvm/" style="font-size: 15px;">tvm</a> <a href="/tags/mlir/" style="font-size: 15px;">mlir</a> <a href="/tags/%E6%80%A7%E8%83%BD%E5%BB%BA%E6%A8%A1/" style="font-size: 15px;">性能建模</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/Nand2Tetris/" style="font-size: 15px;">Nand2Tetris</a> <a href="/tags/ncnn/" style="font-size: 15px;">ncnn</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PCB/" style="font-size: 15px;">PCB</a> <a href="/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">姿态估计</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">人脸检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8F%E5%8C%96/" style="font-size: 15px;">神经网络量化</a> <a href="/tags/Yolo/" style="font-size: 15px;">Yolo</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/NB-IOT/" style="font-size: 15px;">NB-IOT</a> <a href="/tags/Retinaface/" style="font-size: 15px;">Retinaface</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/" style="font-size: 15px;">指令集</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 15px;">人脸识别</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 15px;">优化器</a> <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B/" style="font-size: 15px;">吴恩达课程</a> <a href="/tags/WordCloud/" style="font-size: 15px;">WordCloud</a> <a href="/tags/Zhihu/" style="font-size: 15px;">Zhihu</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8/" style="font-size: 15px;">四轴飞行器</a> <a href="/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/" style="font-size: 15px;">资源汇总</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">无监督学习</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Jittor/" style="font-size: 15px;">Jittor</a> <a href="/tags/Tiramisu/" style="font-size: 15px;">Tiramisu</a> <a href="/tags/Triton/" style="font-size: 15px;">Triton</a> <a href="/tags/vllm/" style="font-size: 15px;">vllm</a> <a href="/tags/%E7%AE%97%E5%AD%90/" style="font-size: 15px;">算子</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/09/26/flashattn/">Flash Attention记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/08/28/chimera/">Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/14/vllm/">推理框架调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/04/distal/">DISTAL: The Distributed Tensor Algebra Compiler</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/04/triton-cpu-lesson-1/">triton-cpu初体验</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/mesh-matmul/">分布式存储架构下的矩阵乘与编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/mlc-tutorial/">机器学习编译概念科普</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/08/08/benchmark-notes/">benchmark的经验与技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/06/14/ampl-learn/">Ampl学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/05/08/constraints-solver-internals/">Constraints Solver Internals</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Zheng's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>